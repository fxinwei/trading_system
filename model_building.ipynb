{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from io import BytesIO\n",
    "import PIL.Image as Image\n",
    "import PIL.ImageOps as ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, Callback, ModelCheckpoint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as lrs\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import pandas_ta as ta\n",
    "# %matplotlib inline\n",
    "from tqsdk2 import TqApi, TargetPosTask, TqSim, TqBacktest, TqAccount, TqAuth, TqKq\n",
    "from tqsdk2.ta import BOLL, PUBU, ENV, ATR, MACD, CCI, RSI\n",
    "from datetime import date\n",
    "import time\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from saved_time_list import saved_time_list # 读取存储的time list\n",
    "from contract_min_max import contract_min_max\n",
    "# from model_list import alexnet, alexnet1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_unix(t):\n",
    "    timearray = time.strptime(t, '%Y-%m-%d %H:%M:%S')\n",
    "    tt = int(time.mktime(timearray))\n",
    "    return tt\n",
    "\n",
    "def timestamp(t):\n",
    "    format = '%Y-%m-%d %H:%M:%S'\n",
    "    t = int(t / 1e9)\n",
    "    t = time.localtime(t)\n",
    "    dt = time.strftime(format, t)\n",
    "    return dt\n",
    "\n",
    "def find_target(symbol, timestr, period):\n",
    "    \n",
    "    # if period == 10:\n",
    "    #     history = 40\n",
    "    # elif period > 10:\n",
    "    #     history = 20\n",
    "    history = 40\n",
    "    df = api.get_kline_serial(symbol, period * 60, 5000)\n",
    "    df = df.dropna()\n",
    "    df.loc[:, \"time\"] = [timestamp(t) for t in df[\"datetime\"]]\n",
    "    df.loc[:, \"macd\"] = MACD(df, *ma_para)[\"bar\"]\n",
    "    df.loc[:, \"cci\"] = CCI(df, 14) # 14 days CCI indicator\n",
    "    df.loc[:, \"rsi\"] = RSI(df, 7) # 7 days RSI\n",
    "    \n",
    "    # print(f\"fartest datetime acquired for {period}min is at {df['time'].iloc[0]}\")\n",
    "    target_index = df.loc[df[\"time\"] == timestr].index[0]\n",
    "    target_values = df[[\"macd\", \"close\", \"cci\", \"rsi\"]].loc[target_index - history + 1:target_index]\n",
    "\n",
    "    return target_values\n",
    "    \n",
    "def prepare_data(symbol, time_list, label, exam=True):\n",
    "    temp_pd = pd.DataFrame()\n",
    "    for item in time_list:\n",
    "        try:\n",
    "            target_values = find_target(symbol, item[0], item[1])\n",
    "            temp_pd[f\"macd{item[1]}\"], temp_pd[f\"close{item[1]}\"] = list(target_values.loc[:, \"macd\"]), list(target_values.loc[:, \"close\"])\n",
    "        except:\n",
    "            print(f\"{item[1]} is problematic\")\n",
    "    \n",
    "    # add cci10 and rsi10 at the end of dataframe\n",
    "    for item in time_list:\n",
    "        if int(item[1]) == 10:\n",
    "            target_values = find_target(symbol, item[0], item[1])\n",
    "            temp_pd[f\"cci{item[1]}\"], temp_pd[f\"rsi{item[1]}\"] = list(target_values.loc[:, \"cci\"] / 200.), list(target_values.loc[:, \"rsi\"] / 100.) # scale cci by 200, rsi by 100\n",
    "            break\n",
    "    \n",
    "    # another way to add cci10 and rsi10 to the end of datafram\n",
    "    # cci_temp = temp_pd.pop(\"cci10\")\n",
    "    # rsi_temp = temp_pd.pop(\"rsi10\")\n",
    "    # temp_pd.insert(8, \"cci10\", cci_temp)\n",
    "    # temp_pd.insert(9, \"rsi10\", rsi_temp)\n",
    "\n",
    "    # for examination\n",
    "    if exam:\n",
    "        fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, sharex='col', constrained_layout=True)\n",
    "        ax1.bar(range(40), temp_pd[\"macd10\"], width=0.5)\n",
    "        ax1.set_title(f'macd 10min at {time_list[0][0]}, mark: {label}')\n",
    "        ax2.bar(range(40), temp_pd[\"macd15\"], width=0.5)\n",
    "        ax2.set_title(f'macd 15min at {time_list[1][0]}, mark: {label}')\n",
    "        ax3.bar(range(40), temp_pd[\"macd60\"], width=0.5)\n",
    "        ax3.set_title(f'macd 60 at {time_list[2][0]}, mark: {label}')\n",
    "        ax4.bar(range(40), temp_pd[\"macd1440\"], width=0.5)\n",
    "        ax4.set_title(f'macd day at {time_list[3][0]}, mark: {label}')\n",
    "        plt.show()\n",
    "    # end examination\n",
    "\n",
    "    return temp_pd, torch.unsqueeze(torch.from_numpy(temp_pd.values), 0), torch.tensor([label])\n",
    "\n",
    "def init_dataset(saved_time_list, exam=0):\n",
    "    data_tot = torch.zeros(1, 40, 10) # initialize data_tot\n",
    "    label_tot = torch.tensor([0.]) # initialize label_tot\n",
    "    for item in saved_time_list:\n",
    "        _, data, label = prepare_data(item[0], item[1], item[2], exam)\n",
    "        data_tot = torch.cat((data_tot, data),dim=0)\n",
    "        label_tot = torch.cat((label_tot, label))\n",
    "    return data_tot[1:], label_tot[1:]  # exclude initial zero data        \n",
    "\n",
    "# find the min and max close value for every symbol in data_tot, and use them to scale data_tot\n",
    "# def scale_data(time_list, data_tot): \n",
    "#     local_data = deepcopy(data_tot)\n",
    "#     symbol_list = []\n",
    "#     for item in time_list:\n",
    "#         if [item[0] , 0, 1e8] in symbol_list:\n",
    "#             pass\n",
    "#         else:\n",
    "#             symbol_list.append([item[0], 0, 1e8])\n",
    "#     for ii, item in enumerate(time_list):\n",
    "#         for si, symbol in enumerate(symbol_list):\n",
    "#             if item[0] == symbol[0]:\n",
    "#                 max_value = torch.max(local_data[ii,:, 1:8:2])\n",
    "#                 min_value = torch.min(local_data[ii,:, 1:8:2])\n",
    "#                 if max_value > float(symbol[1]):\n",
    "#                     symbol_list[si][1] = float(max_value)\n",
    "#                 if min_value < float(symbol[2]):\n",
    "#                     symbol_list[si][2] = float(min_value)\n",
    "#     # print(symbol_list) # inspect symbol_list and their max values\n",
    "#     for ii, item in enumerate(time_list):\n",
    "#         for si, symbol in enumerate(symbol_list):\n",
    "#             if item[0] == symbol[0]:\n",
    "#                 local_data[ii, :, 1:8:2] = (local_data[ii, :, 1:8:2] - symbol[2]) / (symbol[1] - symbol[2]) # scale corresponding close price with min_max method\n",
    "#     return symbol_list, local_data\n",
    "\n",
    "def scale_data(time_list, data_tot, contract_min_max=contract_min_max):\n",
    "    local_data = deepcopy(data_tot)\n",
    "    for ii, item in enumerate(time_list):\n",
    "        for contract in contract_min_max:\n",
    "            if contract[0] in item[0]:\n",
    "                local_data[ii, :, 1:8:2] = (local_data[ii, :, 1:8:2] - contract[1]) / (contract[2] - contract[1])\n",
    "                break\n",
    "        else:\n",
    "            print(f\"\\n{item[0]} is not in contract_list! Scale it manually or add contract to contract_min_max.py.\\n\")\n",
    "    return contract_min_max, local_data\n",
    "\n",
    "\n",
    "def fig_to_mat(data, size=(320,320), lw=7.0):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.plot(data, linewidth=lw)\n",
    "    plt.axis('off')\n",
    "    plt.margins(0, 0)\n",
    "    # plt.show()\n",
    "    buffer_ = BytesIO() # request cache from memory instead of disk\n",
    "    plt.savefig(buffer_, format='png', bbox_inches='tight')\n",
    "    buffer_.seek(0)\n",
    "    pildata = Image.open(buffer_)\n",
    "    pildata = pildata.resize(size) # resize image\n",
    "    pildata = pildata.convert('1') # black and white mode\n",
    "    pildata = ImageOps.invert(pildata) # invert True and False\n",
    "    img_data = np.asarray(pildata, dtype=np.float32) # tranform True False to 1 0\n",
    "    plt.close()\n",
    "    buffer_.close() # release caches\n",
    "    return pildata, torch.from_numpy(img_data) # return image and matrix\n",
    "\n",
    "today = datetime.strftime(datetime.now(), \"%Y-%m-%d\")\n",
    "\n",
    "def read_data(source, date=today):\n",
    "\n",
    "    # date = datetime.strftime(datetime.now(), \"%Y-%m-%d\")\n",
    "    if source == 'tqsdk': # not available in jupyter\n",
    "        global api\n",
    "        api = TqApi(TqKq(), auth=TqAuth(\"\", \"\"))\n",
    "        print(f\"\\nlength of saved_time_list is: {len(saved_time_list)}\\n\")\n",
    "        data_tot, label_tot = init_dataset(saved_time_list, exam=0)\n",
    "        load_data = TensorDataset(data_tot.to(torch.float32), label_tot)\n",
    "        torch.save(load_data, f\"./data/macd_datasets_{date}.pt\") # save data to file\n",
    "        api.close()\n",
    "    else:\n",
    "        load_data = torch.load(f\"./data/macd_datasets_{date}.pt\")\n",
    "        print(f\"\\nshape of loaded data is: {load_data.tensors[0].shape}\\n\")\n",
    "    return load_data\n",
    "\n",
    "def prepare_img_datasets(load_data, data_dim=2, need_symbol_list=False):\n",
    "    target_dataset = load_data.tensors[0].transpose(1,2)\n",
    "\n",
    "    if data_dim == 2:\n",
    "        print(\"\\ntransforming to 2D data\")\n",
    "        ## 转为二维图片\n",
    "        img_size = (300, 300)\n",
    "        linewidth = 9\n",
    "        shape_of_datasets = target_dataset.shape\n",
    "        img_datasets = torch.zeros(shape_of_datasets[0], shape_of_datasets[1], img_size[0], img_size[1])\n",
    "        for i in range(img_datasets.shape[0]):\n",
    "            for j in range(img_datasets.shape[1]):\n",
    "                _, img_datasets[i, j] = fig_to_mat(target_dataset[i, j], size=img_size, lw=linewidth)\n",
    "\n",
    "        print(f\"\\nshape of datasets is: {img_datasets.shape} \\n\")\n",
    "        plt.imshow(img_datasets[0,1]) # show a picture\n",
    "        return img_datasets\n",
    "    else:\n",
    "        print(\"\\nUse 1d dataset\")\n",
    "        symbol_list, img_datasets = scale_data(saved_time_list, target_dataset.transpose(1,2))\n",
    "        img_datasets = img_datasets.transpose(1,2)\n",
    "        print(f\"\\nshape of datasets is: {img_datasets.shape} \\n\")\n",
    "        if need_symbol_list:\n",
    "            return symbol_list, img_datasets\n",
    "        else:\n",
    "            return img_datasets\n",
    "\n",
    "\n",
    "def prepare_dataloader(img_datasets, target_label, bs=32, label_type='binary', test=True, one_hot_label=False):\n",
    "    # build dataloader\n",
    "    scaled_label = target_label / 10.0\n",
    "    binary_label = torch.tensor((scaled_label > 0), dtype=torch.float32) # 只判断macd信号对错不考虑幅值    \n",
    "    \n",
    "    if label_type == \"scaled\":\n",
    "        print(\"\\nUse scaled label \\n\")\n",
    "        datasets_trans = TensorDataset(img_datasets, torch.unsqueeze(scaled_label, dim=1)) # 使用scaled label\n",
    "    elif label_type == \"binary\":\n",
    "        print(\"\\nUse binary label \\n\")\n",
    "        if one_hot_label:\n",
    "            print(\"Enable one-hot encoding\")\n",
    "            datasets_trans = TensorDataset(img_datasets, F.one_hot(torch.tensor([int(i) for i in binary_label])).to(torch.float32)) # 使用binary label并进行one-hot编码\n",
    "        else:\n",
    "            print(\"Disable one-hot encoding\")\n",
    "            datasets_trans = TensorDataset(img_datasets, torch.unsqueeze(binary_label, dim=1)) # 使用binary label不进行one-hot编码\n",
    "\n",
    "    if test: # add test loader\n",
    "        print(\"\\nenable test loader\")\n",
    "        train_data_size = int(len(img_datasets) * 0.7) # 70%\n",
    "        val_data_size = int((len(img_datasets) - train_data_size) / 2) # 15%\n",
    "        test_data_size = len(img_datasets) - train_data_size - val_data_size # 15% \n",
    "\n",
    "        train_dataset, val_dataset, test_dataset = random_split(datasets_trans, [train_data_size, val_data_size, test_data_size])\n",
    "        train_loader = DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size = bs,\n",
    "            shuffle=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            dataset=val_dataset,\n",
    "            batch_size=len(val_dataset),\n",
    "            shuffle=False\n",
    "        )    \n",
    "        test_loader = DataLoader(\n",
    "            dataset=test_dataset,\n",
    "            batch_size=len(test_dataset),\n",
    "            shuffle=False\n",
    "        )\n",
    "        return train_loader, val_loader, test_loader\n",
    "    else: # use only train and val loader\n",
    "        print(\"\\nno test loader\")\n",
    "        train_data_size = int(len(img_datasets) * 0.8) # 80%\n",
    "        val_data_size = int((len(img_datasets) - train_data_size)) # 20%\n",
    "\n",
    "        train_dataset, val_dataset = random_split(datasets_trans, [train_data_size, val_data_size])\n",
    "        train_loader = DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size = bs,\n",
    "            shuffle=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            dataset=val_dataset,\n",
    "            batch_size=len(val_dataset),\n",
    "            shuffle=False\n",
    "        )    \n",
    "        return train_loader, val_loader\n",
    "    \n",
    "\n",
    "## macd and atr parameters\n",
    "ma_para = [13,34,9]\n",
    "atr_para = [13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build model\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self,alpha=0.25, gamma=2.0,use_sigmoid=True):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.use_sigmoid = use_sigmoid\n",
    "        if use_sigmoid:\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Focal loss\n",
    "        :param pred: shape=(B,  HW)\n",
    "        :param label: shape=(B, HW)\n",
    "        \"\"\"\n",
    "        if self.use_sigmoid:\n",
    "            pred = self.sigmoid(pred)\n",
    "        pred = pred.view(-1)\n",
    "        label = target.view(-1)\n",
    "        pos = torch.nonzero(label > 0).squeeze(1)\n",
    "        pos_num = max(pos.numel(),1.0)\n",
    "        mask = ~(label == -1)\n",
    "        pred = pred[mask]\n",
    "        label= label[mask]\n",
    "        focal_weight = self.alpha *(label- pred).abs().pow(self.gamma) * (label> 0.0).float() + (1 - self.alpha) * pred.abs().pow(self.gamma) * (label<= 0.0).float()\n",
    "        loss = F.binary_cross_entropy(pred, label, reduction='none') * focal_weight\n",
    "        return loss.sum()/pos_num\n",
    "\n",
    "class alexnet1d(nn.Module):\n",
    "    def __init__(self, input_channels=8, out_dim=1, first_kernel=7):\n",
    "        super(alexnet1d, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 64, kernel_size=first_kernel, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            nn.Conv1d(64, 128, kernel_size=5, padding=2), nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Conv1d(256, 128, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Conv1d(128, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(2048), nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LazyLinear(1024), nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LazyLinear(512), nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LazyLinear(out_dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        y = self.model(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict exclusive functions\n",
    "def predict_on_target(net, channels, target_data):\n",
    "  with torch.no_grad():\n",
    "    net = net.to('cpu')\n",
    "    net.eval()\n",
    "    predict = F.softmax(net(target_data[:, channels].to(torch.float32)))\n",
    "    return predict\n",
    "  \n",
    "def analysis_result(predict_list):\n",
    "    tensor_list = torch.tensor(sorted(predict_list, reverse=True), dtype=torch.float32)\n",
    "    print(f\"模型最佳准确率为: {tensor_list[0,0]:.2f}\\n\")\n",
    "    print(f\"前二十预测结果中预测为正的占比为: {tensor_list[:20, 1:].max(1)[1].sum() / 20.:.2f}\")\n",
    "    print(f\"所有trials中预测为正的占比为: {tensor_list[:, 1:].max(1)[1].sum() / len(tensor_list):.2f}\")\n",
    "    return tensor_list\n",
    "\n",
    "def get_predictions(target_time_list, target_data, source, nni_model_num=100):\n",
    "\n",
    "    if target_time_list[0] +' '+ target_time_list[1][0][0] in np.array(load_model_predict_record).reshape(-1):\n",
    "    # if 0: # for test\n",
    "        print(\"\\nPrediction for the target has been recorded ! \")\n",
    "        return None\n",
    "\n",
    "    else:\n",
    "        if source == 'batch': # load saved batch models for predictions\n",
    "            print(f\"\\nRunning macd prediction process for {target_time_list[0]} {target_time_list[1][0][0]} by model batch\")\n",
    "            predict_list = []\n",
    "            for load_batch in range(10): \n",
    "                model_info = torch.load(f\"e:\\\\ml_data\\\\model_checkpoints\\\\model_list_batch_{load_batch}.pth\")\n",
    "                load_channels = model_info[\"channels\"]\n",
    "                for state_dict in model_info[\"top_ten_models\"]:\n",
    "                    load_model = alexnet1d(len(load_channels), 2)\n",
    "                    load_model.load_state_dict(state_dict)\n",
    "                    predict = torch.squeeze(predict_on_target(load_model, load_channels, target_data)).tolist()\n",
    "                    predict_list.append(predict)\n",
    "\n",
    "            print(f\"\\nIn {len(predict_list)} predictions from loaded batch model:\\ninstances predict POSITIVE: {torch.tensor(predict_list).max(1)[1].sum()}\\ninstances predict NEGATIVE: {torch.tensor(predict_list).min(1)[1].sum()} \")\n",
    "            return predict_list\n",
    "        \n",
    "        elif source == 'nni': # load saved NNI models for predictions\n",
    "            print(f\"\\nRunning macd prediction process for {target_time_list[0]} {target_time_list[1][0][0]} by NNI models\")\n",
    "            predict_list = []\n",
    "            model_acc_list = []\n",
    "            best_acc = 0\n",
    "            for trial in range(nni_model_num): # should have 100 models \n",
    "                model_info = torch.load(f\"e:\\\\ml_data\\\\model_checkpoints\\\\model_nni_enismkr2_{trial}.pth\")\n",
    "                load_channels = model_info[\"channels\"]\n",
    "                state_dict = model_info[\"best_model\"]\n",
    "                model_acc = model_info[\"acc\"]\n",
    "                load_model = alexnet1d(len(load_channels), 2)\n",
    "                load_model.load_state_dict(state_dict)\n",
    "                predict = torch.squeeze(predict_on_target(load_model, load_channels, target_data)).tolist()\n",
    "                predict_list.append(predict)\n",
    "                model_acc_list.append(model_acc)\n",
    "                if model_acc > best_acc:\n",
    "                    best_acc = model_acc\n",
    "                    best_model_predict = torch.tensor([predict]).max(1)[1]\n",
    "\n",
    "            print(f\"\\nIn {len(predict_list)} predictions from loaded batch model:\\ninstances predict POSITIVE: {torch.tensor(predict_list).max(1)[1].sum()}\\ninstances predict NEGATIVE: {torch.tensor(predict_list).min(1)[1].sum()}\\nBest model with acc: {best_acc}, predict: {best_model_predict.item()} \")\n",
    "\n",
    "            return model_acc_list, predict_list\n",
    "        \n",
    "def get_realtime_prediction(target_data, source='nni', exp_name='enismkr2', nni_model_num=100):\n",
    "    if source == 'batch': # load saved batch models for predictions\n",
    "        # print(f\"\\nRunning macd prediction process for {target_time_list[0]} {target_time_list[1][0][0]} by model batch\")\n",
    "        predict_list = []\n",
    "        for load_batch in range(10): \n",
    "            model_info = torch.load(f\"e:\\\\ml_data\\\\model_checkpoints\\\\model_list_batch_{load_batch}.pth\")\n",
    "            load_channels = model_info[\"channels\"]\n",
    "            for state_dict in model_info[\"top_ten_models\"]:\n",
    "                load_model = alexnet1d(len(load_channels), 2)\n",
    "                load_model.load_state_dict(state_dict)\n",
    "                predict = torch.squeeze(predict_on_target(load_model, load_channels, target_data)).tolist()\n",
    "                predict_list.append(predict)\n",
    "\n",
    "        # print(f\"\\nIn {len(predict_list)} predictions from loaded batch model:\\ninstances predict POSITIVE: {torch.tensor(predict_list).max(1)[1].sum()}\\ninstances predict NEGATIVE: {torch.tensor(predict_list).min(1)[1].sum()} \")\n",
    "        pos_res = torch.tensor(predict_list).max(1)[1].sum().item()\n",
    "        neg_res = torch.tensor(predict_list).min(1)[1].sum().item()\n",
    "        return pos_res, neg_res  \n",
    "    \n",
    "    elif source == 'nni': # load saved NNI models for predictions\n",
    "        # print(f\"\\nRunning macd prediction process for {target_time_list[0]} {target_time_list[1][0][0]} by NNI models\")\n",
    "        predict_list = []\n",
    "        model_acc_list = []\n",
    "        best_acc = 0\n",
    "        for trial in range(nni_model_num): # should have 100 models \n",
    "            model_info = torch.load(f\"e:\\\\ml_data\\\\model_checkpoints\\\\model_nni_{exp_name}_{trial}.pth\")\n",
    "            load_channels = model_info[\"channels\"]\n",
    "            state_dict = model_info[\"best_model\"]\n",
    "            model_acc = model_info[\"acc\"]\n",
    "            if \"first_kernel\" in model_info.keys():\n",
    "                first_kernel = model_info[\"first_kernel\"]\n",
    "                load_model = alexnet1d(len(load_channels), 2, first_kernel)\n",
    "            else:\n",
    "                load_model = alexnet1d(len(load_channels), 2)\n",
    "            load_model.load_state_dict(state_dict)\n",
    "            predict = torch.squeeze(predict_on_target(load_model, load_channels, target_data)).tolist()\n",
    "            predict_list.append(predict)\n",
    "            model_acc_list.append(model_acc)\n",
    "            if model_acc > best_acc:\n",
    "                best_acc = model_acc\n",
    "                best_model_predict = torch.tensor([predict]).max(1)[1]\n",
    "\n",
    "        # print(f\"\\nIn {len(predict_list)} predictions from loaded batch model:\\ninstances predict POSITIVE: {torch.tensor(predict_list).max(1)[1].sum()}\\ninstances predict NEGATIVE: {torch.tensor(predict_list).min(1)[1].sum()}\\nBest model with acc: {best_acc}, predict: {best_model_predict.item()} \")\n",
    "        pos_res = torch.tensor(predict_list).max(1)[1].sum().item()\n",
    "        neg_res = torch.tensor(predict_list).min(1)[1].sum().item()\n",
    "        return pos_res, neg_res, best_model_predict.item(), best_acc\n",
    "    \n",
    "def real_time_predict(exp_name_list=[\"enismkr2\", \"r4c7esy9\"], interval=50): # read saved exp models to predict\n",
    "# realtime predict\n",
    "    df = pd.DataFrame(columns=[\"macd10\",\"close10\", \"macd15\", \"close15\", \"macd60\", \"close60\", \"macd1440\", \"close1440\", \"cci10\", \"rsi10\"])\n",
    "    \n",
    "    while True:\n",
    "        api.wait_update()\n",
    "        for symbol in inspect_symbol_list:\n",
    "            df[\"macd10\"] = MACD(api.get_kline_serial(symbol, 10 * 60, 800), *ma_para)[\"bar\"].iloc[-40:].tolist()\n",
    "            df[\"close10\"] = api.get_kline_serial(symbol, 10 * 60, 800)[\"close\"].iloc[-40:].tolist()\n",
    "            df[\"cci10\"] = (CCI(api.get_kline_serial(symbol, 10 * 60, 800), 14).iloc[-40:] / 200.)[\"cci\"].to_list()\n",
    "            df[\"rsi10\"] = (RSI(api.get_kline_serial(symbol, 10 * 60, 800), 7).iloc[-40:] / 100.)[\"rsi\"].to_list()\n",
    "            df[\"macd15\"] = MACD(api.get_kline_serial(symbol, 15 * 60, 800), *ma_para)[\"bar\"].iloc[-41:-1].tolist()\n",
    "            df[\"close15\"] = api.get_kline_serial(symbol, 15 * 60, 800)[\"close\"].iloc[-41:-1].tolist()\n",
    "            df[\"macd60\"] = MACD(api.get_kline_serial(symbol, 60 * 60, 800), *ma_para)[\"bar\"].iloc[-41:-1].tolist()\n",
    "            df[\"close60\"] = api.get_kline_serial(symbol, 60 * 60, 800)[\"close\"].iloc[-41:-1].tolist()\n",
    "            df[\"macd1440\"] = MACD(api.get_kline_serial(symbol, 24 * 60 * 60, 800), *ma_para)[\"bar\"].iloc[-41:-1].tolist()\n",
    "            df[\"close1440\"] = api.get_kline_serial(symbol, 24 * 60 * 60, 800)[\"close\"].iloc[-41:-1].tolist()\n",
    "\n",
    "            target_data = torch.unsqueeze(torch.from_numpy(df.values), 0)\n",
    "            \n",
    "            if data_dim == 1:\n",
    "                _, target_data = scale_data([[symbol]], target_data)\n",
    "                target_data = target_data.transpose(1,2)\n",
    "\n",
    "            elif data_dim == 2:\n",
    "                img_size = (300, 300)\n",
    "                linewidth = 9\n",
    "                img_data = torch.zeros(target_data.shape[0], target_data.shape[1], img_size[0], img_size[1])    \n",
    "                for j in range(target_data.shape[1]):\n",
    "                    _, img_data[0, j] = fig_to_mat(target_data[0, j], size=img_size, lw=linewidth)\n",
    "\n",
    "            for exp_name in exp_name_list:\n",
    "                if ' 15:05' in datetime.now().strftime('%Y-%m-%d %H:%M:%S') or ' 23:05' in datetime.now().strftime('%Y-%m-%d %H:%M:%S'):\n",
    "                    break\n",
    "                pos_res, neg_res, best_model_predict, best_acc = get_realtime_prediction(target_data, \"nni\", exp_name=exp_name)\n",
    "                print(f\"exp_{exp_name} {symbol}: POS: {pos_res}, NEG: {neg_res}, best_predict: {best_model_predict} with acc {best_acc} at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "                # pos_res, neg_res, best_model_predict = get_realtime_prediction(target_data, \"nni\", 'r4c7esy9')\n",
    "                # print(f\"exp_r4c7esy9 {symbol}: POS: {pos_res}, NEG: {neg_res}, best_predict: {best_model_predict} at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            print(\"\\n\")\n",
    "        print('\\n','-'*50, '\\n')\n",
    "        sleep(interval)\n",
    "        if ' 15:05' in datetime.now().strftime('%Y-%m-%d %H:%M:%S') or ' 23:05' in datetime.now().strftime('%Y-%m-%d %H:%M:%S'):\n",
    "            print(\"Market Close\\n\")\n",
    "            break\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "shape of loaded data is: torch.Size([400, 40, 10])\n",
      "\n",
      "\n",
      "Use 1d dataset\n",
      "\n",
      "shape of datasets is: torch.Size([400, 10, 40]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "inspect_symbol_list = [\n",
    "    \"CZCE.MA309\",\n",
    "    \"SHFE.bu2306\",\n",
    "    \"CZCE.TA309\",\n",
    "    \"SHFE.fu2309\",\n",
    "    \"SHFE.ru2309\",\n",
    "    \"DCE.p2309\",\n",
    "    \"DCE.y2309\",\n",
    "    \"CZCE.OI309\",\n",
    "    \"DCE.a2309\",\n",
    "    \"DCE.m2309\",\n",
    "    \"SHFE.rb2310\",\n",
    "    \"DCE.i2309\",\n",
    "    \"DCE.jm2309\",\n",
    "    \"CZCE.SR309\",\n",
    "    \"CZCE.CF309\",\n",
    "    \"CZCE.SA309\",\n",
    "    \"SHFE.sp2309\",\n",
    "]\n",
    "\n",
    "day_only_symbol = [\n",
    "    \"CZCE.AP310\",\n",
    "    \"DCE.jd2309\",\n",
    "    \"CZCE.SM309\",\n",
    "    \"CZCE.SF309\",\n",
    "]\n",
    "if int(datetime.now().strftime('%Y-%m-%d %H:%M:%S')[11:13]) <= 14:\n",
    "    for symbol in day_only_symbol:\n",
    "        inspect_symbol_list.append(symbol)\n",
    "\n",
    "# model and data parameters\n",
    "pl.seed_everything(42)\n",
    "load_data = read_data(\"file\", date='2023-03-30')\n",
    "saved_time_list = saved_time_list[:len(load_data.tensors[1])]\n",
    "data_dim = 1 # use 1D or 2D data\n",
    "symbol_list, img_datasets = prepare_img_datasets(load_data, data_dim=data_dim, need_symbol_list=True)\n",
    "\n",
    "label_type = \"binary\" # binary or scaled, default binary \n",
    "one_hot_label = True # enable one-hot encoding or not\n",
    "need_test = False # need test loader or not\n",
    "\n",
    "used_net = \"alexnet1d\"\n",
    "batch_size = 96 # 由NNI调参决定\n",
    "channels = [0, 1, 2, 8] # 由NNI调参决定\n",
    "patient = 9 # 训练达到一定精度之后，经过多少epoch停止训练，由NNI调参决定\n",
    "threshold = 0.88 # 由NNI调参决定\n",
    "\n",
    "out_dim = 2 if label_type == \"binary\" and one_hot_label else 1 # 如果使用二元label以及启用one-hot，模型输出维度为2\n",
    "\n",
    "load_model_predict_record = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在使用天勤量化之前，默认您已经知晓并同意以下免责条款，如果不同意请立即停止使用：https://www.shinnytech.com/blog/disclaimer/\n",
      "2023-04-07 21:07:26.820262 - 通知: 账户  与交易服务器的网络连接已建立.\n",
      "2023-04-07 21:07:27.273897 - 通知: 账户 74fcb0b2-3096-4f69-acaf-9e0144c6930f 登录成功\n",
      "2023-04-07 21:07:27.363042 - 通知: 与合约服务器的网络连接已建立.\n"
     ]
    }
   ],
   "source": [
    "api = TqApi(TqKq(), auth=TqAuth(\"jimsapps\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_enismkr2 CZCE.MA309: POS: 78, NEG: 22, best_predict: 1 with acc 0.81429 at 2023-04-07 21:07:31\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 66, NEG: 34, best_predict: 0 with acc 0.75 at 2023-04-07 21:07:33\n",
      "exp_596v38c2_price CZCE.MA309: POS: 57, NEG: 43, best_predict: 1 with acc 0.775 at 2023-04-07 21:07:36\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 71, NEG: 29, best_predict: 1 with acc 0.81429 at 2023-04-07 21:07:38\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 65, NEG: 35, best_predict: 0 with acc 0.75 at 2023-04-07 21:07:41\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 28, NEG: 72, best_predict: 1 with acc 0.775 at 2023-04-07 21:07:43\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 61, NEG: 39, best_predict: 1 with acc 0.81429 at 2023-04-07 21:07:46\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 38, NEG: 62, best_predict: 1 with acc 0.75 at 2023-04-07 21:07:48\n",
      "exp_596v38c2_price CZCE.TA309: POS: 44, NEG: 56, best_predict: 1 with acc 0.775 at 2023-04-07 21:07:50\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 83, NEG: 17, best_predict: 1 with acc 0.81429 at 2023-04-07 21:07:53\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 74, NEG: 26, best_predict: 1 with acc 0.75 at 2023-04-07 21:07:55\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 58, NEG: 42, best_predict: 1 with acc 0.775 at 2023-04-07 21:07:58\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 70, NEG: 30, best_predict: 0 with acc 0.81429 at 2023-04-07 21:08:00\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 69, NEG: 31, best_predict: 0 with acc 0.75 at 2023-04-07 21:08:02\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 57, NEG: 43, best_predict: 0 with acc 0.775 at 2023-04-07 21:08:05\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 54, NEG: 46, best_predict: 1 with acc 0.81429 at 2023-04-07 21:08:08\n",
      "exp_r4c7esy9 DCE.p2309: POS: 57, NEG: 43, best_predict: 1 with acc 0.75 at 2023-04-07 21:08:10\n",
      "exp_596v38c2_price DCE.p2309: POS: 34, NEG: 66, best_predict: 0 with acc 0.775 at 2023-04-07 21:08:12\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 40, NEG: 60, best_predict: 0 with acc 0.81429 at 2023-04-07 21:08:15\n",
      "exp_r4c7esy9 DCE.y2309: POS: 40, NEG: 60, best_predict: 0 with acc 0.75 at 2023-04-07 21:08:17\n",
      "exp_596v38c2_price DCE.y2309: POS: 47, NEG: 53, best_predict: 0 with acc 0.775 at 2023-04-07 21:08:19\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 41, NEG: 59, best_predict: 0 with acc 0.81429 at 2023-04-07 21:08:21\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 30, NEG: 70, best_predict: 0 with acc 0.75 at 2023-04-07 21:08:24\n",
      "exp_596v38c2_price CZCE.OI309: POS: 54, NEG: 46, best_predict: 0 with acc 0.775 at 2023-04-07 21:08:26\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 50, NEG: 50, best_predict: 1 with acc 0.81429 at 2023-04-07 21:08:28\n",
      "exp_r4c7esy9 DCE.a2309: POS: 69, NEG: 31, best_predict: 0 with acc 0.75 at 2023-04-07 21:08:31\n",
      "exp_596v38c2_price DCE.a2309: POS: 26, NEG: 74, best_predict: 0 with acc 0.775 at 2023-04-07 21:08:33\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 27, NEG: 73, best_predict: 0 with acc 0.81429 at 2023-04-07 21:08:35\n",
      "exp_r4c7esy9 DCE.m2309: POS: 28, NEG: 72, best_predict: 0 with acc 0.75 at 2023-04-07 21:08:37\n",
      "exp_596v38c2_price DCE.m2309: POS: 64, NEG: 36, best_predict: 1 with acc 0.775 at 2023-04-07 21:08:40\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 70, NEG: 30, best_predict: 0 with acc 0.81429 at 2023-04-07 21:08:42\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 71, NEG: 29, best_predict: 1 with acc 0.75 at 2023-04-07 21:08:44\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 38, NEG: 62, best_predict: 0 with acc 0.775 at 2023-04-07 21:08:46\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 85, NEG: 15, best_predict: 1 with acc 0.81429 at 2023-04-07 21:08:49\n",
      "exp_r4c7esy9 DCE.i2309: POS: 87, NEG: 13, best_predict: 1 with acc 0.75 at 2023-04-07 21:08:51\n",
      "exp_596v38c2_price DCE.i2309: POS: 37, NEG: 63, best_predict: 1 with acc 0.775 at 2023-04-07 21:08:53\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 79, NEG: 21, best_predict: 1 with acc 0.81429 at 2023-04-07 21:08:55\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 82, NEG: 18, best_predict: 1 with acc 0.75 at 2023-04-07 21:08:57\n",
      "exp_596v38c2_price DCE.jm2309: POS: 24, NEG: 76, best_predict: 0 with acc 0.775 at 2023-04-07 21:09:00\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 69, NEG: 31, best_predict: 0 with acc 0.81429 at 2023-04-07 21:09:02\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 54, NEG: 46, best_predict: 0 with acc 0.75 at 2023-04-07 21:09:04\n",
      "exp_596v38c2_price CZCE.SR309: POS: 78, NEG: 22, best_predict: 1 with acc 0.775 at 2023-04-07 21:09:06\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 38, NEG: 62, best_predict: 0 with acc 0.81429 at 2023-04-07 21:09:09\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 49, NEG: 51, best_predict: 0 with acc 0.75 at 2023-04-07 21:09:11\n",
      "exp_596v38c2_price CZCE.CF309: POS: 50, NEG: 50, best_predict: 0 with acc 0.775 at 2023-04-07 21:09:13\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 90, NEG: 10, best_predict: 1 with acc 0.81429 at 2023-04-07 21:09:15\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 92, NEG: 8, best_predict: 1 with acc 0.75 at 2023-04-07 21:09:17\n",
      "exp_596v38c2_price CZCE.SA309: POS: 14, NEG: 86, best_predict: 1 with acc 0.775 at 2023-04-07 21:09:19\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 56, NEG: 44, best_predict: 0 with acc 0.81429 at 2023-04-07 21:09:22\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 74, NEG: 26, best_predict: 1 with acc 0.75 at 2023-04-07 21:09:24\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 29, NEG: 71, best_predict: 0 with acc 0.775 at 2023-04-07 21:09:26\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 79, NEG: 21, best_predict: 1 with acc 0.81429 at 2023-04-07 21:11:08\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 65, NEG: 35, best_predict: 0 with acc 0.75 at 2023-04-07 21:11:10\n",
      "exp_596v38c2_price CZCE.MA309: POS: 54, NEG: 46, best_predict: 1 with acc 0.775 at 2023-04-07 21:11:12\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 78, NEG: 22, best_predict: 1 with acc 0.81429 at 2023-04-07 21:11:14\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 81, NEG: 19, best_predict: 1 with acc 0.75 at 2023-04-07 21:11:17\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 28, NEG: 72, best_predict: 1 with acc 0.775 at 2023-04-07 21:11:19\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 56, NEG: 44, best_predict: 1 with acc 0.81429 at 2023-04-07 21:11:21\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 34, NEG: 66, best_predict: 0 with acc 0.75 at 2023-04-07 21:11:23\n",
      "exp_596v38c2_price CZCE.TA309: POS: 56, NEG: 44, best_predict: 1 with acc 0.775 at 2023-04-07 21:11:25\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 86, NEG: 14, best_predict: 1 with acc 0.81429 at 2023-04-07 21:11:28\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 82, NEG: 18, best_predict: 1 with acc 0.75 at 2023-04-07 21:11:30\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 63, NEG: 37, best_predict: 1 with acc 0.775 at 2023-04-07 21:11:32\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 62, NEG: 38, best_predict: 0 with acc 0.81429 at 2023-04-07 21:11:34\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 61, NEG: 39, best_predict: 0 with acc 0.75 at 2023-04-07 21:11:36\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 62, NEG: 38, best_predict: 0 with acc 0.775 at 2023-04-07 21:11:38\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 53, NEG: 47, best_predict: 1 with acc 0.81429 at 2023-04-07 21:11:40\n",
      "exp_r4c7esy9 DCE.p2309: POS: 57, NEG: 43, best_predict: 1 with acc 0.75 at 2023-04-07 21:11:42\n",
      "exp_596v38c2_price DCE.p2309: POS: 39, NEG: 61, best_predict: 0 with acc 0.775 at 2023-04-07 21:11:44\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 39, NEG: 61, best_predict: 0 with acc 0.81429 at 2023-04-07 21:11:46\n",
      "exp_r4c7esy9 DCE.y2309: POS: 42, NEG: 58, best_predict: 0 with acc 0.75 at 2023-04-07 21:11:49\n",
      "exp_596v38c2_price DCE.y2309: POS: 51, NEG: 49, best_predict: 0 with acc 0.775 at 2023-04-07 21:11:51\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 41, NEG: 59, best_predict: 0 with acc 0.81429 at 2023-04-07 21:11:53\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 27, NEG: 73, best_predict: 0 with acc 0.75 at 2023-04-07 21:11:55\n",
      "exp_596v38c2_price CZCE.OI309: POS: 64, NEG: 36, best_predict: 0 with acc 0.775 at 2023-04-07 21:11:57\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 49, NEG: 51, best_predict: 1 with acc 0.81429 at 2023-04-07 21:12:00\n",
      "exp_r4c7esy9 DCE.a2309: POS: 73, NEG: 27, best_predict: 0 with acc 0.75 at 2023-04-07 21:12:02\n",
      "exp_596v38c2_price DCE.a2309: POS: 21, NEG: 79, best_predict: 0 with acc 0.775 at 2023-04-07 21:12:04\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 27, NEG: 73, best_predict: 0 with acc 0.81429 at 2023-04-07 21:12:06\n",
      "exp_r4c7esy9 DCE.m2309: POS: 27, NEG: 73, best_predict: 0 with acc 0.75 at 2023-04-07 21:12:08\n",
      "exp_596v38c2_price DCE.m2309: POS: 65, NEG: 35, best_predict: 1 with acc 0.775 at 2023-04-07 21:12:10\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 69, NEG: 31, best_predict: 0 with acc 0.81429 at 2023-04-07 21:12:12\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 74, NEG: 26, best_predict: 1 with acc 0.75 at 2023-04-07 21:12:14\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 42, NEG: 58, best_predict: 0 with acc 0.775 at 2023-04-07 21:12:16\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 83, NEG: 17, best_predict: 1 with acc 0.81429 at 2023-04-07 21:12:18\n",
      "exp_r4c7esy9 DCE.i2309: POS: 87, NEG: 13, best_predict: 1 with acc 0.75 at 2023-04-07 21:12:20\n",
      "exp_596v38c2_price DCE.i2309: POS: 47, NEG: 53, best_predict: 1 with acc 0.775 at 2023-04-07 21:12:23\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 75, NEG: 25, best_predict: 1 with acc 0.81429 at 2023-04-07 21:12:25\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 83, NEG: 17, best_predict: 1 with acc 0.75 at 2023-04-07 21:12:27\n",
      "exp_596v38c2_price DCE.jm2309: POS: 24, NEG: 76, best_predict: 1 with acc 0.775 at 2023-04-07 21:12:29\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 71, NEG: 29, best_predict: 0 with acc 0.81429 at 2023-04-07 21:12:31\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 57, NEG: 43, best_predict: 0 with acc 0.75 at 2023-04-07 21:12:33\n",
      "exp_596v38c2_price CZCE.SR309: POS: 83, NEG: 17, best_predict: 1 with acc 0.775 at 2023-04-07 21:12:35\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 34, NEG: 66, best_predict: 0 with acc 0.81429 at 2023-04-07 21:12:37\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 48, NEG: 52, best_predict: 0 with acc 0.75 at 2023-04-07 21:12:39\n",
      "exp_596v38c2_price CZCE.CF309: POS: 52, NEG: 48, best_predict: 0 with acc 0.775 at 2023-04-07 21:12:42\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 91, NEG: 9, best_predict: 1 with acc 0.81429 at 2023-04-07 21:12:44\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 93, NEG: 7, best_predict: 1 with acc 0.75 at 2023-04-07 21:12:46\n",
      "exp_596v38c2_price CZCE.SA309: POS: 14, NEG: 86, best_predict: 1 with acc 0.775 at 2023-04-07 21:12:48\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 49, NEG: 51, best_predict: 0 with acc 0.81429 at 2023-04-07 21:12:50\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 69, NEG: 31, best_predict: 1 with acc 0.75 at 2023-04-07 21:12:52\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 34, NEG: 66, best_predict: 0 with acc 0.775 at 2023-04-07 21:12:54\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 79, NEG: 21, best_predict: 1 with acc 0.81429 at 2023-04-07 21:14:36\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 65, NEG: 35, best_predict: 0 with acc 0.75 at 2023-04-07 21:14:38\n",
      "exp_596v38c2_price CZCE.MA309: POS: 54, NEG: 46, best_predict: 1 with acc 0.775 at 2023-04-07 21:14:40\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 78, NEG: 22, best_predict: 1 with acc 0.81429 at 2023-04-07 21:14:42\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 81, NEG: 19, best_predict: 1 with acc 0.75 at 2023-04-07 21:14:44\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 28, NEG: 72, best_predict: 1 with acc 0.775 at 2023-04-07 21:14:47\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 56, NEG: 44, best_predict: 1 with acc 0.81429 at 2023-04-07 21:14:49\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 34, NEG: 66, best_predict: 0 with acc 0.75 at 2023-04-07 21:14:51\n",
      "exp_596v38c2_price CZCE.TA309: POS: 56, NEG: 44, best_predict: 1 with acc 0.775 at 2023-04-07 21:14:53\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 86, NEG: 14, best_predict: 1 with acc 0.81429 at 2023-04-07 21:14:55\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 82, NEG: 18, best_predict: 1 with acc 0.75 at 2023-04-07 21:14:57\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 62, NEG: 38, best_predict: 1 with acc 0.775 at 2023-04-07 21:14:59\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 62, NEG: 38, best_predict: 0 with acc 0.81429 at 2023-04-07 21:15:01\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 61, NEG: 39, best_predict: 0 with acc 0.75 at 2023-04-07 21:15:03\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 62, NEG: 38, best_predict: 0 with acc 0.775 at 2023-04-07 21:15:05\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 53, NEG: 47, best_predict: 1 with acc 0.81429 at 2023-04-07 21:15:07\n",
      "exp_r4c7esy9 DCE.p2309: POS: 57, NEG: 43, best_predict: 1 with acc 0.75 at 2023-04-07 21:15:10\n",
      "exp_596v38c2_price DCE.p2309: POS: 39, NEG: 61, best_predict: 0 with acc 0.775 at 2023-04-07 21:15:12\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 39, NEG: 61, best_predict: 0 with acc 0.81429 at 2023-04-07 21:15:14\n",
      "exp_r4c7esy9 DCE.y2309: POS: 42, NEG: 58, best_predict: 0 with acc 0.75 at 2023-04-07 21:15:16\n",
      "exp_596v38c2_price DCE.y2309: POS: 51, NEG: 49, best_predict: 0 with acc 0.775 at 2023-04-07 21:15:18\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 41, NEG: 59, best_predict: 0 with acc 0.81429 at 2023-04-07 21:15:20\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 27, NEG: 73, best_predict: 0 with acc 0.75 at 2023-04-07 21:15:22\n",
      "exp_596v38c2_price CZCE.OI309: POS: 64, NEG: 36, best_predict: 0 with acc 0.775 at 2023-04-07 21:15:24\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 49, NEG: 51, best_predict: 1 with acc 0.81429 at 2023-04-07 21:15:26\n",
      "exp_r4c7esy9 DCE.a2309: POS: 73, NEG: 27, best_predict: 0 with acc 0.75 at 2023-04-07 21:15:28\n",
      "exp_596v38c2_price DCE.a2309: POS: 21, NEG: 79, best_predict: 0 with acc 0.775 at 2023-04-07 21:15:30\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 27, NEG: 73, best_predict: 0 with acc 0.81429 at 2023-04-07 21:15:32\n",
      "exp_r4c7esy9 DCE.m2309: POS: 27, NEG: 73, best_predict: 0 with acc 0.75 at 2023-04-07 21:15:34\n",
      "exp_596v38c2_price DCE.m2309: POS: 65, NEG: 35, best_predict: 1 with acc 0.775 at 2023-04-07 21:15:36\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 69, NEG: 31, best_predict: 0 with acc 0.81429 at 2023-04-07 21:15:39\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 74, NEG: 26, best_predict: 1 with acc 0.75 at 2023-04-07 21:15:41\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 42, NEG: 58, best_predict: 0 with acc 0.775 at 2023-04-07 21:15:43\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 83, NEG: 17, best_predict: 1 with acc 0.81429 at 2023-04-07 21:15:45\n",
      "exp_r4c7esy9 DCE.i2309: POS: 87, NEG: 13, best_predict: 1 with acc 0.75 at 2023-04-07 21:15:47\n",
      "exp_596v38c2_price DCE.i2309: POS: 47, NEG: 53, best_predict: 1 with acc 0.775 at 2023-04-07 21:15:49\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 75, NEG: 25, best_predict: 1 with acc 0.81429 at 2023-04-07 21:15:51\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 83, NEG: 17, best_predict: 1 with acc 0.75 at 2023-04-07 21:15:54\n",
      "exp_596v38c2_price DCE.jm2309: POS: 24, NEG: 76, best_predict: 1 with acc 0.775 at 2023-04-07 21:15:56\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 71, NEG: 29, best_predict: 0 with acc 0.81429 at 2023-04-07 21:15:58\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 57, NEG: 43, best_predict: 0 with acc 0.75 at 2023-04-07 21:16:00\n",
      "exp_596v38c2_price CZCE.SR309: POS: 83, NEG: 17, best_predict: 1 with acc 0.775 at 2023-04-07 21:16:02\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 34, NEG: 66, best_predict: 0 with acc 0.81429 at 2023-04-07 21:16:04\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 48, NEG: 52, best_predict: 0 with acc 0.75 at 2023-04-07 21:16:06\n",
      "exp_596v38c2_price CZCE.CF309: POS: 53, NEG: 47, best_predict: 0 with acc 0.775 at 2023-04-07 21:16:08\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 91, NEG: 9, best_predict: 1 with acc 0.81429 at 2023-04-07 21:16:10\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 93, NEG: 7, best_predict: 1 with acc 0.75 at 2023-04-07 21:16:12\n",
      "exp_596v38c2_price CZCE.SA309: POS: 14, NEG: 86, best_predict: 1 with acc 0.775 at 2023-04-07 21:16:14\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 49, NEG: 51, best_predict: 0 with acc 0.81429 at 2023-04-07 21:16:16\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 69, NEG: 31, best_predict: 1 with acc 0.75 at 2023-04-07 21:16:18\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 34, NEG: 66, best_predict: 0 with acc 0.775 at 2023-04-07 21:16:20\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 79, NEG: 21, best_predict: 1 with acc 0.81429 at 2023-04-07 21:18:02\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 70, NEG: 30, best_predict: 0 with acc 0.75 at 2023-04-07 21:18:05\n",
      "exp_596v38c2_price CZCE.MA309: POS: 55, NEG: 45, best_predict: 1 with acc 0.775 at 2023-04-07 21:18:07\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 75, NEG: 25, best_predict: 1 with acc 0.81429 at 2023-04-07 21:18:09\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 84, NEG: 16, best_predict: 1 with acc 0.75 at 2023-04-07 21:18:11\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 26, NEG: 74, best_predict: 1 with acc 0.775 at 2023-04-07 21:18:13\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 62, NEG: 38, best_predict: 1 with acc 0.81429 at 2023-04-07 21:18:15\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 42, NEG: 58, best_predict: 1 with acc 0.75 at 2023-04-07 21:18:17\n",
      "exp_596v38c2_price CZCE.TA309: POS: 56, NEG: 44, best_predict: 1 with acc 0.775 at 2023-04-07 21:18:20\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 85, NEG: 15, best_predict: 1 with acc 0.81429 at 2023-04-07 21:18:22\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 82, NEG: 18, best_predict: 1 with acc 0.75 at 2023-04-07 21:18:24\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 61, NEG: 39, best_predict: 1 with acc 0.775 at 2023-04-07 21:18:26\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 68, NEG: 32, best_predict: 0 with acc 0.81429 at 2023-04-07 21:18:28\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 66, NEG: 34, best_predict: 0 with acc 0.75 at 2023-04-07 21:18:30\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 59, NEG: 41, best_predict: 0 with acc 0.775 at 2023-04-07 21:18:32\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 48, NEG: 52, best_predict: 1 with acc 0.81429 at 2023-04-07 21:18:34\n",
      "exp_r4c7esy9 DCE.p2309: POS: 57, NEG: 43, best_predict: 1 with acc 0.75 at 2023-04-07 21:18:37\n",
      "exp_596v38c2_price DCE.p2309: POS: 38, NEG: 62, best_predict: 0 with acc 0.775 at 2023-04-07 21:18:39\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 34, NEG: 66, best_predict: 0 with acc 0.81429 at 2023-04-07 21:18:41\n",
      "exp_r4c7esy9 DCE.y2309: POS: 34, NEG: 66, best_predict: 0 with acc 0.75 at 2023-04-07 21:18:43\n",
      "exp_596v38c2_price DCE.y2309: POS: 49, NEG: 51, best_predict: 0 with acc 0.775 at 2023-04-07 21:18:45\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 34, NEG: 66, best_predict: 0 with acc 0.81429 at 2023-04-07 21:18:47\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 30, NEG: 70, best_predict: 0 with acc 0.75 at 2023-04-07 21:18:49\n",
      "exp_596v38c2_price CZCE.OI309: POS: 65, NEG: 35, best_predict: 0 with acc 0.775 at 2023-04-07 21:18:52\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 53, NEG: 47, best_predict: 1 with acc 0.81429 at 2023-04-07 21:18:54\n",
      "exp_r4c7esy9 DCE.a2309: POS: 70, NEG: 30, best_predict: 0 with acc 0.75 at 2023-04-07 21:18:56\n",
      "exp_596v38c2_price DCE.a2309: POS: 21, NEG: 79, best_predict: 0 with acc 0.775 at 2023-04-07 21:18:58\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 26, NEG: 74, best_predict: 0 with acc 0.81429 at 2023-04-07 21:19:00\n",
      "exp_r4c7esy9 DCE.m2309: POS: 26, NEG: 74, best_predict: 0 with acc 0.75 at 2023-04-07 21:19:03\n",
      "exp_596v38c2_price DCE.m2309: POS: 65, NEG: 35, best_predict: 1 with acc 0.775 at 2023-04-07 21:19:05\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 68, NEG: 32, best_predict: 0 with acc 0.81429 at 2023-04-07 21:19:07\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 69, NEG: 31, best_predict: 1 with acc 0.75 at 2023-04-07 21:19:10\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 41, NEG: 59, best_predict: 0 with acc 0.775 at 2023-04-07 21:19:12\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 83, NEG: 17, best_predict: 1 with acc 0.81429 at 2023-04-07 21:19:14\n",
      "exp_r4c7esy9 DCE.i2309: POS: 87, NEG: 13, best_predict: 1 with acc 0.75 at 2023-04-07 21:19:16\n",
      "exp_596v38c2_price DCE.i2309: POS: 46, NEG: 54, best_predict: 1 with acc 0.775 at 2023-04-07 21:19:19\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 77, NEG: 23, best_predict: 1 with acc 0.81429 at 2023-04-07 21:19:21\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 91, NEG: 9, best_predict: 1 with acc 0.75 at 2023-04-07 21:19:23\n",
      "exp_596v38c2_price DCE.jm2309: POS: 23, NEG: 77, best_predict: 1 with acc 0.775 at 2023-04-07 21:19:25\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 74, NEG: 26, best_predict: 0 with acc 0.81429 at 2023-04-07 21:19:27\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 58, NEG: 42, best_predict: 0 with acc 0.75 at 2023-04-07 21:19:29\n",
      "exp_596v38c2_price CZCE.SR309: POS: 82, NEG: 18, best_predict: 1 with acc 0.775 at 2023-04-07 21:19:31\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 32, NEG: 68, best_predict: 0 with acc 0.81429 at 2023-04-07 21:19:33\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 40, NEG: 60, best_predict: 0 with acc 0.75 at 2023-04-07 21:19:35\n",
      "exp_596v38c2_price CZCE.CF309: POS: 53, NEG: 47, best_predict: 0 with acc 0.775 at 2023-04-07 21:19:37\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 93, NEG: 7, best_predict: 1 with acc 0.81429 at 2023-04-07 21:19:40\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 98, NEG: 2, best_predict: 1 with acc 0.75 at 2023-04-07 21:19:42\n",
      "exp_596v38c2_price CZCE.SA309: POS: 13, NEG: 87, best_predict: 1 with acc 0.775 at 2023-04-07 21:19:44\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 55, NEG: 45, best_predict: 0 with acc 0.81429 at 2023-04-07 21:19:46\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 71, NEG: 29, best_predict: 0 with acc 0.75 at 2023-04-07 21:19:48\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 31, NEG: 69, best_predict: 0 with acc 0.775 at 2023-04-07 21:19:50\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 80, NEG: 20, best_predict: 1 with acc 0.81429 at 2023-04-07 21:21:32\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 75, NEG: 25, best_predict: 0 with acc 0.75 at 2023-04-07 21:21:34\n",
      "exp_596v38c2_price CZCE.MA309: POS: 51, NEG: 49, best_predict: 0 with acc 0.775 at 2023-04-07 21:21:36\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 80, NEG: 20, best_predict: 1 with acc 0.81429 at 2023-04-07 21:21:39\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 84, NEG: 16, best_predict: 1 with acc 0.75 at 2023-04-07 21:21:41\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 26, NEG: 74, best_predict: 1 with acc 0.775 at 2023-04-07 21:21:43\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 58, NEG: 42, best_predict: 1 with acc 0.81429 at 2023-04-07 21:21:45\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 45, NEG: 55, best_predict: 1 with acc 0.75 at 2023-04-07 21:21:47\n",
      "exp_596v38c2_price CZCE.TA309: POS: 63, NEG: 37, best_predict: 1 with acc 0.775 at 2023-04-07 21:21:49\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 86, NEG: 14, best_predict: 1 with acc 0.81429 at 2023-04-07 21:21:51\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 91, NEG: 9, best_predict: 1 with acc 0.75 at 2023-04-07 21:21:53\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 73, NEG: 27, best_predict: 1 with acc 0.775 at 2023-04-07 21:21:55\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 52, NEG: 48, best_predict: 0 with acc 0.81429 at 2023-04-07 21:21:57\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 49, NEG: 51, best_predict: 0 with acc 0.75 at 2023-04-07 21:21:59\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 66, NEG: 34, best_predict: 0 with acc 0.775 at 2023-04-07 21:22:01\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 55, NEG: 45, best_predict: 1 with acc 0.81429 at 2023-04-07 21:22:03\n",
      "exp_r4c7esy9 DCE.p2309: POS: 58, NEG: 42, best_predict: 1 with acc 0.75 at 2023-04-07 21:22:05\n",
      "exp_596v38c2_price DCE.p2309: POS: 48, NEG: 52, best_predict: 0 with acc 0.775 at 2023-04-07 21:22:07\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 38, NEG: 62, best_predict: 0 with acc 0.81429 at 2023-04-07 21:22:09\n",
      "exp_r4c7esy9 DCE.y2309: POS: 39, NEG: 61, best_predict: 0 with acc 0.75 at 2023-04-07 21:22:11\n",
      "exp_596v38c2_price DCE.y2309: POS: 54, NEG: 46, best_predict: 0 with acc 0.775 at 2023-04-07 21:22:13\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 35, NEG: 65, best_predict: 0 with acc 0.81429 at 2023-04-07 21:22:15\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 28, NEG: 72, best_predict: 0 with acc 0.75 at 2023-04-07 21:22:17\n",
      "exp_596v38c2_price CZCE.OI309: POS: 69, NEG: 31, best_predict: 0 with acc 0.775 at 2023-04-07 21:22:20\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 62, NEG: 38, best_predict: 1 with acc 0.81429 at 2023-04-07 21:22:22\n",
      "exp_r4c7esy9 DCE.a2309: POS: 75, NEG: 25, best_predict: 0 with acc 0.75 at 2023-04-07 21:22:24\n",
      "exp_596v38c2_price DCE.a2309: POS: 23, NEG: 77, best_predict: 0 with acc 0.775 at 2023-04-07 21:22:26\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 24, NEG: 76, best_predict: 0 with acc 0.81429 at 2023-04-07 21:22:28\n",
      "exp_r4c7esy9 DCE.m2309: POS: 25, NEG: 75, best_predict: 0 with acc 0.75 at 2023-04-07 21:22:30\n",
      "exp_596v38c2_price DCE.m2309: POS: 66, NEG: 34, best_predict: 1 with acc 0.775 at 2023-04-07 21:22:32\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 67, NEG: 33, best_predict: 0 with acc 0.81429 at 2023-04-07 21:22:34\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 69, NEG: 31, best_predict: 1 with acc 0.75 at 2023-04-07 21:22:36\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 40, NEG: 60, best_predict: 0 with acc 0.775 at 2023-04-07 21:22:38\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 83, NEG: 17, best_predict: 1 with acc 0.81429 at 2023-04-07 21:22:40\n",
      "exp_r4c7esy9 DCE.i2309: POS: 87, NEG: 13, best_predict: 1 with acc 0.75 at 2023-04-07 21:22:42\n",
      "exp_596v38c2_price DCE.i2309: POS: 53, NEG: 47, best_predict: 1 with acc 0.775 at 2023-04-07 21:22:44\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 76, NEG: 24, best_predict: 1 with acc 0.81429 at 2023-04-07 21:22:47\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 91, NEG: 9, best_predict: 1 with acc 0.75 at 2023-04-07 21:22:49\n",
      "exp_596v38c2_price DCE.jm2309: POS: 37, NEG: 63, best_predict: 1 with acc 0.775 at 2023-04-07 21:22:51\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 76, NEG: 24, best_predict: 0 with acc 0.81429 at 2023-04-07 21:22:53\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 63, NEG: 37, best_predict: 0 with acc 0.75 at 2023-04-07 21:22:55\n",
      "exp_596v38c2_price CZCE.SR309: POS: 85, NEG: 15, best_predict: 1 with acc 0.775 at 2023-04-07 21:22:57\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 31, NEG: 69, best_predict: 0 with acc 0.81429 at 2023-04-07 21:22:59\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 43, NEG: 57, best_predict: 0 with acc 0.75 at 2023-04-07 21:23:01\n",
      "exp_596v38c2_price CZCE.CF309: POS: 48, NEG: 52, best_predict: 0 with acc 0.775 at 2023-04-07 21:23:03\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 94, NEG: 6, best_predict: 1 with acc 0.81429 at 2023-04-07 21:23:05\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 98, NEG: 2, best_predict: 1 with acc 0.75 at 2023-04-07 21:23:08\n",
      "exp_596v38c2_price CZCE.SA309: POS: 14, NEG: 86, best_predict: 1 with acc 0.775 at 2023-04-07 21:23:10\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 48, NEG: 52, best_predict: 0 with acc 0.81429 at 2023-04-07 21:23:12\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 58, NEG: 42, best_predict: 0 with acc 0.75 at 2023-04-07 21:23:14\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 39, NEG: 61, best_predict: 0 with acc 0.775 at 2023-04-07 21:23:16\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 80, NEG: 20, best_predict: 1 with acc 0.81429 at 2023-04-07 21:24:58\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 75, NEG: 25, best_predict: 0 with acc 0.75 at 2023-04-07 21:25:00\n",
      "exp_596v38c2_price CZCE.MA309: POS: 51, NEG: 49, best_predict: 0 with acc 0.775 at 2023-04-07 21:25:03\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 80, NEG: 20, best_predict: 1 with acc 0.81429 at 2023-04-07 21:25:05\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 85, NEG: 15, best_predict: 1 with acc 0.75 at 2023-04-07 21:25:07\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 26, NEG: 74, best_predict: 1 with acc 0.775 at 2023-04-07 21:25:09\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 59, NEG: 41, best_predict: 1 with acc 0.81429 at 2023-04-07 21:25:11\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 45, NEG: 55, best_predict: 1 with acc 0.75 at 2023-04-07 21:25:13\n",
      "exp_596v38c2_price CZCE.TA309: POS: 62, NEG: 38, best_predict: 1 with acc 0.775 at 2023-04-07 21:25:15\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 86, NEG: 14, best_predict: 1 with acc 0.81429 at 2023-04-07 21:25:17\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 91, NEG: 9, best_predict: 1 with acc 0.75 at 2023-04-07 21:25:19\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 73, NEG: 27, best_predict: 1 with acc 0.775 at 2023-04-07 21:25:21\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 52, NEG: 48, best_predict: 0 with acc 0.81429 at 2023-04-07 21:25:24\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 49, NEG: 51, best_predict: 0 with acc 0.75 at 2023-04-07 21:25:26\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 66, NEG: 34, best_predict: 0 with acc 0.775 at 2023-04-07 21:25:28\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 55, NEG: 45, best_predict: 1 with acc 0.81429 at 2023-04-07 21:25:30\n",
      "exp_r4c7esy9 DCE.p2309: POS: 58, NEG: 42, best_predict: 1 with acc 0.75 at 2023-04-07 21:25:32\n",
      "exp_596v38c2_price DCE.p2309: POS: 48, NEG: 52, best_predict: 0 with acc 0.775 at 2023-04-07 21:25:34\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 38, NEG: 62, best_predict: 0 with acc 0.81429 at 2023-04-07 21:25:36\n",
      "exp_r4c7esy9 DCE.y2309: POS: 38, NEG: 62, best_predict: 0 with acc 0.75 at 2023-04-07 21:25:38\n",
      "exp_596v38c2_price DCE.y2309: POS: 54, NEG: 46, best_predict: 0 with acc 0.775 at 2023-04-07 21:25:40\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 35, NEG: 65, best_predict: 0 with acc 0.81429 at 2023-04-07 21:25:42\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 29, NEG: 71, best_predict: 0 with acc 0.75 at 2023-04-07 21:25:44\n",
      "exp_596v38c2_price CZCE.OI309: POS: 69, NEG: 31, best_predict: 0 with acc 0.775 at 2023-04-07 21:25:46\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 62, NEG: 38, best_predict: 1 with acc 0.81429 at 2023-04-07 21:25:48\n",
      "exp_r4c7esy9 DCE.a2309: POS: 75, NEG: 25, best_predict: 0 with acc 0.75 at 2023-04-07 21:25:50\n",
      "exp_596v38c2_price DCE.a2309: POS: 23, NEG: 77, best_predict: 0 with acc 0.775 at 2023-04-07 21:25:52\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 24, NEG: 76, best_predict: 0 with acc 0.81429 at 2023-04-07 21:25:54\n",
      "exp_r4c7esy9 DCE.m2309: POS: 25, NEG: 75, best_predict: 0 with acc 0.75 at 2023-04-07 21:25:56\n",
      "exp_596v38c2_price DCE.m2309: POS: 66, NEG: 34, best_predict: 1 with acc 0.775 at 2023-04-07 21:25:58\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 67, NEG: 33, best_predict: 0 with acc 0.81429 at 2023-04-07 21:26:00\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 69, NEG: 31, best_predict: 1 with acc 0.75 at 2023-04-07 21:26:02\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 40, NEG: 60, best_predict: 0 with acc 0.775 at 2023-04-07 21:26:04\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 83, NEG: 17, best_predict: 1 with acc 0.81429 at 2023-04-07 21:26:06\n",
      "exp_r4c7esy9 DCE.i2309: POS: 87, NEG: 13, best_predict: 1 with acc 0.75 at 2023-04-07 21:26:08\n",
      "exp_596v38c2_price DCE.i2309: POS: 53, NEG: 47, best_predict: 1 with acc 0.775 at 2023-04-07 21:26:10\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 76, NEG: 24, best_predict: 1 with acc 0.81429 at 2023-04-07 21:26:12\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 91, NEG: 9, best_predict: 1 with acc 0.75 at 2023-04-07 21:26:14\n",
      "exp_596v38c2_price DCE.jm2309: POS: 37, NEG: 63, best_predict: 1 with acc 0.775 at 2023-04-07 21:26:16\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 77, NEG: 23, best_predict: 0 with acc 0.81429 at 2023-04-07 21:26:19\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 63, NEG: 37, best_predict: 0 with acc 0.75 at 2023-04-07 21:26:21\n",
      "exp_596v38c2_price CZCE.SR309: POS: 85, NEG: 15, best_predict: 1 with acc 0.775 at 2023-04-07 21:26:23\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 31, NEG: 69, best_predict: 0 with acc 0.81429 at 2023-04-07 21:26:25\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 42, NEG: 58, best_predict: 0 with acc 0.75 at 2023-04-07 21:26:27\n",
      "exp_596v38c2_price CZCE.CF309: POS: 48, NEG: 52, best_predict: 0 with acc 0.775 at 2023-04-07 21:26:29\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 94, NEG: 6, best_predict: 1 with acc 0.81429 at 2023-04-07 21:26:31\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 98, NEG: 2, best_predict: 1 with acc 0.75 at 2023-04-07 21:26:33\n",
      "exp_596v38c2_price CZCE.SA309: POS: 14, NEG: 86, best_predict: 1 with acc 0.775 at 2023-04-07 21:26:35\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 47, NEG: 53, best_predict: 0 with acc 0.81429 at 2023-04-07 21:26:37\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 58, NEG: 42, best_predict: 0 with acc 0.75 at 2023-04-07 21:26:39\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 39, NEG: 61, best_predict: 0 with acc 0.775 at 2023-04-07 21:26:41\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 80, NEG: 20, best_predict: 1 with acc 0.81429 at 2023-04-07 21:28:23\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 75, NEG: 25, best_predict: 0 with acc 0.75 at 2023-04-07 21:28:25\n",
      "exp_596v38c2_price CZCE.MA309: POS: 51, NEG: 49, best_predict: 0 with acc 0.775 at 2023-04-07 21:28:27\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 80, NEG: 20, best_predict: 1 with acc 0.81429 at 2023-04-07 21:28:29\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 85, NEG: 15, best_predict: 1 with acc 0.75 at 2023-04-07 21:28:31\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 26, NEG: 74, best_predict: 1 with acc 0.775 at 2023-04-07 21:28:33\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 59, NEG: 41, best_predict: 1 with acc 0.81429 at 2023-04-07 21:28:35\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 45, NEG: 55, best_predict: 1 with acc 0.75 at 2023-04-07 21:28:38\n",
      "exp_596v38c2_price CZCE.TA309: POS: 62, NEG: 38, best_predict: 1 with acc 0.775 at 2023-04-07 21:28:40\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 86, NEG: 14, best_predict: 1 with acc 0.81429 at 2023-04-07 21:28:42\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 90, NEG: 10, best_predict: 1 with acc 0.75 at 2023-04-07 21:28:44\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 73, NEG: 27, best_predict: 1 with acc 0.775 at 2023-04-07 21:28:46\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 52, NEG: 48, best_predict: 0 with acc 0.81429 at 2023-04-07 21:28:48\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 49, NEG: 51, best_predict: 0 with acc 0.75 at 2023-04-07 21:28:50\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 66, NEG: 34, best_predict: 0 with acc 0.775 at 2023-04-07 21:28:52\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 55, NEG: 45, best_predict: 1 with acc 0.81429 at 2023-04-07 21:28:54\n",
      "exp_r4c7esy9 DCE.p2309: POS: 58, NEG: 42, best_predict: 1 with acc 0.75 at 2023-04-07 21:28:56\n",
      "exp_596v38c2_price DCE.p2309: POS: 48, NEG: 52, best_predict: 0 with acc 0.775 at 2023-04-07 21:28:58\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 38, NEG: 62, best_predict: 0 with acc 0.81429 at 2023-04-07 21:29:00\n",
      "exp_r4c7esy9 DCE.y2309: POS: 39, NEG: 61, best_predict: 0 with acc 0.75 at 2023-04-07 21:29:02\n",
      "exp_596v38c2_price DCE.y2309: POS: 54, NEG: 46, best_predict: 0 with acc 0.775 at 2023-04-07 21:29:04\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 35, NEG: 65, best_predict: 0 with acc 0.81429 at 2023-04-07 21:29:07\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 29, NEG: 71, best_predict: 0 with acc 0.75 at 2023-04-07 21:29:09\n",
      "exp_596v38c2_price CZCE.OI309: POS: 69, NEG: 31, best_predict: 0 with acc 0.775 at 2023-04-07 21:29:11\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 61, NEG: 39, best_predict: 1 with acc 0.81429 at 2023-04-07 21:29:13\n",
      "exp_r4c7esy9 DCE.a2309: POS: 75, NEG: 25, best_predict: 0 with acc 0.75 at 2023-04-07 21:29:15\n",
      "exp_596v38c2_price DCE.a2309: POS: 23, NEG: 77, best_predict: 0 with acc 0.775 at 2023-04-07 21:29:17\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 24, NEG: 76, best_predict: 0 with acc 0.81429 at 2023-04-07 21:29:19\n",
      "exp_r4c7esy9 DCE.m2309: POS: 25, NEG: 75, best_predict: 0 with acc 0.75 at 2023-04-07 21:29:21\n",
      "exp_596v38c2_price DCE.m2309: POS: 66, NEG: 34, best_predict: 1 with acc 0.775 at 2023-04-07 21:29:23\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 67, NEG: 33, best_predict: 0 with acc 0.81429 at 2023-04-07 21:29:25\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 69, NEG: 31, best_predict: 1 with acc 0.75 at 2023-04-07 21:29:27\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 40, NEG: 60, best_predict: 0 with acc 0.775 at 2023-04-07 21:29:29\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 83, NEG: 17, best_predict: 1 with acc 0.81429 at 2023-04-07 21:29:31\n",
      "exp_r4c7esy9 DCE.i2309: POS: 87, NEG: 13, best_predict: 1 with acc 0.75 at 2023-04-07 21:29:34\n",
      "exp_596v38c2_price DCE.i2309: POS: 53, NEG: 47, best_predict: 1 with acc 0.775 at 2023-04-07 21:29:36\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 76, NEG: 24, best_predict: 1 with acc 0.81429 at 2023-04-07 21:29:39\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 91, NEG: 9, best_predict: 1 with acc 0.75 at 2023-04-07 21:29:41\n",
      "exp_596v38c2_price DCE.jm2309: POS: 38, NEG: 62, best_predict: 1 with acc 0.775 at 2023-04-07 21:29:43\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 77, NEG: 23, best_predict: 0 with acc 0.81429 at 2023-04-07 21:29:46\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 63, NEG: 37, best_predict: 0 with acc 0.75 at 2023-04-07 21:29:48\n",
      "exp_596v38c2_price CZCE.SR309: POS: 85, NEG: 15, best_predict: 1 with acc 0.775 at 2023-04-07 21:29:50\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 31, NEG: 69, best_predict: 0 with acc 0.81429 at 2023-04-07 21:29:53\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 42, NEG: 58, best_predict: 0 with acc 0.75 at 2023-04-07 21:29:55\n",
      "exp_596v38c2_price CZCE.CF309: POS: 48, NEG: 52, best_predict: 0 with acc 0.775 at 2023-04-07 21:29:57\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 94, NEG: 6, best_predict: 1 with acc 0.81429 at 2023-04-07 21:29:59\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 98, NEG: 2, best_predict: 1 with acc 0.75 at 2023-04-07 21:30:01\n",
      "exp_596v38c2_price CZCE.SA309: POS: 14, NEG: 86, best_predict: 1 with acc 0.775 at 2023-04-07 21:30:03\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 47, NEG: 53, best_predict: 0 with acc 0.81429 at 2023-04-07 21:30:05\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 58, NEG: 42, best_predict: 0 with acc 0.75 at 2023-04-07 21:30:07\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 40, NEG: 60, best_predict: 0 with acc 0.775 at 2023-04-07 21:30:09\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 83, NEG: 17, best_predict: 1 with acc 0.81429 at 2023-04-07 21:31:51\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 90, NEG: 10, best_predict: 1 with acc 0.75 at 2023-04-07 21:31:54\n",
      "exp_596v38c2_price CZCE.MA309: POS: 42, NEG: 58, best_predict: 0 with acc 0.775 at 2023-04-07 21:31:56\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 81, NEG: 19, best_predict: 1 with acc 0.81429 at 2023-04-07 21:31:58\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 89, NEG: 11, best_predict: 1 with acc 0.75 at 2023-04-07 21:32:00\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 27, NEG: 73, best_predict: 1 with acc 0.775 at 2023-04-07 21:32:02\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 63, NEG: 37, best_predict: 1 with acc 0.81429 at 2023-04-07 21:32:04\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 52, NEG: 48, best_predict: 1 with acc 0.75 at 2023-04-07 21:32:06\n",
      "exp_596v38c2_price CZCE.TA309: POS: 61, NEG: 39, best_predict: 1 with acc 0.775 at 2023-04-07 21:32:08\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 86, NEG: 14, best_predict: 1 with acc 0.81429 at 2023-04-07 21:32:10\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 21:32:12\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 80, NEG: 20, best_predict: 1 with acc 0.775 at 2023-04-07 21:32:15\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 50, NEG: 50, best_predict: 0 with acc 0.81429 at 2023-04-07 21:32:17\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 41, NEG: 59, best_predict: 0 with acc 0.75 at 2023-04-07 21:32:19\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 62, NEG: 38, best_predict: 0 with acc 0.775 at 2023-04-07 21:32:21\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 52, NEG: 48, best_predict: 1 with acc 0.81429 at 2023-04-07 21:32:23\n",
      "exp_r4c7esy9 DCE.p2309: POS: 59, NEG: 41, best_predict: 1 with acc 0.75 at 2023-04-07 21:32:25\n",
      "exp_596v38c2_price DCE.p2309: POS: 48, NEG: 52, best_predict: 0 with acc 0.775 at 2023-04-07 21:32:27\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 34, NEG: 66, best_predict: 1 with acc 0.81429 at 2023-04-07 21:32:29\n",
      "exp_r4c7esy9 DCE.y2309: POS: 44, NEG: 56, best_predict: 0 with acc 0.75 at 2023-04-07 21:32:31\n",
      "exp_596v38c2_price DCE.y2309: POS: 53, NEG: 47, best_predict: 0 with acc 0.775 at 2023-04-07 21:32:33\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 36, NEG: 64, best_predict: 1 with acc 0.81429 at 2023-04-07 21:32:35\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 34, NEG: 66, best_predict: 0 with acc 0.75 at 2023-04-07 21:32:37\n",
      "exp_596v38c2_price CZCE.OI309: POS: 69, NEG: 31, best_predict: 0 with acc 0.775 at 2023-04-07 21:32:39\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 62, NEG: 38, best_predict: 1 with acc 0.81429 at 2023-04-07 21:32:41\n",
      "exp_r4c7esy9 DCE.a2309: POS: 71, NEG: 29, best_predict: 0 with acc 0.75 at 2023-04-07 21:32:43\n",
      "exp_596v38c2_price DCE.a2309: POS: 23, NEG: 77, best_predict: 0 with acc 0.775 at 2023-04-07 21:32:45\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 28, NEG: 72, best_predict: 0 with acc 0.81429 at 2023-04-07 21:32:47\n",
      "exp_r4c7esy9 DCE.m2309: POS: 31, NEG: 69, best_predict: 0 with acc 0.75 at 2023-04-07 21:32:49\n",
      "exp_596v38c2_price DCE.m2309: POS: 64, NEG: 36, best_predict: 1 with acc 0.775 at 2023-04-07 21:32:51\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 62, NEG: 38, best_predict: 0 with acc 0.81429 at 2023-04-07 21:32:54\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 61, NEG: 39, best_predict: 0 with acc 0.75 at 2023-04-07 21:32:56\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 41, NEG: 59, best_predict: 0 with acc 0.775 at 2023-04-07 21:32:58\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 83, NEG: 17, best_predict: 1 with acc 0.81429 at 2023-04-07 21:33:00\n",
      "exp_r4c7esy9 DCE.i2309: POS: 87, NEG: 13, best_predict: 1 with acc 0.75 at 2023-04-07 21:33:02\n",
      "exp_596v38c2_price DCE.i2309: POS: 56, NEG: 44, best_predict: 1 with acc 0.775 at 2023-04-07 21:33:04\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 82, NEG: 18, best_predict: 1 with acc 0.81429 at 2023-04-07 21:33:06\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 92, NEG: 8, best_predict: 1 with acc 0.75 at 2023-04-07 21:33:08\n",
      "exp_596v38c2_price DCE.jm2309: POS: 44, NEG: 56, best_predict: 1 with acc 0.775 at 2023-04-07 21:33:10\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 83, NEG: 17, best_predict: 0 with acc 0.81429 at 2023-04-07 21:33:12\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 69, NEG: 31, best_predict: 0 with acc 0.75 at 2023-04-07 21:33:14\n",
      "exp_596v38c2_price CZCE.SR309: POS: 90, NEG: 10, best_predict: 1 with acc 0.775 at 2023-04-07 21:33:16\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 26, NEG: 74, best_predict: 0 with acc 0.81429 at 2023-04-07 21:33:18\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 33, NEG: 67, best_predict: 0 with acc 0.75 at 2023-04-07 21:33:20\n",
      "exp_596v38c2_price CZCE.CF309: POS: 37, NEG: 63, best_predict: 0 with acc 0.775 at 2023-04-07 21:33:22\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 95, NEG: 5, best_predict: 1 with acc 0.81429 at 2023-04-07 21:33:24\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 98, NEG: 2, best_predict: 1 with acc 0.75 at 2023-04-07 21:33:27\n",
      "exp_596v38c2_price CZCE.SA309: POS: 19, NEG: 81, best_predict: 1 with acc 0.775 at 2023-04-07 21:33:29\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 44, NEG: 56, best_predict: 0 with acc 0.81429 at 2023-04-07 21:33:31\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 55, NEG: 45, best_predict: 0 with acc 0.75 at 2023-04-07 21:33:33\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 41, NEG: 59, best_predict: 0 with acc 0.775 at 2023-04-07 21:33:35\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 83, NEG: 17, best_predict: 1 with acc 0.81429 at 2023-04-07 21:35:17\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 90, NEG: 10, best_predict: 1 with acc 0.75 at 2023-04-07 21:35:19\n",
      "exp_596v38c2_price CZCE.MA309: POS: 42, NEG: 58, best_predict: 0 with acc 0.775 at 2023-04-07 21:35:21\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 81, NEG: 19, best_predict: 1 with acc 0.81429 at 2023-04-07 21:35:23\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 89, NEG: 11, best_predict: 1 with acc 0.75 at 2023-04-07 21:35:25\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 27, NEG: 73, best_predict: 1 with acc 0.775 at 2023-04-07 21:35:27\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 63, NEG: 37, best_predict: 1 with acc 0.81429 at 2023-04-07 21:35:30\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 52, NEG: 48, best_predict: 1 with acc 0.75 at 2023-04-07 21:35:32\n",
      "exp_596v38c2_price CZCE.TA309: POS: 61, NEG: 39, best_predict: 1 with acc 0.775 at 2023-04-07 21:35:34\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 86, NEG: 14, best_predict: 1 with acc 0.81429 at 2023-04-07 21:35:36\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 21:35:38\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 80, NEG: 20, best_predict: 1 with acc 0.775 at 2023-04-07 21:35:40\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 50, NEG: 50, best_predict: 0 with acc 0.81429 at 2023-04-07 21:35:42\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 42, NEG: 58, best_predict: 0 with acc 0.75 at 2023-04-07 21:35:45\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 62, NEG: 38, best_predict: 0 with acc 0.775 at 2023-04-07 21:35:47\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 52, NEG: 48, best_predict: 1 with acc 0.81429 at 2023-04-07 21:35:49\n",
      "exp_r4c7esy9 DCE.p2309: POS: 59, NEG: 41, best_predict: 1 with acc 0.75 at 2023-04-07 21:35:51\n",
      "exp_596v38c2_price DCE.p2309: POS: 48, NEG: 52, best_predict: 0 with acc 0.775 at 2023-04-07 21:35:53\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 34, NEG: 66, best_predict: 1 with acc 0.81429 at 2023-04-07 21:35:55\n",
      "exp_r4c7esy9 DCE.y2309: POS: 44, NEG: 56, best_predict: 0 with acc 0.75 at 2023-04-07 21:35:58\n",
      "exp_596v38c2_price DCE.y2309: POS: 53, NEG: 47, best_predict: 0 with acc 0.775 at 2023-04-07 21:36:00\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 36, NEG: 64, best_predict: 1 with acc 0.81429 at 2023-04-07 21:36:02\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 34, NEG: 66, best_predict: 0 with acc 0.75 at 2023-04-07 21:36:04\n",
      "exp_596v38c2_price CZCE.OI309: POS: 69, NEG: 31, best_predict: 0 with acc 0.775 at 2023-04-07 21:36:06\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 62, NEG: 38, best_predict: 1 with acc 0.81429 at 2023-04-07 21:36:08\n",
      "exp_r4c7esy9 DCE.a2309: POS: 71, NEG: 29, best_predict: 0 with acc 0.75 at 2023-04-07 21:36:10\n",
      "exp_596v38c2_price DCE.a2309: POS: 23, NEG: 77, best_predict: 0 with acc 0.775 at 2023-04-07 21:36:13\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 28, NEG: 72, best_predict: 0 with acc 0.81429 at 2023-04-07 21:36:15\n",
      "exp_r4c7esy9 DCE.m2309: POS: 31, NEG: 69, best_predict: 0 with acc 0.75 at 2023-04-07 21:36:17\n",
      "exp_596v38c2_price DCE.m2309: POS: 64, NEG: 36, best_predict: 1 with acc 0.775 at 2023-04-07 21:36:19\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 62, NEG: 38, best_predict: 0 with acc 0.81429 at 2023-04-07 21:36:21\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 61, NEG: 39, best_predict: 0 with acc 0.75 at 2023-04-07 21:36:23\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 41, NEG: 59, best_predict: 0 with acc 0.775 at 2023-04-07 21:36:25\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 83, NEG: 17, best_predict: 1 with acc 0.81429 at 2023-04-07 21:36:27\n",
      "exp_r4c7esy9 DCE.i2309: POS: 87, NEG: 13, best_predict: 1 with acc 0.75 at 2023-04-07 21:36:30\n",
      "exp_596v38c2_price DCE.i2309: POS: 56, NEG: 44, best_predict: 1 with acc 0.775 at 2023-04-07 21:36:32\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 82, NEG: 18, best_predict: 1 with acc 0.81429 at 2023-04-07 21:36:34\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 92, NEG: 8, best_predict: 1 with acc 0.75 at 2023-04-07 21:36:36\n",
      "exp_596v38c2_price DCE.jm2309: POS: 44, NEG: 56, best_predict: 1 with acc 0.775 at 2023-04-07 21:36:38\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 83, NEG: 17, best_predict: 0 with acc 0.81429 at 2023-04-07 21:36:40\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 68, NEG: 32, best_predict: 0 with acc 0.75 at 2023-04-07 21:36:43\n",
      "exp_596v38c2_price CZCE.SR309: POS: 90, NEG: 10, best_predict: 1 with acc 0.775 at 2023-04-07 21:36:45\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 26, NEG: 74, best_predict: 0 with acc 0.81429 at 2023-04-07 21:36:47\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 33, NEG: 67, best_predict: 0 with acc 0.75 at 2023-04-07 21:36:49\n",
      "exp_596v38c2_price CZCE.CF309: POS: 37, NEG: 63, best_predict: 0 with acc 0.775 at 2023-04-07 21:36:51\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 95, NEG: 5, best_predict: 1 with acc 0.81429 at 2023-04-07 21:36:53\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 98, NEG: 2, best_predict: 1 with acc 0.75 at 2023-04-07 21:36:55\n",
      "exp_596v38c2_price CZCE.SA309: POS: 19, NEG: 81, best_predict: 1 with acc 0.775 at 2023-04-07 21:36:57\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 45, NEG: 55, best_predict: 0 with acc 0.81429 at 2023-04-07 21:36:59\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 56, NEG: 44, best_predict: 0 with acc 0.75 at 2023-04-07 21:37:02\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 41, NEG: 59, best_predict: 0 with acc 0.775 at 2023-04-07 21:37:04\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 83, NEG: 17, best_predict: 1 with acc 0.81429 at 2023-04-07 21:38:46\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 90, NEG: 10, best_predict: 1 with acc 0.75 at 2023-04-07 21:38:48\n",
      "exp_596v38c2_price CZCE.MA309: POS: 42, NEG: 58, best_predict: 0 with acc 0.775 at 2023-04-07 21:38:50\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 81, NEG: 19, best_predict: 1 with acc 0.81429 at 2023-04-07 21:38:52\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 89, NEG: 11, best_predict: 1 with acc 0.75 at 2023-04-07 21:38:55\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 27, NEG: 73, best_predict: 1 with acc 0.775 at 2023-04-07 21:38:57\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 63, NEG: 37, best_predict: 1 with acc 0.81429 at 2023-04-07 21:38:59\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 53, NEG: 47, best_predict: 1 with acc 0.75 at 2023-04-07 21:39:02\n",
      "exp_596v38c2_price CZCE.TA309: POS: 61, NEG: 39, best_predict: 1 with acc 0.775 at 2023-04-07 21:39:04\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 86, NEG: 14, best_predict: 1 with acc 0.81429 at 2023-04-07 21:39:06\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 21:39:08\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 80, NEG: 20, best_predict: 1 with acc 0.775 at 2023-04-07 21:39:10\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 50, NEG: 50, best_predict: 0 with acc 0.81429 at 2023-04-07 21:39:13\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 42, NEG: 58, best_predict: 0 with acc 0.75 at 2023-04-07 21:39:15\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 62, NEG: 38, best_predict: 0 with acc 0.775 at 2023-04-07 21:39:17\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 52, NEG: 48, best_predict: 1 with acc 0.81429 at 2023-04-07 21:39:19\n",
      "exp_r4c7esy9 DCE.p2309: POS: 59, NEG: 41, best_predict: 1 with acc 0.75 at 2023-04-07 21:39:21\n",
      "exp_596v38c2_price DCE.p2309: POS: 48, NEG: 52, best_predict: 0 with acc 0.775 at 2023-04-07 21:39:24\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 34, NEG: 66, best_predict: 1 with acc 0.81429 at 2023-04-07 21:39:26\n",
      "exp_r4c7esy9 DCE.y2309: POS: 44, NEG: 56, best_predict: 0 with acc 0.75 at 2023-04-07 21:39:28\n",
      "exp_596v38c2_price DCE.y2309: POS: 53, NEG: 47, best_predict: 0 with acc 0.775 at 2023-04-07 21:39:31\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 36, NEG: 64, best_predict: 1 with acc 0.81429 at 2023-04-07 21:39:33\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 34, NEG: 66, best_predict: 0 with acc 0.75 at 2023-04-07 21:39:35\n",
      "exp_596v38c2_price CZCE.OI309: POS: 69, NEG: 31, best_predict: 0 with acc 0.775 at 2023-04-07 21:39:37\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 62, NEG: 38, best_predict: 1 with acc 0.81429 at 2023-04-07 21:39:39\n",
      "exp_r4c7esy9 DCE.a2309: POS: 71, NEG: 29, best_predict: 0 with acc 0.75 at 2023-04-07 21:39:41\n",
      "exp_596v38c2_price DCE.a2309: POS: 23, NEG: 77, best_predict: 0 with acc 0.775 at 2023-04-07 21:39:43\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 28, NEG: 72, best_predict: 0 with acc 0.81429 at 2023-04-07 21:39:45\n",
      "exp_r4c7esy9 DCE.m2309: POS: 31, NEG: 69, best_predict: 0 with acc 0.75 at 2023-04-07 21:39:48\n",
      "exp_596v38c2_price DCE.m2309: POS: 64, NEG: 36, best_predict: 1 with acc 0.775 at 2023-04-07 21:39:50\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 62, NEG: 38, best_predict: 0 with acc 0.81429 at 2023-04-07 21:39:52\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 60, NEG: 40, best_predict: 0 with acc 0.75 at 2023-04-07 21:39:54\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 41, NEG: 59, best_predict: 0 with acc 0.775 at 2023-04-07 21:39:56\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 83, NEG: 17, best_predict: 1 with acc 0.81429 at 2023-04-07 21:39:58\n",
      "exp_r4c7esy9 DCE.i2309: POS: 87, NEG: 13, best_predict: 1 with acc 0.75 at 2023-04-07 21:40:00\n",
      "exp_596v38c2_price DCE.i2309: POS: 56, NEG: 44, best_predict: 1 with acc 0.775 at 2023-04-07 21:40:02\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 82, NEG: 18, best_predict: 1 with acc 0.81429 at 2023-04-07 21:40:04\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 92, NEG: 8, best_predict: 1 with acc 0.75 at 2023-04-07 21:40:06\n",
      "exp_596v38c2_price DCE.jm2309: POS: 44, NEG: 56, best_predict: 1 with acc 0.775 at 2023-04-07 21:40:09\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 83, NEG: 17, best_predict: 0 with acc 0.81429 at 2023-04-07 21:40:11\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 68, NEG: 32, best_predict: 0 with acc 0.75 at 2023-04-07 21:40:13\n",
      "exp_596v38c2_price CZCE.SR309: POS: 90, NEG: 10, best_predict: 1 with acc 0.775 at 2023-04-07 21:40:15\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 26, NEG: 74, best_predict: 0 with acc 0.81429 at 2023-04-07 21:40:17\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 33, NEG: 67, best_predict: 0 with acc 0.75 at 2023-04-07 21:40:19\n",
      "exp_596v38c2_price CZCE.CF309: POS: 37, NEG: 63, best_predict: 0 with acc 0.775 at 2023-04-07 21:40:21\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 95, NEG: 5, best_predict: 1 with acc 0.81429 at 2023-04-07 21:40:24\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 98, NEG: 2, best_predict: 1 with acc 0.75 at 2023-04-07 21:40:26\n",
      "exp_596v38c2_price CZCE.SA309: POS: 19, NEG: 81, best_predict: 1 with acc 0.775 at 2023-04-07 21:40:28\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 45, NEG: 55, best_predict: 0 with acc 0.81429 at 2023-04-07 21:40:30\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 56, NEG: 44, best_predict: 0 with acc 0.75 at 2023-04-07 21:40:32\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 41, NEG: 59, best_predict: 0 with acc 0.775 at 2023-04-07 21:40:34\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 87, NEG: 13, best_predict: 1 with acc 0.81429 at 2023-04-07 21:42:16\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 93, NEG: 7, best_predict: 1 with acc 0.75 at 2023-04-07 21:42:19\n",
      "exp_596v38c2_price CZCE.MA309: POS: 37, NEG: 63, best_predict: 0 with acc 0.775 at 2023-04-07 21:42:21\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 85, NEG: 15, best_predict: 1 with acc 0.81429 at 2023-04-07 21:42:23\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 21:42:25\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 26, NEG: 74, best_predict: 1 with acc 0.775 at 2023-04-07 21:42:27\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 72, NEG: 28, best_predict: 1 with acc 0.81429 at 2023-04-07 21:42:29\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 68, NEG: 32, best_predict: 1 with acc 0.75 at 2023-04-07 21:42:31\n",
      "exp_596v38c2_price CZCE.TA309: POS: 70, NEG: 30, best_predict: 1 with acc 0.775 at 2023-04-07 21:42:33\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 87, NEG: 13, best_predict: 1 with acc 0.81429 at 2023-04-07 21:42:35\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 96, NEG: 4, best_predict: 1 with acc 0.75 at 2023-04-07 21:42:38\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 81, NEG: 19, best_predict: 1 with acc 0.775 at 2023-04-07 21:42:40\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 44, NEG: 56, best_predict: 0 with acc 0.81429 at 2023-04-07 21:42:42\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 34, NEG: 66, best_predict: 0 with acc 0.75 at 2023-04-07 21:42:44\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 59, NEG: 41, best_predict: 0 with acc 0.775 at 2023-04-07 21:42:46\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 51, NEG: 49, best_predict: 1 with acc 0.81429 at 2023-04-07 21:42:48\n",
      "exp_r4c7esy9 DCE.p2309: POS: 63, NEG: 37, best_predict: 1 with acc 0.75 at 2023-04-07 21:42:50\n",
      "exp_596v38c2_price DCE.p2309: POS: 55, NEG: 45, best_predict: 0 with acc 0.775 at 2023-04-07 21:42:53\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 41, NEG: 59, best_predict: 1 with acc 0.81429 at 2023-04-07 21:42:55\n",
      "exp_r4c7esy9 DCE.y2309: POS: 56, NEG: 44, best_predict: 1 with acc 0.75 at 2023-04-07 21:42:57\n",
      "exp_596v38c2_price DCE.y2309: POS: 57, NEG: 43, best_predict: 0 with acc 0.775 at 2023-04-07 21:42:59\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 47, NEG: 53, best_predict: 1 with acc 0.81429 at 2023-04-07 21:43:01\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 44, NEG: 56, best_predict: 1 with acc 0.75 at 2023-04-07 21:43:03\n",
      "exp_596v38c2_price CZCE.OI309: POS: 69, NEG: 31, best_predict: 0 with acc 0.775 at 2023-04-07 21:43:05\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 71, NEG: 29, best_predict: 1 with acc 0.81429 at 2023-04-07 21:43:07\n",
      "exp_r4c7esy9 DCE.a2309: POS: 67, NEG: 33, best_predict: 0 with acc 0.75 at 2023-04-07 21:43:10\n",
      "exp_596v38c2_price DCE.a2309: POS: 27, NEG: 73, best_predict: 0 with acc 0.775 at 2023-04-07 21:43:12\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 39, NEG: 61, best_predict: 0 with acc 0.81429 at 2023-04-07 21:43:14\n",
      "exp_r4c7esy9 DCE.m2309: POS: 34, NEG: 66, best_predict: 0 with acc 0.75 at 2023-04-07 21:43:16\n",
      "exp_596v38c2_price DCE.m2309: POS: 61, NEG: 39, best_predict: 1 with acc 0.775 at 2023-04-07 21:43:18\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 61, NEG: 39, best_predict: 0 with acc 0.81429 at 2023-04-07 21:43:20\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 61, NEG: 39, best_predict: 0 with acc 0.75 at 2023-04-07 21:43:22\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 35, NEG: 65, best_predict: 0 with acc 0.775 at 2023-04-07 21:43:24\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 81, NEG: 19, best_predict: 1 with acc 0.81429 at 2023-04-07 21:43:27\n",
      "exp_r4c7esy9 DCE.i2309: POS: 89, NEG: 11, best_predict: 1 with acc 0.75 at 2023-04-07 21:43:29\n",
      "exp_596v38c2_price DCE.i2309: POS: 58, NEG: 42, best_predict: 1 with acc 0.775 at 2023-04-07 21:43:31\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 85, NEG: 15, best_predict: 1 with acc 0.81429 at 2023-04-07 21:43:33\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 95, NEG: 5, best_predict: 1 with acc 0.75 at 2023-04-07 21:43:36\n",
      "exp_596v38c2_price DCE.jm2309: POS: 56, NEG: 44, best_predict: 0 with acc 0.775 at 2023-04-07 21:43:38\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 85, NEG: 15, best_predict: 0 with acc 0.81429 at 2023-04-07 21:43:40\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 78, NEG: 22, best_predict: 1 with acc 0.75 at 2023-04-07 21:43:43\n",
      "exp_596v38c2_price CZCE.SR309: POS: 91, NEG: 9, best_predict: 1 with acc 0.775 at 2023-04-07 21:43:45\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 26, NEG: 74, best_predict: 0 with acc 0.81429 at 2023-04-07 21:43:47\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 35, NEG: 65, best_predict: 0 with acc 0.75 at 2023-04-07 21:43:49\n",
      "exp_596v38c2_price CZCE.CF309: POS: 28, NEG: 72, best_predict: 0 with acc 0.775 at 2023-04-07 21:43:52\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 95, NEG: 5, best_predict: 1 with acc 0.81429 at 2023-04-07 21:43:54\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 98, NEG: 2, best_predict: 1 with acc 0.75 at 2023-04-07 21:43:56\n",
      "exp_596v38c2_price CZCE.SA309: POS: 20, NEG: 80, best_predict: 1 with acc 0.775 at 2023-04-07 21:43:58\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 38, NEG: 62, best_predict: 0 with acc 0.81429 at 2023-04-07 21:44:00\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 51, NEG: 49, best_predict: 0 with acc 0.75 at 2023-04-07 21:44:02\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 40, NEG: 60, best_predict: 1 with acc 0.775 at 2023-04-07 21:44:05\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 90, NEG: 10, best_predict: 1 with acc 0.81429 at 2023-04-07 21:45:47\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 97, NEG: 3, best_predict: 1 with acc 0.75 at 2023-04-07 21:45:49\n",
      "exp_596v38c2_price CZCE.MA309: POS: 37, NEG: 63, best_predict: 0 with acc 0.775 at 2023-04-07 21:45:51\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 84, NEG: 16, best_predict: 1 with acc 0.81429 at 2023-04-07 21:45:53\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 93, NEG: 7, best_predict: 1 with acc 0.75 at 2023-04-07 21:45:55\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 26, NEG: 74, best_predict: 1 with acc 0.775 at 2023-04-07 21:45:57\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 69, NEG: 31, best_predict: 1 with acc 0.81429 at 2023-04-07 21:46:00\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 69, NEG: 31, best_predict: 1 with acc 0.75 at 2023-04-07 21:46:02\n",
      "exp_596v38c2_price CZCE.TA309: POS: 70, NEG: 30, best_predict: 1 with acc 0.775 at 2023-04-07 21:46:04\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 90, NEG: 10, best_predict: 1 with acc 0.81429 at 2023-04-07 21:46:06\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 96, NEG: 4, best_predict: 1 with acc 0.75 at 2023-04-07 21:46:08\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 83, NEG: 17, best_predict: 1 with acc 0.775 at 2023-04-07 21:46:10\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 46, NEG: 54, best_predict: 0 with acc 0.81429 at 2023-04-07 21:46:12\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 36, NEG: 64, best_predict: 0 with acc 0.75 at 2023-04-07 21:46:15\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 58, NEG: 42, best_predict: 0 with acc 0.775 at 2023-04-07 21:46:17\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 52, NEG: 48, best_predict: 1 with acc 0.81429 at 2023-04-07 21:46:19\n",
      "exp_r4c7esy9 DCE.p2309: POS: 59, NEG: 41, best_predict: 1 with acc 0.75 at 2023-04-07 21:46:21\n",
      "exp_596v38c2_price DCE.p2309: POS: 54, NEG: 46, best_predict: 0 with acc 0.775 at 2023-04-07 21:46:24\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 38, NEG: 62, best_predict: 1 with acc 0.81429 at 2023-04-07 21:46:26\n",
      "exp_r4c7esy9 DCE.y2309: POS: 51, NEG: 49, best_predict: 0 with acc 0.75 at 2023-04-07 21:46:28\n",
      "exp_596v38c2_price DCE.y2309: POS: 58, NEG: 42, best_predict: 0 with acc 0.775 at 2023-04-07 21:46:30\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 36, NEG: 64, best_predict: 1 with acc 0.81429 at 2023-04-07 21:46:32\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 42, NEG: 58, best_predict: 0 with acc 0.75 at 2023-04-07 21:46:34\n",
      "exp_596v38c2_price CZCE.OI309: POS: 69, NEG: 31, best_predict: 0 with acc 0.775 at 2023-04-07 21:46:36\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 72, NEG: 28, best_predict: 1 with acc 0.81429 at 2023-04-07 21:46:38\n",
      "exp_r4c7esy9 DCE.a2309: POS: 62, NEG: 38, best_predict: 1 with acc 0.75 at 2023-04-07 21:46:40\n",
      "exp_596v38c2_price DCE.a2309: POS: 24, NEG: 76, best_predict: 0 with acc 0.775 at 2023-04-07 21:46:42\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 37, NEG: 63, best_predict: 0 with acc 0.81429 at 2023-04-07 21:46:44\n",
      "exp_r4c7esy9 DCE.m2309: POS: 31, NEG: 69, best_predict: 0 with acc 0.75 at 2023-04-07 21:46:46\n",
      "exp_596v38c2_price DCE.m2309: POS: 62, NEG: 38, best_predict: 1 with acc 0.775 at 2023-04-07 21:46:49\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 59, NEG: 41, best_predict: 0 with acc 0.81429 at 2023-04-07 21:46:51\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 56, NEG: 44, best_predict: 0 with acc 0.75 at 2023-04-07 21:46:53\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 34, NEG: 66, best_predict: 0 with acc 0.775 at 2023-04-07 21:46:55\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 83, NEG: 17, best_predict: 1 with acc 0.81429 at 2023-04-07 21:46:58\n",
      "exp_r4c7esy9 DCE.i2309: POS: 89, NEG: 11, best_predict: 1 with acc 0.75 at 2023-04-07 21:47:00\n",
      "exp_596v38c2_price DCE.i2309: POS: 57, NEG: 43, best_predict: 1 with acc 0.775 at 2023-04-07 21:47:02\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 86, NEG: 14, best_predict: 1 with acc 0.81429 at 2023-04-07 21:47:04\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 96, NEG: 4, best_predict: 1 with acc 0.75 at 2023-04-07 21:47:06\n",
      "exp_596v38c2_price DCE.jm2309: POS: 53, NEG: 47, best_predict: 0 with acc 0.775 at 2023-04-07 21:47:08\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 85, NEG: 15, best_predict: 0 with acc 0.81429 at 2023-04-07 21:47:10\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 78, NEG: 22, best_predict: 0 with acc 0.75 at 2023-04-07 21:47:12\n",
      "exp_596v38c2_price CZCE.SR309: POS: 92, NEG: 8, best_predict: 1 with acc 0.775 at 2023-04-07 21:47:14\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 25, NEG: 75, best_predict: 0 with acc 0.81429 at 2023-04-07 21:47:17\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 25, NEG: 75, best_predict: 0 with acc 0.75 at 2023-04-07 21:47:19\n",
      "exp_596v38c2_price CZCE.CF309: POS: 21, NEG: 79, best_predict: 0 with acc 0.775 at 2023-04-07 21:47:21\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 95, NEG: 5, best_predict: 1 with acc 0.81429 at 2023-04-07 21:47:23\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 98, NEG: 2, best_predict: 1 with acc 0.75 at 2023-04-07 21:47:25\n",
      "exp_596v38c2_price CZCE.SA309: POS: 22, NEG: 78, best_predict: 1 with acc 0.775 at 2023-04-07 21:47:27\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 38, NEG: 62, best_predict: 0 with acc 0.81429 at 2023-04-07 21:47:29\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 53, NEG: 47, best_predict: 0 with acc 0.75 at 2023-04-07 21:47:31\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 40, NEG: 60, best_predict: 1 with acc 0.775 at 2023-04-07 21:47:33\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 90, NEG: 10, best_predict: 1 with acc 0.81429 at 2023-04-07 21:49:15\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 97, NEG: 3, best_predict: 1 with acc 0.75 at 2023-04-07 21:49:17\n",
      "exp_596v38c2_price CZCE.MA309: POS: 37, NEG: 63, best_predict: 0 with acc 0.775 at 2023-04-07 21:49:20\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 84, NEG: 16, best_predict: 1 with acc 0.81429 at 2023-04-07 21:49:22\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 93, NEG: 7, best_predict: 1 with acc 0.75 at 2023-04-07 21:49:24\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 26, NEG: 74, best_predict: 1 with acc 0.775 at 2023-04-07 21:49:26\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 69, NEG: 31, best_predict: 1 with acc 0.81429 at 2023-04-07 21:49:28\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 69, NEG: 31, best_predict: 1 with acc 0.75 at 2023-04-07 21:49:30\n",
      "exp_596v38c2_price CZCE.TA309: POS: 70, NEG: 30, best_predict: 1 with acc 0.775 at 2023-04-07 21:49:32\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 90, NEG: 10, best_predict: 1 with acc 0.81429 at 2023-04-07 21:49:34\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 96, NEG: 4, best_predict: 1 with acc 0.75 at 2023-04-07 21:49:36\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 83, NEG: 17, best_predict: 1 with acc 0.775 at 2023-04-07 21:49:38\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 46, NEG: 54, best_predict: 0 with acc 0.81429 at 2023-04-07 21:49:40\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 36, NEG: 64, best_predict: 0 with acc 0.75 at 2023-04-07 21:49:42\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 58, NEG: 42, best_predict: 0 with acc 0.775 at 2023-04-07 21:49:44\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 52, NEG: 48, best_predict: 1 with acc 0.81429 at 2023-04-07 21:49:46\n",
      "exp_r4c7esy9 DCE.p2309: POS: 59, NEG: 41, best_predict: 1 with acc 0.75 at 2023-04-07 21:49:48\n",
      "exp_596v38c2_price DCE.p2309: POS: 54, NEG: 46, best_predict: 0 with acc 0.775 at 2023-04-07 21:49:50\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 38, NEG: 62, best_predict: 1 with acc 0.81429 at 2023-04-07 21:49:53\n",
      "exp_r4c7esy9 DCE.y2309: POS: 51, NEG: 49, best_predict: 0 with acc 0.75 at 2023-04-07 21:49:55\n",
      "exp_596v38c2_price DCE.y2309: POS: 58, NEG: 42, best_predict: 0 with acc 0.775 at 2023-04-07 21:49:57\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 36, NEG: 64, best_predict: 1 with acc 0.81429 at 2023-04-07 21:49:59\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 42, NEG: 58, best_predict: 0 with acc 0.75 at 2023-04-07 21:50:01\n",
      "exp_596v38c2_price CZCE.OI309: POS: 69, NEG: 31, best_predict: 0 with acc 0.775 at 2023-04-07 21:50:03\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 72, NEG: 28, best_predict: 1 with acc 0.81429 at 2023-04-07 21:50:05\n",
      "exp_r4c7esy9 DCE.a2309: POS: 62, NEG: 38, best_predict: 1 with acc 0.75 at 2023-04-07 21:50:07\n",
      "exp_596v38c2_price DCE.a2309: POS: 24, NEG: 76, best_predict: 0 with acc 0.775 at 2023-04-07 21:50:09\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 37, NEG: 63, best_predict: 0 with acc 0.81429 at 2023-04-07 21:50:11\n",
      "exp_r4c7esy9 DCE.m2309: POS: 31, NEG: 69, best_predict: 0 with acc 0.75 at 2023-04-07 21:50:13\n",
      "exp_596v38c2_price DCE.m2309: POS: 62, NEG: 38, best_predict: 1 with acc 0.775 at 2023-04-07 21:50:15\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 59, NEG: 41, best_predict: 0 with acc 0.81429 at 2023-04-07 21:50:18\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 56, NEG: 44, best_predict: 0 with acc 0.75 at 2023-04-07 21:50:20\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 34, NEG: 66, best_predict: 0 with acc 0.775 at 2023-04-07 21:50:22\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 83, NEG: 17, best_predict: 1 with acc 0.81429 at 2023-04-07 21:50:24\n",
      "exp_r4c7esy9 DCE.i2309: POS: 89, NEG: 11, best_predict: 1 with acc 0.75 at 2023-04-07 21:50:26\n",
      "exp_596v38c2_price DCE.i2309: POS: 57, NEG: 43, best_predict: 1 with acc 0.775 at 2023-04-07 21:50:28\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 86, NEG: 14, best_predict: 1 with acc 0.81429 at 2023-04-07 21:50:30\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 96, NEG: 4, best_predict: 1 with acc 0.75 at 2023-04-07 21:50:32\n",
      "exp_596v38c2_price DCE.jm2309: POS: 53, NEG: 47, best_predict: 0 with acc 0.775 at 2023-04-07 21:50:34\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 85, NEG: 15, best_predict: 0 with acc 0.81429 at 2023-04-07 21:50:36\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 78, NEG: 22, best_predict: 0 with acc 0.75 at 2023-04-07 21:50:39\n",
      "exp_596v38c2_price CZCE.SR309: POS: 92, NEG: 8, best_predict: 1 with acc 0.775 at 2023-04-07 21:50:41\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 25, NEG: 75, best_predict: 0 with acc 0.81429 at 2023-04-07 21:50:43\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 25, NEG: 75, best_predict: 0 with acc 0.75 at 2023-04-07 21:50:46\n",
      "exp_596v38c2_price CZCE.CF309: POS: 21, NEG: 79, best_predict: 0 with acc 0.775 at 2023-04-07 21:50:48\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 95, NEG: 5, best_predict: 1 with acc 0.81429 at 2023-04-07 21:50:50\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 98, NEG: 2, best_predict: 1 with acc 0.75 at 2023-04-07 21:50:52\n",
      "exp_596v38c2_price CZCE.SA309: POS: 22, NEG: 78, best_predict: 1 with acc 0.775 at 2023-04-07 21:50:54\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 38, NEG: 62, best_predict: 0 with acc 0.81429 at 2023-04-07 21:50:57\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 53, NEG: 47, best_predict: 0 with acc 0.75 at 2023-04-07 21:50:59\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 40, NEG: 60, best_predict: 1 with acc 0.775 at 2023-04-07 21:51:01\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 92, NEG: 8, best_predict: 1 with acc 0.81429 at 2023-04-07 21:52:43\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 99, NEG: 1, best_predict: 1 with acc 0.75 at 2023-04-07 21:52:45\n",
      "exp_596v38c2_price CZCE.MA309: POS: 31, NEG: 69, best_predict: 0 with acc 0.775 at 2023-04-07 21:52:47\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 86, NEG: 14, best_predict: 1 with acc 0.81429 at 2023-04-07 21:52:49\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 21:52:52\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 29, NEG: 71, best_predict: 1 with acc 0.775 at 2023-04-07 21:52:54\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 71, NEG: 29, best_predict: 1 with acc 0.81429 at 2023-04-07 21:52:56\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 77, NEG: 23, best_predict: 1 with acc 0.75 at 2023-04-07 21:52:58\n",
      "exp_596v38c2_price CZCE.TA309: POS: 75, NEG: 25, best_predict: 1 with acc 0.775 at 2023-04-07 21:53:00\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 89, NEG: 11, best_predict: 1 with acc 0.81429 at 2023-04-07 21:53:02\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 21:53:04\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 79, NEG: 21, best_predict: 1 with acc 0.775 at 2023-04-07 21:53:06\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 50, NEG: 50, best_predict: 0 with acc 0.81429 at 2023-04-07 21:53:08\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 40, NEG: 60, best_predict: 0 with acc 0.75 at 2023-04-07 21:53:10\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 48, NEG: 52, best_predict: 0 with acc 0.775 at 2023-04-07 21:53:12\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 49, NEG: 51, best_predict: 1 with acc 0.81429 at 2023-04-07 21:53:14\n",
      "exp_r4c7esy9 DCE.p2309: POS: 67, NEG: 33, best_predict: 1 with acc 0.75 at 2023-04-07 21:53:16\n",
      "exp_596v38c2_price DCE.p2309: POS: 58, NEG: 42, best_predict: 0 with acc 0.775 at 2023-04-07 21:53:18\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 55, NEG: 45, best_predict: 1 with acc 0.81429 at 2023-04-07 21:53:20\n",
      "exp_r4c7esy9 DCE.y2309: POS: 66, NEG: 34, best_predict: 1 with acc 0.75 at 2023-04-07 21:53:23\n",
      "exp_596v38c2_price DCE.y2309: POS: 57, NEG: 43, best_predict: 0 with acc 0.775 at 2023-04-07 21:53:25\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 55, NEG: 45, best_predict: 1 with acc 0.81429 at 2023-04-07 21:53:27\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 54, NEG: 46, best_predict: 1 with acc 0.75 at 2023-04-07 21:53:29\n",
      "exp_596v38c2_price CZCE.OI309: POS: 75, NEG: 25, best_predict: 0 with acc 0.775 at 2023-04-07 21:53:31\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 65, NEG: 35, best_predict: 1 with acc 0.81429 at 2023-04-07 21:53:33\n",
      "exp_r4c7esy9 DCE.a2309: POS: 51, NEG: 49, best_predict: 1 with acc 0.75 at 2023-04-07 21:53:35\n",
      "exp_596v38c2_price DCE.a2309: POS: 30, NEG: 70, best_predict: 0 with acc 0.775 at 2023-04-07 21:53:37\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 48, NEG: 52, best_predict: 1 with acc 0.81429 at 2023-04-07 21:53:39\n",
      "exp_r4c7esy9 DCE.m2309: POS: 34, NEG: 66, best_predict: 0 with acc 0.75 at 2023-04-07 21:53:41\n",
      "exp_596v38c2_price DCE.m2309: POS: 44, NEG: 56, best_predict: 1 with acc 0.775 at 2023-04-07 21:53:44\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 53, NEG: 47, best_predict: 0 with acc 0.81429 at 2023-04-07 21:53:46\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 59, NEG: 41, best_predict: 0 with acc 0.75 at 2023-04-07 21:53:48\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 28, NEG: 72, best_predict: 0 with acc 0.775 at 2023-04-07 21:53:50\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 87, NEG: 13, best_predict: 1 with acc 0.81429 at 2023-04-07 21:53:52\n",
      "exp_r4c7esy9 DCE.i2309: POS: 89, NEG: 11, best_predict: 1 with acc 0.75 at 2023-04-07 21:53:54\n",
      "exp_596v38c2_price DCE.i2309: POS: 56, NEG: 44, best_predict: 1 with acc 0.775 at 2023-04-07 21:53:56\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 85, NEG: 15, best_predict: 1 with acc 0.81429 at 2023-04-07 21:53:58\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 96, NEG: 4, best_predict: 1 with acc 0.75 at 2023-04-07 21:54:00\n",
      "exp_596v38c2_price DCE.jm2309: POS: 54, NEG: 46, best_predict: 0 with acc 0.775 at 2023-04-07 21:54:02\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 85, NEG: 15, best_predict: 0 with acc 0.81429 at 2023-04-07 21:54:04\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 83, NEG: 17, best_predict: 1 with acc 0.75 at 2023-04-07 21:54:06\n",
      "exp_596v38c2_price CZCE.SR309: POS: 91, NEG: 9, best_predict: 1 with acc 0.775 at 2023-04-07 21:54:08\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 21, NEG: 79, best_predict: 0 with acc 0.81429 at 2023-04-07 21:54:10\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 22, NEG: 78, best_predict: 0 with acc 0.75 at 2023-04-07 21:54:12\n",
      "exp_596v38c2_price CZCE.CF309: POS: 13, NEG: 87, best_predict: 0 with acc 0.775 at 2023-04-07 21:54:14\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 95, NEG: 5, best_predict: 1 with acc 0.81429 at 2023-04-07 21:54:16\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 98, NEG: 2, best_predict: 1 with acc 0.75 at 2023-04-07 21:54:19\n",
      "exp_596v38c2_price CZCE.SA309: POS: 24, NEG: 76, best_predict: 1 with acc 0.775 at 2023-04-07 21:54:21\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 41, NEG: 59, best_predict: 0 with acc 0.81429 at 2023-04-07 21:54:23\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 51, NEG: 49, best_predict: 0 with acc 0.75 at 2023-04-07 21:54:25\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 34, NEG: 66, best_predict: 0 with acc 0.775 at 2023-04-07 21:54:27\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 92, NEG: 8, best_predict: 1 with acc 0.81429 at 2023-04-07 21:56:09\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 99, NEG: 1, best_predict: 1 with acc 0.75 at 2023-04-07 21:56:11\n",
      "exp_596v38c2_price CZCE.MA309: POS: 31, NEG: 69, best_predict: 0 with acc 0.775 at 2023-04-07 21:56:13\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 86, NEG: 14, best_predict: 1 with acc 0.81429 at 2023-04-07 21:56:15\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 21:56:18\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 29, NEG: 71, best_predict: 1 with acc 0.775 at 2023-04-07 21:56:20\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 71, NEG: 29, best_predict: 1 with acc 0.81429 at 2023-04-07 21:56:22\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 77, NEG: 23, best_predict: 1 with acc 0.75 at 2023-04-07 21:56:24\n",
      "exp_596v38c2_price CZCE.TA309: POS: 75, NEG: 25, best_predict: 1 with acc 0.775 at 2023-04-07 21:56:26\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 89, NEG: 11, best_predict: 1 with acc 0.81429 at 2023-04-07 21:56:28\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 21:56:30\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 79, NEG: 21, best_predict: 1 with acc 0.775 at 2023-04-07 21:56:32\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 50, NEG: 50, best_predict: 0 with acc 0.81429 at 2023-04-07 21:56:34\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 40, NEG: 60, best_predict: 0 with acc 0.75 at 2023-04-07 21:56:36\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 48, NEG: 52, best_predict: 0 with acc 0.775 at 2023-04-07 21:56:38\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 49, NEG: 51, best_predict: 1 with acc 0.81429 at 2023-04-07 21:56:40\n",
      "exp_r4c7esy9 DCE.p2309: POS: 67, NEG: 33, best_predict: 1 with acc 0.75 at 2023-04-07 21:56:42\n",
      "exp_596v38c2_price DCE.p2309: POS: 58, NEG: 42, best_predict: 0 with acc 0.775 at 2023-04-07 21:56:44\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 55, NEG: 45, best_predict: 1 with acc 0.81429 at 2023-04-07 21:56:46\n",
      "exp_r4c7esy9 DCE.y2309: POS: 66, NEG: 34, best_predict: 1 with acc 0.75 at 2023-04-07 21:56:48\n",
      "exp_596v38c2_price DCE.y2309: POS: 57, NEG: 43, best_predict: 0 with acc 0.775 at 2023-04-07 21:56:51\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 55, NEG: 45, best_predict: 1 with acc 0.81429 at 2023-04-07 21:56:53\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 54, NEG: 46, best_predict: 1 with acc 0.75 at 2023-04-07 21:56:55\n",
      "exp_596v38c2_price CZCE.OI309: POS: 75, NEG: 25, best_predict: 0 with acc 0.775 at 2023-04-07 21:56:57\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 65, NEG: 35, best_predict: 1 with acc 0.81429 at 2023-04-07 21:56:59\n",
      "exp_r4c7esy9 DCE.a2309: POS: 51, NEG: 49, best_predict: 1 with acc 0.75 at 2023-04-07 21:57:01\n",
      "exp_596v38c2_price DCE.a2309: POS: 30, NEG: 70, best_predict: 0 with acc 0.775 at 2023-04-07 21:57:03\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 48, NEG: 52, best_predict: 1 with acc 0.81429 at 2023-04-07 21:57:05\n",
      "exp_r4c7esy9 DCE.m2309: POS: 34, NEG: 66, best_predict: 0 with acc 0.75 at 2023-04-07 21:57:07\n",
      "exp_596v38c2_price DCE.m2309: POS: 44, NEG: 56, best_predict: 1 with acc 0.775 at 2023-04-07 21:57:09\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 53, NEG: 47, best_predict: 0 with acc 0.81429 at 2023-04-07 21:57:11\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 59, NEG: 41, best_predict: 0 with acc 0.75 at 2023-04-07 21:57:13\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 28, NEG: 72, best_predict: 0 with acc 0.775 at 2023-04-07 21:57:15\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 87, NEG: 13, best_predict: 1 with acc 0.81429 at 2023-04-07 21:57:17\n",
      "exp_r4c7esy9 DCE.i2309: POS: 89, NEG: 11, best_predict: 1 with acc 0.75 at 2023-04-07 21:57:19\n",
      "exp_596v38c2_price DCE.i2309: POS: 56, NEG: 44, best_predict: 1 with acc 0.775 at 2023-04-07 21:57:21\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 85, NEG: 15, best_predict: 1 with acc 0.81429 at 2023-04-07 21:57:23\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 96, NEG: 4, best_predict: 1 with acc 0.75 at 2023-04-07 21:57:25\n",
      "exp_596v38c2_price DCE.jm2309: POS: 53, NEG: 47, best_predict: 0 with acc 0.775 at 2023-04-07 21:57:27\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 85, NEG: 15, best_predict: 0 with acc 0.81429 at 2023-04-07 21:57:29\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 83, NEG: 17, best_predict: 1 with acc 0.75 at 2023-04-07 21:57:31\n",
      "exp_596v38c2_price CZCE.SR309: POS: 91, NEG: 9, best_predict: 1 with acc 0.775 at 2023-04-07 21:57:33\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 21, NEG: 79, best_predict: 0 with acc 0.81429 at 2023-04-07 21:57:35\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 22, NEG: 78, best_predict: 0 with acc 0.75 at 2023-04-07 21:57:37\n",
      "exp_596v38c2_price CZCE.CF309: POS: 13, NEG: 87, best_predict: 0 with acc 0.775 at 2023-04-07 21:57:39\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 95, NEG: 5, best_predict: 1 with acc 0.81429 at 2023-04-07 21:57:41\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 98, NEG: 2, best_predict: 1 with acc 0.75 at 2023-04-07 21:57:43\n",
      "exp_596v38c2_price CZCE.SA309: POS: 25, NEG: 75, best_predict: 1 with acc 0.775 at 2023-04-07 21:57:45\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 41, NEG: 59, best_predict: 0 with acc 0.81429 at 2023-04-07 21:57:47\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 51, NEG: 49, best_predict: 0 with acc 0.75 at 2023-04-07 21:57:49\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 34, NEG: 66, best_predict: 0 with acc 0.775 at 2023-04-07 21:57:51\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 92, NEG: 8, best_predict: 1 with acc 0.81429 at 2023-04-07 21:59:33\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 99, NEG: 1, best_predict: 1 with acc 0.75 at 2023-04-07 21:59:35\n",
      "exp_596v38c2_price CZCE.MA309: POS: 31, NEG: 69, best_predict: 0 with acc 0.775 at 2023-04-07 21:59:37\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 86, NEG: 14, best_predict: 1 with acc 0.81429 at 2023-04-07 21:59:39\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 21:59:41\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 29, NEG: 71, best_predict: 1 with acc 0.775 at 2023-04-07 21:59:43\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 71, NEG: 29, best_predict: 1 with acc 0.81429 at 2023-04-07 21:59:45\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 77, NEG: 23, best_predict: 1 with acc 0.75 at 2023-04-07 21:59:47\n",
      "exp_596v38c2_price CZCE.TA309: POS: 75, NEG: 25, best_predict: 1 with acc 0.775 at 2023-04-07 21:59:49\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 89, NEG: 11, best_predict: 1 with acc 0.81429 at 2023-04-07 21:59:51\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 21:59:53\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 79, NEG: 21, best_predict: 1 with acc 0.775 at 2023-04-07 21:59:55\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 50, NEG: 50, best_predict: 0 with acc 0.81429 at 2023-04-07 21:59:57\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 40, NEG: 60, best_predict: 0 with acc 0.75 at 2023-04-07 21:59:59\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 48, NEG: 52, best_predict: 0 with acc 0.775 at 2023-04-07 22:00:01\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 49, NEG: 51, best_predict: 1 with acc 0.81429 at 2023-04-07 22:00:03\n",
      "exp_r4c7esy9 DCE.p2309: POS: 67, NEG: 33, best_predict: 1 with acc 0.75 at 2023-04-07 22:00:05\n",
      "exp_596v38c2_price DCE.p2309: POS: 58, NEG: 42, best_predict: 0 with acc 0.775 at 2023-04-07 22:00:07\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 55, NEG: 45, best_predict: 1 with acc 0.81429 at 2023-04-07 22:00:09\n",
      "exp_r4c7esy9 DCE.y2309: POS: 66, NEG: 34, best_predict: 1 with acc 0.75 at 2023-04-07 22:00:11\n",
      "exp_596v38c2_price DCE.y2309: POS: 57, NEG: 43, best_predict: 0 with acc 0.775 at 2023-04-07 22:00:13\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 55, NEG: 45, best_predict: 1 with acc 0.81429 at 2023-04-07 22:00:15\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 54, NEG: 46, best_predict: 1 with acc 0.75 at 2023-04-07 22:00:18\n",
      "exp_596v38c2_price CZCE.OI309: POS: 75, NEG: 25, best_predict: 0 with acc 0.775 at 2023-04-07 22:00:20\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 65, NEG: 35, best_predict: 1 with acc 0.81429 at 2023-04-07 22:00:22\n",
      "exp_r4c7esy9 DCE.a2309: POS: 51, NEG: 49, best_predict: 1 with acc 0.75 at 2023-04-07 22:00:24\n",
      "exp_596v38c2_price DCE.a2309: POS: 30, NEG: 70, best_predict: 0 with acc 0.775 at 2023-04-07 22:00:26\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 48, NEG: 52, best_predict: 1 with acc 0.81429 at 2023-04-07 22:00:28\n",
      "exp_r4c7esy9 DCE.m2309: POS: 34, NEG: 66, best_predict: 0 with acc 0.75 at 2023-04-07 22:00:30\n",
      "exp_596v38c2_price DCE.m2309: POS: 44, NEG: 56, best_predict: 1 with acc 0.775 at 2023-04-07 22:00:32\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 53, NEG: 47, best_predict: 0 with acc 0.81429 at 2023-04-07 22:00:35\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 59, NEG: 41, best_predict: 0 with acc 0.75 at 2023-04-07 22:00:37\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 28, NEG: 72, best_predict: 0 with acc 0.775 at 2023-04-07 22:00:39\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 87, NEG: 13, best_predict: 1 with acc 0.81429 at 2023-04-07 22:00:41\n",
      "exp_r4c7esy9 DCE.i2309: POS: 89, NEG: 11, best_predict: 1 with acc 0.75 at 2023-04-07 22:00:43\n",
      "exp_596v38c2_price DCE.i2309: POS: 56, NEG: 44, best_predict: 1 with acc 0.775 at 2023-04-07 22:00:45\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 85, NEG: 15, best_predict: 1 with acc 0.81429 at 2023-04-07 22:00:47\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 96, NEG: 4, best_predict: 1 with acc 0.75 at 2023-04-07 22:00:49\n",
      "exp_596v38c2_price DCE.jm2309: POS: 54, NEG: 46, best_predict: 0 with acc 0.775 at 2023-04-07 22:00:51\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 85, NEG: 15, best_predict: 0 with acc 0.81429 at 2023-04-07 22:00:53\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 83, NEG: 17, best_predict: 1 with acc 0.75 at 2023-04-07 22:00:55\n",
      "exp_596v38c2_price CZCE.SR309: POS: 91, NEG: 9, best_predict: 1 with acc 0.775 at 2023-04-07 22:00:57\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 21, NEG: 79, best_predict: 0 with acc 0.81429 at 2023-04-07 22:00:59\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 22, NEG: 78, best_predict: 0 with acc 0.75 at 2023-04-07 22:01:01\n",
      "exp_596v38c2_price CZCE.CF309: POS: 13, NEG: 87, best_predict: 0 with acc 0.775 at 2023-04-07 22:01:03\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 95, NEG: 5, best_predict: 1 with acc 0.81429 at 2023-04-07 22:01:05\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 98, NEG: 2, best_predict: 1 with acc 0.75 at 2023-04-07 22:01:07\n",
      "exp_596v38c2_price CZCE.SA309: POS: 24, NEG: 76, best_predict: 1 with acc 0.775 at 2023-04-07 22:01:09\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 41, NEG: 59, best_predict: 0 with acc 0.81429 at 2023-04-07 22:01:11\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 51, NEG: 49, best_predict: 0 with acc 0.75 at 2023-04-07 22:01:13\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 34, NEG: 66, best_predict: 0 with acc 0.775 at 2023-04-07 22:01:16\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 93, NEG: 7, best_predict: 1 with acc 0.81429 at 2023-04-07 22:02:58\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 99, NEG: 1, best_predict: 1 with acc 0.75 at 2023-04-07 22:03:00\n",
      "exp_596v38c2_price CZCE.MA309: POS: 27, NEG: 73, best_predict: 0 with acc 0.775 at 2023-04-07 22:03:02\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 86, NEG: 14, best_predict: 1 with acc 0.81429 at 2023-04-07 22:03:04\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 22:03:06\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 40, NEG: 60, best_predict: 1 with acc 0.775 at 2023-04-07 22:03:08\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 76, NEG: 24, best_predict: 1 with acc 0.81429 at 2023-04-07 22:03:10\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 90, NEG: 10, best_predict: 1 with acc 0.75 at 2023-04-07 22:03:12\n",
      "exp_596v38c2_price CZCE.TA309: POS: 80, NEG: 20, best_predict: 1 with acc 0.775 at 2023-04-07 22:03:14\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 87, NEG: 13, best_predict: 1 with acc 0.81429 at 2023-04-07 22:03:16\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 95, NEG: 5, best_predict: 1 with acc 0.75 at 2023-04-07 22:03:18\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 80, NEG: 20, best_predict: 1 with acc 0.775 at 2023-04-07 22:03:20\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 49, NEG: 51, best_predict: 0 with acc 0.81429 at 2023-04-07 22:03:22\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 40, NEG: 60, best_predict: 0 with acc 0.75 at 2023-04-07 22:03:24\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 38, NEG: 62, best_predict: 0 with acc 0.775 at 2023-04-07 22:03:26\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 49, NEG: 51, best_predict: 1 with acc 0.81429 at 2023-04-07 22:03:28\n",
      "exp_r4c7esy9 DCE.p2309: POS: 65, NEG: 35, best_predict: 1 with acc 0.75 at 2023-04-07 22:03:30\n",
      "exp_596v38c2_price DCE.p2309: POS: 64, NEG: 36, best_predict: 0 with acc 0.775 at 2023-04-07 22:03:32\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 66, NEG: 34, best_predict: 1 with acc 0.81429 at 2023-04-07 22:03:34\n",
      "exp_r4c7esy9 DCE.y2309: POS: 75, NEG: 25, best_predict: 1 with acc 0.75 at 2023-04-07 22:03:37\n",
      "exp_596v38c2_price DCE.y2309: POS: 56, NEG: 44, best_predict: 0 with acc 0.775 at 2023-04-07 22:03:39\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 58, NEG: 42, best_predict: 1 with acc 0.81429 at 2023-04-07 22:03:41\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 66, NEG: 34, best_predict: 1 with acc 0.75 at 2023-04-07 22:03:43\n",
      "exp_596v38c2_price CZCE.OI309: POS: 79, NEG: 21, best_predict: 0 with acc 0.775 at 2023-04-07 22:03:45\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 63, NEG: 37, best_predict: 1 with acc 0.81429 at 2023-04-07 22:03:47\n",
      "exp_r4c7esy9 DCE.a2309: POS: 44, NEG: 56, best_predict: 1 with acc 0.75 at 2023-04-07 22:03:49\n",
      "exp_596v38c2_price DCE.a2309: POS: 39, NEG: 61, best_predict: 0 with acc 0.775 at 2023-04-07 22:03:51\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 48, NEG: 52, best_predict: 1 with acc 0.81429 at 2023-04-07 22:03:53\n",
      "exp_r4c7esy9 DCE.m2309: POS: 32, NEG: 68, best_predict: 0 with acc 0.75 at 2023-04-07 22:03:55\n",
      "exp_596v38c2_price DCE.m2309: POS: 30, NEG: 70, best_predict: 1 with acc 0.775 at 2023-04-07 22:03:57\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 56, NEG: 44, best_predict: 0 with acc 0.81429 at 2023-04-07 22:03:59\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 63, NEG: 37, best_predict: 0 with acc 0.75 at 2023-04-07 22:04:01\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 27, NEG: 73, best_predict: 0 with acc 0.775 at 2023-04-07 22:04:03\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 89, NEG: 11, best_predict: 1 with acc 0.81429 at 2023-04-07 22:04:05\n",
      "exp_r4c7esy9 DCE.i2309: POS: 93, NEG: 7, best_predict: 1 with acc 0.75 at 2023-04-07 22:04:07\n",
      "exp_596v38c2_price DCE.i2309: POS: 58, NEG: 42, best_predict: 1 with acc 0.775 at 2023-04-07 22:04:09\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 87, NEG: 13, best_predict: 1 with acc 0.81429 at 2023-04-07 22:04:11\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 97, NEG: 3, best_predict: 1 with acc 0.75 at 2023-04-07 22:04:13\n",
      "exp_596v38c2_price DCE.jm2309: POS: 55, NEG: 45, best_predict: 0 with acc 0.775 at 2023-04-07 22:04:16\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 86, NEG: 14, best_predict: 0 with acc 0.81429 at 2023-04-07 22:04:18\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 86, NEG: 14, best_predict: 1 with acc 0.75 at 2023-04-07 22:04:20\n",
      "exp_596v38c2_price CZCE.SR309: POS: 92, NEG: 8, best_predict: 1 with acc 0.775 at 2023-04-07 22:04:22\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 23, NEG: 77, best_predict: 0 with acc 0.81429 at 2023-04-07 22:04:24\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 25, NEG: 75, best_predict: 0 with acc 0.75 at 2023-04-07 22:04:26\n",
      "exp_596v38c2_price CZCE.CF309: POS: 10, NEG: 90, best_predict: 0 with acc 0.775 at 2023-04-07 22:04:28\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 96, NEG: 4, best_predict: 1 with acc 0.81429 at 2023-04-07 22:04:30\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 98, NEG: 2, best_predict: 1 with acc 0.75 at 2023-04-07 22:04:32\n",
      "exp_596v38c2_price CZCE.SA309: POS: 27, NEG: 73, best_predict: 1 with acc 0.775 at 2023-04-07 22:04:34\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 44, NEG: 56, best_predict: 1 with acc 0.81429 at 2023-04-07 22:04:36\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 49, NEG: 51, best_predict: 0 with acc 0.75 at 2023-04-07 22:04:38\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 25, NEG: 75, best_predict: 0 with acc 0.775 at 2023-04-07 22:04:40\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 93, NEG: 7, best_predict: 1 with acc 0.81429 at 2023-04-07 22:06:22\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 99, NEG: 1, best_predict: 1 with acc 0.75 at 2023-04-07 22:06:25\n",
      "exp_596v38c2_price CZCE.MA309: POS: 27, NEG: 73, best_predict: 0 with acc 0.775 at 2023-04-07 22:06:27\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 86, NEG: 14, best_predict: 1 with acc 0.81429 at 2023-04-07 22:06:29\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 22:06:31\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 40, NEG: 60, best_predict: 1 with acc 0.775 at 2023-04-07 22:06:33\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 76, NEG: 24, best_predict: 1 with acc 0.81429 at 2023-04-07 22:06:35\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 90, NEG: 10, best_predict: 1 with acc 0.75 at 2023-04-07 22:06:37\n",
      "exp_596v38c2_price CZCE.TA309: POS: 80, NEG: 20, best_predict: 1 with acc 0.775 at 2023-04-07 22:06:39\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 87, NEG: 13, best_predict: 1 with acc 0.81429 at 2023-04-07 22:06:41\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 95, NEG: 5, best_predict: 1 with acc 0.75 at 2023-04-07 22:06:43\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 80, NEG: 20, best_predict: 1 with acc 0.775 at 2023-04-07 22:06:45\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 49, NEG: 51, best_predict: 0 with acc 0.81429 at 2023-04-07 22:06:47\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 40, NEG: 60, best_predict: 0 with acc 0.75 at 2023-04-07 22:06:49\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 38, NEG: 62, best_predict: 0 with acc 0.775 at 2023-04-07 22:06:51\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 49, NEG: 51, best_predict: 1 with acc 0.81429 at 2023-04-07 22:06:53\n",
      "exp_r4c7esy9 DCE.p2309: POS: 65, NEG: 35, best_predict: 1 with acc 0.75 at 2023-04-07 22:06:55\n",
      "exp_596v38c2_price DCE.p2309: POS: 64, NEG: 36, best_predict: 0 with acc 0.775 at 2023-04-07 22:06:57\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 66, NEG: 34, best_predict: 1 with acc 0.81429 at 2023-04-07 22:07:00\n",
      "exp_r4c7esy9 DCE.y2309: POS: 75, NEG: 25, best_predict: 1 with acc 0.75 at 2023-04-07 22:07:02\n",
      "exp_596v38c2_price DCE.y2309: POS: 55, NEG: 45, best_predict: 0 with acc 0.775 at 2023-04-07 22:07:04\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 58, NEG: 42, best_predict: 1 with acc 0.81429 at 2023-04-07 22:07:06\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 66, NEG: 34, best_predict: 1 with acc 0.75 at 2023-04-07 22:07:08\n",
      "exp_596v38c2_price CZCE.OI309: POS: 79, NEG: 21, best_predict: 0 with acc 0.775 at 2023-04-07 22:07:10\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 63, NEG: 37, best_predict: 1 with acc 0.81429 at 2023-04-07 22:07:12\n",
      "exp_r4c7esy9 DCE.a2309: POS: 44, NEG: 56, best_predict: 1 with acc 0.75 at 2023-04-07 22:07:14\n",
      "exp_596v38c2_price DCE.a2309: POS: 39, NEG: 61, best_predict: 0 with acc 0.775 at 2023-04-07 22:07:16\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 48, NEG: 52, best_predict: 1 with acc 0.81429 at 2023-04-07 22:07:18\n",
      "exp_r4c7esy9 DCE.m2309: POS: 32, NEG: 68, best_predict: 0 with acc 0.75 at 2023-04-07 22:07:20\n",
      "exp_596v38c2_price DCE.m2309: POS: 30, NEG: 70, best_predict: 1 with acc 0.775 at 2023-04-07 22:07:22\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 56, NEG: 44, best_predict: 0 with acc 0.81429 at 2023-04-07 22:07:24\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 63, NEG: 37, best_predict: 0 with acc 0.75 at 2023-04-07 22:07:27\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 27, NEG: 73, best_predict: 0 with acc 0.775 at 2023-04-07 22:07:29\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 89, NEG: 11, best_predict: 1 with acc 0.81429 at 2023-04-07 22:07:31\n",
      "exp_r4c7esy9 DCE.i2309: POS: 93, NEG: 7, best_predict: 1 with acc 0.75 at 2023-04-07 22:07:33\n",
      "exp_596v38c2_price DCE.i2309: POS: 58, NEG: 42, best_predict: 1 with acc 0.775 at 2023-04-07 22:07:35\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 87, NEG: 13, best_predict: 1 with acc 0.81429 at 2023-04-07 22:07:37\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 97, NEG: 3, best_predict: 1 with acc 0.75 at 2023-04-07 22:07:39\n",
      "exp_596v38c2_price DCE.jm2309: POS: 55, NEG: 45, best_predict: 0 with acc 0.775 at 2023-04-07 22:07:41\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 86, NEG: 14, best_predict: 0 with acc 0.81429 at 2023-04-07 22:07:43\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 86, NEG: 14, best_predict: 1 with acc 0.75 at 2023-04-07 22:07:45\n",
      "exp_596v38c2_price CZCE.SR309: POS: 92, NEG: 8, best_predict: 1 with acc 0.775 at 2023-04-07 22:07:48\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 23, NEG: 77, best_predict: 0 with acc 0.81429 at 2023-04-07 22:07:50\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 24, NEG: 76, best_predict: 0 with acc 0.75 at 2023-04-07 22:07:52\n",
      "exp_596v38c2_price CZCE.CF309: POS: 10, NEG: 90, best_predict: 0 with acc 0.775 at 2023-04-07 22:07:54\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 96, NEG: 4, best_predict: 1 with acc 0.81429 at 2023-04-07 22:07:56\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 98, NEG: 2, best_predict: 1 with acc 0.75 at 2023-04-07 22:07:59\n",
      "exp_596v38c2_price CZCE.SA309: POS: 27, NEG: 73, best_predict: 1 with acc 0.775 at 2023-04-07 22:08:01\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 44, NEG: 56, best_predict: 1 with acc 0.81429 at 2023-04-07 22:08:03\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 49, NEG: 51, best_predict: 0 with acc 0.75 at 2023-04-07 22:08:05\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 25, NEG: 75, best_predict: 0 with acc 0.775 at 2023-04-07 22:08:07\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 93, NEG: 7, best_predict: 1 with acc 0.81429 at 2023-04-07 22:09:50\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 99, NEG: 1, best_predict: 1 with acc 0.75 at 2023-04-07 22:09:52\n",
      "exp_596v38c2_price CZCE.MA309: POS: 27, NEG: 73, best_predict: 0 with acc 0.775 at 2023-04-07 22:09:54\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 86, NEG: 14, best_predict: 1 with acc 0.81429 at 2023-04-07 22:09:56\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 22:09:58\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 40, NEG: 60, best_predict: 1 with acc 0.775 at 2023-04-07 22:10:01\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 76, NEG: 24, best_predict: 1 with acc 0.81429 at 2023-04-07 22:10:03\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 90, NEG: 10, best_predict: 1 with acc 0.75 at 2023-04-07 22:10:05\n",
      "exp_596v38c2_price CZCE.TA309: POS: 80, NEG: 20, best_predict: 1 with acc 0.775 at 2023-04-07 22:10:07\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 87, NEG: 13, best_predict: 1 with acc 0.81429 at 2023-04-07 22:10:09\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 95, NEG: 5, best_predict: 1 with acc 0.75 at 2023-04-07 22:10:11\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 80, NEG: 20, best_predict: 1 with acc 0.775 at 2023-04-07 22:10:13\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 49, NEG: 51, best_predict: 0 with acc 0.81429 at 2023-04-07 22:10:15\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 40, NEG: 60, best_predict: 0 with acc 0.75 at 2023-04-07 22:10:17\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 38, NEG: 62, best_predict: 0 with acc 0.775 at 2023-04-07 22:10:19\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 49, NEG: 51, best_predict: 1 with acc 0.81429 at 2023-04-07 22:10:22\n",
      "exp_r4c7esy9 DCE.p2309: POS: 65, NEG: 35, best_predict: 1 with acc 0.75 at 2023-04-07 22:10:24\n",
      "exp_596v38c2_price DCE.p2309: POS: 64, NEG: 36, best_predict: 0 with acc 0.775 at 2023-04-07 22:10:26\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 66, NEG: 34, best_predict: 1 with acc 0.81429 at 2023-04-07 22:10:28\n",
      "exp_r4c7esy9 DCE.y2309: POS: 75, NEG: 25, best_predict: 1 with acc 0.75 at 2023-04-07 22:10:30\n",
      "exp_596v38c2_price DCE.y2309: POS: 56, NEG: 44, best_predict: 0 with acc 0.775 at 2023-04-07 22:10:32\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 58, NEG: 42, best_predict: 1 with acc 0.81429 at 2023-04-07 22:10:34\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 66, NEG: 34, best_predict: 1 with acc 0.75 at 2023-04-07 22:10:36\n",
      "exp_596v38c2_price CZCE.OI309: POS: 79, NEG: 21, best_predict: 0 with acc 0.775 at 2023-04-07 22:10:38\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 63, NEG: 37, best_predict: 1 with acc 0.81429 at 2023-04-07 22:10:40\n",
      "exp_r4c7esy9 DCE.a2309: POS: 44, NEG: 56, best_predict: 1 with acc 0.75 at 2023-04-07 22:10:43\n",
      "exp_596v38c2_price DCE.a2309: POS: 39, NEG: 61, best_predict: 0 with acc 0.775 at 2023-04-07 22:10:45\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 48, NEG: 52, best_predict: 1 with acc 0.81429 at 2023-04-07 22:10:47\n",
      "exp_r4c7esy9 DCE.m2309: POS: 32, NEG: 68, best_predict: 0 with acc 0.75 at 2023-04-07 22:10:49\n",
      "exp_596v38c2_price DCE.m2309: POS: 30, NEG: 70, best_predict: 1 with acc 0.775 at 2023-04-07 22:10:51\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 56, NEG: 44, best_predict: 0 with acc 0.81429 at 2023-04-07 22:10:53\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 63, NEG: 37, best_predict: 0 with acc 0.75 at 2023-04-07 22:10:55\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 28, NEG: 72, best_predict: 0 with acc 0.775 at 2023-04-07 22:10:57\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 89, NEG: 11, best_predict: 1 with acc 0.81429 at 2023-04-07 22:10:59\n",
      "exp_r4c7esy9 DCE.i2309: POS: 93, NEG: 7, best_predict: 1 with acc 0.75 at 2023-04-07 22:11:01\n",
      "exp_596v38c2_price DCE.i2309: POS: 58, NEG: 42, best_predict: 1 with acc 0.775 at 2023-04-07 22:11:03\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 87, NEG: 13, best_predict: 1 with acc 0.81429 at 2023-04-07 22:11:05\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 97, NEG: 3, best_predict: 1 with acc 0.75 at 2023-04-07 22:11:08\n",
      "exp_596v38c2_price DCE.jm2309: POS: 55, NEG: 45, best_predict: 0 with acc 0.775 at 2023-04-07 22:11:10\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 86, NEG: 14, best_predict: 0 with acc 0.81429 at 2023-04-07 22:11:12\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 86, NEG: 14, best_predict: 1 with acc 0.75 at 2023-04-07 22:11:14\n",
      "exp_596v38c2_price CZCE.SR309: POS: 92, NEG: 8, best_predict: 1 with acc 0.775 at 2023-04-07 22:11:16\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 23, NEG: 77, best_predict: 0 with acc 0.81429 at 2023-04-07 22:11:18\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 24, NEG: 76, best_predict: 0 with acc 0.75 at 2023-04-07 22:11:20\n",
      "exp_596v38c2_price CZCE.CF309: POS: 10, NEG: 90, best_predict: 0 with acc 0.775 at 2023-04-07 22:11:22\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 96, NEG: 4, best_predict: 1 with acc 0.81429 at 2023-04-07 22:11:25\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 98, NEG: 2, best_predict: 1 with acc 0.75 at 2023-04-07 22:11:27\n",
      "exp_596v38c2_price CZCE.SA309: POS: 27, NEG: 73, best_predict: 1 with acc 0.775 at 2023-04-07 22:11:29\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 44, NEG: 56, best_predict: 1 with acc 0.81429 at 2023-04-07 22:11:31\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 49, NEG: 51, best_predict: 0 with acc 0.75 at 2023-04-07 22:11:33\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 25, NEG: 75, best_predict: 0 with acc 0.775 at 2023-04-07 22:11:35\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 93, NEG: 7, best_predict: 1 with acc 0.81429 at 2023-04-07 22:13:17\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 100, NEG: 0, best_predict: 1 with acc 0.75 at 2023-04-07 22:13:19\n",
      "exp_596v38c2_price CZCE.MA309: POS: 27, NEG: 73, best_predict: 0 with acc 0.775 at 2023-04-07 22:13:22\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 85, NEG: 15, best_predict: 1 with acc 0.81429 at 2023-04-07 22:13:24\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 22:13:26\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 46, NEG: 54, best_predict: 1 with acc 0.775 at 2023-04-07 22:13:28\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 85, NEG: 15, best_predict: 1 with acc 0.81429 at 2023-04-07 22:13:30\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 22:13:33\n",
      "exp_596v38c2_price CZCE.TA309: POS: 78, NEG: 22, best_predict: 1 with acc 0.775 at 2023-04-07 22:13:35\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 84, NEG: 16, best_predict: 1 with acc 0.81429 at 2023-04-07 22:13:37\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 88, NEG: 12, best_predict: 1 with acc 0.75 at 2023-04-07 22:13:39\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 79, NEG: 21, best_predict: 1 with acc 0.775 at 2023-04-07 22:13:41\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 56, NEG: 44, best_predict: 0 with acc 0.81429 at 2023-04-07 22:13:43\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 51, NEG: 49, best_predict: 0 with acc 0.75 at 2023-04-07 22:13:45\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 35, NEG: 65, best_predict: 0 with acc 0.775 at 2023-04-07 22:13:47\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 55, NEG: 45, best_predict: 1 with acc 0.81429 at 2023-04-07 22:13:49\n",
      "exp_r4c7esy9 DCE.p2309: POS: 72, NEG: 28, best_predict: 1 with acc 0.75 at 2023-04-07 22:13:52\n",
      "exp_596v38c2_price DCE.p2309: POS: 60, NEG: 40, best_predict: 0 with acc 0.775 at 2023-04-07 22:13:54\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 69, NEG: 31, best_predict: 1 with acc 0.81429 at 2023-04-07 22:13:56\n",
      "exp_r4c7esy9 DCE.y2309: POS: 76, NEG: 24, best_predict: 1 with acc 0.75 at 2023-04-07 22:13:58\n",
      "exp_596v38c2_price DCE.y2309: POS: 53, NEG: 47, best_predict: 0 with acc 0.775 at 2023-04-07 22:14:00\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 76, NEG: 24, best_predict: 1 with acc 0.81429 at 2023-04-07 22:14:02\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 82, NEG: 18, best_predict: 1 with acc 0.75 at 2023-04-07 22:14:04\n",
      "exp_596v38c2_price CZCE.OI309: POS: 82, NEG: 18, best_predict: 0 with acc 0.775 at 2023-04-07 22:14:06\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 65, NEG: 35, best_predict: 1 with acc 0.81429 at 2023-04-07 22:14:09\n",
      "exp_r4c7esy9 DCE.a2309: POS: 40, NEG: 60, best_predict: 1 with acc 0.75 at 2023-04-07 22:14:11\n",
      "exp_596v38c2_price DCE.a2309: POS: 48, NEG: 52, best_predict: 0 with acc 0.775 at 2023-04-07 22:14:13\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 52, NEG: 48, best_predict: 1 with acc 0.81429 at 2023-04-07 22:14:15\n",
      "exp_r4c7esy9 DCE.m2309: POS: 37, NEG: 63, best_predict: 0 with acc 0.75 at 2023-04-07 22:14:17\n",
      "exp_596v38c2_price DCE.m2309: POS: 23, NEG: 77, best_predict: 1 with acc 0.775 at 2023-04-07 22:14:19\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 58, NEG: 42, best_predict: 0 with acc 0.81429 at 2023-04-07 22:14:21\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 60, NEG: 40, best_predict: 1 with acc 0.75 at 2023-04-07 22:14:23\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 24, NEG: 76, best_predict: 0 with acc 0.775 at 2023-04-07 22:14:25\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 87, NEG: 13, best_predict: 1 with acc 0.81429 at 2023-04-07 22:14:27\n",
      "exp_r4c7esy9 DCE.i2309: POS: 91, NEG: 9, best_predict: 1 with acc 0.75 at 2023-04-07 22:14:29\n",
      "exp_596v38c2_price DCE.i2309: POS: 53, NEG: 47, best_predict: 0 with acc 0.775 at 2023-04-07 22:14:32\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 91, NEG: 9, best_predict: 1 with acc 0.81429 at 2023-04-07 22:14:34\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 98, NEG: 2, best_predict: 1 with acc 0.75 at 2023-04-07 22:14:36\n",
      "exp_596v38c2_price DCE.jm2309: POS: 50, NEG: 50, best_predict: 0 with acc 0.775 at 2023-04-07 22:14:38\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 88, NEG: 12, best_predict: 0 with acc 0.81429 at 2023-04-07 22:14:40\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 87, NEG: 13, best_predict: 1 with acc 0.75 at 2023-04-07 22:14:42\n",
      "exp_596v38c2_price CZCE.SR309: POS: 91, NEG: 9, best_predict: 1 with acc 0.775 at 2023-04-07 22:14:44\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 17, NEG: 83, best_predict: 0 with acc 0.81429 at 2023-04-07 22:14:46\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 21, NEG: 79, best_predict: 0 with acc 0.75 at 2023-04-07 22:14:48\n",
      "exp_596v38c2_price CZCE.CF309: POS: 10, NEG: 90, best_predict: 0 with acc 0.775 at 2023-04-07 22:14:50\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 96, NEG: 4, best_predict: 1 with acc 0.81429 at 2023-04-07 22:14:52\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 99, NEG: 1, best_predict: 1 with acc 0.75 at 2023-04-07 22:14:54\n",
      "exp_596v38c2_price CZCE.SA309: POS: 29, NEG: 71, best_predict: 1 with acc 0.775 at 2023-04-07 22:14:56\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 42, NEG: 58, best_predict: 0 with acc 0.81429 at 2023-04-07 22:14:59\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 50, NEG: 50, best_predict: 0 with acc 0.75 at 2023-04-07 22:15:01\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 18, NEG: 82, best_predict: 0 with acc 0.775 at 2023-04-07 22:15:03\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 95, NEG: 5, best_predict: 1 with acc 0.81429 at 2023-04-07 22:16:45\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 100, NEG: 0, best_predict: 1 with acc 0.75 at 2023-04-07 22:16:47\n",
      "exp_596v38c2_price CZCE.MA309: POS: 27, NEG: 73, best_predict: 0 with acc 0.775 at 2023-04-07 22:16:49\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 87, NEG: 13, best_predict: 1 with acc 0.81429 at 2023-04-07 22:16:51\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 22:16:53\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 45, NEG: 55, best_predict: 1 with acc 0.775 at 2023-04-07 22:16:55\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 83, NEG: 17, best_predict: 0 with acc 0.81429 at 2023-04-07 22:16:57\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 22:16:59\n",
      "exp_596v38c2_price CZCE.TA309: POS: 78, NEG: 22, best_predict: 1 with acc 0.775 at 2023-04-07 22:17:01\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 86, NEG: 14, best_predict: 1 with acc 0.81429 at 2023-04-07 22:17:03\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 91, NEG: 9, best_predict: 1 with acc 0.75 at 2023-04-07 22:17:06\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 78, NEG: 22, best_predict: 1 with acc 0.775 at 2023-04-07 22:17:08\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 53, NEG: 47, best_predict: 0 with acc 0.81429 at 2023-04-07 22:17:10\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 51, NEG: 49, best_predict: 0 with acc 0.75 at 2023-04-07 22:17:12\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 35, NEG: 65, best_predict: 0 with acc 0.775 at 2023-04-07 22:17:14\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 54, NEG: 46, best_predict: 1 with acc 0.81429 at 2023-04-07 22:17:16\n",
      "exp_r4c7esy9 DCE.p2309: POS: 71, NEG: 29, best_predict: 1 with acc 0.75 at 2023-04-07 22:17:18\n",
      "exp_596v38c2_price DCE.p2309: POS: 63, NEG: 37, best_predict: 0 with acc 0.775 at 2023-04-07 22:17:20\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 71, NEG: 29, best_predict: 1 with acc 0.81429 at 2023-04-07 22:17:22\n",
      "exp_r4c7esy9 DCE.y2309: POS: 76, NEG: 24, best_predict: 1 with acc 0.75 at 2023-04-07 22:17:24\n",
      "exp_596v38c2_price DCE.y2309: POS: 52, NEG: 48, best_predict: 0 with acc 0.775 at 2023-04-07 22:17:26\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 75, NEG: 25, best_predict: 1 with acc 0.81429 at 2023-04-07 22:17:28\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 81, NEG: 19, best_predict: 1 with acc 0.75 at 2023-04-07 22:17:30\n",
      "exp_596v38c2_price CZCE.OI309: POS: 81, NEG: 19, best_predict: 0 with acc 0.775 at 2023-04-07 22:17:32\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 65, NEG: 35, best_predict: 1 with acc 0.81429 at 2023-04-07 22:17:35\n",
      "exp_r4c7esy9 DCE.a2309: POS: 41, NEG: 59, best_predict: 1 with acc 0.75 at 2023-04-07 22:17:37\n",
      "exp_596v38c2_price DCE.a2309: POS: 47, NEG: 53, best_predict: 0 with acc 0.775 at 2023-04-07 22:17:39\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 51, NEG: 49, best_predict: 1 with acc 0.81429 at 2023-04-07 22:17:41\n",
      "exp_r4c7esy9 DCE.m2309: POS: 36, NEG: 64, best_predict: 0 with acc 0.75 at 2023-04-07 22:17:43\n",
      "exp_596v38c2_price DCE.m2309: POS: 29, NEG: 71, best_predict: 1 with acc 0.775 at 2023-04-07 22:17:45\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 53, NEG: 47, best_predict: 0 with acc 0.81429 at 2023-04-07 22:17:47\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 56, NEG: 44, best_predict: 1 with acc 0.75 at 2023-04-07 22:17:49\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 24, NEG: 76, best_predict: 0 with acc 0.775 at 2023-04-07 22:17:51\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 88, NEG: 12, best_predict: 1 with acc 0.81429 at 2023-04-07 22:17:53\n",
      "exp_r4c7esy9 DCE.i2309: POS: 92, NEG: 8, best_predict: 1 with acc 0.75 at 2023-04-07 22:17:55\n",
      "exp_596v38c2_price DCE.i2309: POS: 53, NEG: 47, best_predict: 0 with acc 0.775 at 2023-04-07 22:17:57\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 90, NEG: 10, best_predict: 1 with acc 0.81429 at 2023-04-07 22:17:59\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 98, NEG: 2, best_predict: 1 with acc 0.75 at 2023-04-07 22:18:01\n",
      "exp_596v38c2_price DCE.jm2309: POS: 47, NEG: 53, best_predict: 0 with acc 0.775 at 2023-04-07 22:18:04\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 88, NEG: 12, best_predict: 0 with acc 0.81429 at 2023-04-07 22:18:06\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 88, NEG: 12, best_predict: 1 with acc 0.75 at 2023-04-07 22:18:08\n",
      "exp_596v38c2_price CZCE.SR309: POS: 91, NEG: 9, best_predict: 1 with acc 0.775 at 2023-04-07 22:18:10\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 13, NEG: 87, best_predict: 0 with acc 0.81429 at 2023-04-07 22:18:12\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 18, NEG: 82, best_predict: 0 with acc 0.75 at 2023-04-07 22:18:14\n",
      "exp_596v38c2_price CZCE.CF309: POS: 10, NEG: 90, best_predict: 0 with acc 0.775 at 2023-04-07 22:18:16\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 96, NEG: 4, best_predict: 1 with acc 0.81429 at 2023-04-07 22:18:18\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 99, NEG: 1, best_predict: 1 with acc 0.75 at 2023-04-07 22:18:20\n",
      "exp_596v38c2_price CZCE.SA309: POS: 29, NEG: 71, best_predict: 1 with acc 0.775 at 2023-04-07 22:18:22\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 48, NEG: 52, best_predict: 0 with acc 0.81429 at 2023-04-07 22:18:24\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 51, NEG: 49, best_predict: 0 with acc 0.75 at 2023-04-07 22:18:26\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 17, NEG: 83, best_predict: 0 with acc 0.775 at 2023-04-07 22:18:28\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 94, NEG: 6, best_predict: 1 with acc 0.81429 at 2023-04-07 22:20:10\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 100, NEG: 0, best_predict: 1 with acc 0.75 at 2023-04-07 22:20:12\n",
      "exp_596v38c2_price CZCE.MA309: POS: 27, NEG: 73, best_predict: 0 with acc 0.775 at 2023-04-07 22:20:15\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 86, NEG: 14, best_predict: 1 with acc 0.81429 at 2023-04-07 22:20:17\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 93, NEG: 7, best_predict: 1 with acc 0.75 at 2023-04-07 22:20:19\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 45, NEG: 55, best_predict: 1 with acc 0.775 at 2023-04-07 22:20:21\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 81, NEG: 19, best_predict: 0 with acc 0.81429 at 2023-04-07 22:20:23\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 96, NEG: 4, best_predict: 1 with acc 0.75 at 2023-04-07 22:20:25\n",
      "exp_596v38c2_price CZCE.TA309: POS: 77, NEG: 23, best_predict: 1 with acc 0.775 at 2023-04-07 22:20:27\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 82, NEG: 18, best_predict: 1 with acc 0.81429 at 2023-04-07 22:20:29\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 83, NEG: 17, best_predict: 1 with acc 0.75 at 2023-04-07 22:20:31\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 78, NEG: 22, best_predict: 1 with acc 0.775 at 2023-04-07 22:20:34\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 53, NEG: 47, best_predict: 0 with acc 0.81429 at 2023-04-07 22:20:36\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 52, NEG: 48, best_predict: 0 with acc 0.75 at 2023-04-07 22:20:38\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 32, NEG: 68, best_predict: 0 with acc 0.775 at 2023-04-07 22:20:40\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 50, NEG: 50, best_predict: 1 with acc 0.81429 at 2023-04-07 22:20:42\n",
      "exp_r4c7esy9 DCE.p2309: POS: 74, NEG: 26, best_predict: 1 with acc 0.75 at 2023-04-07 22:20:44\n",
      "exp_596v38c2_price DCE.p2309: POS: 58, NEG: 42, best_predict: 0 with acc 0.775 at 2023-04-07 22:20:47\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 76, NEG: 24, best_predict: 1 with acc 0.81429 at 2023-04-07 22:20:49\n",
      "exp_r4c7esy9 DCE.y2309: POS: 77, NEG: 23, best_predict: 1 with acc 0.75 at 2023-04-07 22:20:51\n",
      "exp_596v38c2_price DCE.y2309: POS: 44, NEG: 56, best_predict: 0 with acc 0.775 at 2023-04-07 22:20:53\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 82, NEG: 18, best_predict: 1 with acc 0.81429 at 2023-04-07 22:20:55\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 91, NEG: 9, best_predict: 1 with acc 0.75 at 2023-04-07 22:20:57\n",
      "exp_596v38c2_price CZCE.OI309: POS: 79, NEG: 21, best_predict: 0 with acc 0.775 at 2023-04-07 22:20:59\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 66, NEG: 34, best_predict: 1 with acc 0.81429 at 2023-04-07 22:21:01\n",
      "exp_r4c7esy9 DCE.a2309: POS: 39, NEG: 61, best_predict: 1 with acc 0.75 at 2023-04-07 22:21:03\n",
      "exp_596v38c2_price DCE.a2309: POS: 53, NEG: 47, best_predict: 0 with acc 0.775 at 2023-04-07 22:21:05\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 55, NEG: 45, best_predict: 1 with acc 0.81429 at 2023-04-07 22:21:07\n",
      "exp_r4c7esy9 DCE.m2309: POS: 44, NEG: 56, best_predict: 0 with acc 0.75 at 2023-04-07 22:21:09\n",
      "exp_596v38c2_price DCE.m2309: POS: 24, NEG: 76, best_predict: 1 with acc 0.775 at 2023-04-07 22:21:11\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 66, NEG: 34, best_predict: 0 with acc 0.81429 at 2023-04-07 22:21:13\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 61, NEG: 39, best_predict: 1 with acc 0.75 at 2023-04-07 22:21:15\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 31, NEG: 69, best_predict: 0 with acc 0.775 at 2023-04-07 22:21:17\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 89, NEG: 11, best_predict: 1 with acc 0.81429 at 2023-04-07 22:21:19\n",
      "exp_r4c7esy9 DCE.i2309: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 22:21:21\n",
      "exp_596v38c2_price DCE.i2309: POS: 50, NEG: 50, best_predict: 0 with acc 0.775 at 2023-04-07 22:21:24\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 90, NEG: 10, best_predict: 1 with acc 0.81429 at 2023-04-07 22:21:26\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 98, NEG: 2, best_predict: 1 with acc 0.75 at 2023-04-07 22:21:28\n",
      "exp_596v38c2_price DCE.jm2309: POS: 38, NEG: 62, best_predict: 0 with acc 0.775 at 2023-04-07 22:21:30\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 88, NEG: 12, best_predict: 0 with acc 0.81429 at 2023-04-07 22:21:32\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 88, NEG: 12, best_predict: 1 with acc 0.75 at 2023-04-07 22:21:34\n",
      "exp_596v38c2_price CZCE.SR309: POS: 89, NEG: 11, best_predict: 1 with acc 0.775 at 2023-04-07 22:21:36\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 12, NEG: 88, best_predict: 0 with acc 0.81429 at 2023-04-07 22:21:38\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 17, NEG: 83, best_predict: 0 with acc 0.75 at 2023-04-07 22:21:40\n",
      "exp_596v38c2_price CZCE.CF309: POS: 15, NEG: 85, best_predict: 0 with acc 0.775 at 2023-04-07 22:21:42\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 95, NEG: 5, best_predict: 1 with acc 0.81429 at 2023-04-07 22:21:44\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 98, NEG: 2, best_predict: 1 with acc 0.75 at 2023-04-07 22:21:46\n",
      "exp_596v38c2_price CZCE.SA309: POS: 32, NEG: 68, best_predict: 1 with acc 0.775 at 2023-04-07 22:21:48\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 50, NEG: 50, best_predict: 0 with acc 0.81429 at 2023-04-07 22:21:50\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 51, NEG: 49, best_predict: 0 with acc 0.75 at 2023-04-07 22:21:52\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 11, NEG: 89, best_predict: 0 with acc 0.775 at 2023-04-07 22:21:54\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 94, NEG: 6, best_predict: 1 with acc 0.81429 at 2023-04-07 22:23:36\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 100, NEG: 0, best_predict: 1 with acc 0.75 at 2023-04-07 22:23:38\n",
      "exp_596v38c2_price CZCE.MA309: POS: 27, NEG: 73, best_predict: 0 with acc 0.775 at 2023-04-07 22:23:40\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 86, NEG: 14, best_predict: 1 with acc 0.81429 at 2023-04-07 22:23:42\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 93, NEG: 7, best_predict: 1 with acc 0.75 at 2023-04-07 22:23:44\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 45, NEG: 55, best_predict: 1 with acc 0.775 at 2023-04-07 22:23:46\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 82, NEG: 18, best_predict: 0 with acc 0.81429 at 2023-04-07 22:23:48\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 96, NEG: 4, best_predict: 1 with acc 0.75 at 2023-04-07 22:23:50\n",
      "exp_596v38c2_price CZCE.TA309: POS: 77, NEG: 23, best_predict: 1 with acc 0.775 at 2023-04-07 22:23:52\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 82, NEG: 18, best_predict: 1 with acc 0.81429 at 2023-04-07 22:23:54\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 83, NEG: 17, best_predict: 1 with acc 0.75 at 2023-04-07 22:23:56\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 78, NEG: 22, best_predict: 1 with acc 0.775 at 2023-04-07 22:23:58\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 53, NEG: 47, best_predict: 0 with acc 0.81429 at 2023-04-07 22:24:00\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 52, NEG: 48, best_predict: 0 with acc 0.75 at 2023-04-07 22:24:02\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 32, NEG: 68, best_predict: 0 with acc 0.775 at 2023-04-07 22:24:04\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 50, NEG: 50, best_predict: 1 with acc 0.81429 at 2023-04-07 22:24:06\n",
      "exp_r4c7esy9 DCE.p2309: POS: 74, NEG: 26, best_predict: 1 with acc 0.75 at 2023-04-07 22:24:08\n",
      "exp_596v38c2_price DCE.p2309: POS: 58, NEG: 42, best_predict: 0 with acc 0.775 at 2023-04-07 22:24:10\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 76, NEG: 24, best_predict: 1 with acc 0.81429 at 2023-04-07 22:24:12\n",
      "exp_r4c7esy9 DCE.y2309: POS: 77, NEG: 23, best_predict: 1 with acc 0.75 at 2023-04-07 22:24:14\n",
      "exp_596v38c2_price DCE.y2309: POS: 44, NEG: 56, best_predict: 0 with acc 0.775 at 2023-04-07 22:24:16\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 82, NEG: 18, best_predict: 1 with acc 0.81429 at 2023-04-07 22:24:19\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 91, NEG: 9, best_predict: 1 with acc 0.75 at 2023-04-07 22:24:21\n",
      "exp_596v38c2_price CZCE.OI309: POS: 79, NEG: 21, best_predict: 0 with acc 0.775 at 2023-04-07 22:24:23\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 66, NEG: 34, best_predict: 1 with acc 0.81429 at 2023-04-07 22:24:25\n",
      "exp_r4c7esy9 DCE.a2309: POS: 38, NEG: 62, best_predict: 1 with acc 0.75 at 2023-04-07 22:24:27\n",
      "exp_596v38c2_price DCE.a2309: POS: 53, NEG: 47, best_predict: 0 with acc 0.775 at 2023-04-07 22:24:29\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 55, NEG: 45, best_predict: 1 with acc 0.81429 at 2023-04-07 22:24:31\n",
      "exp_r4c7esy9 DCE.m2309: POS: 44, NEG: 56, best_predict: 0 with acc 0.75 at 2023-04-07 22:24:33\n",
      "exp_596v38c2_price DCE.m2309: POS: 24, NEG: 76, best_predict: 1 with acc 0.775 at 2023-04-07 22:24:35\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 66, NEG: 34, best_predict: 0 with acc 0.81429 at 2023-04-07 22:24:38\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 61, NEG: 39, best_predict: 1 with acc 0.75 at 2023-04-07 22:24:40\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 31, NEG: 69, best_predict: 0 with acc 0.775 at 2023-04-07 22:24:42\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 89, NEG: 11, best_predict: 1 with acc 0.81429 at 2023-04-07 22:24:44\n",
      "exp_r4c7esy9 DCE.i2309: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 22:24:46\n",
      "exp_596v38c2_price DCE.i2309: POS: 50, NEG: 50, best_predict: 0 with acc 0.775 at 2023-04-07 22:24:48\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 90, NEG: 10, best_predict: 1 with acc 0.81429 at 2023-04-07 22:24:50\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 98, NEG: 2, best_predict: 1 with acc 0.75 at 2023-04-07 22:24:52\n",
      "exp_596v38c2_price DCE.jm2309: POS: 38, NEG: 62, best_predict: 0 with acc 0.775 at 2023-04-07 22:24:54\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 88, NEG: 12, best_predict: 0 with acc 0.81429 at 2023-04-07 22:24:56\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 88, NEG: 12, best_predict: 1 with acc 0.75 at 2023-04-07 22:24:58\n",
      "exp_596v38c2_price CZCE.SR309: POS: 89, NEG: 11, best_predict: 1 with acc 0.775 at 2023-04-07 22:25:00\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 12, NEG: 88, best_predict: 0 with acc 0.81429 at 2023-04-07 22:25:03\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 17, NEG: 83, best_predict: 0 with acc 0.75 at 2023-04-07 22:25:05\n",
      "exp_596v38c2_price CZCE.CF309: POS: 15, NEG: 85, best_predict: 0 with acc 0.775 at 2023-04-07 22:25:07\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 95, NEG: 5, best_predict: 1 with acc 0.81429 at 2023-04-07 22:25:09\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 98, NEG: 2, best_predict: 1 with acc 0.75 at 2023-04-07 22:25:11\n",
      "exp_596v38c2_price CZCE.SA309: POS: 32, NEG: 68, best_predict: 1 with acc 0.775 at 2023-04-07 22:25:13\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 50, NEG: 50, best_predict: 0 with acc 0.81429 at 2023-04-07 22:25:16\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 52, NEG: 48, best_predict: 0 with acc 0.75 at 2023-04-07 22:25:18\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 11, NEG: 89, best_predict: 0 with acc 0.775 at 2023-04-07 22:25:20\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 94, NEG: 6, best_predict: 1 with acc 0.81429 at 2023-04-07 22:27:02\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 100, NEG: 0, best_predict: 1 with acc 0.75 at 2023-04-07 22:27:04\n",
      "exp_596v38c2_price CZCE.MA309: POS: 27, NEG: 73, best_predict: 0 with acc 0.775 at 2023-04-07 22:27:06\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 86, NEG: 14, best_predict: 1 with acc 0.81429 at 2023-04-07 22:27:08\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 93, NEG: 7, best_predict: 1 with acc 0.75 at 2023-04-07 22:27:11\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 45, NEG: 55, best_predict: 1 with acc 0.775 at 2023-04-07 22:27:13\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 82, NEG: 18, best_predict: 0 with acc 0.81429 at 2023-04-07 22:27:15\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 96, NEG: 4, best_predict: 1 with acc 0.75 at 2023-04-07 22:27:17\n",
      "exp_596v38c2_price CZCE.TA309: POS: 77, NEG: 23, best_predict: 1 with acc 0.775 at 2023-04-07 22:27:19\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 82, NEG: 18, best_predict: 1 with acc 0.81429 at 2023-04-07 22:27:21\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 83, NEG: 17, best_predict: 1 with acc 0.75 at 2023-04-07 22:27:24\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 78, NEG: 22, best_predict: 1 with acc 0.775 at 2023-04-07 22:27:26\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 53, NEG: 47, best_predict: 0 with acc 0.81429 at 2023-04-07 22:27:28\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 52, NEG: 48, best_predict: 0 with acc 0.75 at 2023-04-07 22:27:30\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 32, NEG: 68, best_predict: 0 with acc 0.775 at 2023-04-07 22:27:32\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 50, NEG: 50, best_predict: 1 with acc 0.81429 at 2023-04-07 22:27:34\n",
      "exp_r4c7esy9 DCE.p2309: POS: 74, NEG: 26, best_predict: 1 with acc 0.75 at 2023-04-07 22:27:36\n",
      "exp_596v38c2_price DCE.p2309: POS: 58, NEG: 42, best_predict: 0 with acc 0.775 at 2023-04-07 22:27:38\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 76, NEG: 24, best_predict: 1 with acc 0.81429 at 2023-04-07 22:27:41\n",
      "exp_r4c7esy9 DCE.y2309: POS: 77, NEG: 23, best_predict: 1 with acc 0.75 at 2023-04-07 22:27:43\n",
      "exp_596v38c2_price DCE.y2309: POS: 44, NEG: 56, best_predict: 0 with acc 0.775 at 2023-04-07 22:27:45\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 82, NEG: 18, best_predict: 1 with acc 0.81429 at 2023-04-07 22:27:47\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 91, NEG: 9, best_predict: 1 with acc 0.75 at 2023-04-07 22:27:49\n",
      "exp_596v38c2_price CZCE.OI309: POS: 79, NEG: 21, best_predict: 0 with acc 0.775 at 2023-04-07 22:27:51\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 66, NEG: 34, best_predict: 1 with acc 0.81429 at 2023-04-07 22:27:53\n",
      "exp_r4c7esy9 DCE.a2309: POS: 39, NEG: 61, best_predict: 1 with acc 0.75 at 2023-04-07 22:27:55\n",
      "exp_596v38c2_price DCE.a2309: POS: 53, NEG: 47, best_predict: 0 with acc 0.775 at 2023-04-07 22:27:57\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 55, NEG: 45, best_predict: 1 with acc 0.81429 at 2023-04-07 22:27:59\n",
      "exp_r4c7esy9 DCE.m2309: POS: 44, NEG: 56, best_predict: 0 with acc 0.75 at 2023-04-07 22:28:02\n",
      "exp_596v38c2_price DCE.m2309: POS: 24, NEG: 76, best_predict: 1 with acc 0.775 at 2023-04-07 22:28:04\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 66, NEG: 34, best_predict: 0 with acc 0.81429 at 2023-04-07 22:28:06\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 61, NEG: 39, best_predict: 1 with acc 0.75 at 2023-04-07 22:28:08\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 31, NEG: 69, best_predict: 0 with acc 0.775 at 2023-04-07 22:28:10\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 89, NEG: 11, best_predict: 1 with acc 0.81429 at 2023-04-07 22:28:12\n",
      "exp_r4c7esy9 DCE.i2309: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 22:28:14\n",
      "exp_596v38c2_price DCE.i2309: POS: 50, NEG: 50, best_predict: 0 with acc 0.775 at 2023-04-07 22:28:16\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 90, NEG: 10, best_predict: 1 with acc 0.81429 at 2023-04-07 22:28:19\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 98, NEG: 2, best_predict: 1 with acc 0.75 at 2023-04-07 22:28:21\n",
      "exp_596v38c2_price DCE.jm2309: POS: 38, NEG: 62, best_predict: 0 with acc 0.775 at 2023-04-07 22:28:23\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 88, NEG: 12, best_predict: 0 with acc 0.81429 at 2023-04-07 22:28:25\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 88, NEG: 12, best_predict: 1 with acc 0.75 at 2023-04-07 22:28:27\n",
      "exp_596v38c2_price CZCE.SR309: POS: 89, NEG: 11, best_predict: 1 with acc 0.775 at 2023-04-07 22:28:29\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 12, NEG: 88, best_predict: 0 with acc 0.81429 at 2023-04-07 22:28:31\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 17, NEG: 83, best_predict: 0 with acc 0.75 at 2023-04-07 22:28:33\n",
      "exp_596v38c2_price CZCE.CF309: POS: 15, NEG: 85, best_predict: 0 with acc 0.775 at 2023-04-07 22:28:35\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 95, NEG: 5, best_predict: 1 with acc 0.81429 at 2023-04-07 22:28:37\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 98, NEG: 2, best_predict: 1 with acc 0.75 at 2023-04-07 22:28:40\n",
      "exp_596v38c2_price CZCE.SA309: POS: 32, NEG: 68, best_predict: 1 with acc 0.775 at 2023-04-07 22:28:42\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 50, NEG: 50, best_predict: 0 with acc 0.81429 at 2023-04-07 22:28:44\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 52, NEG: 48, best_predict: 0 with acc 0.75 at 2023-04-07 22:28:46\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 11, NEG: 89, best_predict: 0 with acc 0.775 at 2023-04-07 22:28:48\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 96, NEG: 4, best_predict: 1 with acc 0.81429 at 2023-04-07 22:30:30\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 100, NEG: 0, best_predict: 1 with acc 0.75 at 2023-04-07 22:30:32\n",
      "exp_596v38c2_price CZCE.MA309: POS: 28, NEG: 72, best_predict: 0 with acc 0.775 at 2023-04-07 22:30:34\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 88, NEG: 12, best_predict: 1 with acc 0.81429 at 2023-04-07 22:30:36\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 22:30:38\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 48, NEG: 52, best_predict: 1 with acc 0.775 at 2023-04-07 22:30:40\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 85, NEG: 15, best_predict: 0 with acc 0.81429 at 2023-04-07 22:30:42\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 97, NEG: 3, best_predict: 1 with acc 0.75 at 2023-04-07 22:30:44\n",
      "exp_596v38c2_price CZCE.TA309: POS: 73, NEG: 27, best_predict: 1 with acc 0.775 at 2023-04-07 22:30:46\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 77, NEG: 23, best_predict: 1 with acc 0.81429 at 2023-04-07 22:30:48\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 75, NEG: 25, best_predict: 1 with acc 0.75 at 2023-04-07 22:30:50\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 74, NEG: 26, best_predict: 1 with acc 0.775 at 2023-04-07 22:30:52\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 56, NEG: 44, best_predict: 0 with acc 0.81429 at 2023-04-07 22:30:55\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 62, NEG: 38, best_predict: 1 with acc 0.75 at 2023-04-07 22:30:57\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 29, NEG: 71, best_predict: 0 with acc 0.775 at 2023-04-07 22:30:59\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 44, NEG: 56, best_predict: 1 with acc 0.81429 at 2023-04-07 22:31:01\n",
      "exp_r4c7esy9 DCE.p2309: POS: 75, NEG: 25, best_predict: 1 with acc 0.75 at 2023-04-07 22:31:03\n",
      "exp_596v38c2_price DCE.p2309: POS: 55, NEG: 45, best_predict: 0 with acc 0.775 at 2023-04-07 22:31:05\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 76, NEG: 24, best_predict: 1 with acc 0.81429 at 2023-04-07 22:31:07\n",
      "exp_r4c7esy9 DCE.y2309: POS: 75, NEG: 25, best_predict: 1 with acc 0.75 at 2023-04-07 22:31:09\n",
      "exp_596v38c2_price DCE.y2309: POS: 35, NEG: 65, best_predict: 0 with acc 0.775 at 2023-04-07 22:31:11\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 81, NEG: 19, best_predict: 0 with acc 0.81429 at 2023-04-07 22:31:13\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 91, NEG: 9, best_predict: 1 with acc 0.75 at 2023-04-07 22:31:15\n",
      "exp_596v38c2_price CZCE.OI309: POS: 74, NEG: 26, best_predict: 0 with acc 0.775 at 2023-04-07 22:31:17\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 63, NEG: 37, best_predict: 1 with acc 0.81429 at 2023-04-07 22:31:19\n",
      "exp_r4c7esy9 DCE.a2309: POS: 34, NEG: 66, best_predict: 0 with acc 0.75 at 2023-04-07 22:31:21\n",
      "exp_596v38c2_price DCE.a2309: POS: 56, NEG: 44, best_predict: 0 with acc 0.775 at 2023-04-07 22:31:23\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 57, NEG: 43, best_predict: 1 with acc 0.81429 at 2023-04-07 22:31:25\n",
      "exp_r4c7esy9 DCE.m2309: POS: 54, NEG: 46, best_predict: 1 with acc 0.75 at 2023-04-07 22:31:27\n",
      "exp_596v38c2_price DCE.m2309: POS: 25, NEG: 75, best_predict: 1 with acc 0.775 at 2023-04-07 22:31:29\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 56, NEG: 44, best_predict: 0 with acc 0.81429 at 2023-04-07 22:31:31\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 55, NEG: 45, best_predict: 1 with acc 0.75 at 2023-04-07 22:31:33\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 27, NEG: 73, best_predict: 0 with acc 0.775 at 2023-04-07 22:31:35\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 90, NEG: 10, best_predict: 1 with acc 0.81429 at 2023-04-07 22:31:37\n",
      "exp_r4c7esy9 DCE.i2309: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 22:31:39\n",
      "exp_596v38c2_price DCE.i2309: POS: 44, NEG: 56, best_predict: 0 with acc 0.775 at 2023-04-07 22:31:41\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 91, NEG: 9, best_predict: 1 with acc 0.81429 at 2023-04-07 22:31:43\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 95, NEG: 5, best_predict: 1 with acc 0.75 at 2023-04-07 22:31:45\n",
      "exp_596v38c2_price DCE.jm2309: POS: 34, NEG: 66, best_predict: 0 with acc 0.775 at 2023-04-07 22:31:47\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 83, NEG: 17, best_predict: 0 with acc 0.81429 at 2023-04-07 22:31:49\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 89, NEG: 11, best_predict: 1 with acc 0.75 at 2023-04-07 22:31:51\n",
      "exp_596v38c2_price CZCE.SR309: POS: 84, NEG: 16, best_predict: 1 with acc 0.775 at 2023-04-07 22:31:53\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 11, NEG: 89, best_predict: 0 with acc 0.81429 at 2023-04-07 22:31:55\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 13, NEG: 87, best_predict: 0 with acc 0.75 at 2023-04-07 22:31:57\n",
      "exp_596v38c2_price CZCE.CF309: POS: 16, NEG: 84, best_predict: 0 with acc 0.775 at 2023-04-07 22:31:59\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 92, NEG: 8, best_predict: 1 with acc 0.81429 at 2023-04-07 22:32:01\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 97, NEG: 3, best_predict: 1 with acc 0.75 at 2023-04-07 22:32:03\n",
      "exp_596v38c2_price CZCE.SA309: POS: 35, NEG: 65, best_predict: 1 with acc 0.775 at 2023-04-07 22:32:05\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 58, NEG: 42, best_predict: 1 with acc 0.81429 at 2023-04-07 22:32:07\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 60, NEG: 40, best_predict: 1 with acc 0.75 at 2023-04-07 22:32:09\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 5, NEG: 95, best_predict: 0 with acc 0.775 at 2023-04-07 22:32:11\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 96, NEG: 4, best_predict: 1 with acc 0.81429 at 2023-04-07 22:33:53\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 100, NEG: 0, best_predict: 1 with acc 0.75 at 2023-04-07 22:33:55\n",
      "exp_596v38c2_price CZCE.MA309: POS: 28, NEG: 72, best_predict: 0 with acc 0.775 at 2023-04-07 22:33:57\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 88, NEG: 12, best_predict: 1 with acc 0.81429 at 2023-04-07 22:34:00\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 22:34:02\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 48, NEG: 52, best_predict: 1 with acc 0.775 at 2023-04-07 22:34:03\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 85, NEG: 15, best_predict: 0 with acc 0.81429 at 2023-04-07 22:34:06\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 97, NEG: 3, best_predict: 1 with acc 0.75 at 2023-04-07 22:34:08\n",
      "exp_596v38c2_price CZCE.TA309: POS: 73, NEG: 27, best_predict: 1 with acc 0.775 at 2023-04-07 22:34:10\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 77, NEG: 23, best_predict: 1 with acc 0.81429 at 2023-04-07 22:34:12\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 75, NEG: 25, best_predict: 1 with acc 0.75 at 2023-04-07 22:34:14\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 74, NEG: 26, best_predict: 1 with acc 0.775 at 2023-04-07 22:34:16\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 56, NEG: 44, best_predict: 0 with acc 0.81429 at 2023-04-07 22:34:18\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 62, NEG: 38, best_predict: 1 with acc 0.75 at 2023-04-07 22:34:20\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 29, NEG: 71, best_predict: 0 with acc 0.775 at 2023-04-07 22:34:22\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 44, NEG: 56, best_predict: 1 with acc 0.81429 at 2023-04-07 22:34:24\n",
      "exp_r4c7esy9 DCE.p2309: POS: 75, NEG: 25, best_predict: 1 with acc 0.75 at 2023-04-07 22:34:26\n",
      "exp_596v38c2_price DCE.p2309: POS: 55, NEG: 45, best_predict: 0 with acc 0.775 at 2023-04-07 22:34:28\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 76, NEG: 24, best_predict: 1 with acc 0.81429 at 2023-04-07 22:34:30\n",
      "exp_r4c7esy9 DCE.y2309: POS: 75, NEG: 25, best_predict: 1 with acc 0.75 at 2023-04-07 22:34:32\n",
      "exp_596v38c2_price DCE.y2309: POS: 35, NEG: 65, best_predict: 0 with acc 0.775 at 2023-04-07 22:34:34\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 81, NEG: 19, best_predict: 0 with acc 0.81429 at 2023-04-07 22:34:36\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 91, NEG: 9, best_predict: 1 with acc 0.75 at 2023-04-07 22:34:38\n",
      "exp_596v38c2_price CZCE.OI309: POS: 74, NEG: 26, best_predict: 0 with acc 0.775 at 2023-04-07 22:34:40\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 63, NEG: 37, best_predict: 1 with acc 0.81429 at 2023-04-07 22:34:42\n",
      "exp_r4c7esy9 DCE.a2309: POS: 35, NEG: 65, best_predict: 0 with acc 0.75 at 2023-04-07 22:34:44\n",
      "exp_596v38c2_price DCE.a2309: POS: 56, NEG: 44, best_predict: 0 with acc 0.775 at 2023-04-07 22:34:46\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 57, NEG: 43, best_predict: 1 with acc 0.81429 at 2023-04-07 22:34:48\n",
      "exp_r4c7esy9 DCE.m2309: POS: 54, NEG: 46, best_predict: 1 with acc 0.75 at 2023-04-07 22:34:50\n",
      "exp_596v38c2_price DCE.m2309: POS: 25, NEG: 75, best_predict: 1 with acc 0.775 at 2023-04-07 22:34:52\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 56, NEG: 44, best_predict: 0 with acc 0.81429 at 2023-04-07 22:34:54\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 55, NEG: 45, best_predict: 1 with acc 0.75 at 2023-04-07 22:34:56\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 27, NEG: 73, best_predict: 0 with acc 0.775 at 2023-04-07 22:34:58\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 90, NEG: 10, best_predict: 1 with acc 0.81429 at 2023-04-07 22:35:00\n",
      "exp_r4c7esy9 DCE.i2309: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 22:35:02\n",
      "exp_596v38c2_price DCE.i2309: POS: 44, NEG: 56, best_predict: 0 with acc 0.775 at 2023-04-07 22:35:04\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 91, NEG: 9, best_predict: 1 with acc 0.81429 at 2023-04-07 22:35:06\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 95, NEG: 5, best_predict: 1 with acc 0.75 at 2023-04-07 22:35:08\n",
      "exp_596v38c2_price DCE.jm2309: POS: 34, NEG: 66, best_predict: 0 with acc 0.775 at 2023-04-07 22:35:10\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 83, NEG: 17, best_predict: 0 with acc 0.81429 at 2023-04-07 22:35:12\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 89, NEG: 11, best_predict: 1 with acc 0.75 at 2023-04-07 22:35:14\n",
      "exp_596v38c2_price CZCE.SR309: POS: 84, NEG: 16, best_predict: 1 with acc 0.775 at 2023-04-07 22:35:16\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 11, NEG: 89, best_predict: 0 with acc 0.81429 at 2023-04-07 22:35:18\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 13, NEG: 87, best_predict: 0 with acc 0.75 at 2023-04-07 22:35:20\n",
      "exp_596v38c2_price CZCE.CF309: POS: 16, NEG: 84, best_predict: 0 with acc 0.775 at 2023-04-07 22:35:22\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 92, NEG: 8, best_predict: 1 with acc 0.81429 at 2023-04-07 22:35:24\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 97, NEG: 3, best_predict: 1 with acc 0.75 at 2023-04-07 22:35:26\n",
      "exp_596v38c2_price CZCE.SA309: POS: 35, NEG: 65, best_predict: 1 with acc 0.775 at 2023-04-07 22:35:28\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 58, NEG: 42, best_predict: 1 with acc 0.81429 at 2023-04-07 22:35:30\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 60, NEG: 40, best_predict: 1 with acc 0.75 at 2023-04-07 22:35:32\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 5, NEG: 95, best_predict: 0 with acc 0.775 at 2023-04-07 22:35:34\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 96, NEG: 4, best_predict: 1 with acc 0.81429 at 2023-04-07 22:37:16\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 100, NEG: 0, best_predict: 1 with acc 0.75 at 2023-04-07 22:37:18\n",
      "exp_596v38c2_price CZCE.MA309: POS: 28, NEG: 72, best_predict: 0 with acc 0.775 at 2023-04-07 22:37:20\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 88, NEG: 12, best_predict: 1 with acc 0.81429 at 2023-04-07 22:37:22\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 22:37:24\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 48, NEG: 52, best_predict: 1 with acc 0.775 at 2023-04-07 22:37:26\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 85, NEG: 15, best_predict: 0 with acc 0.81429 at 2023-04-07 22:37:28\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 97, NEG: 3, best_predict: 1 with acc 0.75 at 2023-04-07 22:37:30\n",
      "exp_596v38c2_price CZCE.TA309: POS: 73, NEG: 27, best_predict: 1 with acc 0.775 at 2023-04-07 22:37:32\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 77, NEG: 23, best_predict: 1 with acc 0.81429 at 2023-04-07 22:37:34\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 75, NEG: 25, best_predict: 1 with acc 0.75 at 2023-04-07 22:37:36\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 74, NEG: 26, best_predict: 1 with acc 0.775 at 2023-04-07 22:37:38\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 56, NEG: 44, best_predict: 0 with acc 0.81429 at 2023-04-07 22:37:40\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 62, NEG: 38, best_predict: 1 with acc 0.75 at 2023-04-07 22:37:42\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 29, NEG: 71, best_predict: 0 with acc 0.775 at 2023-04-07 22:37:44\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 44, NEG: 56, best_predict: 1 with acc 0.81429 at 2023-04-07 22:37:47\n",
      "exp_r4c7esy9 DCE.p2309: POS: 75, NEG: 25, best_predict: 1 with acc 0.75 at 2023-04-07 22:37:49\n",
      "exp_596v38c2_price DCE.p2309: POS: 55, NEG: 45, best_predict: 0 with acc 0.775 at 2023-04-07 22:37:51\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 76, NEG: 24, best_predict: 1 with acc 0.81429 at 2023-04-07 22:37:53\n",
      "exp_r4c7esy9 DCE.y2309: POS: 75, NEG: 25, best_predict: 1 with acc 0.75 at 2023-04-07 22:37:55\n",
      "exp_596v38c2_price DCE.y2309: POS: 35, NEG: 65, best_predict: 0 with acc 0.775 at 2023-04-07 22:37:57\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 81, NEG: 19, best_predict: 0 with acc 0.81429 at 2023-04-07 22:37:59\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 91, NEG: 9, best_predict: 1 with acc 0.75 at 2023-04-07 22:38:01\n",
      "exp_596v38c2_price CZCE.OI309: POS: 74, NEG: 26, best_predict: 0 with acc 0.775 at 2023-04-07 22:38:03\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 63, NEG: 37, best_predict: 1 with acc 0.81429 at 2023-04-07 22:38:05\n",
      "exp_r4c7esy9 DCE.a2309: POS: 34, NEG: 66, best_predict: 0 with acc 0.75 at 2023-04-07 22:38:07\n",
      "exp_596v38c2_price DCE.a2309: POS: 56, NEG: 44, best_predict: 0 with acc 0.775 at 2023-04-07 22:38:09\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 57, NEG: 43, best_predict: 1 with acc 0.81429 at 2023-04-07 22:38:11\n",
      "exp_r4c7esy9 DCE.m2309: POS: 54, NEG: 46, best_predict: 1 with acc 0.75 at 2023-04-07 22:38:13\n",
      "exp_596v38c2_price DCE.m2309: POS: 25, NEG: 75, best_predict: 1 with acc 0.775 at 2023-04-07 22:38:15\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 56, NEG: 44, best_predict: 0 with acc 0.81429 at 2023-04-07 22:38:17\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 55, NEG: 45, best_predict: 1 with acc 0.75 at 2023-04-07 22:38:19\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 27, NEG: 73, best_predict: 0 with acc 0.775 at 2023-04-07 22:38:21\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 90, NEG: 10, best_predict: 1 with acc 0.81429 at 2023-04-07 22:38:24\n",
      "exp_r4c7esy9 DCE.i2309: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 22:38:26\n",
      "exp_596v38c2_price DCE.i2309: POS: 44, NEG: 56, best_predict: 0 with acc 0.775 at 2023-04-07 22:38:28\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 91, NEG: 9, best_predict: 1 with acc 0.81429 at 2023-04-07 22:38:30\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 95, NEG: 5, best_predict: 1 with acc 0.75 at 2023-04-07 22:38:32\n",
      "exp_596v38c2_price DCE.jm2309: POS: 34, NEG: 66, best_predict: 0 with acc 0.775 at 2023-04-07 22:38:34\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 83, NEG: 17, best_predict: 0 with acc 0.81429 at 2023-04-07 22:38:36\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 89, NEG: 11, best_predict: 1 with acc 0.75 at 2023-04-07 22:38:38\n",
      "exp_596v38c2_price CZCE.SR309: POS: 84, NEG: 16, best_predict: 1 with acc 0.775 at 2023-04-07 22:38:40\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 11, NEG: 89, best_predict: 0 with acc 0.81429 at 2023-04-07 22:38:42\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 13, NEG: 87, best_predict: 0 with acc 0.75 at 2023-04-07 22:38:44\n",
      "exp_596v38c2_price CZCE.CF309: POS: 16, NEG: 84, best_predict: 0 with acc 0.775 at 2023-04-07 22:38:46\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 92, NEG: 8, best_predict: 1 with acc 0.81429 at 2023-04-07 22:38:48\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 97, NEG: 3, best_predict: 1 with acc 0.75 at 2023-04-07 22:38:50\n",
      "exp_596v38c2_price CZCE.SA309: POS: 35, NEG: 65, best_predict: 1 with acc 0.775 at 2023-04-07 22:38:52\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 58, NEG: 42, best_predict: 1 with acc 0.81429 at 2023-04-07 22:38:54\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 60, NEG: 40, best_predict: 1 with acc 0.75 at 2023-04-07 22:38:56\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 5, NEG: 95, best_predict: 0 with acc 0.775 at 2023-04-07 22:38:58\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 94, NEG: 6, best_predict: 1 with acc 0.81429 at 2023-04-07 22:40:41\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 100, NEG: 0, best_predict: 1 with acc 0.75 at 2023-04-07 22:40:43\n",
      "exp_596v38c2_price CZCE.MA309: POS: 30, NEG: 70, best_predict: 0 with acc 0.775 at 2023-04-07 22:40:45\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 82, NEG: 18, best_predict: 1 with acc 0.81429 at 2023-04-07 22:40:47\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 22:40:49\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 45, NEG: 55, best_predict: 1 with acc 0.775 at 2023-04-07 22:40:51\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 80, NEG: 20, best_predict: 1 with acc 0.81429 at 2023-04-07 22:40:53\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 95, NEG: 5, best_predict: 1 with acc 0.75 at 2023-04-07 22:40:55\n",
      "exp_596v38c2_price CZCE.TA309: POS: 65, NEG: 35, best_predict: 1 with acc 0.775 at 2023-04-07 22:40:57\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 67, NEG: 33, best_predict: 1 with acc 0.81429 at 2023-04-07 22:40:59\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 65, NEG: 35, best_predict: 1 with acc 0.75 at 2023-04-07 22:41:02\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 73, NEG: 27, best_predict: 1 with acc 0.775 at 2023-04-07 22:41:04\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 59, NEG: 41, best_predict: 0 with acc 0.81429 at 2023-04-07 22:41:06\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 58, NEG: 42, best_predict: 1 with acc 0.75 at 2023-04-07 22:41:08\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 28, NEG: 72, best_predict: 0 with acc 0.775 at 2023-04-07 22:41:10\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 44, NEG: 56, best_predict: 1 with acc 0.81429 at 2023-04-07 22:41:12\n",
      "exp_r4c7esy9 DCE.p2309: POS: 67, NEG: 33, best_predict: 0 with acc 0.75 at 2023-04-07 22:41:14\n",
      "exp_596v38c2_price DCE.p2309: POS: 54, NEG: 46, best_predict: 0 with acc 0.775 at 2023-04-07 22:41:16\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 66, NEG: 34, best_predict: 1 with acc 0.81429 at 2023-04-07 22:41:18\n",
      "exp_r4c7esy9 DCE.y2309: POS: 75, NEG: 25, best_predict: 1 with acc 0.75 at 2023-04-07 22:41:20\n",
      "exp_596v38c2_price DCE.y2309: POS: 30, NEG: 70, best_predict: 0 with acc 0.775 at 2023-04-07 22:41:22\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 76, NEG: 24, best_predict: 0 with acc 0.81429 at 2023-04-07 22:41:24\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 88, NEG: 12, best_predict: 1 with acc 0.75 at 2023-04-07 22:41:26\n",
      "exp_596v38c2_price CZCE.OI309: POS: 68, NEG: 32, best_predict: 0 with acc 0.775 at 2023-04-07 22:41:29\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 63, NEG: 37, best_predict: 1 with acc 0.81429 at 2023-04-07 22:41:31\n",
      "exp_r4c7esy9 DCE.a2309: POS: 32, NEG: 68, best_predict: 0 with acc 0.75 at 2023-04-07 22:41:33\n",
      "exp_596v38c2_price DCE.a2309: POS: 57, NEG: 43, best_predict: 0 with acc 0.775 at 2023-04-07 22:41:35\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 64, NEG: 36, best_predict: 1 with acc 0.81429 at 2023-04-07 22:41:37\n",
      "exp_r4c7esy9 DCE.m2309: POS: 63, NEG: 37, best_predict: 1 with acc 0.75 at 2023-04-07 22:41:39\n",
      "exp_596v38c2_price DCE.m2309: POS: 24, NEG: 76, best_predict: 1 with acc 0.775 at 2023-04-07 22:41:41\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 59, NEG: 41, best_predict: 0 with acc 0.81429 at 2023-04-07 22:41:43\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 55, NEG: 45, best_predict: 1 with acc 0.75 at 2023-04-07 22:41:45\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 27, NEG: 73, best_predict: 0 with acc 0.775 at 2023-04-07 22:41:47\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 90, NEG: 10, best_predict: 1 with acc 0.81429 at 2023-04-07 22:41:49\n",
      "exp_r4c7esy9 DCE.i2309: POS: 93, NEG: 7, best_predict: 1 with acc 0.75 at 2023-04-07 22:41:52\n",
      "exp_596v38c2_price DCE.i2309: POS: 41, NEG: 59, best_predict: 0 with acc 0.775 at 2023-04-07 22:41:54\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 87, NEG: 13, best_predict: 1 with acc 0.81429 at 2023-04-07 22:41:56\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 22:41:58\n",
      "exp_596v38c2_price DCE.jm2309: POS: 33, NEG: 67, best_predict: 0 with acc 0.775 at 2023-04-07 22:42:00\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 79, NEG: 21, best_predict: 0 with acc 0.81429 at 2023-04-07 22:42:02\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 89, NEG: 11, best_predict: 1 with acc 0.75 at 2023-04-07 22:42:04\n",
      "exp_596v38c2_price CZCE.SR309: POS: 78, NEG: 22, best_predict: 1 with acc 0.775 at 2023-04-07 22:42:06\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 13, NEG: 87, best_predict: 0 with acc 0.81429 at 2023-04-07 22:42:08\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 12, NEG: 88, best_predict: 0 with acc 0.75 at 2023-04-07 22:42:10\n",
      "exp_596v38c2_price CZCE.CF309: POS: 11, NEG: 89, best_predict: 0 with acc 0.775 at 2023-04-07 22:42:13\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 87, NEG: 13, best_predict: 1 with acc 0.81429 at 2023-04-07 22:42:15\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 22:42:17\n",
      "exp_596v38c2_price CZCE.SA309: POS: 36, NEG: 64, best_predict: 1 with acc 0.775 at 2023-04-07 22:42:19\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 57, NEG: 43, best_predict: 1 with acc 0.81429 at 2023-04-07 22:42:21\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 68, NEG: 32, best_predict: 1 with acc 0.75 at 2023-04-07 22:42:23\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 4, NEG: 96, best_predict: 0 with acc 0.775 at 2023-04-07 22:42:25\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 94, NEG: 6, best_predict: 1 with acc 0.81429 at 2023-04-07 22:44:07\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 100, NEG: 0, best_predict: 1 with acc 0.75 at 2023-04-07 22:44:09\n",
      "exp_596v38c2_price CZCE.MA309: POS: 30, NEG: 70, best_predict: 0 with acc 0.775 at 2023-04-07 22:44:11\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 82, NEG: 18, best_predict: 1 with acc 0.81429 at 2023-04-07 22:44:13\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 22:44:15\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 45, NEG: 55, best_predict: 1 with acc 0.775 at 2023-04-07 22:44:17\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 80, NEG: 20, best_predict: 1 with acc 0.81429 at 2023-04-07 22:44:19\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 95, NEG: 5, best_predict: 1 with acc 0.75 at 2023-04-07 22:44:21\n",
      "exp_596v38c2_price CZCE.TA309: POS: 65, NEG: 35, best_predict: 1 with acc 0.775 at 2023-04-07 22:44:23\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 67, NEG: 33, best_predict: 1 with acc 0.81429 at 2023-04-07 22:44:26\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 65, NEG: 35, best_predict: 1 with acc 0.75 at 2023-04-07 22:44:28\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 73, NEG: 27, best_predict: 1 with acc 0.775 at 2023-04-07 22:44:29\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 59, NEG: 41, best_predict: 0 with acc 0.81429 at 2023-04-07 22:44:32\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 58, NEG: 42, best_predict: 1 with acc 0.75 at 2023-04-07 22:44:34\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 28, NEG: 72, best_predict: 0 with acc 0.775 at 2023-04-07 22:44:36\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 44, NEG: 56, best_predict: 1 with acc 0.81429 at 2023-04-07 22:44:38\n",
      "exp_r4c7esy9 DCE.p2309: POS: 67, NEG: 33, best_predict: 0 with acc 0.75 at 2023-04-07 22:44:40\n",
      "exp_596v38c2_price DCE.p2309: POS: 54, NEG: 46, best_predict: 0 with acc 0.775 at 2023-04-07 22:44:42\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 65, NEG: 35, best_predict: 1 with acc 0.81429 at 2023-04-07 22:44:44\n",
      "exp_r4c7esy9 DCE.y2309: POS: 75, NEG: 25, best_predict: 1 with acc 0.75 at 2023-04-07 22:44:46\n",
      "exp_596v38c2_price DCE.y2309: POS: 30, NEG: 70, best_predict: 0 with acc 0.775 at 2023-04-07 22:44:48\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 76, NEG: 24, best_predict: 0 with acc 0.81429 at 2023-04-07 22:44:50\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 88, NEG: 12, best_predict: 1 with acc 0.75 at 2023-04-07 22:44:52\n",
      "exp_596v38c2_price CZCE.OI309: POS: 68, NEG: 32, best_predict: 0 with acc 0.775 at 2023-04-07 22:44:54\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 63, NEG: 37, best_predict: 1 with acc 0.81429 at 2023-04-07 22:44:56\n",
      "exp_r4c7esy9 DCE.a2309: POS: 32, NEG: 68, best_predict: 0 with acc 0.75 at 2023-04-07 22:44:58\n",
      "exp_596v38c2_price DCE.a2309: POS: 57, NEG: 43, best_predict: 0 with acc 0.775 at 2023-04-07 22:45:00\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 64, NEG: 36, best_predict: 1 with acc 0.81429 at 2023-04-07 22:45:02\n",
      "exp_r4c7esy9 DCE.m2309: POS: 63, NEG: 37, best_predict: 1 with acc 0.75 at 2023-04-07 22:45:04\n",
      "exp_596v38c2_price DCE.m2309: POS: 24, NEG: 76, best_predict: 1 with acc 0.775 at 2023-04-07 22:45:06\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 59, NEG: 41, best_predict: 0 with acc 0.81429 at 2023-04-07 22:45:08\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 55, NEG: 45, best_predict: 1 with acc 0.75 at 2023-04-07 22:45:10\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 27, NEG: 73, best_predict: 0 with acc 0.775 at 2023-04-07 22:45:12\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 90, NEG: 10, best_predict: 1 with acc 0.81429 at 2023-04-07 22:45:14\n",
      "exp_r4c7esy9 DCE.i2309: POS: 93, NEG: 7, best_predict: 1 with acc 0.75 at 2023-04-07 22:45:16\n",
      "exp_596v38c2_price DCE.i2309: POS: 41, NEG: 59, best_predict: 0 with acc 0.775 at 2023-04-07 22:45:18\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 87, NEG: 13, best_predict: 1 with acc 0.81429 at 2023-04-07 22:45:20\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 22:45:22\n",
      "exp_596v38c2_price DCE.jm2309: POS: 33, NEG: 67, best_predict: 0 with acc 0.775 at 2023-04-07 22:45:24\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 79, NEG: 21, best_predict: 0 with acc 0.81429 at 2023-04-07 22:45:26\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 89, NEG: 11, best_predict: 1 with acc 0.75 at 2023-04-07 22:45:28\n",
      "exp_596v38c2_price CZCE.SR309: POS: 78, NEG: 22, best_predict: 1 with acc 0.775 at 2023-04-07 22:45:30\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 13, NEG: 87, best_predict: 0 with acc 0.81429 at 2023-04-07 22:45:32\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 12, NEG: 88, best_predict: 0 with acc 0.75 at 2023-04-07 22:45:34\n",
      "exp_596v38c2_price CZCE.CF309: POS: 11, NEG: 89, best_predict: 0 with acc 0.775 at 2023-04-07 22:45:36\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 87, NEG: 13, best_predict: 1 with acc 0.81429 at 2023-04-07 22:45:38\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 22:45:41\n",
      "exp_596v38c2_price CZCE.SA309: POS: 36, NEG: 64, best_predict: 1 with acc 0.775 at 2023-04-07 22:45:42\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 57, NEG: 43, best_predict: 1 with acc 0.81429 at 2023-04-07 22:45:45\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 68, NEG: 32, best_predict: 1 with acc 0.75 at 2023-04-07 22:45:47\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 4, NEG: 96, best_predict: 0 with acc 0.775 at 2023-04-07 22:45:49\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 95, NEG: 5, best_predict: 1 with acc 0.81429 at 2023-04-07 22:47:31\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 100, NEG: 0, best_predict: 1 with acc 0.75 at 2023-04-07 22:47:33\n",
      "exp_596v38c2_price CZCE.MA309: POS: 28, NEG: 72, best_predict: 0 with acc 0.775 at 2023-04-07 22:47:35\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 82, NEG: 18, best_predict: 1 with acc 0.81429 at 2023-04-07 22:47:37\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 22:47:39\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 45, NEG: 55, best_predict: 1 with acc 0.775 at 2023-04-07 22:47:41\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 80, NEG: 20, best_predict: 1 with acc 0.81429 at 2023-04-07 22:47:43\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 95, NEG: 5, best_predict: 1 with acc 0.75 at 2023-04-07 22:47:45\n",
      "exp_596v38c2_price CZCE.TA309: POS: 66, NEG: 34, best_predict: 1 with acc 0.775 at 2023-04-07 22:47:47\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 68, NEG: 32, best_predict: 1 with acc 0.81429 at 2023-04-07 22:47:49\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 65, NEG: 35, best_predict: 1 with acc 0.75 at 2023-04-07 22:47:52\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 73, NEG: 27, best_predict: 1 with acc 0.775 at 2023-04-07 22:47:54\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 56, NEG: 44, best_predict: 0 with acc 0.81429 at 2023-04-07 22:47:56\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 62, NEG: 38, best_predict: 1 with acc 0.75 at 2023-04-07 22:47:58\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 31, NEG: 69, best_predict: 0 with acc 0.775 at 2023-04-07 22:48:00\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 47, NEG: 53, best_predict: 0 with acc 0.81429 at 2023-04-07 22:48:02\n",
      "exp_r4c7esy9 DCE.p2309: POS: 74, NEG: 26, best_predict: 0 with acc 0.75 at 2023-04-07 22:48:04\n",
      "exp_596v38c2_price DCE.p2309: POS: 56, NEG: 44, best_predict: 0 with acc 0.775 at 2023-04-07 22:48:06\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 72, NEG: 28, best_predict: 1 with acc 0.81429 at 2023-04-07 22:48:08\n",
      "exp_r4c7esy9 DCE.y2309: POS: 77, NEG: 23, best_predict: 1 with acc 0.75 at 2023-04-07 22:48:10\n",
      "exp_596v38c2_price DCE.y2309: POS: 32, NEG: 68, best_predict: 0 with acc 0.775 at 2023-04-07 22:48:12\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 80, NEG: 20, best_predict: 0 with acc 0.81429 at 2023-04-07 22:48:14\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 90, NEG: 10, best_predict: 1 with acc 0.75 at 2023-04-07 22:48:16\n",
      "exp_596v38c2_price CZCE.OI309: POS: 68, NEG: 32, best_predict: 0 with acc 0.775 at 2023-04-07 22:48:18\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 62, NEG: 38, best_predict: 1 with acc 0.81429 at 2023-04-07 22:48:20\n",
      "exp_r4c7esy9 DCE.a2309: POS: 35, NEG: 65, best_predict: 0 with acc 0.75 at 2023-04-07 22:48:22\n",
      "exp_596v38c2_price DCE.a2309: POS: 58, NEG: 42, best_predict: 0 with acc 0.775 at 2023-04-07 22:48:24\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 66, NEG: 34, best_predict: 1 with acc 0.81429 at 2023-04-07 22:48:27\n",
      "exp_r4c7esy9 DCE.m2309: POS: 65, NEG: 35, best_predict: 1 with acc 0.75 at 2023-04-07 22:48:29\n",
      "exp_596v38c2_price DCE.m2309: POS: 25, NEG: 75, best_predict: 1 with acc 0.775 at 2023-04-07 22:48:31\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 53, NEG: 47, best_predict: 0 with acc 0.81429 at 2023-04-07 22:48:33\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 49, NEG: 51, best_predict: 1 with acc 0.75 at 2023-04-07 22:48:35\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 27, NEG: 73, best_predict: 0 with acc 0.775 at 2023-04-07 22:48:37\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 90, NEG: 10, best_predict: 1 with acc 0.81429 at 2023-04-07 22:48:39\n",
      "exp_r4c7esy9 DCE.i2309: POS: 93, NEG: 7, best_predict: 1 with acc 0.75 at 2023-04-07 22:48:41\n",
      "exp_596v38c2_price DCE.i2309: POS: 38, NEG: 62, best_predict: 0 with acc 0.775 at 2023-04-07 22:48:44\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 88, NEG: 12, best_predict: 1 with acc 0.81429 at 2023-04-07 22:48:46\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 22:48:48\n",
      "exp_596v38c2_price DCE.jm2309: POS: 32, NEG: 68, best_predict: 0 with acc 0.775 at 2023-04-07 22:48:50\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 71, NEG: 29, best_predict: 0 with acc 0.81429 at 2023-04-07 22:48:52\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 81, NEG: 19, best_predict: 1 with acc 0.75 at 2023-04-07 22:48:54\n",
      "exp_596v38c2_price CZCE.SR309: POS: 77, NEG: 23, best_predict: 1 with acc 0.775 at 2023-04-07 22:48:56\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 10, NEG: 90, best_predict: 0 with acc 0.81429 at 2023-04-07 22:48:58\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 10, NEG: 90, best_predict: 0 with acc 0.75 at 2023-04-07 22:49:00\n",
      "exp_596v38c2_price CZCE.CF309: POS: 11, NEG: 89, best_predict: 0 with acc 0.775 at 2023-04-07 22:49:02\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 87, NEG: 13, best_predict: 1 with acc 0.81429 at 2023-04-07 22:49:04\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 93, NEG: 7, best_predict: 1 with acc 0.75 at 2023-04-07 22:49:06\n",
      "exp_596v38c2_price CZCE.SA309: POS: 39, NEG: 61, best_predict: 1 with acc 0.775 at 2023-04-07 22:49:08\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 60, NEG: 40, best_predict: 1 with acc 0.81429 at 2023-04-07 22:49:11\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 74, NEG: 26, best_predict: 1 with acc 0.75 at 2023-04-07 22:49:13\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 4, NEG: 96, best_predict: 0 with acc 0.775 at 2023-04-07 22:49:15\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 96, NEG: 4, best_predict: 1 with acc 0.81429 at 2023-04-07 22:50:57\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 100, NEG: 0, best_predict: 1 with acc 0.75 at 2023-04-07 22:50:59\n",
      "exp_596v38c2_price CZCE.MA309: POS: 31, NEG: 69, best_predict: 0 with acc 0.775 at 2023-04-07 22:51:01\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 79, NEG: 21, best_predict: 1 with acc 0.81429 at 2023-04-07 22:51:03\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 95, NEG: 5, best_predict: 1 with acc 0.75 at 2023-04-07 22:51:05\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 45, NEG: 55, best_predict: 1 with acc 0.775 at 2023-04-07 22:51:07\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 75, NEG: 25, best_predict: 1 with acc 0.81429 at 2023-04-07 22:51:09\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 92, NEG: 8, best_predict: 1 with acc 0.75 at 2023-04-07 22:51:11\n",
      "exp_596v38c2_price CZCE.TA309: POS: 50, NEG: 50, best_predict: 1 with acc 0.775 at 2023-04-07 22:51:13\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 68, NEG: 32, best_predict: 1 with acc 0.81429 at 2023-04-07 22:51:15\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 57, NEG: 43, best_predict: 1 with acc 0.75 at 2023-04-07 22:51:17\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 67, NEG: 33, best_predict: 1 with acc 0.775 at 2023-04-07 22:51:19\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 62, NEG: 38, best_predict: 0 with acc 0.81429 at 2023-04-07 22:51:21\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 61, NEG: 39, best_predict: 1 with acc 0.75 at 2023-04-07 22:51:23\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 27, NEG: 73, best_predict: 0 with acc 0.775 at 2023-04-07 22:51:25\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 44, NEG: 56, best_predict: 0 with acc 0.81429 at 2023-04-07 22:51:27\n",
      "exp_r4c7esy9 DCE.p2309: POS: 67, NEG: 33, best_predict: 0 with acc 0.75 at 2023-04-07 22:51:29\n",
      "exp_596v38c2_price DCE.p2309: POS: 50, NEG: 50, best_predict: 0 with acc 0.775 at 2023-04-07 22:51:31\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 56, NEG: 44, best_predict: 0 with acc 0.81429 at 2023-04-07 22:51:33\n",
      "exp_r4c7esy9 DCE.y2309: POS: 74, NEG: 26, best_predict: 1 with acc 0.75 at 2023-04-07 22:51:35\n",
      "exp_596v38c2_price DCE.y2309: POS: 33, NEG: 67, best_predict: 0 with acc 0.775 at 2023-04-07 22:51:37\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 77, NEG: 23, best_predict: 0 with acc 0.81429 at 2023-04-07 22:51:39\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 85, NEG: 15, best_predict: 1 with acc 0.75 at 2023-04-07 22:51:41\n",
      "exp_596v38c2_price CZCE.OI309: POS: 52, NEG: 48, best_predict: 0 with acc 0.775 at 2023-04-07 22:51:43\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 59, NEG: 41, best_predict: 1 with acc 0.81429 at 2023-04-07 22:51:45\n",
      "exp_r4c7esy9 DCE.a2309: POS: 33, NEG: 67, best_predict: 0 with acc 0.75 at 2023-04-07 22:51:47\n",
      "exp_596v38c2_price DCE.a2309: POS: 66, NEG: 34, best_predict: 0 with acc 0.775 at 2023-04-07 22:51:49\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 71, NEG: 29, best_predict: 1 with acc 0.81429 at 2023-04-07 22:51:51\n",
      "exp_r4c7esy9 DCE.m2309: POS: 74, NEG: 26, best_predict: 1 with acc 0.75 at 2023-04-07 22:51:53\n",
      "exp_596v38c2_price DCE.m2309: POS: 24, NEG: 76, best_predict: 1 with acc 0.775 at 2023-04-07 22:51:56\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 46, NEG: 54, best_predict: 0 with acc 0.81429 at 2023-04-07 22:51:58\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 44, NEG: 56, best_predict: 1 with acc 0.75 at 2023-04-07 22:52:00\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 24, NEG: 76, best_predict: 0 with acc 0.775 at 2023-04-07 22:52:02\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 91, NEG: 9, best_predict: 1 with acc 0.81429 at 2023-04-07 22:52:04\n",
      "exp_r4c7esy9 DCE.i2309: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 22:52:06\n",
      "exp_596v38c2_price DCE.i2309: POS: 39, NEG: 61, best_predict: 0 with acc 0.775 at 2023-04-07 22:52:08\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 86, NEG: 14, best_predict: 1 with acc 0.81429 at 2023-04-07 22:52:10\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 91, NEG: 9, best_predict: 1 with acc 0.75 at 2023-04-07 22:52:12\n",
      "exp_596v38c2_price DCE.jm2309: POS: 34, NEG: 66, best_predict: 1 with acc 0.775 at 2023-04-07 22:52:14\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 65, NEG: 35, best_predict: 0 with acc 0.81429 at 2023-04-07 22:52:16\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 77, NEG: 23, best_predict: 1 with acc 0.75 at 2023-04-07 22:52:18\n",
      "exp_596v38c2_price CZCE.SR309: POS: 70, NEG: 30, best_predict: 1 with acc 0.775 at 2023-04-07 22:52:20\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 11, NEG: 89, best_predict: 0 with acc 0.81429 at 2023-04-07 22:52:22\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 9, NEG: 91, best_predict: 0 with acc 0.75 at 2023-04-07 22:52:24\n",
      "exp_596v38c2_price CZCE.CF309: POS: 17, NEG: 83, best_predict: 0 with acc 0.775 at 2023-04-07 22:52:26\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 83, NEG: 17, best_predict: 1 with acc 0.81429 at 2023-04-07 22:52:28\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 86, NEG: 14, best_predict: 1 with acc 0.75 at 2023-04-07 22:52:30\n",
      "exp_596v38c2_price CZCE.SA309: POS: 41, NEG: 59, best_predict: 1 with acc 0.775 at 2023-04-07 22:52:32\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 61, NEG: 39, best_predict: 1 with acc 0.81429 at 2023-04-07 22:52:35\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 75, NEG: 25, best_predict: 1 with acc 0.75 at 2023-04-07 22:52:37\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 2, NEG: 98, best_predict: 0 with acc 0.775 at 2023-04-07 22:52:39\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 96, NEG: 4, best_predict: 1 with acc 0.81429 at 2023-04-07 22:54:21\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 100, NEG: 0, best_predict: 1 with acc 0.75 at 2023-04-07 22:54:23\n",
      "exp_596v38c2_price CZCE.MA309: POS: 31, NEG: 69, best_predict: 0 with acc 0.775 at 2023-04-07 22:54:25\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 79, NEG: 21, best_predict: 1 with acc 0.81429 at 2023-04-07 22:54:27\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 95, NEG: 5, best_predict: 1 with acc 0.75 at 2023-04-07 22:54:29\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 45, NEG: 55, best_predict: 1 with acc 0.775 at 2023-04-07 22:54:31\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 75, NEG: 25, best_predict: 1 with acc 0.81429 at 2023-04-07 22:54:33\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 92, NEG: 8, best_predict: 1 with acc 0.75 at 2023-04-07 22:54:35\n",
      "exp_596v38c2_price CZCE.TA309: POS: 50, NEG: 50, best_predict: 1 with acc 0.775 at 2023-04-07 22:54:37\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 68, NEG: 32, best_predict: 1 with acc 0.81429 at 2023-04-07 22:54:39\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 57, NEG: 43, best_predict: 1 with acc 0.75 at 2023-04-07 22:54:41\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 67, NEG: 33, best_predict: 1 with acc 0.775 at 2023-04-07 22:54:43\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 62, NEG: 38, best_predict: 0 with acc 0.81429 at 2023-04-07 22:54:45\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 62, NEG: 38, best_predict: 1 with acc 0.75 at 2023-04-07 22:54:47\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 27, NEG: 73, best_predict: 0 with acc 0.775 at 2023-04-07 22:54:49\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 44, NEG: 56, best_predict: 0 with acc 0.81429 at 2023-04-07 22:54:51\n",
      "exp_r4c7esy9 DCE.p2309: POS: 67, NEG: 33, best_predict: 0 with acc 0.75 at 2023-04-07 22:54:53\n",
      "exp_596v38c2_price DCE.p2309: POS: 50, NEG: 50, best_predict: 0 with acc 0.775 at 2023-04-07 22:54:55\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 56, NEG: 44, best_predict: 0 with acc 0.81429 at 2023-04-07 22:54:57\n",
      "exp_r4c7esy9 DCE.y2309: POS: 74, NEG: 26, best_predict: 1 with acc 0.75 at 2023-04-07 22:54:59\n",
      "exp_596v38c2_price DCE.y2309: POS: 33, NEG: 67, best_predict: 0 with acc 0.775 at 2023-04-07 22:55:01\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 77, NEG: 23, best_predict: 0 with acc 0.81429 at 2023-04-07 22:55:03\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 85, NEG: 15, best_predict: 1 with acc 0.75 at 2023-04-07 22:55:05\n",
      "exp_596v38c2_price CZCE.OI309: POS: 52, NEG: 48, best_predict: 0 with acc 0.775 at 2023-04-07 22:55:07\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 59, NEG: 41, best_predict: 1 with acc 0.81429 at 2023-04-07 22:55:10\n",
      "exp_r4c7esy9 DCE.a2309: POS: 33, NEG: 67, best_predict: 0 with acc 0.75 at 2023-04-07 22:55:12\n",
      "exp_596v38c2_price DCE.a2309: POS: 66, NEG: 34, best_predict: 0 with acc 0.775 at 2023-04-07 22:55:14\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 71, NEG: 29, best_predict: 1 with acc 0.81429 at 2023-04-07 22:55:16\n",
      "exp_r4c7esy9 DCE.m2309: POS: 74, NEG: 26, best_predict: 1 with acc 0.75 at 2023-04-07 22:55:18\n",
      "exp_596v38c2_price DCE.m2309: POS: 24, NEG: 76, best_predict: 1 with acc 0.775 at 2023-04-07 22:55:20\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 46, NEG: 54, best_predict: 0 with acc 0.81429 at 2023-04-07 22:55:22\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 44, NEG: 56, best_predict: 1 with acc 0.75 at 2023-04-07 22:55:24\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 23, NEG: 77, best_predict: 0 with acc 0.775 at 2023-04-07 22:55:26\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 91, NEG: 9, best_predict: 1 with acc 0.81429 at 2023-04-07 22:55:28\n",
      "exp_r4c7esy9 DCE.i2309: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 22:55:30\n",
      "exp_596v38c2_price DCE.i2309: POS: 39, NEG: 61, best_predict: 0 with acc 0.775 at 2023-04-07 22:55:32\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 86, NEG: 14, best_predict: 1 with acc 0.81429 at 2023-04-07 22:55:34\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 91, NEG: 9, best_predict: 1 with acc 0.75 at 2023-04-07 22:55:36\n",
      "exp_596v38c2_price DCE.jm2309: POS: 34, NEG: 66, best_predict: 1 with acc 0.775 at 2023-04-07 22:55:38\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 65, NEG: 35, best_predict: 0 with acc 0.81429 at 2023-04-07 22:55:40\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 77, NEG: 23, best_predict: 1 with acc 0.75 at 2023-04-07 22:55:42\n",
      "exp_596v38c2_price CZCE.SR309: POS: 70, NEG: 30, best_predict: 1 with acc 0.775 at 2023-04-07 22:55:44\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 11, NEG: 89, best_predict: 0 with acc 0.81429 at 2023-04-07 22:55:46\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 10, NEG: 90, best_predict: 0 with acc 0.75 at 2023-04-07 22:55:48\n",
      "exp_596v38c2_price CZCE.CF309: POS: 16, NEG: 84, best_predict: 0 with acc 0.775 at 2023-04-07 22:55:50\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 83, NEG: 17, best_predict: 1 with acc 0.81429 at 2023-04-07 22:55:52\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 86, NEG: 14, best_predict: 1 with acc 0.75 at 2023-04-07 22:55:55\n",
      "exp_596v38c2_price CZCE.SA309: POS: 41, NEG: 59, best_predict: 1 with acc 0.775 at 2023-04-07 22:55:57\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 61, NEG: 39, best_predict: 1 with acc 0.81429 at 2023-04-07 22:55:59\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 75, NEG: 25, best_predict: 1 with acc 0.75 at 2023-04-07 22:56:01\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 2, NEG: 98, best_predict: 0 with acc 0.775 at 2023-04-07 22:56:03\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 96, NEG: 4, best_predict: 1 with acc 0.81429 at 2023-04-07 22:57:45\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 100, NEG: 0, best_predict: 1 with acc 0.75 at 2023-04-07 22:57:47\n",
      "exp_596v38c2_price CZCE.MA309: POS: 31, NEG: 69, best_predict: 0 with acc 0.775 at 2023-04-07 22:57:49\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 80, NEG: 20, best_predict: 1 with acc 0.81429 at 2023-04-07 22:57:51\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 95, NEG: 5, best_predict: 1 with acc 0.75 at 2023-04-07 22:57:53\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 45, NEG: 55, best_predict: 1 with acc 0.775 at 2023-04-07 22:57:55\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 75, NEG: 25, best_predict: 1 with acc 0.81429 at 2023-04-07 22:57:57\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 92, NEG: 8, best_predict: 1 with acc 0.75 at 2023-04-07 22:57:59\n",
      "exp_596v38c2_price CZCE.TA309: POS: 50, NEG: 50, best_predict: 1 with acc 0.775 at 2023-04-07 22:58:01\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 68, NEG: 32, best_predict: 1 with acc 0.81429 at 2023-04-07 22:58:04\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 57, NEG: 43, best_predict: 1 with acc 0.75 at 2023-04-07 22:58:06\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 67, NEG: 33, best_predict: 1 with acc 0.775 at 2023-04-07 22:58:08\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 62, NEG: 38, best_predict: 0 with acc 0.81429 at 2023-04-07 22:58:10\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 62, NEG: 38, best_predict: 1 with acc 0.75 at 2023-04-07 22:58:12\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 27, NEG: 73, best_predict: 0 with acc 0.775 at 2023-04-07 22:58:14\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 44, NEG: 56, best_predict: 0 with acc 0.81429 at 2023-04-07 22:58:16\n",
      "exp_r4c7esy9 DCE.p2309: POS: 67, NEG: 33, best_predict: 0 with acc 0.75 at 2023-04-07 22:58:18\n",
      "exp_596v38c2_price DCE.p2309: POS: 50, NEG: 50, best_predict: 0 with acc 0.775 at 2023-04-07 22:58:20\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 56, NEG: 44, best_predict: 0 with acc 0.81429 at 2023-04-07 22:58:22\n",
      "exp_r4c7esy9 DCE.y2309: POS: 74, NEG: 26, best_predict: 1 with acc 0.75 at 2023-04-07 22:58:24\n",
      "exp_596v38c2_price DCE.y2309: POS: 33, NEG: 67, best_predict: 0 with acc 0.775 at 2023-04-07 22:58:26\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 77, NEG: 23, best_predict: 0 with acc 0.81429 at 2023-04-07 22:58:28\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 85, NEG: 15, best_predict: 1 with acc 0.75 at 2023-04-07 22:58:30\n",
      "exp_596v38c2_price CZCE.OI309: POS: 52, NEG: 48, best_predict: 0 with acc 0.775 at 2023-04-07 22:58:33\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 59, NEG: 41, best_predict: 1 with acc 0.81429 at 2023-04-07 22:58:35\n",
      "exp_r4c7esy9 DCE.a2309: POS: 33, NEG: 67, best_predict: 0 with acc 0.75 at 2023-04-07 22:58:37\n",
      "exp_596v38c2_price DCE.a2309: POS: 66, NEG: 34, best_predict: 0 with acc 0.775 at 2023-04-07 22:58:39\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 71, NEG: 29, best_predict: 1 with acc 0.81429 at 2023-04-07 22:58:41\n",
      "exp_r4c7esy9 DCE.m2309: POS: 74, NEG: 26, best_predict: 1 with acc 0.75 at 2023-04-07 22:58:43\n",
      "exp_596v38c2_price DCE.m2309: POS: 24, NEG: 76, best_predict: 1 with acc 0.775 at 2023-04-07 22:58:45\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 46, NEG: 54, best_predict: 0 with acc 0.81429 at 2023-04-07 22:58:47\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 46, NEG: 54, best_predict: 1 with acc 0.75 at 2023-04-07 22:58:49\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 22, NEG: 78, best_predict: 0 with acc 0.775 at 2023-04-07 22:58:51\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 91, NEG: 9, best_predict: 1 with acc 0.81429 at 2023-04-07 22:58:53\n",
      "exp_r4c7esy9 DCE.i2309: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 22:58:55\n",
      "exp_596v38c2_price DCE.i2309: POS: 39, NEG: 61, best_predict: 0 with acc 0.775 at 2023-04-07 22:58:57\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 86, NEG: 14, best_predict: 1 with acc 0.81429 at 2023-04-07 22:58:59\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 91, NEG: 9, best_predict: 1 with acc 0.75 at 2023-04-07 22:59:01\n",
      "exp_596v38c2_price DCE.jm2309: POS: 34, NEG: 66, best_predict: 1 with acc 0.775 at 2023-04-07 22:59:03\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 65, NEG: 35, best_predict: 0 with acc 0.81429 at 2023-04-07 22:59:05\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 77, NEG: 23, best_predict: 1 with acc 0.75 at 2023-04-07 22:59:07\n",
      "exp_596v38c2_price CZCE.SR309: POS: 70, NEG: 30, best_predict: 1 with acc 0.775 at 2023-04-07 22:59:09\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 11, NEG: 89, best_predict: 0 with acc 0.81429 at 2023-04-07 22:59:11\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 10, NEG: 90, best_predict: 0 with acc 0.75 at 2023-04-07 22:59:13\n",
      "exp_596v38c2_price CZCE.CF309: POS: 17, NEG: 83, best_predict: 0 with acc 0.775 at 2023-04-07 22:59:15\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 83, NEG: 17, best_predict: 1 with acc 0.81429 at 2023-04-07 22:59:17\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 86, NEG: 14, best_predict: 1 with acc 0.75 at 2023-04-07 22:59:19\n",
      "exp_596v38c2_price CZCE.SA309: POS: 41, NEG: 59, best_predict: 1 with acc 0.775 at 2023-04-07 22:59:21\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 61, NEG: 39, best_predict: 1 with acc 0.81429 at 2023-04-07 22:59:23\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 75, NEG: 25, best_predict: 1 with acc 0.75 at 2023-04-07 22:59:25\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 2, NEG: 98, best_predict: 0 with acc 0.775 at 2023-04-07 22:59:28\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 96, NEG: 4, best_predict: 1 with acc 0.81429 at 2023-04-07 23:01:10\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 100, NEG: 0, best_predict: 1 with acc 0.75 at 2023-04-07 23:01:12\n",
      "exp_596v38c2_price CZCE.MA309: POS: 31, NEG: 69, best_predict: 0 with acc 0.775 at 2023-04-07 23:01:14\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 79, NEG: 21, best_predict: 1 with acc 0.81429 at 2023-04-07 23:01:16\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 95, NEG: 5, best_predict: 1 with acc 0.75 at 2023-04-07 23:01:18\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 45, NEG: 55, best_predict: 1 with acc 0.775 at 2023-04-07 23:01:20\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 75, NEG: 25, best_predict: 1 with acc 0.81429 at 2023-04-07 23:01:22\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 92, NEG: 8, best_predict: 1 with acc 0.75 at 2023-04-07 23:01:24\n",
      "exp_596v38c2_price CZCE.TA309: POS: 50, NEG: 50, best_predict: 1 with acc 0.775 at 2023-04-07 23:01:25\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 68, NEG: 32, best_predict: 1 with acc 0.81429 at 2023-04-07 23:01:27\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 57, NEG: 43, best_predict: 1 with acc 0.75 at 2023-04-07 23:01:29\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 67, NEG: 33, best_predict: 1 with acc 0.775 at 2023-04-07 23:01:31\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 62, NEG: 38, best_predict: 0 with acc 0.81429 at 2023-04-07 23:01:33\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 62, NEG: 38, best_predict: 1 with acc 0.75 at 2023-04-07 23:01:35\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 27, NEG: 73, best_predict: 0 with acc 0.775 at 2023-04-07 23:01:37\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 44, NEG: 56, best_predict: 0 with acc 0.81429 at 2023-04-07 23:01:39\n",
      "exp_r4c7esy9 DCE.p2309: POS: 67, NEG: 33, best_predict: 0 with acc 0.75 at 2023-04-07 23:01:41\n",
      "exp_596v38c2_price DCE.p2309: POS: 50, NEG: 50, best_predict: 0 with acc 0.775 at 2023-04-07 23:01:43\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 56, NEG: 44, best_predict: 0 with acc 0.81429 at 2023-04-07 23:01:45\n",
      "exp_r4c7esy9 DCE.y2309: POS: 74, NEG: 26, best_predict: 1 with acc 0.75 at 2023-04-07 23:01:47\n",
      "exp_596v38c2_price DCE.y2309: POS: 33, NEG: 67, best_predict: 0 with acc 0.775 at 2023-04-07 23:01:49\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 77, NEG: 23, best_predict: 0 with acc 0.81429 at 2023-04-07 23:01:51\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 85, NEG: 15, best_predict: 1 with acc 0.75 at 2023-04-07 23:01:53\n",
      "exp_596v38c2_price CZCE.OI309: POS: 52, NEG: 48, best_predict: 0 with acc 0.775 at 2023-04-07 23:01:55\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 59, NEG: 41, best_predict: 1 with acc 0.81429 at 2023-04-07 23:01:57\n",
      "exp_r4c7esy9 DCE.a2309: POS: 33, NEG: 67, best_predict: 0 with acc 0.75 at 2023-04-07 23:01:59\n",
      "exp_596v38c2_price DCE.a2309: POS: 66, NEG: 34, best_predict: 0 with acc 0.775 at 2023-04-07 23:02:01\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 71, NEG: 29, best_predict: 1 with acc 0.81429 at 2023-04-07 23:02:03\n",
      "exp_r4c7esy9 DCE.m2309: POS: 74, NEG: 26, best_predict: 1 with acc 0.75 at 2023-04-07 23:02:05\n",
      "exp_596v38c2_price DCE.m2309: POS: 24, NEG: 76, best_predict: 1 with acc 0.775 at 2023-04-07 23:02:07\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 46, NEG: 54, best_predict: 0 with acc 0.81429 at 2023-04-07 23:02:09\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 46, NEG: 54, best_predict: 1 with acc 0.75 at 2023-04-07 23:02:11\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 22, NEG: 78, best_predict: 0 with acc 0.775 at 2023-04-07 23:02:13\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 91, NEG: 9, best_predict: 1 with acc 0.81429 at 2023-04-07 23:02:15\n",
      "exp_r4c7esy9 DCE.i2309: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-07 23:02:17\n",
      "exp_596v38c2_price DCE.i2309: POS: 39, NEG: 61, best_predict: 0 with acc 0.775 at 2023-04-07 23:02:19\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 86, NEG: 14, best_predict: 1 with acc 0.81429 at 2023-04-07 23:02:20\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 91, NEG: 9, best_predict: 1 with acc 0.75 at 2023-04-07 23:02:22\n",
      "exp_596v38c2_price DCE.jm2309: POS: 34, NEG: 66, best_predict: 1 with acc 0.775 at 2023-04-07 23:02:24\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 65, NEG: 35, best_predict: 0 with acc 0.81429 at 2023-04-07 23:02:26\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 77, NEG: 23, best_predict: 1 with acc 0.75 at 2023-04-07 23:02:28\n",
      "exp_596v38c2_price CZCE.SR309: POS: 70, NEG: 30, best_predict: 1 with acc 0.775 at 2023-04-07 23:02:30\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 11, NEG: 89, best_predict: 0 with acc 0.81429 at 2023-04-07 23:02:32\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 10, NEG: 90, best_predict: 0 with acc 0.75 at 2023-04-07 23:02:34\n",
      "exp_596v38c2_price CZCE.CF309: POS: 16, NEG: 84, best_predict: 0 with acc 0.775 at 2023-04-07 23:02:36\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 83, NEG: 17, best_predict: 1 with acc 0.81429 at 2023-04-07 23:02:38\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 86, NEG: 14, best_predict: 1 with acc 0.75 at 2023-04-07 23:02:40\n",
      "exp_596v38c2_price CZCE.SA309: POS: 41, NEG: 59, best_predict: 1 with acc 0.775 at 2023-04-07 23:02:42\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 61, NEG: 39, best_predict: 1 with acc 0.81429 at 2023-04-07 23:02:44\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 75, NEG: 25, best_predict: 1 with acc 0.75 at 2023-04-07 23:02:46\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 2, NEG: 98, best_predict: 0 with acc 0.775 at 2023-04-07 23:02:48\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 96, NEG: 4, best_predict: 1 with acc 0.81429 at 2023-04-08 01:00:03\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 100, NEG: 0, best_predict: 1 with acc 0.75 at 2023-04-08 01:00:05\n",
      "exp_596v38c2_price CZCE.MA309: POS: 31, NEG: 69, best_predict: 0 with acc 0.775 at 2023-04-08 01:00:07\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 79, NEG: 21, best_predict: 1 with acc 0.81429 at 2023-04-08 01:00:09\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 95, NEG: 5, best_predict: 1 with acc 0.75 at 2023-04-08 01:00:11\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 45, NEG: 55, best_predict: 1 with acc 0.775 at 2023-04-08 01:00:13\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 75, NEG: 25, best_predict: 1 with acc 0.81429 at 2023-04-08 01:00:15\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 92, NEG: 8, best_predict: 1 with acc 0.75 at 2023-04-08 01:00:17\n",
      "exp_596v38c2_price CZCE.TA309: POS: 50, NEG: 50, best_predict: 1 with acc 0.775 at 2023-04-08 01:00:19\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 68, NEG: 32, best_predict: 1 with acc 0.81429 at 2023-04-08 01:00:21\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 57, NEG: 43, best_predict: 1 with acc 0.75 at 2023-04-08 01:00:23\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 67, NEG: 33, best_predict: 1 with acc 0.775 at 2023-04-08 01:00:25\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 62, NEG: 38, best_predict: 0 with acc 0.81429 at 2023-04-08 01:00:27\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 62, NEG: 38, best_predict: 1 with acc 0.75 at 2023-04-08 01:00:29\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 27, NEG: 73, best_predict: 0 with acc 0.775 at 2023-04-08 01:00:32\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 44, NEG: 56, best_predict: 0 with acc 0.81429 at 2023-04-08 01:00:34\n",
      "exp_r4c7esy9 DCE.p2309: POS: 67, NEG: 33, best_predict: 0 with acc 0.75 at 2023-04-08 01:00:36\n",
      "exp_596v38c2_price DCE.p2309: POS: 50, NEG: 50, best_predict: 0 with acc 0.775 at 2023-04-08 01:00:38\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 56, NEG: 44, best_predict: 0 with acc 0.81429 at 2023-04-08 01:00:40\n",
      "exp_r4c7esy9 DCE.y2309: POS: 74, NEG: 26, best_predict: 1 with acc 0.75 at 2023-04-08 01:00:42\n",
      "exp_596v38c2_price DCE.y2309: POS: 33, NEG: 67, best_predict: 0 with acc 0.775 at 2023-04-08 01:00:44\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 77, NEG: 23, best_predict: 0 with acc 0.81429 at 2023-04-08 01:00:46\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 85, NEG: 15, best_predict: 1 with acc 0.75 at 2023-04-08 01:00:48\n",
      "exp_596v38c2_price CZCE.OI309: POS: 52, NEG: 48, best_predict: 0 with acc 0.775 at 2023-04-08 01:00:50\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 59, NEG: 41, best_predict: 1 with acc 0.81429 at 2023-04-08 01:00:52\n",
      "exp_r4c7esy9 DCE.a2309: POS: 33, NEG: 67, best_predict: 0 with acc 0.75 at 2023-04-08 01:00:54\n",
      "exp_596v38c2_price DCE.a2309: POS: 66, NEG: 34, best_predict: 0 with acc 0.775 at 2023-04-08 01:00:57\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 71, NEG: 29, best_predict: 1 with acc 0.81429 at 2023-04-08 01:00:59\n",
      "exp_r4c7esy9 DCE.m2309: POS: 74, NEG: 26, best_predict: 1 with acc 0.75 at 2023-04-08 01:01:01\n",
      "exp_596v38c2_price DCE.m2309: POS: 24, NEG: 76, best_predict: 1 with acc 0.775 at 2023-04-08 01:01:03\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 46, NEG: 54, best_predict: 0 with acc 0.81429 at 2023-04-08 01:01:05\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 46, NEG: 54, best_predict: 1 with acc 0.75 at 2023-04-08 01:01:07\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 22, NEG: 78, best_predict: 0 with acc 0.775 at 2023-04-08 01:01:09\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 91, NEG: 9, best_predict: 1 with acc 0.81429 at 2023-04-08 01:01:11\n",
      "exp_r4c7esy9 DCE.i2309: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-08 01:01:13\n",
      "exp_596v38c2_price DCE.i2309: POS: 39, NEG: 61, best_predict: 0 with acc 0.775 at 2023-04-08 01:01:15\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 86, NEG: 14, best_predict: 1 with acc 0.81429 at 2023-04-08 01:01:17\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 91, NEG: 9, best_predict: 1 with acc 0.75 at 2023-04-08 01:01:19\n",
      "exp_596v38c2_price DCE.jm2309: POS: 34, NEG: 66, best_predict: 1 with acc 0.775 at 2023-04-08 01:01:21\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 65, NEG: 35, best_predict: 0 with acc 0.81429 at 2023-04-08 01:01:24\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 77, NEG: 23, best_predict: 1 with acc 0.75 at 2023-04-08 01:01:26\n",
      "exp_596v38c2_price CZCE.SR309: POS: 70, NEG: 30, best_predict: 1 with acc 0.775 at 2023-04-08 01:01:28\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 11, NEG: 89, best_predict: 0 with acc 0.81429 at 2023-04-08 01:01:30\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 10, NEG: 90, best_predict: 0 with acc 0.75 at 2023-04-08 01:01:32\n",
      "exp_596v38c2_price CZCE.CF309: POS: 16, NEG: 84, best_predict: 0 with acc 0.775 at 2023-04-08 01:01:34\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 83, NEG: 17, best_predict: 1 with acc 0.81429 at 2023-04-08 01:01:36\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 86, NEG: 14, best_predict: 1 with acc 0.75 at 2023-04-08 01:01:38\n",
      "exp_596v38c2_price CZCE.SA309: POS: 41, NEG: 59, best_predict: 1 with acc 0.775 at 2023-04-08 01:01:40\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 61, NEG: 39, best_predict: 1 with acc 0.81429 at 2023-04-08 01:01:42\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 75, NEG: 25, best_predict: 1 with acc 0.75 at 2023-04-08 01:01:44\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 2, NEG: 98, best_predict: 0 with acc 0.775 at 2023-04-08 01:01:46\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 96, NEG: 4, best_predict: 1 with acc 0.81429 at 2023-04-08 01:03:29\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 100, NEG: 0, best_predict: 1 with acc 0.75 at 2023-04-08 01:03:31\n",
      "exp_596v38c2_price CZCE.MA309: POS: 31, NEG: 69, best_predict: 0 with acc 0.775 at 2023-04-08 01:03:33\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 79, NEG: 21, best_predict: 1 with acc 0.81429 at 2023-04-08 01:03:35\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 95, NEG: 5, best_predict: 1 with acc 0.75 at 2023-04-08 01:03:37\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 45, NEG: 55, best_predict: 1 with acc 0.775 at 2023-04-08 01:03:39\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 75, NEG: 25, best_predict: 1 with acc 0.81429 at 2023-04-08 01:03:41\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 92, NEG: 8, best_predict: 1 with acc 0.75 at 2023-04-08 01:03:43\n",
      "exp_596v38c2_price CZCE.TA309: POS: 50, NEG: 50, best_predict: 1 with acc 0.775 at 2023-04-08 01:03:45\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 68, NEG: 32, best_predict: 1 with acc 0.81429 at 2023-04-08 01:03:47\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 57, NEG: 43, best_predict: 1 with acc 0.75 at 2023-04-08 01:03:49\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 67, NEG: 33, best_predict: 1 with acc 0.775 at 2023-04-08 01:03:51\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 62, NEG: 38, best_predict: 0 with acc 0.81429 at 2023-04-08 01:03:53\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 62, NEG: 38, best_predict: 1 with acc 0.75 at 2023-04-08 01:03:55\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 27, NEG: 73, best_predict: 0 with acc 0.775 at 2023-04-08 01:03:57\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 44, NEG: 56, best_predict: 0 with acc 0.81429 at 2023-04-08 01:03:59\n",
      "exp_r4c7esy9 DCE.p2309: POS: 67, NEG: 33, best_predict: 0 with acc 0.75 at 2023-04-08 01:04:02\n",
      "exp_596v38c2_price DCE.p2309: POS: 50, NEG: 50, best_predict: 0 with acc 0.775 at 2023-04-08 01:04:04\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 56, NEG: 44, best_predict: 0 with acc 0.81429 at 2023-04-08 01:04:06\n",
      "exp_r4c7esy9 DCE.y2309: POS: 74, NEG: 26, best_predict: 1 with acc 0.75 at 2023-04-08 01:04:08\n",
      "exp_596v38c2_price DCE.y2309: POS: 33, NEG: 67, best_predict: 0 with acc 0.775 at 2023-04-08 01:04:10\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 77, NEG: 23, best_predict: 0 with acc 0.81429 at 2023-04-08 01:04:12\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 85, NEG: 15, best_predict: 1 with acc 0.75 at 2023-04-08 01:04:14\n",
      "exp_596v38c2_price CZCE.OI309: POS: 52, NEG: 48, best_predict: 0 with acc 0.775 at 2023-04-08 01:04:16\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 59, NEG: 41, best_predict: 1 with acc 0.81429 at 2023-04-08 01:04:18\n",
      "exp_r4c7esy9 DCE.a2309: POS: 33, NEG: 67, best_predict: 0 with acc 0.75 at 2023-04-08 01:04:20\n",
      "exp_596v38c2_price DCE.a2309: POS: 66, NEG: 34, best_predict: 0 with acc 0.775 at 2023-04-08 01:04:22\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 71, NEG: 29, best_predict: 1 with acc 0.81429 at 2023-04-08 01:04:24\n",
      "exp_r4c7esy9 DCE.m2309: POS: 74, NEG: 26, best_predict: 1 with acc 0.75 at 2023-04-08 01:04:26\n",
      "exp_596v38c2_price DCE.m2309: POS: 24, NEG: 76, best_predict: 1 with acc 0.775 at 2023-04-08 01:04:28\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 46, NEG: 54, best_predict: 0 with acc 0.81429 at 2023-04-08 01:04:31\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 46, NEG: 54, best_predict: 1 with acc 0.75 at 2023-04-08 01:04:33\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 22, NEG: 78, best_predict: 0 with acc 0.775 at 2023-04-08 01:04:35\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 91, NEG: 9, best_predict: 1 with acc 0.81429 at 2023-04-08 01:04:37\n",
      "exp_r4c7esy9 DCE.i2309: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-08 01:04:39\n",
      "exp_596v38c2_price DCE.i2309: POS: 39, NEG: 61, best_predict: 0 with acc 0.775 at 2023-04-08 01:04:41\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 86, NEG: 14, best_predict: 1 with acc 0.81429 at 2023-04-08 01:04:43\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 91, NEG: 9, best_predict: 1 with acc 0.75 at 2023-04-08 01:04:45\n",
      "exp_596v38c2_price DCE.jm2309: POS: 34, NEG: 66, best_predict: 1 with acc 0.775 at 2023-04-08 01:04:47\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 65, NEG: 35, best_predict: 0 with acc 0.81429 at 2023-04-08 01:04:49\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 77, NEG: 23, best_predict: 1 with acc 0.75 at 2023-04-08 01:04:52\n",
      "exp_596v38c2_price CZCE.SR309: POS: 70, NEG: 30, best_predict: 1 with acc 0.775 at 2023-04-08 01:04:54\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 11, NEG: 89, best_predict: 0 with acc 0.81429 at 2023-04-08 01:04:56\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 10, NEG: 90, best_predict: 0 with acc 0.75 at 2023-04-08 01:04:58\n",
      "exp_596v38c2_price CZCE.CF309: POS: 16, NEG: 84, best_predict: 0 with acc 0.775 at 2023-04-08 01:05:00\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 83, NEG: 17, best_predict: 1 with acc 0.81429 at 2023-04-08 01:05:02\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 86, NEG: 14, best_predict: 1 with acc 0.75 at 2023-04-08 01:05:04\n",
      "exp_596v38c2_price CZCE.SA309: POS: 41, NEG: 59, best_predict: 1 with acc 0.775 at 2023-04-08 01:05:06\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 61, NEG: 39, best_predict: 1 with acc 0.81429 at 2023-04-08 01:05:08\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 75, NEG: 25, best_predict: 1 with acc 0.75 at 2023-04-08 01:05:10\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 2, NEG: 98, best_predict: 0 with acc 0.775 at 2023-04-08 01:05:12\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 96, NEG: 4, best_predict: 1 with acc 0.81429 at 2023-04-08 02:30:03\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 100, NEG: 0, best_predict: 1 with acc 0.75 at 2023-04-08 02:30:05\n",
      "exp_596v38c2_price CZCE.MA309: POS: 31, NEG: 69, best_predict: 0 with acc 0.775 at 2023-04-08 02:30:07\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 79, NEG: 21, best_predict: 1 with acc 0.81429 at 2023-04-08 02:30:09\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 95, NEG: 5, best_predict: 1 with acc 0.75 at 2023-04-08 02:30:11\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 45, NEG: 55, best_predict: 1 with acc 0.775 at 2023-04-08 02:30:13\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 75, NEG: 25, best_predict: 1 with acc 0.81429 at 2023-04-08 02:30:15\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 92, NEG: 8, best_predict: 1 with acc 0.75 at 2023-04-08 02:30:16\n",
      "exp_596v38c2_price CZCE.TA309: POS: 50, NEG: 50, best_predict: 1 with acc 0.775 at 2023-04-08 02:30:18\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 68, NEG: 32, best_predict: 1 with acc 0.81429 at 2023-04-08 02:30:20\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 57, NEG: 43, best_predict: 1 with acc 0.75 at 2023-04-08 02:30:22\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 67, NEG: 33, best_predict: 1 with acc 0.775 at 2023-04-08 02:30:24\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 62, NEG: 38, best_predict: 0 with acc 0.81429 at 2023-04-08 02:30:26\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 62, NEG: 38, best_predict: 1 with acc 0.75 at 2023-04-08 02:30:28\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 27, NEG: 73, best_predict: 0 with acc 0.775 at 2023-04-08 02:30:30\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 44, NEG: 56, best_predict: 0 with acc 0.81429 at 2023-04-08 02:30:32\n",
      "exp_r4c7esy9 DCE.p2309: POS: 67, NEG: 33, best_predict: 0 with acc 0.75 at 2023-04-08 02:30:34\n",
      "exp_596v38c2_price DCE.p2309: POS: 50, NEG: 50, best_predict: 0 with acc 0.775 at 2023-04-08 02:30:36\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 56, NEG: 44, best_predict: 0 with acc 0.81429 at 2023-04-08 02:30:38\n",
      "exp_r4c7esy9 DCE.y2309: POS: 74, NEG: 26, best_predict: 1 with acc 0.75 at 2023-04-08 02:30:40\n",
      "exp_596v38c2_price DCE.y2309: POS: 33, NEG: 67, best_predict: 0 with acc 0.775 at 2023-04-08 02:30:42\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 77, NEG: 23, best_predict: 0 with acc 0.81429 at 2023-04-08 02:30:43\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 85, NEG: 15, best_predict: 1 with acc 0.75 at 2023-04-08 02:30:45\n",
      "exp_596v38c2_price CZCE.OI309: POS: 52, NEG: 48, best_predict: 0 with acc 0.775 at 2023-04-08 02:30:47\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 59, NEG: 41, best_predict: 1 with acc 0.81429 at 2023-04-08 02:30:49\n",
      "exp_r4c7esy9 DCE.a2309: POS: 33, NEG: 67, best_predict: 0 with acc 0.75 at 2023-04-08 02:30:51\n",
      "exp_596v38c2_price DCE.a2309: POS: 66, NEG: 34, best_predict: 0 with acc 0.775 at 2023-04-08 02:30:53\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 71, NEG: 29, best_predict: 1 with acc 0.81429 at 2023-04-08 02:30:55\n",
      "exp_r4c7esy9 DCE.m2309: POS: 74, NEG: 26, best_predict: 1 with acc 0.75 at 2023-04-08 02:30:57\n",
      "exp_596v38c2_price DCE.m2309: POS: 24, NEG: 76, best_predict: 1 with acc 0.775 at 2023-04-08 02:30:59\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 46, NEG: 54, best_predict: 0 with acc 0.81429 at 2023-04-08 02:31:01\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 46, NEG: 54, best_predict: 1 with acc 0.75 at 2023-04-08 02:31:03\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 22, NEG: 78, best_predict: 0 with acc 0.775 at 2023-04-08 02:31:05\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 91, NEG: 9, best_predict: 1 with acc 0.81429 at 2023-04-08 02:31:07\n",
      "exp_r4c7esy9 DCE.i2309: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-08 02:31:09\n",
      "exp_596v38c2_price DCE.i2309: POS: 39, NEG: 61, best_predict: 0 with acc 0.775 at 2023-04-08 02:31:11\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 86, NEG: 14, best_predict: 1 with acc 0.81429 at 2023-04-08 02:31:13\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 91, NEG: 9, best_predict: 1 with acc 0.75 at 2023-04-08 02:31:14\n",
      "exp_596v38c2_price DCE.jm2309: POS: 34, NEG: 66, best_predict: 1 with acc 0.775 at 2023-04-08 02:31:16\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 65, NEG: 35, best_predict: 0 with acc 0.81429 at 2023-04-08 02:31:18\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 77, NEG: 23, best_predict: 1 with acc 0.75 at 2023-04-08 02:31:20\n",
      "exp_596v38c2_price CZCE.SR309: POS: 70, NEG: 30, best_predict: 1 with acc 0.775 at 2023-04-08 02:31:22\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 11, NEG: 89, best_predict: 0 with acc 0.81429 at 2023-04-08 02:31:24\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 10, NEG: 90, best_predict: 0 with acc 0.75 at 2023-04-08 02:31:26\n",
      "exp_596v38c2_price CZCE.CF309: POS: 16, NEG: 84, best_predict: 0 with acc 0.775 at 2023-04-08 02:31:28\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 83, NEG: 17, best_predict: 1 with acc 0.81429 at 2023-04-08 02:31:30\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 86, NEG: 14, best_predict: 1 with acc 0.75 at 2023-04-08 02:31:31\n",
      "exp_596v38c2_price CZCE.SA309: POS: 41, NEG: 59, best_predict: 1 with acc 0.775 at 2023-04-08 02:31:33\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 61, NEG: 39, best_predict: 1 with acc 0.81429 at 2023-04-08 02:31:35\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 75, NEG: 25, best_predict: 1 with acc 0.75 at 2023-04-08 02:31:37\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 2, NEG: 98, best_predict: 0 with acc 0.775 at 2023-04-08 02:31:39\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "exp_enismkr2 CZCE.MA309: POS: 96, NEG: 4, best_predict: 1 with acc 0.81429 at 2023-04-08 02:33:21\n",
      "exp_r4c7esy9 CZCE.MA309: POS: 100, NEG: 0, best_predict: 1 with acc 0.75 at 2023-04-08 02:33:23\n",
      "exp_596v38c2_price CZCE.MA309: POS: 31, NEG: 69, best_predict: 0 with acc 0.775 at 2023-04-08 02:33:25\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.bu2306: POS: 79, NEG: 21, best_predict: 1 with acc 0.81429 at 2023-04-08 02:33:27\n",
      "exp_r4c7esy9 SHFE.bu2306: POS: 95, NEG: 5, best_predict: 1 with acc 0.75 at 2023-04-08 02:33:29\n",
      "exp_596v38c2_price SHFE.bu2306: POS: 45, NEG: 55, best_predict: 1 with acc 0.775 at 2023-04-08 02:33:31\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.TA309: POS: 75, NEG: 25, best_predict: 1 with acc 0.81429 at 2023-04-08 02:33:33\n",
      "exp_r4c7esy9 CZCE.TA309: POS: 92, NEG: 8, best_predict: 1 with acc 0.75 at 2023-04-08 02:33:34\n",
      "exp_596v38c2_price CZCE.TA309: POS: 50, NEG: 50, best_predict: 1 with acc 0.775 at 2023-04-08 02:33:36\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.fu2309: POS: 68, NEG: 32, best_predict: 1 with acc 0.81429 at 2023-04-08 02:33:38\n",
      "exp_r4c7esy9 SHFE.fu2309: POS: 57, NEG: 43, best_predict: 1 with acc 0.75 at 2023-04-08 02:33:40\n",
      "exp_596v38c2_price SHFE.fu2309: POS: 67, NEG: 33, best_predict: 1 with acc 0.775 at 2023-04-08 02:33:42\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.ru2309: POS: 62, NEG: 38, best_predict: 0 with acc 0.81429 at 2023-04-08 02:33:44\n",
      "exp_r4c7esy9 SHFE.ru2309: POS: 62, NEG: 38, best_predict: 1 with acc 0.75 at 2023-04-08 02:33:46\n",
      "exp_596v38c2_price SHFE.ru2309: POS: 27, NEG: 73, best_predict: 0 with acc 0.775 at 2023-04-08 02:33:48\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.p2309: POS: 44, NEG: 56, best_predict: 0 with acc 0.81429 at 2023-04-08 02:33:50\n",
      "exp_r4c7esy9 DCE.p2309: POS: 67, NEG: 33, best_predict: 0 with acc 0.75 at 2023-04-08 02:33:52\n",
      "exp_596v38c2_price DCE.p2309: POS: 50, NEG: 50, best_predict: 0 with acc 0.775 at 2023-04-08 02:33:54\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.y2309: POS: 56, NEG: 44, best_predict: 0 with acc 0.81429 at 2023-04-08 02:33:56\n",
      "exp_r4c7esy9 DCE.y2309: POS: 74, NEG: 26, best_predict: 1 with acc 0.75 at 2023-04-08 02:33:58\n",
      "exp_596v38c2_price DCE.y2309: POS: 33, NEG: 67, best_predict: 0 with acc 0.775 at 2023-04-08 02:33:59\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.OI309: POS: 77, NEG: 23, best_predict: 0 with acc 0.81429 at 2023-04-08 02:34:01\n",
      "exp_r4c7esy9 CZCE.OI309: POS: 85, NEG: 15, best_predict: 1 with acc 0.75 at 2023-04-08 02:34:03\n",
      "exp_596v38c2_price CZCE.OI309: POS: 52, NEG: 48, best_predict: 0 with acc 0.775 at 2023-04-08 02:34:05\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.a2309: POS: 59, NEG: 41, best_predict: 1 with acc 0.81429 at 2023-04-08 02:34:07\n",
      "exp_r4c7esy9 DCE.a2309: POS: 33, NEG: 67, best_predict: 0 with acc 0.75 at 2023-04-08 02:34:09\n",
      "exp_596v38c2_price DCE.a2309: POS: 66, NEG: 34, best_predict: 0 with acc 0.775 at 2023-04-08 02:34:11\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.m2309: POS: 71, NEG: 29, best_predict: 1 with acc 0.81429 at 2023-04-08 02:34:13\n",
      "exp_r4c7esy9 DCE.m2309: POS: 74, NEG: 26, best_predict: 1 with acc 0.75 at 2023-04-08 02:34:15\n",
      "exp_596v38c2_price DCE.m2309: POS: 24, NEG: 76, best_predict: 1 with acc 0.775 at 2023-04-08 02:34:17\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.rb2310: POS: 46, NEG: 54, best_predict: 0 with acc 0.81429 at 2023-04-08 02:34:19\n",
      "exp_r4c7esy9 SHFE.rb2310: POS: 46, NEG: 54, best_predict: 1 with acc 0.75 at 2023-04-08 02:34:21\n",
      "exp_596v38c2_price SHFE.rb2310: POS: 22, NEG: 78, best_predict: 0 with acc 0.775 at 2023-04-08 02:34:23\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.i2309: POS: 91, NEG: 9, best_predict: 1 with acc 0.81429 at 2023-04-08 02:34:25\n",
      "exp_r4c7esy9 DCE.i2309: POS: 94, NEG: 6, best_predict: 1 with acc 0.75 at 2023-04-08 02:34:26\n",
      "exp_596v38c2_price DCE.i2309: POS: 39, NEG: 61, best_predict: 0 with acc 0.775 at 2023-04-08 02:34:28\n",
      "\n",
      "\n",
      "exp_enismkr2 DCE.jm2309: POS: 86, NEG: 14, best_predict: 1 with acc 0.81429 at 2023-04-08 02:34:30\n",
      "exp_r4c7esy9 DCE.jm2309: POS: 91, NEG: 9, best_predict: 1 with acc 0.75 at 2023-04-08 02:34:32\n",
      "exp_596v38c2_price DCE.jm2309: POS: 34, NEG: 66, best_predict: 1 with acc 0.775 at 2023-04-08 02:34:34\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SR309: POS: 65, NEG: 35, best_predict: 0 with acc 0.81429 at 2023-04-08 02:34:36\n",
      "exp_r4c7esy9 CZCE.SR309: POS: 77, NEG: 23, best_predict: 1 with acc 0.75 at 2023-04-08 02:34:38\n",
      "exp_596v38c2_price CZCE.SR309: POS: 70, NEG: 30, best_predict: 1 with acc 0.775 at 2023-04-08 02:34:40\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.CF309: POS: 11, NEG: 89, best_predict: 0 with acc 0.81429 at 2023-04-08 02:34:42\n",
      "exp_r4c7esy9 CZCE.CF309: POS: 10, NEG: 90, best_predict: 0 with acc 0.75 at 2023-04-08 02:34:44\n",
      "exp_596v38c2_price CZCE.CF309: POS: 16, NEG: 84, best_predict: 0 with acc 0.775 at 2023-04-08 02:34:46\n",
      "\n",
      "\n",
      "exp_enismkr2 CZCE.SA309: POS: 83, NEG: 17, best_predict: 1 with acc 0.81429 at 2023-04-08 02:34:48\n",
      "exp_r4c7esy9 CZCE.SA309: POS: 86, NEG: 14, best_predict: 1 with acc 0.75 at 2023-04-08 02:34:50\n",
      "exp_596v38c2_price CZCE.SA309: POS: 41, NEG: 59, best_predict: 1 with acc 0.775 at 2023-04-08 02:34:52\n",
      "\n",
      "\n",
      "exp_enismkr2 SHFE.sp2309: POS: 61, NEG: 39, best_predict: 1 with acc 0.81429 at 2023-04-08 02:34:53\n",
      "exp_r4c7esy9 SHFE.sp2309: POS: 75, NEG: 25, best_predict: 1 with acc 0.75 at 2023-04-08 02:34:55\n",
      "exp_596v38c2_price SHFE.sp2309: POS: 2, NEG: 98, best_predict: 0 with acc 0.775 at 2023-04-08 02:34:57\n",
      "\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "real_time_predict(exp_name_list=[\"enismkr2\", \"r4c7esy9\", \"596v38c2_price\"], interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 查看保存的模型统计\n",
    "df = pd.DataFrame(columns=[\"596v38c2\", \"r4c7esy9\", \"enismkr2\", \"596v38c2_price\"], data=[])\n",
    "\n",
    "for name in [\"596v38c2\", \"r4c7esy9\", \"enismkr2\", \"596v38c2_price\"]:\n",
    "    acc_list = []\n",
    "    for i in range(100):\n",
    "        model_info = torch.load(f\"e:\\\\ml_data\\\\model_checkpoints\\\\model_nni_{name}_{i}.pth\")\n",
    "        acc = model_info[\"acc\"]\n",
    "        acc_list.append(acc)\n",
    "\n",
    "    df[f\"{name}\"] = acc_list\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running macd prediction process for SHFE.sp2305 2023-03-31 10:30:00 by NNI models\n",
      "\n",
      "In 100 predictions from loaded batch model:\n",
      "instances predict POSITIVE: 51\n",
      "instances predict NEGATIVE: 49\n",
      "Best model with acc: 0.81429, predict: 1 \n"
     ]
    }
   ],
   "source": [
    "# single run\n",
    "# api.wait_update() # 盘中可以取消注释\n",
    "# target time list\n",
    "target_time_list = ['SHFE.sp2305',\n",
    "  [['2023-03-31 10:30:00', 10],\n",
    "   ['2023-03-31 10:00:00', 15],\n",
    "   ['2023-03-31 09:00:00', 60],\n",
    "   ['2023-03-31 00:00:00', 1440]], 0.0]\n",
    "\n",
    "target_df, target_data, _ = prepare_data(target_time_list[0], target_time_list[1], target_time_list[2], exam=True)\n",
    "\n",
    "if data_dim == 1:\n",
    "   _, target_data = scale_data([target_time_list], target_data)\n",
    "   target_data = target_data.transpose(1,2)\n",
    "\n",
    "elif data_dim == 2:\n",
    "  img_size = (300, 300)\n",
    "  linewidth = 9\n",
    "  img_data = torch.zeros(target_data.shape[0], target_data.shape[1], img_size[0], img_size[1])    \n",
    "  for j in range(target_data.shape[1]):\n",
    "      _, img_data[0, j] = fig_to_mat(target_data[0, j], size=img_size, lw=linewidth)    \n",
    "\n",
    "\n",
    "model_acc_list, predict_list = get_predictions(target_time_list, target_data, \"nni\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['DCE.p2305 2023-03-30 09:40:00', '84', '16'],\n",
       " ['DCE.jm2305 2023-03-30 11:00:00', '43', '57'],\n",
       " ['SHFE.sp2305 2023-03-30 11:20:00', '62', '38'],\n",
       " ['SHFE.sp2305 2023-03-30 11:20:00', '62', '38'],\n",
       " ['CZCE.SA305 2023-03-30 13:40:00', '88', '12'],\n",
       " ['CZCE.CF305 2023-03-30 11:00:00', '30', '70'],\n",
       " ['CZCE.CF305 2023-03-30 10:40:00', '29', '71'],\n",
       " ['CZCE.CF305 2023-03-30 13:50:00', '84', '16'],\n",
       " ['SHFE.ru2305 2023-03-30 14:00:00', '80', '20']]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # record the prediction\n",
    "today = datetime.strftime(datetime.now(), \"%Y-%m-%d\")\n",
    "\n",
    "load_model_predict_record.append(\n",
    "[\n",
    "    target_time_list[0] +' '+ target_time_list[1][0][0],\n",
    "    str(torch.tensor(predict_list).max(1)[1].sum().item()),\n",
    "    str(torch.tensor(predict_list).min(1)[1].sum().item()),\n",
    "]\n",
    ")\n",
    "predict_record_df = pd.DataFrame(columns=[\"contract and time point\", \"positive\", \"negative\"], data=load_model_predict_record)\n",
    "predict_record_df.to_csv(f\"e:\\\\ml_data\\\\predict_record\\\\record_{today}.csv\", index=False)\n",
    "load_model_predict_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_acc_list = []\n",
    "max_trial = 100 if data_dim == 1 else 50\n",
    "\n",
    "print(f\"\\nRunning macd prediction process for {target_time_list[0]} {target_time_list[1][0][0]}\")\n",
    "for trial in range(max_trial):\n",
    "    train_loader, test_loader = prepare_dataloader(img_datasets, load_data.tensors[1], batch_size, label_type, test=False, one_hot_label=one_hot_label) # use different dataloader for every trial\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    # device = 'cpu'\n",
    "    # used_net = \"alexnet1d\"\n",
    "    net = alexnet1d(len(channels), out_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss() if out_dim == 2 else nn.MSELoss()\n",
    "    # criterion = nn.BCEWithLogitsLoss()\n",
    "    # criterion = FocalLoss()\n",
    "    scheduler = lrs.StepLR(optimizer, step_size=5 ,gamma=0.85)\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    tolerance = 0 # 由NNI调参决定\n",
    "    max_epochs = 200\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        net.train()\n",
    "        for batch, (data, label) in enumerate(train_loader):\n",
    "            data = data[:,channels].to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            logits = net(data)\n",
    "            loss = criterion(logits, label)\n",
    "            train_loss.append(loss.item() / len(data))\n",
    "\n",
    "            if label_type == 'binary':\n",
    "                if one_hot_label: \n",
    "                    acc = logits.max(1)[1].eq(label.max(1)[1]).sum().item() / len(label)\n",
    "                else:\n",
    "                    acc = ((logits.flatten() > 0.5 ).flatten() == label.flatten()).sum().item() / len(label)\n",
    "            else:\n",
    "                count = 0\n",
    "                for item in (torch.cat((logits, label), 1) > 0):\n",
    "                    if item[0] == item[1]:\n",
    "                        count += 1\n",
    "                acc = count / len(label)\n",
    "            train_acc.append(acc)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch % 2 == 0:\n",
    "                print(f\"Train Epoch {epoch}, lr: {optimizer.param_groups[0]['lr']:.6f}, {batch * len(data)}/{len(train_loader.dataset)}, avg loss: {loss.item() / len(data):.6f}, train_acc: {train_acc[-1]:.2f}\")\n",
    "        scheduler.step() # update learning rate after an epoch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            for data, label in test_loader:\n",
    "                data = data[:, channels].to(device)\n",
    "                label = label.to(device)\n",
    "                \n",
    "                logits = net(data)\n",
    "                loss = criterion(logits, label)\n",
    "                \n",
    "                if label_type == 'binary':\n",
    "                    if one_hot_label: \n",
    "                        acc = logits.max(1)[1].eq(label.max(1)[1]).sum().item() / len(label)\n",
    "                    else:\n",
    "                        acc = ((logits.flatten() > 0.5 ).flatten() == label.flatten()).sum().item() / len(label)\n",
    "                else:\n",
    "                    count = 0\n",
    "                    for item in (torch.cat((logits, label), 1) > 0):\n",
    "                        if item[0] == item[1]:\n",
    "                            count += 1\n",
    "                    acc = count / len(label)\n",
    "                test_acc.append(acc)\n",
    "\n",
    "                # print(((net(data).flatten() > 0.5 ) == label.flatten()).sum() / len(data))\n",
    "\n",
    "                test_loss.append(loss.item() / len(data))\n",
    "\n",
    "            print(f\"{used_net}, trial.{trial}:\\nEpoch {epoch}, avg test_loss: {loss.item() / len(data):.6f}, test_acc: {acc:.2f}\")\n",
    "        \n",
    "        if train_acc[-1] >= threshold: # early stop\n",
    "            tolerance += 1\n",
    "            print(f\"\\ntrain data has been fully fitted. Stop training process in 5 epochs. tolerance = {tolerance}\")\n",
    "            if tolerance >= patient:\n",
    "                print(\"\\nTraining stop\")\n",
    "                break\n",
    "\n",
    "    # torch.save(net.state_dict(), f\"e:\\\\ml_data\\\\model_checkpoints\\\\{used_net}_{today}_{len(saved_time_list)}_{int(test_acc[-1] * 100)}_{channels}_{label_type}.pth\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        net = net.to('cpu')\n",
    "        net.eval()\n",
    "        for data, label in test_loader:\n",
    "            # if label.max() == 1. and label.min() == 0.:\n",
    "            if label_type == \"binary\" and not one_hot_label:\n",
    "                result = torch.cat((torch.tensor(net(data[:, channels]) > 0.5, dtype=torch.float32), label),1)\n",
    "            elif label_type == \"binary\" and one_hot_label:\n",
    "                result = torch.stack((net(data[:, channels]).max(1)[1], label.max(1)[1]), 1)\n",
    "                pos_corr = 0\n",
    "                neg_corr = 0\n",
    "                for res in result:\n",
    "                    if res[0] == res[1] and res[1] == 1.:\n",
    "                        pos_corr += 1\n",
    "                    elif res[0] == res[1] and res[1] == 0.:\n",
    "                        neg_corr += 1\n",
    "                print(f\"在{len(result)}个结果中:\\n正确信号{sum([item[1] == 1 for item in result])}个\\n错误信号{sum([item[1] == 0 for item in result])}个\\n信号正确并预测正确的概率为{round(pos_corr/len(result), 3)}\\n信号错误并预测正确的概率为{round(neg_corr/len(result),3)}\\n总正确率为{sum([item[0] == item[1] for item in result])/len(result):.2f}\")\n",
    "            \n",
    "            else:\n",
    "                count = 0\n",
    "                for item in torch.cat((net(data[:, channels]), label), 1) > 0:\n",
    "                    if item[0] == item[1]:\n",
    "                        count += 1\n",
    "                result = torch.cat((net(data[:, channels]), label), 1) > 0\n",
    "                pos_corr = 0\n",
    "                neg_corr = 0\n",
    "                for res in result:\n",
    "                    if res[0] == res[1] and res[1] == True:\n",
    "                        pos_corr += 1\n",
    "                    elif res[0] == res[1] and res[1] == False:\n",
    "                        neg_corr += 1          \n",
    "                print(f\"在{len(result)}个结果中:\\n正确信号{sum([item[1] == 1 for item in result])}个\\n错误信号{sum([item[1] == 0 for item in result])}个\\n信号正确并预测正确的概率为{round(pos_corr/len(result), 3)}\\n信号错误并预测正确的概率为{round(neg_corr/len(result),3)}\\n总正确率为{sum([item[0] == item[1] for item in result])/len(result):.2f}\")\n",
    "    predict = torch.squeeze(predict_on_target(net, channels, target_data)).tolist()\n",
    "    trial_acc_list.append([test_acc[-1], *predict])\n",
    "\n",
    "predict_tensor_list = analysis_result(trial_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_tensor_list[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 3, 9] 56 0.88 4\n"
     ]
    }
   ],
   "source": [
    "parameter_set = {\n",
    "    \"channels\": [\n",
    "        0,\n",
    "        1,\n",
    "        3,\n",
    "        9\n",
    "    ],\n",
    "    \"batch_size\": 56,\n",
    "    \"threshold\": 0.88,\n",
    "    \"patient\": 4\n",
    "} # copy from NNI web as json file\n",
    "\n",
    "channels = parameter_set[\"channels\"] # 由NNI调参决定\n",
    "batch_size = parameter_set[\"batch_size\"] # 由NNI调参决定\n",
    "threshold = parameter_set[\"threshold\"] # 由NNI调参决定\n",
    "patient = parameter_set[\"patient\"] # 训练达到一定精度之后，经过多少epoch停止训练，由NNI调参决定\n",
    "print(channels, batch_size, threshold, patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generate candidate models for evaluations\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012417, train_acc: 0.43\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012356, train_acc: 0.62\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012718, train_acc: 0.61\n",
      "alexnet1d, trial.0:\n",
      "Epoch 0, avg test_loss: 0.009794, test_acc: 0.60\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012387, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012303, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012545, train_acc: 0.54\n",
      "alexnet1d, trial.0:\n",
      "Epoch 1, avg test_loss: 0.009882, test_acc: 0.60\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011775, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012733, train_acc: 0.48\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012045, train_acc: 0.59\n",
      "alexnet1d, trial.0:\n",
      "Epoch 2, avg test_loss: 0.009901, test_acc: 0.60\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011840, train_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011974, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.013621, train_acc: 0.46\n",
      "alexnet1d, trial.0:\n",
      "Epoch 3, avg test_loss: 0.009738, test_acc: 0.60\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012297, train_acc: 0.50\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012500, train_acc: 0.43\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012447, train_acc: 0.45\n",
      "alexnet1d, trial.0:\n",
      "Epoch 4, avg test_loss: 0.009814, test_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012364, train_acc: 0.45\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012135, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012553, train_acc: 0.52\n",
      "alexnet1d, trial.0:\n",
      "Epoch 5, avg test_loss: 0.009699, test_acc: 0.60\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012347, train_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012267, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012278, train_acc: 0.54\n",
      "alexnet1d, trial.0:\n",
      "Epoch 6, avg test_loss: 0.009593, test_acc: 0.60\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012064, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011823, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012142, train_acc: 0.62\n",
      "alexnet1d, trial.0:\n",
      "Epoch 7, avg test_loss: 0.009586, test_acc: 0.60\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012313, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011673, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012379, train_acc: 0.57\n",
      "alexnet1d, trial.0:\n",
      "Epoch 8, avg test_loss: 0.009602, test_acc: 0.60\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012058, train_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012449, train_acc: 0.46\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011405, train_acc: 0.71\n",
      "alexnet1d, trial.0:\n",
      "Epoch 9, avg test_loss: 0.009498, test_acc: 0.60\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011744, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011567, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011418, train_acc: 0.55\n",
      "alexnet1d, trial.0:\n",
      "Epoch 10, avg test_loss: 0.009684, test_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011759, train_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011472, train_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010772, train_acc: 0.70\n",
      "alexnet1d, trial.0:\n",
      "Epoch 11, avg test_loss: 0.010144, test_acc: 0.63\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.012487, train_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010512, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011078, train_acc: 0.64\n",
      "alexnet1d, trial.0:\n",
      "Epoch 12, avg test_loss: 0.009222, test_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009618, train_acc: 0.77\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010061, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011393, train_acc: 0.59\n",
      "alexnet1d, trial.0:\n",
      "Epoch 13, avg test_loss: 0.010647, test_acc: 0.56\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010294, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011156, train_acc: 0.64\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011113, train_acc: 0.70\n",
      "alexnet1d, trial.0:\n",
      "Epoch 14, avg test_loss: 0.010077, test_acc: 0.61\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009279, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.012475, train_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009840, train_acc: 0.71\n",
      "alexnet1d, trial.0:\n",
      "Epoch 15, avg test_loss: 0.009625, test_acc: 0.59\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009835, train_acc: 0.80\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010054, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010107, train_acc: 0.75\n",
      "alexnet1d, trial.0:\n",
      "Epoch 16, avg test_loss: 0.009713, test_acc: 0.61\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009449, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.011368, train_acc: 0.66\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009585, train_acc: 0.73\n",
      "alexnet1d, trial.0:\n",
      "Epoch 17, avg test_loss: 0.010943, test_acc: 0.59\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.011430, train_acc: 0.66\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007943, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008236, train_acc: 0.79\n",
      "alexnet1d, trial.0:\n",
      "Epoch 18, avg test_loss: 0.009989, test_acc: 0.60\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008518, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008470, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007006, train_acc: 0.84\n",
      "alexnet1d, trial.0:\n",
      "Epoch 19, avg test_loss: 0.011420, test_acc: 0.54\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007324, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007581, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008876, train_acc: 0.79\n",
      "alexnet1d, trial.0:\n",
      "Epoch 20, avg test_loss: 0.011929, test_acc: 0.67\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007514, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007473, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006692, train_acc: 0.88\n",
      "alexnet1d, trial.0:\n",
      "Epoch 21, avg test_loss: 0.010127, test_acc: 0.69\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006656, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006221, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.009771, train_acc: 0.75\n",
      "alexnet1d, trial.0:\n",
      "Epoch 22, avg test_loss: 0.014721, test_acc: 0.56\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007359, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.008567, train_acc: 0.77\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.009671, train_acc: 0.75\n",
      "alexnet1d, trial.0:\n",
      "Epoch 23, avg test_loss: 0.013627, test_acc: 0.71\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005274, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005316, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006981, train_acc: 0.86\n",
      "alexnet1d, trial.0:\n",
      "Epoch 24, avg test_loss: 0.012020, test_acc: 0.60\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.008659, train_acc: 0.80\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006866, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006414, train_acc: 0.86\n",
      "alexnet1d, trial.0:\n",
      "Epoch 25, avg test_loss: 0.012851, test_acc: 0.67\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004661, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.007736, train_acc: 0.82\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004720, train_acc: 0.93\n",
      "alexnet1d, trial.0:\n",
      "Epoch 26, avg test_loss: 0.013134, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003867, train_acc: 0.98\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004452, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.005323, train_acc: 0.89\n",
      "alexnet1d, trial.0:\n",
      "Epoch 27, avg test_loss: 0.015024, test_acc: 0.67\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003666, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.003600, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.003914, train_acc: 0.89\n",
      "alexnet1d, trial.0:\n",
      "Epoch 28, avg test_loss: 0.016294, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.002521, train_acc: 0.96\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.003354, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.005318, train_acc: 0.86\n",
      "alexnet1d, trial.0:\n",
      "Epoch 29, avg test_loss: 0.017647, test_acc: 0.66\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.001808, train_acc: 0.96\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.004047, train_acc: 0.95\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.001370, train_acc: 0.98\n",
      "alexnet1d, trial.0:\n",
      "Epoch 30, avg test_loss: 0.020668, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号42个\n",
      "错误信号28个\n",
      "信号正确并预测正确的概率为0.386\n",
      "信号错误并预测正确的概率为0.243\n",
      "总正确率为0.63\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012454, train_acc: 0.41\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.013443, train_acc: 0.62\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012382, train_acc: 0.55\n",
      "alexnet1d, trial.1:\n",
      "Epoch 0, avg test_loss: 0.010096, test_acc: 0.49\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012140, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.011790, train_acc: 0.62\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011768, train_acc: 0.61\n",
      "alexnet1d, trial.1:\n",
      "Epoch 1, avg test_loss: 0.011459, test_acc: 0.49\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011944, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011621, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012374, train_acc: 0.57\n",
      "alexnet1d, trial.1:\n",
      "Epoch 2, avg test_loss: 0.010454, test_acc: 0.49\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011272, train_acc: 0.70\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011932, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012357, train_acc: 0.54\n",
      "alexnet1d, trial.1:\n",
      "Epoch 3, avg test_loss: 0.010917, test_acc: 0.49\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011674, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012084, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011006, train_acc: 0.61\n",
      "alexnet1d, trial.1:\n",
      "Epoch 4, avg test_loss: 0.011048, test_acc: 0.49\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012188, train_acc: 0.52\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011287, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011000, train_acc: 0.70\n",
      "alexnet1d, trial.1:\n",
      "Epoch 5, avg test_loss: 0.012596, test_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011078, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011709, train_acc: 0.50\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011172, train_acc: 0.62\n",
      "alexnet1d, trial.1:\n",
      "Epoch 6, avg test_loss: 0.012271, test_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011355, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012363, train_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011187, train_acc: 0.64\n",
      "alexnet1d, trial.1:\n",
      "Epoch 7, avg test_loss: 0.011019, test_acc: 0.63\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011442, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011510, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.010723, train_acc: 0.66\n",
      "alexnet1d, trial.1:\n",
      "Epoch 8, avg test_loss: 0.012724, test_acc: 0.50\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.009978, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011450, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012110, train_acc: 0.71\n",
      "alexnet1d, trial.1:\n",
      "Epoch 9, avg test_loss: 0.011819, test_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.009855, train_acc: 0.73\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010402, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012319, train_acc: 0.59\n",
      "alexnet1d, trial.1:\n",
      "Epoch 10, avg test_loss: 0.011636, test_acc: 0.50\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011126, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010922, train_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.009074, train_acc: 0.79\n",
      "alexnet1d, trial.1:\n",
      "Epoch 11, avg test_loss: 0.013155, test_acc: 0.54\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010378, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.009510, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.008840, train_acc: 0.75\n",
      "alexnet1d, trial.1:\n",
      "Epoch 12, avg test_loss: 0.011642, test_acc: 0.54\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009650, train_acc: 0.75\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009220, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011833, train_acc: 0.61\n",
      "alexnet1d, trial.1:\n",
      "Epoch 13, avg test_loss: 0.013350, test_acc: 0.56\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009534, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.008943, train_acc: 0.80\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009864, train_acc: 0.73\n",
      "alexnet1d, trial.1:\n",
      "Epoch 14, avg test_loss: 0.014672, test_acc: 0.54\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008693, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.006802, train_acc: 0.88\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.013013, train_acc: 0.59\n",
      "alexnet1d, trial.1:\n",
      "Epoch 15, avg test_loss: 0.013693, test_acc: 0.59\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008253, train_acc: 0.82\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.007955, train_acc: 0.82\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.007160, train_acc: 0.80\n",
      "alexnet1d, trial.1:\n",
      "Epoch 16, avg test_loss: 0.019161, test_acc: 0.57\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009084, train_acc: 0.70\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009140, train_acc: 0.71\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007478, train_acc: 0.77\n",
      "alexnet1d, trial.1:\n",
      "Epoch 17, avg test_loss: 0.016270, test_acc: 0.54\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008495, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008403, train_acc: 0.71\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007445, train_acc: 0.86\n",
      "alexnet1d, trial.1:\n",
      "Epoch 18, avg test_loss: 0.014415, test_acc: 0.60\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007376, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008092, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008707, train_acc: 0.77\n",
      "alexnet1d, trial.1:\n",
      "Epoch 19, avg test_loss: 0.025008, test_acc: 0.50\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008142, train_acc: 0.73\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.006743, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009510, train_acc: 0.75\n",
      "alexnet1d, trial.1:\n",
      "Epoch 20, avg test_loss: 0.014857, test_acc: 0.61\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007981, train_acc: 0.73\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007976, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008569, train_acc: 0.82\n",
      "alexnet1d, trial.1:\n",
      "Epoch 21, avg test_loss: 0.014568, test_acc: 0.56\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006257, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006907, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007291, train_acc: 0.77\n",
      "alexnet1d, trial.1:\n",
      "Epoch 22, avg test_loss: 0.023401, test_acc: 0.57\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005280, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007641, train_acc: 0.80\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006380, train_acc: 0.82\n",
      "alexnet1d, trial.1:\n",
      "Epoch 23, avg test_loss: 0.021425, test_acc: 0.56\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.003167, train_acc: 0.95\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006744, train_acc: 0.79\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006640, train_acc: 0.84\n",
      "alexnet1d, trial.1:\n",
      "Epoch 24, avg test_loss: 0.027164, test_acc: 0.61\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.002440, train_acc: 0.96\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005014, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.007643, train_acc: 0.73\n",
      "alexnet1d, trial.1:\n",
      "Epoch 25, avg test_loss: 0.030348, test_acc: 0.56\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.003302, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004047, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.003991, train_acc: 0.89\n",
      "alexnet1d, trial.1:\n",
      "Epoch 26, avg test_loss: 0.025953, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003327, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004436, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004875, train_acc: 0.88\n",
      "alexnet1d, trial.1:\n",
      "Epoch 27, avg test_loss: 0.027386, test_acc: 0.60\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003239, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.003843, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.003407, train_acc: 0.93\n",
      "alexnet1d, trial.1:\n",
      "Epoch 28, avg test_loss: 0.032620, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.002411, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.004492, train_acc: 0.88\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.004561, train_acc: 0.89\n",
      "alexnet1d, trial.1:\n",
      "Epoch 29, avg test_loss: 0.044426, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.002830, train_acc: 0.95\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.004434, train_acc: 0.88\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.002844, train_acc: 0.93\n",
      "alexnet1d, trial.1:\n",
      "Epoch 30, avg test_loss: 0.031616, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号34个\n",
      "错误信号36个\n",
      "信号正确并预测正确的概率为0.386\n",
      "信号错误并预测正确的概率为0.214\n",
      "总正确率为0.60\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012447, train_acc: 0.43\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012238, train_acc: 0.62\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.011994, train_acc: 0.68\n",
      "alexnet1d, trial.2:\n",
      "Epoch 0, avg test_loss: 0.009776, test_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012072, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012382, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012980, train_acc: 0.48\n",
      "alexnet1d, trial.2:\n",
      "Epoch 1, avg test_loss: 0.009755, test_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012382, train_acc: 0.50\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012181, train_acc: 0.66\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012336, train_acc: 0.52\n",
      "alexnet1d, trial.2:\n",
      "Epoch 2, avg test_loss: 0.009774, test_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012160, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012017, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011949, train_acc: 0.62\n",
      "alexnet1d, trial.2:\n",
      "Epoch 3, avg test_loss: 0.009737, test_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011656, train_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012030, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.013226, train_acc: 0.46\n",
      "alexnet1d, trial.2:\n",
      "Epoch 4, avg test_loss: 0.009754, test_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011745, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012926, train_acc: 0.46\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012014, train_acc: 0.62\n",
      "alexnet1d, trial.2:\n",
      "Epoch 5, avg test_loss: 0.009750, test_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011918, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012175, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012138, train_acc: 0.59\n",
      "alexnet1d, trial.2:\n",
      "Epoch 6, avg test_loss: 0.009738, test_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011989, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012393, train_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012021, train_acc: 0.59\n",
      "alexnet1d, trial.2:\n",
      "Epoch 7, avg test_loss: 0.009697, test_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012686, train_acc: 0.48\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011876, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012170, train_acc: 0.57\n",
      "alexnet1d, trial.2:\n",
      "Epoch 8, avg test_loss: 0.009588, test_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011926, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011916, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012310, train_acc: 0.52\n",
      "alexnet1d, trial.2:\n",
      "Epoch 9, avg test_loss: 0.009403, test_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011517, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011592, train_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011756, train_acc: 0.64\n",
      "alexnet1d, trial.2:\n",
      "Epoch 10, avg test_loss: 0.009238, test_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011184, train_acc: 0.73\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010640, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.012585, train_acc: 0.59\n",
      "alexnet1d, trial.2:\n",
      "Epoch 11, avg test_loss: 0.008871, test_acc: 0.69\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011040, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011694, train_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010545, train_acc: 0.68\n",
      "alexnet1d, trial.2:\n",
      "Epoch 12, avg test_loss: 0.009258, test_acc: 0.60\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010629, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011212, train_acc: 0.57\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010933, train_acc: 0.68\n",
      "alexnet1d, trial.2:\n",
      "Epoch 13, avg test_loss: 0.008872, test_acc: 0.60\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010604, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010346, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009930, train_acc: 0.73\n",
      "alexnet1d, trial.2:\n",
      "Epoch 14, avg test_loss: 0.008749, test_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010551, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.011289, train_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010289, train_acc: 0.70\n",
      "alexnet1d, trial.2:\n",
      "Epoch 15, avg test_loss: 0.009562, test_acc: 0.57\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008120, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010564, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.012653, train_acc: 0.68\n",
      "alexnet1d, trial.2:\n",
      "Epoch 16, avg test_loss: 0.009109, test_acc: 0.61\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008420, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009142, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010977, train_acc: 0.70\n",
      "alexnet1d, trial.2:\n",
      "Epoch 17, avg test_loss: 0.008849, test_acc: 0.60\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.010006, train_acc: 0.71\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009001, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008728, train_acc: 0.79\n",
      "alexnet1d, trial.2:\n",
      "Epoch 18, avg test_loss: 0.008791, test_acc: 0.60\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008570, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008876, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009787, train_acc: 0.71\n",
      "alexnet1d, trial.2:\n",
      "Epoch 19, avg test_loss: 0.010461, test_acc: 0.63\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007924, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.010884, train_acc: 0.68\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009165, train_acc: 0.71\n",
      "alexnet1d, trial.2:\n",
      "Epoch 20, avg test_loss: 0.009571, test_acc: 0.63\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007218, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.009940, train_acc: 0.73\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.009941, train_acc: 0.79\n",
      "alexnet1d, trial.2:\n",
      "Epoch 21, avg test_loss: 0.009957, test_acc: 0.60\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.008060, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007417, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007702, train_acc: 0.88\n",
      "alexnet1d, trial.2:\n",
      "Epoch 22, avg test_loss: 0.010666, test_acc: 0.57\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007295, train_acc: 0.80\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007689, train_acc: 0.79\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006965, train_acc: 0.82\n",
      "alexnet1d, trial.2:\n",
      "Epoch 23, avg test_loss: 0.010815, test_acc: 0.64\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.008514, train_acc: 0.77\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005818, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006804, train_acc: 0.84\n",
      "alexnet1d, trial.2:\n",
      "Epoch 24, avg test_loss: 0.012696, test_acc: 0.61\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005115, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005538, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005728, train_acc: 0.88\n",
      "alexnet1d, trial.2:\n",
      "Epoch 25, avg test_loss: 0.015770, test_acc: 0.56\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005581, train_acc: 0.82\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.003469, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.006215, train_acc: 0.82\n",
      "alexnet1d, trial.2:\n",
      "Epoch 26, avg test_loss: 0.014591, test_acc: 0.66\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.006163, train_acc: 0.82\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.003130, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.006591, train_acc: 0.91\n",
      "alexnet1d, trial.2:\n",
      "Epoch 27, avg test_loss: 0.014396, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004476, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.003403, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.006346, train_acc: 0.80\n",
      "alexnet1d, trial.2:\n",
      "Epoch 28, avg test_loss: 0.015644, test_acc: 0.66\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.003122, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.004469, train_acc: 0.89\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.006353, train_acc: 0.89\n",
      "alexnet1d, trial.2:\n",
      "Epoch 29, avg test_loss: 0.015514, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.003600, train_acc: 0.93\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.003922, train_acc: 0.89\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.003533, train_acc: 0.89\n",
      "alexnet1d, trial.2:\n",
      "Epoch 30, avg test_loss: 0.015585, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.002587, train_acc: 0.96\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.003933, train_acc: 0.91\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.001128, train_acc: 1.00\n",
      "alexnet1d, trial.2:\n",
      "Epoch 31, avg test_loss: 0.018950, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号40个\n",
      "错误信号30个\n",
      "信号正确并预测正确的概率为0.271\n",
      "信号错误并预测正确的概率为0.257\n",
      "总正确率为0.53\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012341, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012458, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012299, train_acc: 0.57\n",
      "alexnet1d, trial.3:\n",
      "Epoch 0, avg test_loss: 0.009818, test_acc: 0.63\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012089, train_acc: 0.62\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012221, train_acc: 0.52\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012440, train_acc: 0.54\n",
      "alexnet1d, trial.3:\n",
      "Epoch 1, avg test_loss: 0.009843, test_acc: 0.63\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012146, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012242, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012204, train_acc: 0.57\n",
      "alexnet1d, trial.3:\n",
      "Epoch 2, avg test_loss: 0.009773, test_acc: 0.63\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012422, train_acc: 0.46\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012101, train_acc: 0.50\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011266, train_acc: 0.66\n",
      "alexnet1d, trial.3:\n",
      "Epoch 3, avg test_loss: 0.010319, test_acc: 0.63\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011087, train_acc: 0.66\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.013926, train_acc: 0.46\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011859, train_acc: 0.70\n",
      "alexnet1d, trial.3:\n",
      "Epoch 4, avg test_loss: 0.009791, test_acc: 0.63\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011991, train_acc: 0.68\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012237, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012668, train_acc: 0.46\n",
      "alexnet1d, trial.3:\n",
      "Epoch 5, avg test_loss: 0.009610, test_acc: 0.63\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012038, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012598, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012376, train_acc: 0.52\n",
      "alexnet1d, trial.3:\n",
      "Epoch 6, avg test_loss: 0.009617, test_acc: 0.63\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012376, train_acc: 0.50\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011997, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011781, train_acc: 0.62\n",
      "alexnet1d, trial.3:\n",
      "Epoch 7, avg test_loss: 0.009584, test_acc: 0.63\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011830, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012428, train_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012672, train_acc: 0.46\n",
      "alexnet1d, trial.3:\n",
      "Epoch 8, avg test_loss: 0.009792, test_acc: 0.63\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012027, train_acc: 0.48\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012082, train_acc: 0.55\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011982, train_acc: 0.54\n",
      "alexnet1d, trial.3:\n",
      "Epoch 9, avg test_loss: 0.009850, test_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011964, train_acc: 0.46\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012040, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012403, train_acc: 0.54\n",
      "alexnet1d, trial.3:\n",
      "Epoch 10, avg test_loss: 0.009634, test_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.012164, train_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011536, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011925, train_acc: 0.61\n",
      "alexnet1d, trial.3:\n",
      "Epoch 11, avg test_loss: 0.009914, test_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010809, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.012034, train_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.012012, train_acc: 0.55\n",
      "alexnet1d, trial.3:\n",
      "Epoch 12, avg test_loss: 0.009474, test_acc: 0.63\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010391, train_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011142, train_acc: 0.55\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.012519, train_acc: 0.59\n",
      "alexnet1d, trial.3:\n",
      "Epoch 13, avg test_loss: 0.009738, test_acc: 0.60\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.011044, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011652, train_acc: 0.64\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010927, train_acc: 0.62\n",
      "alexnet1d, trial.3:\n",
      "Epoch 14, avg test_loss: 0.009942, test_acc: 0.59\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010580, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.011334, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009849, train_acc: 0.73\n",
      "alexnet1d, trial.3:\n",
      "Epoch 15, avg test_loss: 0.009691, test_acc: 0.61\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010010, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009976, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.011373, train_acc: 0.64\n",
      "alexnet1d, trial.3:\n",
      "Epoch 16, avg test_loss: 0.009950, test_acc: 0.61\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.010029, train_acc: 0.70\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009538, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.012304, train_acc: 0.64\n",
      "alexnet1d, trial.3:\n",
      "Epoch 17, avg test_loss: 0.010007, test_acc: 0.64\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007460, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.011004, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009515, train_acc: 0.77\n",
      "alexnet1d, trial.3:\n",
      "Epoch 18, avg test_loss: 0.010910, test_acc: 0.54\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009345, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009225, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.010462, train_acc: 0.66\n",
      "alexnet1d, trial.3:\n",
      "Epoch 19, avg test_loss: 0.010200, test_acc: 0.61\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.009304, train_acc: 0.77\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008598, train_acc: 0.75\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.011258, train_acc: 0.73\n",
      "alexnet1d, trial.3:\n",
      "Epoch 20, avg test_loss: 0.010476, test_acc: 0.59\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008090, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.008231, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006132, train_acc: 0.89\n",
      "alexnet1d, trial.3:\n",
      "Epoch 21, avg test_loss: 0.010885, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.009513, train_acc: 0.71\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.009662, train_acc: 0.73\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007592, train_acc: 0.79\n",
      "alexnet1d, trial.3:\n",
      "Epoch 22, avg test_loss: 0.013430, test_acc: 0.51\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.008944, train_acc: 0.80\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007015, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.009506, train_acc: 0.80\n",
      "alexnet1d, trial.3:\n",
      "Epoch 23, avg test_loss: 0.012414, test_acc: 0.61\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006318, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.009367, train_acc: 0.77\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.004496, train_acc: 0.88\n",
      "alexnet1d, trial.3:\n",
      "Epoch 24, avg test_loss: 0.012778, test_acc: 0.61\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.007763, train_acc: 0.82\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.007730, train_acc: 0.79\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005335, train_acc: 0.91\n",
      "alexnet1d, trial.3:\n",
      "Epoch 25, avg test_loss: 0.013624, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005971, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005434, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004773, train_acc: 0.91\n",
      "alexnet1d, trial.3:\n",
      "Epoch 26, avg test_loss: 0.013429, test_acc: 0.67\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004347, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004346, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004157, train_acc: 0.91\n",
      "alexnet1d, trial.3:\n",
      "Epoch 27, avg test_loss: 0.017247, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号44个\n",
      "错误信号26个\n",
      "信号正确并预测正确的概率为0.386\n",
      "信号错误并预测正确的概率为0.243\n",
      "总正确率为0.63\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012375, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.011886, train_acc: 0.64\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012238, train_acc: 0.50\n",
      "alexnet1d, trial.4:\n",
      "Epoch 0, avg test_loss: 0.009801, test_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012089, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012198, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012104, train_acc: 0.61\n",
      "alexnet1d, trial.4:\n",
      "Epoch 1, avg test_loss: 0.009762, test_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011879, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011821, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012248, train_acc: 0.57\n",
      "alexnet1d, trial.4:\n",
      "Epoch 2, avg test_loss: 0.009779, test_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011732, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012137, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012326, train_acc: 0.54\n",
      "alexnet1d, trial.4:\n",
      "Epoch 3, avg test_loss: 0.009847, test_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.010912, train_acc: 0.70\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011996, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012550, train_acc: 0.52\n",
      "alexnet1d, trial.4:\n",
      "Epoch 4, avg test_loss: 0.009850, test_acc: 0.60\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012145, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011794, train_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012465, train_acc: 0.55\n",
      "alexnet1d, trial.4:\n",
      "Epoch 5, avg test_loss: 0.009689, test_acc: 0.63\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011600, train_acc: 0.71\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011851, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011066, train_acc: 0.62\n",
      "alexnet1d, trial.4:\n",
      "Epoch 6, avg test_loss: 0.009896, test_acc: 0.60\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011683, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011479, train_acc: 0.55\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011164, train_acc: 0.68\n",
      "alexnet1d, trial.4:\n",
      "Epoch 7, avg test_loss: 0.010097, test_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011527, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012655, train_acc: 0.52\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012124, train_acc: 0.68\n",
      "alexnet1d, trial.4:\n",
      "Epoch 8, avg test_loss: 0.009402, test_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011619, train_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011778, train_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012507, train_acc: 0.57\n",
      "alexnet1d, trial.4:\n",
      "Epoch 9, avg test_loss: 0.009455, test_acc: 0.60\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011803, train_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011410, train_acc: 0.71\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011040, train_acc: 0.64\n",
      "alexnet1d, trial.4:\n",
      "Epoch 10, avg test_loss: 0.010237, test_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011674, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.012360, train_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010091, train_acc: 0.70\n",
      "alexnet1d, trial.4:\n",
      "Epoch 11, avg test_loss: 0.009864, test_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009311, train_acc: 0.75\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010520, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011828, train_acc: 0.66\n",
      "alexnet1d, trial.4:\n",
      "Epoch 12, avg test_loss: 0.009467, test_acc: 0.74\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011843, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.008870, train_acc: 0.86\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010147, train_acc: 0.68\n",
      "alexnet1d, trial.4:\n",
      "Epoch 13, avg test_loss: 0.009672, test_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.008193, train_acc: 0.82\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010913, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010039, train_acc: 0.70\n",
      "alexnet1d, trial.4:\n",
      "Epoch 14, avg test_loss: 0.011312, test_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009800, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.011935, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009735, train_acc: 0.75\n",
      "alexnet1d, trial.4:\n",
      "Epoch 15, avg test_loss: 0.011412, test_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008380, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008367, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008066, train_acc: 0.77\n",
      "alexnet1d, trial.4:\n",
      "Epoch 16, avg test_loss: 0.011128, test_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007771, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007846, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010015, train_acc: 0.75\n",
      "alexnet1d, trial.4:\n",
      "Epoch 17, avg test_loss: 0.011575, test_acc: 0.66\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008833, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007658, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007293, train_acc: 0.88\n",
      "alexnet1d, trial.4:\n",
      "Epoch 18, avg test_loss: 0.014647, test_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008105, train_acc: 0.75\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007780, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.005239, train_acc: 0.93\n",
      "alexnet1d, trial.4:\n",
      "Epoch 19, avg test_loss: 0.012425, test_acc: 0.70\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008116, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.006827, train_acc: 0.88\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.005406, train_acc: 0.86\n",
      "alexnet1d, trial.4:\n",
      "Epoch 20, avg test_loss: 0.015277, test_acc: 0.71\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.004492, train_acc: 0.89\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.005072, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007508, train_acc: 0.86\n",
      "alexnet1d, trial.4:\n",
      "Epoch 21, avg test_loss: 0.016213, test_acc: 0.70\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.003456, train_acc: 0.93\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.003517, train_acc: 0.96\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.004943, train_acc: 0.86\n",
      "alexnet1d, trial.4:\n",
      "Epoch 22, avg test_loss: 0.017935, test_acc: 0.67\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.003346, train_acc: 0.95\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.003626, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.003736, train_acc: 0.91\n",
      "alexnet1d, trial.4:\n",
      "Epoch 23, avg test_loss: 0.021412, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005159, train_acc: 0.80\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005503, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.004114, train_acc: 0.93\n",
      "alexnet1d, trial.4:\n",
      "Epoch 24, avg test_loss: 0.020029, test_acc: 0.70\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.003976, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.003625, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004729, train_acc: 0.89\n",
      "alexnet1d, trial.4:\n",
      "Epoch 25, avg test_loss: 0.021835, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号40个\n",
      "错误信号30个\n",
      "信号正确并预测正确的概率为0.457\n",
      "信号错误并预测正确的概率为0.157\n",
      "总正确率为0.61\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012378, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012316, train_acc: 0.52\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.011891, train_acc: 0.62\n",
      "alexnet1d, trial.5:\n",
      "Epoch 0, avg test_loss: 0.010062, test_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012295, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.011822, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012259, train_acc: 0.55\n",
      "alexnet1d, trial.5:\n",
      "Epoch 1, avg test_loss: 0.009766, test_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012317, train_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011930, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012175, train_acc: 0.59\n",
      "alexnet1d, trial.5:\n",
      "Epoch 2, avg test_loss: 0.009783, test_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.010760, train_acc: 0.68\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.013781, train_acc: 0.43\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012093, train_acc: 0.61\n",
      "alexnet1d, trial.5:\n",
      "Epoch 3, avg test_loss: 0.009776, test_acc: 0.60\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012235, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012235, train_acc: 0.50\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012034, train_acc: 0.59\n",
      "alexnet1d, trial.5:\n",
      "Epoch 4, avg test_loss: 0.009724, test_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012118, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012419, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011824, train_acc: 0.64\n",
      "alexnet1d, trial.5:\n",
      "Epoch 5, avg test_loss: 0.009670, test_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011489, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011867, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.013375, train_acc: 0.45\n",
      "alexnet1d, trial.5:\n",
      "Epoch 6, avg test_loss: 0.009640, test_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.010706, train_acc: 0.68\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011938, train_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012856, train_acc: 0.61\n",
      "alexnet1d, trial.5:\n",
      "Epoch 7, avg test_loss: 0.009516, test_acc: 0.60\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012517, train_acc: 0.55\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011581, train_acc: 0.68\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011531, train_acc: 0.62\n",
      "alexnet1d, trial.5:\n",
      "Epoch 8, avg test_loss: 0.009535, test_acc: 0.63\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010270, train_acc: 0.75\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012138, train_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010476, train_acc: 0.71\n",
      "alexnet1d, trial.5:\n",
      "Epoch 9, avg test_loss: 0.009490, test_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010423, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010328, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011293, train_acc: 0.66\n",
      "alexnet1d, trial.5:\n",
      "Epoch 10, avg test_loss: 0.009933, test_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.009583, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010947, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.009762, train_acc: 0.73\n",
      "alexnet1d, trial.5:\n",
      "Epoch 11, avg test_loss: 0.010292, test_acc: 0.63\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009493, train_acc: 0.79\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010433, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011570, train_acc: 0.70\n",
      "alexnet1d, trial.5:\n",
      "Epoch 12, avg test_loss: 0.010885, test_acc: 0.63\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010262, train_acc: 0.82\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010602, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.008837, train_acc: 0.77\n",
      "alexnet1d, trial.5:\n",
      "Epoch 13, avg test_loss: 0.010934, test_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009900, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009967, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010022, train_acc: 0.68\n",
      "alexnet1d, trial.5:\n",
      "Epoch 14, avg test_loss: 0.011524, test_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009438, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008042, train_acc: 0.82\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008668, train_acc: 0.77\n",
      "alexnet1d, trial.5:\n",
      "Epoch 15, avg test_loss: 0.011947, test_acc: 0.63\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009052, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.007567, train_acc: 0.82\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010044, train_acc: 0.66\n",
      "alexnet1d, trial.5:\n",
      "Epoch 16, avg test_loss: 0.013835, test_acc: 0.60\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.006332, train_acc: 0.86\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008115, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008596, train_acc: 0.73\n",
      "alexnet1d, trial.5:\n",
      "Epoch 17, avg test_loss: 0.013181, test_acc: 0.64\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007174, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008798, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.006099, train_acc: 0.84\n",
      "alexnet1d, trial.5:\n",
      "Epoch 18, avg test_loss: 0.017102, test_acc: 0.63\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.005440, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.006078, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009316, train_acc: 0.77\n",
      "alexnet1d, trial.5:\n",
      "Epoch 19, avg test_loss: 0.018900, test_acc: 0.60\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007258, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.005622, train_acc: 0.88\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.004590, train_acc: 0.91\n",
      "alexnet1d, trial.5:\n",
      "Epoch 20, avg test_loss: 0.021069, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006337, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006320, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.005889, train_acc: 0.88\n",
      "alexnet1d, trial.5:\n",
      "Epoch 21, avg test_loss: 0.020280, test_acc: 0.57\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.004284, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005509, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006625, train_acc: 0.88\n",
      "alexnet1d, trial.5:\n",
      "Epoch 22, avg test_loss: 0.016352, test_acc: 0.61\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005264, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006211, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.004725, train_acc: 0.88\n",
      "alexnet1d, trial.5:\n",
      "Epoch 23, avg test_loss: 0.024185, test_acc: 0.60\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006802, train_acc: 0.82\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005400, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005563, train_acc: 0.86\n",
      "alexnet1d, trial.5:\n",
      "Epoch 24, avg test_loss: 0.016969, test_acc: 0.61\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.008435, train_acc: 0.79\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.007181, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004212, train_acc: 0.93\n",
      "alexnet1d, trial.5:\n",
      "Epoch 25, avg test_loss: 0.021570, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005052, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004683, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.003101, train_acc: 0.98\n",
      "alexnet1d, trial.5:\n",
      "Epoch 26, avg test_loss: 0.023162, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004719, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.003385, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003555, train_acc: 0.93\n",
      "alexnet1d, trial.5:\n",
      "Epoch 27, avg test_loss: 0.022379, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号40个\n",
      "错误信号30个\n",
      "信号正确并预测正确的概率为0.4\n",
      "信号错误并预测正确的概率为0.2\n",
      "总正确率为0.60\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012334, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.013667, train_acc: 0.52\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012198, train_acc: 0.55\n",
      "alexnet1d, trial.6:\n",
      "Epoch 0, avg test_loss: 0.009848, test_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012360, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012488, train_acc: 0.48\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011976, train_acc: 0.62\n",
      "alexnet1d, trial.6:\n",
      "Epoch 1, avg test_loss: 0.010847, test_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012937, train_acc: 0.50\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012174, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012052, train_acc: 0.59\n",
      "alexnet1d, trial.6:\n",
      "Epoch 2, avg test_loss: 0.009901, test_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011847, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012949, train_acc: 0.50\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012041, train_acc: 0.57\n",
      "alexnet1d, trial.6:\n",
      "Epoch 3, avg test_loss: 0.010001, test_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012054, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012145, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011355, train_acc: 0.70\n",
      "alexnet1d, trial.6:\n",
      "Epoch 4, avg test_loss: 0.009857, test_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011799, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012432, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012491, train_acc: 0.52\n",
      "alexnet1d, trial.6:\n",
      "Epoch 5, avg test_loss: 0.010012, test_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011723, train_acc: 0.52\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012422, train_acc: 0.41\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011621, train_acc: 0.61\n",
      "alexnet1d, trial.6:\n",
      "Epoch 6, avg test_loss: 0.009891, test_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011749, train_acc: 0.50\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011343, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011785, train_acc: 0.61\n",
      "alexnet1d, trial.6:\n",
      "Epoch 7, avg test_loss: 0.010292, test_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011290, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012174, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011303, train_acc: 0.77\n",
      "alexnet1d, trial.6:\n",
      "Epoch 8, avg test_loss: 0.010148, test_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011942, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010082, train_acc: 0.82\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011445, train_acc: 0.55\n",
      "alexnet1d, trial.6:\n",
      "Epoch 9, avg test_loss: 0.010099, test_acc: 0.53\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011809, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010746, train_acc: 0.77\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010620, train_acc: 0.64\n",
      "alexnet1d, trial.6:\n",
      "Epoch 10, avg test_loss: 0.009859, test_acc: 0.56\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010548, train_acc: 0.75\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.009740, train_acc: 0.75\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010661, train_acc: 0.66\n",
      "alexnet1d, trial.6:\n",
      "Epoch 11, avg test_loss: 0.010919, test_acc: 0.54\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009239, train_acc: 0.79\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.008839, train_acc: 0.77\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.009971, train_acc: 0.71\n",
      "alexnet1d, trial.6:\n",
      "Epoch 12, avg test_loss: 0.009785, test_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009555, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.008298, train_acc: 0.79\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010298, train_acc: 0.71\n",
      "alexnet1d, trial.6:\n",
      "Epoch 13, avg test_loss: 0.010354, test_acc: 0.64\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.011575, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009130, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009505, train_acc: 0.80\n",
      "alexnet1d, trial.6:\n",
      "Epoch 14, avg test_loss: 0.011170, test_acc: 0.54\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008466, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008402, train_acc: 0.86\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009718, train_acc: 0.68\n",
      "alexnet1d, trial.6:\n",
      "Epoch 15, avg test_loss: 0.010474, test_acc: 0.56\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010940, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.007860, train_acc: 0.80\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.007003, train_acc: 0.86\n",
      "alexnet1d, trial.6:\n",
      "Epoch 16, avg test_loss: 0.010389, test_acc: 0.64\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007948, train_acc: 0.86\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007780, train_acc: 0.84\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007760, train_acc: 0.82\n",
      "alexnet1d, trial.6:\n",
      "Epoch 17, avg test_loss: 0.012614, test_acc: 0.59\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.006421, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.006605, train_acc: 0.86\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.010749, train_acc: 0.73\n",
      "alexnet1d, trial.6:\n",
      "Epoch 18, avg test_loss: 0.013303, test_acc: 0.57\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007694, train_acc: 0.70\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.006616, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008223, train_acc: 0.79\n",
      "alexnet1d, trial.6:\n",
      "Epoch 19, avg test_loss: 0.011631, test_acc: 0.60\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007468, train_acc: 0.88\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007162, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006395, train_acc: 0.86\n",
      "alexnet1d, trial.6:\n",
      "Epoch 20, avg test_loss: 0.013548, test_acc: 0.63\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.004931, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006277, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006958, train_acc: 0.84\n",
      "alexnet1d, trial.6:\n",
      "Epoch 21, avg test_loss: 0.014468, test_acc: 0.60\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007075, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006226, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006196, train_acc: 0.86\n",
      "alexnet1d, trial.6:\n",
      "Epoch 22, avg test_loss: 0.013126, test_acc: 0.64\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.008471, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.004284, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.005523, train_acc: 0.93\n",
      "alexnet1d, trial.6:\n",
      "Epoch 23, avg test_loss: 0.014373, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005311, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004501, train_acc: 0.95\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006810, train_acc: 0.84\n",
      "alexnet1d, trial.6:\n",
      "Epoch 24, avg test_loss: 0.016033, test_acc: 0.66\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005376, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006003, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004939, train_acc: 0.89\n",
      "alexnet1d, trial.6:\n",
      "Epoch 25, avg test_loss: 0.016647, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004914, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005416, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.002998, train_acc: 0.93\n",
      "alexnet1d, trial.6:\n",
      "Epoch 26, avg test_loss: 0.017498, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.002609, train_acc: 0.96\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004096, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003895, train_acc: 0.93\n",
      "alexnet1d, trial.6:\n",
      "Epoch 27, avg test_loss: 0.021170, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号41个\n",
      "错误信号29个\n",
      "信号正确并预测正确的概率为0.386\n",
      "信号错误并预测正确的概率为0.229\n",
      "总正确率为0.61\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012331, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.013545, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012362, train_acc: 0.52\n",
      "alexnet1d, trial.7:\n",
      "Epoch 0, avg test_loss: 0.009950, test_acc: 0.46\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012477, train_acc: 0.39\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012431, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012348, train_acc: 0.52\n",
      "alexnet1d, trial.7:\n",
      "Epoch 1, avg test_loss: 0.009772, test_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012142, train_acc: 0.66\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012281, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011792, train_acc: 0.68\n",
      "alexnet1d, trial.7:\n",
      "Epoch 2, avg test_loss: 0.009704, test_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012059, train_acc: 0.62\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012564, train_acc: 0.50\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012473, train_acc: 0.54\n",
      "alexnet1d, trial.7:\n",
      "Epoch 3, avg test_loss: 0.009674, test_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011727, train_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011925, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012267, train_acc: 0.57\n",
      "alexnet1d, trial.7:\n",
      "Epoch 4, avg test_loss: 0.009655, test_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012065, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012795, train_acc: 0.46\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012194, train_acc: 0.59\n",
      "alexnet1d, trial.7:\n",
      "Epoch 5, avg test_loss: 0.009684, test_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012281, train_acc: 0.52\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012262, train_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011915, train_acc: 0.64\n",
      "alexnet1d, trial.7:\n",
      "Epoch 6, avg test_loss: 0.009651, test_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012810, train_acc: 0.46\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012266, train_acc: 0.55\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012202, train_acc: 0.59\n",
      "alexnet1d, trial.7:\n",
      "Epoch 7, avg test_loss: 0.009464, test_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012450, train_acc: 0.52\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012025, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011692, train_acc: 0.62\n",
      "alexnet1d, trial.7:\n",
      "Epoch 8, avg test_loss: 0.009491, test_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012322, train_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011615, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011803, train_acc: 0.55\n",
      "alexnet1d, trial.7:\n",
      "Epoch 9, avg test_loss: 0.009574, test_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011789, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012000, train_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011449, train_acc: 0.66\n",
      "alexnet1d, trial.7:\n",
      "Epoch 10, avg test_loss: 0.009242, test_acc: 0.63\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011953, train_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011979, train_acc: 0.55\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011793, train_acc: 0.64\n",
      "alexnet1d, trial.7:\n",
      "Epoch 11, avg test_loss: 0.009524, test_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.012118, train_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011218, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.012333, train_acc: 0.61\n",
      "alexnet1d, trial.7:\n",
      "Epoch 12, avg test_loss: 0.009378, test_acc: 0.60\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011435, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.012029, train_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011105, train_acc: 0.64\n",
      "alexnet1d, trial.7:\n",
      "Epoch 13, avg test_loss: 0.008964, test_acc: 0.67\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010389, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011015, train_acc: 0.62\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011879, train_acc: 0.59\n",
      "alexnet1d, trial.7:\n",
      "Epoch 14, avg test_loss: 0.009497, test_acc: 0.61\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009828, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010721, train_acc: 0.68\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011036, train_acc: 0.61\n",
      "alexnet1d, trial.7:\n",
      "Epoch 15, avg test_loss: 0.008814, test_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.011627, train_acc: 0.55\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010653, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.011651, train_acc: 0.61\n",
      "alexnet1d, trial.7:\n",
      "Epoch 16, avg test_loss: 0.009458, test_acc: 0.61\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.011770, train_acc: 0.68\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010144, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010815, train_acc: 0.66\n",
      "alexnet1d, trial.7:\n",
      "Epoch 17, avg test_loss: 0.009190, test_acc: 0.70\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009190, train_acc: 0.71\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008787, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.012440, train_acc: 0.64\n",
      "alexnet1d, trial.7:\n",
      "Epoch 18, avg test_loss: 0.010225, test_acc: 0.61\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008665, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009676, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009931, train_acc: 0.71\n",
      "alexnet1d, trial.7:\n",
      "Epoch 19, avg test_loss: 0.010688, test_acc: 0.64\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007417, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.010147, train_acc: 0.71\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.010237, train_acc: 0.61\n",
      "alexnet1d, trial.7:\n",
      "Epoch 20, avg test_loss: 0.009813, test_acc: 0.69\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008387, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.009353, train_acc: 0.77\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.011048, train_acc: 0.71\n",
      "alexnet1d, trial.7:\n",
      "Epoch 21, avg test_loss: 0.012200, test_acc: 0.64\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.008133, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007003, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.009030, train_acc: 0.73\n",
      "alexnet1d, trial.7:\n",
      "Epoch 22, avg test_loss: 0.010984, test_acc: 0.64\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.008091, train_acc: 0.79\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006574, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.009066, train_acc: 0.75\n",
      "alexnet1d, trial.7:\n",
      "Epoch 23, avg test_loss: 0.012177, test_acc: 0.67\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005527, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.008715, train_acc: 0.75\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.009706, train_acc: 0.79\n",
      "alexnet1d, trial.7:\n",
      "Epoch 24, avg test_loss: 0.011099, test_acc: 0.70\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.006268, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.009868, train_acc: 0.73\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006428, train_acc: 0.82\n",
      "alexnet1d, trial.7:\n",
      "Epoch 25, avg test_loss: 0.013667, test_acc: 0.59\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.006989, train_acc: 0.79\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.007183, train_acc: 0.82\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.005896, train_acc: 0.84\n",
      "alexnet1d, trial.7:\n",
      "Epoch 26, avg test_loss: 0.011252, test_acc: 0.69\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.005620, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.009530, train_acc: 0.73\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.006224, train_acc: 0.82\n",
      "alexnet1d, trial.7:\n",
      "Epoch 27, avg test_loss: 0.014328, test_acc: 0.61\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.007811, train_acc: 0.80\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.005061, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.005669, train_acc: 0.89\n",
      "alexnet1d, trial.7:\n",
      "Epoch 28, avg test_loss: 0.015221, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.004513, train_acc: 0.89\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.005452, train_acc: 0.84\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.005769, train_acc: 0.86\n",
      "alexnet1d, trial.7:\n",
      "Epoch 29, avg test_loss: 0.018898, test_acc: 0.61\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.003744, train_acc: 0.93\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.007291, train_acc: 0.88\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.004946, train_acc: 0.89\n",
      "alexnet1d, trial.7:\n",
      "Epoch 30, avg test_loss: 0.016915, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.004013, train_acc: 0.93\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.003051, train_acc: 0.95\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.005099, train_acc: 0.86\n",
      "alexnet1d, trial.7:\n",
      "Epoch 31, avg test_loss: 0.020182, test_acc: 0.61\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.003047, train_acc: 0.96\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.004988, train_acc: 0.88\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.004495, train_acc: 0.89\n",
      "alexnet1d, trial.7:\n",
      "Epoch 32, avg test_loss: 0.019531, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 33, lr: 0.000377, 0/280, avg loss: 0.004132, train_acc: 0.89\n",
      "Train Epoch 33, lr: 0.000377, 112/280, avg loss: 0.003549, train_acc: 0.96\n",
      "Train Epoch 33, lr: 0.000377, 224/280, avg loss: 0.003077, train_acc: 0.95\n",
      "alexnet1d, trial.7:\n",
      "Epoch 33, avg test_loss: 0.022499, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号41个\n",
      "错误信号29个\n",
      "信号正确并预测正确的概率为0.414\n",
      "信号错误并预测正确的概率为0.214\n",
      "总正确率为0.63\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012386, train_acc: 0.48\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012889, train_acc: 0.50\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012250, train_acc: 0.57\n",
      "alexnet1d, trial.8:\n",
      "Epoch 0, avg test_loss: 0.010234, test_acc: 0.51\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012929, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012099, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011761, train_acc: 0.70\n",
      "alexnet1d, trial.8:\n",
      "Epoch 1, avg test_loss: 0.010003, test_acc: 0.51\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011963, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011928, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012791, train_acc: 0.52\n",
      "alexnet1d, trial.8:\n",
      "Epoch 2, avg test_loss: 0.010249, test_acc: 0.51\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.013001, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011835, train_acc: 0.62\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011785, train_acc: 0.62\n",
      "alexnet1d, trial.8:\n",
      "Epoch 3, avg test_loss: 0.009950, test_acc: 0.51\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012129, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012250, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012209, train_acc: 0.57\n",
      "alexnet1d, trial.8:\n",
      "Epoch 4, avg test_loss: 0.010100, test_acc: 0.51\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012465, train_acc: 0.48\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011637, train_acc: 0.68\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011351, train_acc: 0.70\n",
      "alexnet1d, trial.8:\n",
      "Epoch 5, avg test_loss: 0.010328, test_acc: 0.51\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011936, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012568, train_acc: 0.52\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011015, train_acc: 0.66\n",
      "alexnet1d, trial.8:\n",
      "Epoch 6, avg test_loss: 0.010334, test_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011352, train_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011921, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.013293, train_acc: 0.54\n",
      "alexnet1d, trial.8:\n",
      "Epoch 7, avg test_loss: 0.010718, test_acc: 0.53\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011512, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011538, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011253, train_acc: 0.57\n",
      "alexnet1d, trial.8:\n",
      "Epoch 8, avg test_loss: 0.010555, test_acc: 0.53\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011600, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011983, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011517, train_acc: 0.62\n",
      "alexnet1d, trial.8:\n",
      "Epoch 9, avg test_loss: 0.011570, test_acc: 0.56\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.012636, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010604, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010294, train_acc: 0.59\n",
      "alexnet1d, trial.8:\n",
      "Epoch 10, avg test_loss: 0.013521, test_acc: 0.56\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010352, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011251, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010503, train_acc: 0.75\n",
      "alexnet1d, trial.8:\n",
      "Epoch 11, avg test_loss: 0.009955, test_acc: 0.54\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009870, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.012158, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.009687, train_acc: 0.70\n",
      "alexnet1d, trial.8:\n",
      "Epoch 12, avg test_loss: 0.010633, test_acc: 0.56\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009119, train_acc: 0.79\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010581, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.012957, train_acc: 0.62\n",
      "alexnet1d, trial.8:\n",
      "Epoch 13, avg test_loss: 0.011787, test_acc: 0.54\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.011260, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.008984, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.007956, train_acc: 0.80\n",
      "alexnet1d, trial.8:\n",
      "Epoch 14, avg test_loss: 0.010825, test_acc: 0.54\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009790, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008256, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008393, train_acc: 0.79\n",
      "alexnet1d, trial.8:\n",
      "Epoch 15, avg test_loss: 0.012063, test_acc: 0.53\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.007838, train_acc: 0.80\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.014893, train_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.007421, train_acc: 0.84\n",
      "alexnet1d, trial.8:\n",
      "Epoch 16, avg test_loss: 0.011013, test_acc: 0.57\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008521, train_acc: 0.70\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009957, train_acc: 0.71\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008604, train_acc: 0.82\n",
      "alexnet1d, trial.8:\n",
      "Epoch 17, avg test_loss: 0.015726, test_acc: 0.49\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008792, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009248, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008457, train_acc: 0.82\n",
      "alexnet1d, trial.8:\n",
      "Epoch 18, avg test_loss: 0.012241, test_acc: 0.50\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008587, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007925, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007715, train_acc: 0.84\n",
      "alexnet1d, trial.8:\n",
      "Epoch 19, avg test_loss: 0.011476, test_acc: 0.54\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007349, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007835, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008188, train_acc: 0.75\n",
      "alexnet1d, trial.8:\n",
      "Epoch 20, avg test_loss: 0.013873, test_acc: 0.50\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006105, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006985, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.010528, train_acc: 0.73\n",
      "alexnet1d, trial.8:\n",
      "Epoch 21, avg test_loss: 0.015916, test_acc: 0.51\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007094, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007233, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006812, train_acc: 0.88\n",
      "alexnet1d, trial.8:\n",
      "Epoch 22, avg test_loss: 0.015513, test_acc: 0.53\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007571, train_acc: 0.80\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006125, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.007437, train_acc: 0.82\n",
      "alexnet1d, trial.8:\n",
      "Epoch 23, avg test_loss: 0.017226, test_acc: 0.51\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004912, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005385, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.003538, train_acc: 0.91\n",
      "alexnet1d, trial.8:\n",
      "Epoch 24, avg test_loss: 0.026962, test_acc: 0.50\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.003276, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005350, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006771, train_acc: 0.82\n",
      "alexnet1d, trial.8:\n",
      "Epoch 25, avg test_loss: 0.022487, test_acc: 0.51\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.002548, train_acc: 0.98\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.009492, train_acc: 0.79\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.003443, train_acc: 0.96\n",
      "alexnet1d, trial.8:\n",
      "Epoch 26, avg test_loss: 0.020707, test_acc: 0.50\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004664, train_acc: 0.88\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.006408, train_acc: 0.80\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004275, train_acc: 0.91\n",
      "alexnet1d, trial.8:\n",
      "Epoch 27, avg test_loss: 0.028635, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004194, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004226, train_acc: 0.88\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.003749, train_acc: 0.95\n",
      "alexnet1d, trial.8:\n",
      "Epoch 28, avg test_loss: 0.020149, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号36个\n",
      "错误信号34个\n",
      "信号正确并预测正确的概率为0.286\n",
      "信号错误并预测正确的概率为0.257\n",
      "总正确率为0.54\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012318, train_acc: 0.64\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012184, train_acc: 0.61\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012448, train_acc: 0.50\n",
      "alexnet1d, trial.9:\n",
      "Epoch 0, avg test_loss: 0.009614, test_acc: 0.60\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012110, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012494, train_acc: 0.52\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011891, train_acc: 0.68\n",
      "alexnet1d, trial.9:\n",
      "Epoch 1, avg test_loss: 0.009740, test_acc: 0.60\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012362, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012091, train_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012265, train_acc: 0.55\n",
      "alexnet1d, trial.9:\n",
      "Epoch 2, avg test_loss: 0.009572, test_acc: 0.60\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012019, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011578, train_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.014158, train_acc: 0.45\n",
      "alexnet1d, trial.9:\n",
      "Epoch 3, avg test_loss: 0.009486, test_acc: 0.60\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011859, train_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011823, train_acc: 0.70\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012660, train_acc: 0.46\n",
      "alexnet1d, trial.9:\n",
      "Epoch 4, avg test_loss: 0.009710, test_acc: 0.60\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011872, train_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012513, train_acc: 0.43\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012089, train_acc: 0.57\n",
      "alexnet1d, trial.9:\n",
      "Epoch 5, avg test_loss: 0.009690, test_acc: 0.60\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012133, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011647, train_acc: 0.68\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012159, train_acc: 0.50\n",
      "alexnet1d, trial.9:\n",
      "Epoch 6, avg test_loss: 0.009534, test_acc: 0.60\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012144, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012131, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012261, train_acc: 0.52\n",
      "alexnet1d, trial.9:\n",
      "Epoch 7, avg test_loss: 0.009454, test_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011876, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012073, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012118, train_acc: 0.57\n",
      "alexnet1d, trial.9:\n",
      "Epoch 8, avg test_loss: 0.009358, test_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012110, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011942, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011430, train_acc: 0.57\n",
      "alexnet1d, trial.9:\n",
      "Epoch 9, avg test_loss: 0.009329, test_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010821, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011588, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012263, train_acc: 0.64\n",
      "alexnet1d, trial.9:\n",
      "Epoch 10, avg test_loss: 0.009254, test_acc: 0.63\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011483, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010775, train_acc: 0.75\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010898, train_acc: 0.64\n",
      "alexnet1d, trial.9:\n",
      "Epoch 11, avg test_loss: 0.009220, test_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010571, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010708, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011471, train_acc: 0.66\n",
      "alexnet1d, trial.9:\n",
      "Epoch 12, avg test_loss: 0.008875, test_acc: 0.63\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011177, train_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011650, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009853, train_acc: 0.71\n",
      "alexnet1d, trial.9:\n",
      "Epoch 13, avg test_loss: 0.009110, test_acc: 0.60\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009955, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010461, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009700, train_acc: 0.70\n",
      "alexnet1d, trial.9:\n",
      "Epoch 14, avg test_loss: 0.008990, test_acc: 0.61\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.011754, train_acc: 0.68\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009300, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010801, train_acc: 0.62\n",
      "alexnet1d, trial.9:\n",
      "Epoch 15, avg test_loss: 0.009662, test_acc: 0.60\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.011251, train_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010114, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009830, train_acc: 0.71\n",
      "alexnet1d, trial.9:\n",
      "Epoch 16, avg test_loss: 0.009268, test_acc: 0.67\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009774, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010472, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007100, train_acc: 0.86\n",
      "alexnet1d, trial.9:\n",
      "Epoch 17, avg test_loss: 0.009953, test_acc: 0.64\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007596, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008193, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007962, train_acc: 0.86\n",
      "alexnet1d, trial.9:\n",
      "Epoch 18, avg test_loss: 0.011007, test_acc: 0.57\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006550, train_acc: 0.88\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008996, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.006938, train_acc: 0.89\n",
      "alexnet1d, trial.9:\n",
      "Epoch 19, avg test_loss: 0.012456, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007470, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.005792, train_acc: 0.91\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007291, train_acc: 0.79\n",
      "alexnet1d, trial.9:\n",
      "Epoch 20, avg test_loss: 0.012035, test_acc: 0.69\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.005393, train_acc: 0.89\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.008585, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.010273, train_acc: 0.79\n",
      "alexnet1d, trial.9:\n",
      "Epoch 21, avg test_loss: 0.016077, test_acc: 0.60\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.004940, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005407, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008276, train_acc: 0.79\n",
      "alexnet1d, trial.9:\n",
      "Epoch 22, avg test_loss: 0.015271, test_acc: 0.57\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006438, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.008460, train_acc: 0.79\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.005897, train_acc: 0.91\n",
      "alexnet1d, trial.9:\n",
      "Epoch 23, avg test_loss: 0.013788, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006232, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004691, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006432, train_acc: 0.89\n",
      "alexnet1d, trial.9:\n",
      "Epoch 24, avg test_loss: 0.014640, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004676, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.007974, train_acc: 0.80\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.002558, train_acc: 0.98\n",
      "alexnet1d, trial.9:\n",
      "Epoch 25, avg test_loss: 0.015691, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号42个\n",
      "错误信号28个\n",
      "信号正确并预测正确的概率为0.343\n",
      "信号错误并预测正确的概率为0.271\n",
      "总正确率为0.61\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012336, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012407, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012227, train_acc: 0.59\n",
      "alexnet1d, trial.10:\n",
      "Epoch 0, avg test_loss: 0.009763, test_acc: 0.56\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012055, train_acc: 0.46\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.011672, train_acc: 0.70\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012129, train_acc: 0.66\n",
      "alexnet1d, trial.10:\n",
      "Epoch 1, avg test_loss: 0.009787, test_acc: 0.56\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011796, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011836, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011634, train_acc: 0.66\n",
      "alexnet1d, trial.10:\n",
      "Epoch 2, avg test_loss: 0.009706, test_acc: 0.56\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011724, train_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012606, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012782, train_acc: 0.52\n",
      "alexnet1d, trial.10:\n",
      "Epoch 3, avg test_loss: 0.009760, test_acc: 0.56\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011425, train_acc: 0.68\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011456, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011443, train_acc: 0.57\n",
      "alexnet1d, trial.10:\n",
      "Epoch 4, avg test_loss: 0.009841, test_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.010601, train_acc: 0.77\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.010926, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012924, train_acc: 0.62\n",
      "alexnet1d, trial.10:\n",
      "Epoch 5, avg test_loss: 0.009681, test_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011363, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011947, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011240, train_acc: 0.71\n",
      "alexnet1d, trial.10:\n",
      "Epoch 6, avg test_loss: 0.009452, test_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.010984, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.010873, train_acc: 0.68\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012245, train_acc: 0.57\n",
      "alexnet1d, trial.10:\n",
      "Epoch 7, avg test_loss: 0.009780, test_acc: 0.60\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010951, train_acc: 0.68\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011552, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.010812, train_acc: 0.66\n",
      "alexnet1d, trial.10:\n",
      "Epoch 8, avg test_loss: 0.009565, test_acc: 0.56\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010323, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010272, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.013575, train_acc: 0.59\n",
      "alexnet1d, trial.10:\n",
      "Epoch 9, avg test_loss: 0.009961, test_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010296, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010327, train_acc: 0.71\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011939, train_acc: 0.61\n",
      "alexnet1d, trial.10:\n",
      "Epoch 10, avg test_loss: 0.009434, test_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010734, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.009432, train_acc: 0.86\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.009569, train_acc: 0.73\n",
      "alexnet1d, trial.10:\n",
      "Epoch 11, avg test_loss: 0.009828, test_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010636, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.009518, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.008584, train_acc: 0.75\n",
      "alexnet1d, trial.10:\n",
      "Epoch 12, avg test_loss: 0.010719, test_acc: 0.60\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.008204, train_acc: 0.82\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009739, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010769, train_acc: 0.66\n",
      "alexnet1d, trial.10:\n",
      "Epoch 13, avg test_loss: 0.009636, test_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010540, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010689, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011141, train_acc: 0.66\n",
      "alexnet1d, trial.10:\n",
      "Epoch 14, avg test_loss: 0.010815, test_acc: 0.60\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010296, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.011149, train_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009875, train_acc: 0.71\n",
      "alexnet1d, trial.10:\n",
      "Epoch 15, avg test_loss: 0.009856, test_acc: 0.60\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008585, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.007685, train_acc: 0.91\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009897, train_acc: 0.73\n",
      "alexnet1d, trial.10:\n",
      "Epoch 16, avg test_loss: 0.010141, test_acc: 0.63\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007061, train_acc: 0.84\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008736, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008490, train_acc: 0.79\n",
      "alexnet1d, trial.10:\n",
      "Epoch 17, avg test_loss: 0.010752, test_acc: 0.61\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.006586, train_acc: 0.89\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009359, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008393, train_acc: 0.79\n",
      "alexnet1d, trial.10:\n",
      "Epoch 18, avg test_loss: 0.011348, test_acc: 0.59\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009585, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007033, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007930, train_acc: 0.80\n",
      "alexnet1d, trial.10:\n",
      "Epoch 19, avg test_loss: 0.011345, test_acc: 0.61\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008359, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.005811, train_acc: 0.89\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006589, train_acc: 0.88\n",
      "alexnet1d, trial.10:\n",
      "Epoch 20, avg test_loss: 0.011546, test_acc: 0.63\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.005231, train_acc: 0.91\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.009772, train_acc: 0.77\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007328, train_acc: 0.80\n",
      "alexnet1d, trial.10:\n",
      "Epoch 21, avg test_loss: 0.013731, test_acc: 0.56\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006113, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007287, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008565, train_acc: 0.77\n",
      "alexnet1d, trial.10:\n",
      "Epoch 22, avg test_loss: 0.014101, test_acc: 0.50\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005573, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.003558, train_acc: 0.96\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.007059, train_acc: 0.80\n",
      "alexnet1d, trial.10:\n",
      "Epoch 23, avg test_loss: 0.012492, test_acc: 0.61\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005927, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.003854, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006799, train_acc: 0.84\n",
      "alexnet1d, trial.10:\n",
      "Epoch 24, avg test_loss: 0.015506, test_acc: 0.59\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004315, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004362, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005057, train_acc: 0.89\n",
      "alexnet1d, trial.10:\n",
      "Epoch 25, avg test_loss: 0.016015, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.003525, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004041, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.003930, train_acc: 0.95\n",
      "alexnet1d, trial.10:\n",
      "Epoch 26, avg test_loss: 0.016111, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003093, train_acc: 0.96\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004259, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004023, train_acc: 0.93\n",
      "alexnet1d, trial.10:\n",
      "Epoch 27, avg test_loss: 0.017550, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.002932, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.003237, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.001614, train_acc: 0.96\n",
      "alexnet1d, trial.10:\n",
      "Epoch 28, avg test_loss: 0.021032, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号39个\n",
      "错误信号31个\n",
      "信号正确并预测正确的概率为0.4\n",
      "信号错误并预测正确的概率为0.143\n",
      "总正确率为0.54\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012417, train_acc: 0.48\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012367, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.015124, train_acc: 0.61\n",
      "alexnet1d, trial.11:\n",
      "Epoch 0, avg test_loss: 0.010352, test_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.014350, train_acc: 0.38\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012381, train_acc: 0.45\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012421, train_acc: 0.45\n",
      "alexnet1d, trial.11:\n",
      "Epoch 1, avg test_loss: 0.009874, test_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012355, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012303, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012351, train_acc: 0.54\n",
      "alexnet1d, trial.11:\n",
      "Epoch 2, avg test_loss: 0.009761, test_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012472, train_acc: 0.50\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012006, train_acc: 0.62\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011922, train_acc: 0.64\n",
      "alexnet1d, trial.11:\n",
      "Epoch 3, avg test_loss: 0.009776, test_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012084, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011543, train_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011902, train_acc: 0.64\n",
      "alexnet1d, trial.11:\n",
      "Epoch 4, avg test_loss: 0.009838, test_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011844, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012102, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012001, train_acc: 0.57\n",
      "alexnet1d, trial.11:\n",
      "Epoch 5, avg test_loss: 0.009781, test_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012416, train_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011655, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012528, train_acc: 0.54\n",
      "alexnet1d, trial.11:\n",
      "Epoch 6, avg test_loss: 0.009922, test_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011919, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012773, train_acc: 0.50\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012650, train_acc: 0.48\n",
      "alexnet1d, trial.11:\n",
      "Epoch 7, avg test_loss: 0.009870, test_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011875, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012391, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012113, train_acc: 0.57\n",
      "alexnet1d, trial.11:\n",
      "Epoch 8, avg test_loss: 0.009820, test_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012230, train_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011249, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012383, train_acc: 0.45\n",
      "alexnet1d, trial.11:\n",
      "Epoch 9, avg test_loss: 0.010056, test_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.012345, train_acc: 0.50\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012195, train_acc: 0.55\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010275, train_acc: 0.77\n",
      "alexnet1d, trial.11:\n",
      "Epoch 10, avg test_loss: 0.009981, test_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011931, train_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.012972, train_acc: 0.50\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010899, train_acc: 0.75\n",
      "alexnet1d, trial.11:\n",
      "Epoch 11, avg test_loss: 0.009999, test_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011087, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011372, train_acc: 0.54\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011374, train_acc: 0.55\n",
      "alexnet1d, trial.11:\n",
      "Epoch 12, avg test_loss: 0.009837, test_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011618, train_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011875, train_acc: 0.55\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011746, train_acc: 0.61\n",
      "alexnet1d, trial.11:\n",
      "Epoch 13, avg test_loss: 0.010257, test_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010773, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011031, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.013299, train_acc: 0.48\n",
      "alexnet1d, trial.11:\n",
      "Epoch 14, avg test_loss: 0.010164, test_acc: 0.60\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010111, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.012190, train_acc: 0.62\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010512, train_acc: 0.68\n",
      "alexnet1d, trial.11:\n",
      "Epoch 15, avg test_loss: 0.009649, test_acc: 0.64\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010455, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.011746, train_acc: 0.59\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.013381, train_acc: 0.62\n",
      "alexnet1d, trial.11:\n",
      "Epoch 16, avg test_loss: 0.010182, test_acc: 0.63\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.010135, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.011426, train_acc: 0.62\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.012082, train_acc: 0.70\n",
      "alexnet1d, trial.11:\n",
      "Epoch 17, avg test_loss: 0.010871, test_acc: 0.57\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.010448, train_acc: 0.70\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.011099, train_acc: 0.70\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.011035, train_acc: 0.68\n",
      "alexnet1d, trial.11:\n",
      "Epoch 18, avg test_loss: 0.009715, test_acc: 0.61\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.010310, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.010708, train_acc: 0.68\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009748, train_acc: 0.77\n",
      "alexnet1d, trial.11:\n",
      "Epoch 19, avg test_loss: 0.011513, test_acc: 0.61\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.010148, train_acc: 0.73\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.010500, train_acc: 0.68\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.010542, train_acc: 0.64\n",
      "alexnet1d, trial.11:\n",
      "Epoch 20, avg test_loss: 0.009953, test_acc: 0.63\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.010052, train_acc: 0.75\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.009972, train_acc: 0.73\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.010978, train_acc: 0.62\n",
      "alexnet1d, trial.11:\n",
      "Epoch 21, avg test_loss: 0.010517, test_acc: 0.57\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.009866, train_acc: 0.77\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007999, train_acc: 0.75\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.010274, train_acc: 0.73\n",
      "alexnet1d, trial.11:\n",
      "Epoch 22, avg test_loss: 0.010990, test_acc: 0.59\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.010629, train_acc: 0.71\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.008509, train_acc: 0.77\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.009905, train_acc: 0.73\n",
      "alexnet1d, trial.11:\n",
      "Epoch 23, avg test_loss: 0.010649, test_acc: 0.63\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.008399, train_acc: 0.79\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.007671, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.007906, train_acc: 0.79\n",
      "alexnet1d, trial.11:\n",
      "Epoch 24, avg test_loss: 0.012026, test_acc: 0.56\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.008788, train_acc: 0.79\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.007562, train_acc: 0.82\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.008613, train_acc: 0.80\n",
      "alexnet1d, trial.11:\n",
      "Epoch 25, avg test_loss: 0.012810, test_acc: 0.51\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.006243, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.007782, train_acc: 0.79\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.007871, train_acc: 0.79\n",
      "alexnet1d, trial.11:\n",
      "Epoch 26, avg test_loss: 0.011994, test_acc: 0.59\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.008945, train_acc: 0.82\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.005905, train_acc: 0.88\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.008353, train_acc: 0.79\n",
      "alexnet1d, trial.11:\n",
      "Epoch 27, avg test_loss: 0.013575, test_acc: 0.51\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.007780, train_acc: 0.80\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.008138, train_acc: 0.84\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.006114, train_acc: 0.93\n",
      "alexnet1d, trial.11:\n",
      "Epoch 28, avg test_loss: 0.013124, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.006300, train_acc: 0.82\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.007971, train_acc: 0.80\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.005669, train_acc: 0.84\n",
      "alexnet1d, trial.11:\n",
      "Epoch 29, avg test_loss: 0.015754, test_acc: 0.56\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.006534, train_acc: 0.88\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.005747, train_acc: 0.84\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.009589, train_acc: 0.75\n",
      "alexnet1d, trial.11:\n",
      "Epoch 30, avg test_loss: 0.013989, test_acc: 0.60\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.006040, train_acc: 0.84\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.005593, train_acc: 0.91\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.007735, train_acc: 0.84\n",
      "alexnet1d, trial.11:\n",
      "Epoch 31, avg test_loss: 0.014243, test_acc: 0.54\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.006887, train_acc: 0.82\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.005112, train_acc: 0.93\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.008055, train_acc: 0.82\n",
      "alexnet1d, trial.11:\n",
      "Epoch 32, avg test_loss: 0.012768, test_acc: 0.57\n",
      "Train Epoch 33, lr: 0.000377, 0/280, avg loss: 0.006466, train_acc: 0.88\n",
      "Train Epoch 33, lr: 0.000377, 112/280, avg loss: 0.005006, train_acc: 0.88\n",
      "Train Epoch 33, lr: 0.000377, 224/280, avg loss: 0.005434, train_acc: 0.89\n",
      "alexnet1d, trial.11:\n",
      "Epoch 33, avg test_loss: 0.016795, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 34, lr: 0.000377, 0/280, avg loss: 0.005688, train_acc: 0.86\n",
      "Train Epoch 34, lr: 0.000377, 112/280, avg loss: 0.005975, train_acc: 0.84\n",
      "Train Epoch 34, lr: 0.000377, 224/280, avg loss: 0.005922, train_acc: 0.88\n",
      "alexnet1d, trial.11:\n",
      "Epoch 34, avg test_loss: 0.017189, test_acc: 0.57\n",
      "Train Epoch 35, lr: 0.000321, 0/280, avg loss: 0.005419, train_acc: 0.88\n",
      "Train Epoch 35, lr: 0.000321, 112/280, avg loss: 0.004216, train_acc: 0.89\n",
      "Train Epoch 35, lr: 0.000321, 224/280, avg loss: 0.004419, train_acc: 0.89\n",
      "alexnet1d, trial.11:\n",
      "Epoch 35, avg test_loss: 0.018336, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 36, lr: 0.000321, 0/280, avg loss: 0.003063, train_acc: 0.91\n",
      "Train Epoch 36, lr: 0.000321, 112/280, avg loss: 0.003902, train_acc: 0.89\n",
      "Train Epoch 36, lr: 0.000321, 224/280, avg loss: 0.003275, train_acc: 0.96\n",
      "alexnet1d, trial.11:\n",
      "Epoch 36, avg test_loss: 0.019092, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号41个\n",
      "错误信号29个\n",
      "信号正确并预测正确的概率为0.429\n",
      "信号错误并预测正确的概率为0.157\n",
      "总正确率为0.59\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012387, train_acc: 0.43\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012148, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.011762, train_acc: 0.68\n",
      "alexnet1d, trial.12:\n",
      "Epoch 0, avg test_loss: 0.010926, test_acc: 0.46\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.013458, train_acc: 0.46\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.011976, train_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011803, train_acc: 0.64\n",
      "alexnet1d, trial.12:\n",
      "Epoch 1, avg test_loss: 0.010342, test_acc: 0.46\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012053, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012427, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011686, train_acc: 0.68\n",
      "alexnet1d, trial.12:\n",
      "Epoch 2, avg test_loss: 0.010636, test_acc: 0.46\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012360, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012375, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011429, train_acc: 0.66\n",
      "alexnet1d, trial.12:\n",
      "Epoch 3, avg test_loss: 0.010851, test_acc: 0.46\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011337, train_acc: 0.66\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012176, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012273, train_acc: 0.55\n",
      "alexnet1d, trial.12:\n",
      "Epoch 4, avg test_loss: 0.011513, test_acc: 0.46\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011456, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012572, train_acc: 0.50\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011338, train_acc: 0.68\n",
      "alexnet1d, trial.12:\n",
      "Epoch 5, avg test_loss: 0.011209, test_acc: 0.46\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012860, train_acc: 0.45\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011264, train_acc: 0.70\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012293, train_acc: 0.61\n",
      "alexnet1d, trial.12:\n",
      "Epoch 6, avg test_loss: 0.013519, test_acc: 0.44\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.010936, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011474, train_acc: 0.73\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011767, train_acc: 0.68\n",
      "alexnet1d, trial.12:\n",
      "Epoch 7, avg test_loss: 0.010934, test_acc: 0.44\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011054, train_acc: 0.73\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011952, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011455, train_acc: 0.57\n",
      "alexnet1d, trial.12:\n",
      "Epoch 8, avg test_loss: 0.012573, test_acc: 0.47\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010420, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012034, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010849, train_acc: 0.66\n",
      "alexnet1d, trial.12:\n",
      "Epoch 9, avg test_loss: 0.012429, test_acc: 0.43\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011723, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010417, train_acc: 0.71\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010954, train_acc: 0.64\n",
      "alexnet1d, trial.12:\n",
      "Epoch 10, avg test_loss: 0.012019, test_acc: 0.49\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010774, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011273, train_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010034, train_acc: 0.70\n",
      "alexnet1d, trial.12:\n",
      "Epoch 11, avg test_loss: 0.012961, test_acc: 0.44\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010268, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.008588, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010247, train_acc: 0.73\n",
      "alexnet1d, trial.12:\n",
      "Epoch 12, avg test_loss: 0.013776, test_acc: 0.51\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009926, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009689, train_acc: 0.79\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011020, train_acc: 0.70\n",
      "alexnet1d, trial.12:\n",
      "Epoch 13, avg test_loss: 0.014640, test_acc: 0.50\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.008247, train_acc: 0.79\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.008974, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.013181, train_acc: 0.61\n",
      "alexnet1d, trial.12:\n",
      "Epoch 14, avg test_loss: 0.014893, test_acc: 0.51\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008805, train_acc: 0.82\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009311, train_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009188, train_acc: 0.70\n",
      "alexnet1d, trial.12:\n",
      "Epoch 15, avg test_loss: 0.014612, test_acc: 0.49\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.007570, train_acc: 0.80\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008062, train_acc: 0.82\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009142, train_acc: 0.77\n",
      "alexnet1d, trial.12:\n",
      "Epoch 16, avg test_loss: 0.013631, test_acc: 0.57\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007132, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009229, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007925, train_acc: 0.75\n",
      "alexnet1d, trial.12:\n",
      "Epoch 17, avg test_loss: 0.018861, test_acc: 0.54\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.005129, train_acc: 0.86\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007048, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009041, train_acc: 0.82\n",
      "alexnet1d, trial.12:\n",
      "Epoch 18, avg test_loss: 0.016782, test_acc: 0.56\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006047, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008101, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007402, train_acc: 0.86\n",
      "alexnet1d, trial.12:\n",
      "Epoch 19, avg test_loss: 0.022625, test_acc: 0.53\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008520, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.005904, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.010206, train_acc: 0.73\n",
      "alexnet1d, trial.12:\n",
      "Epoch 20, avg test_loss: 0.019971, test_acc: 0.56\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007588, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007821, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007022, train_acc: 0.88\n",
      "alexnet1d, trial.12:\n",
      "Epoch 21, avg test_loss: 0.017816, test_acc: 0.46\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.008424, train_acc: 0.75\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006662, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.004642, train_acc: 0.93\n",
      "alexnet1d, trial.12:\n",
      "Epoch 22, avg test_loss: 0.018855, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005151, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.004467, train_acc: 0.95\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.005838, train_acc: 0.88\n",
      "alexnet1d, trial.12:\n",
      "Epoch 23, avg test_loss: 0.020735, test_acc: 0.53\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.003674, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006734, train_acc: 0.82\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.003528, train_acc: 0.95\n",
      "alexnet1d, trial.12:\n",
      "Epoch 24, avg test_loss: 0.023973, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.003018, train_acc: 0.96\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004840, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004479, train_acc: 0.91\n",
      "alexnet1d, trial.12:\n",
      "Epoch 25, avg test_loss: 0.026083, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.002291, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.003621, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.003135, train_acc: 0.95\n",
      "alexnet1d, trial.12:\n",
      "Epoch 26, avg test_loss: 0.028102, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号32个\n",
      "错误信号38个\n",
      "信号正确并预测正确的概率为0.329\n",
      "信号错误并预测正确的概率为0.214\n",
      "总正确率为0.54\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012352, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.011919, train_acc: 0.68\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.022203, train_acc: 0.55\n",
      "alexnet1d, trial.13:\n",
      "Epoch 0, avg test_loss: 0.010428, test_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012619, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012442, train_acc: 0.39\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012493, train_acc: 0.41\n",
      "alexnet1d, trial.13:\n",
      "Epoch 1, avg test_loss: 0.009898, test_acc: 0.50\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012429, train_acc: 0.48\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012360, train_acc: 0.52\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012245, train_acc: 0.64\n",
      "alexnet1d, trial.13:\n",
      "Epoch 2, avg test_loss: 0.009945, test_acc: 0.50\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012321, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011767, train_acc: 0.68\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011889, train_acc: 0.59\n",
      "alexnet1d, trial.13:\n",
      "Epoch 3, avg test_loss: 0.010944, test_acc: 0.50\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011103, train_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011566, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012573, train_acc: 0.55\n",
      "alexnet1d, trial.13:\n",
      "Epoch 4, avg test_loss: 0.010130, test_acc: 0.50\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012002, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012081, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011881, train_acc: 0.64\n",
      "alexnet1d, trial.13:\n",
      "Epoch 5, avg test_loss: 0.009977, test_acc: 0.50\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011780, train_acc: 0.68\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012175, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012272, train_acc: 0.55\n",
      "alexnet1d, trial.13:\n",
      "Epoch 6, avg test_loss: 0.010054, test_acc: 0.50\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011918, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011496, train_acc: 0.68\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012371, train_acc: 0.54\n",
      "alexnet1d, trial.13:\n",
      "Epoch 7, avg test_loss: 0.010321, test_acc: 0.50\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011517, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012988, train_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012185, train_acc: 0.55\n",
      "alexnet1d, trial.13:\n",
      "Epoch 8, avg test_loss: 0.010439, test_acc: 0.50\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011641, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011425, train_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.013248, train_acc: 0.45\n",
      "alexnet1d, trial.13:\n",
      "Epoch 9, avg test_loss: 0.010454, test_acc: 0.50\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.012334, train_acc: 0.55\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011234, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012330, train_acc: 0.57\n",
      "alexnet1d, trial.13:\n",
      "Epoch 10, avg test_loss: 0.010300, test_acc: 0.50\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011811, train_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011536, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011821, train_acc: 0.62\n",
      "alexnet1d, trial.13:\n",
      "Epoch 11, avg test_loss: 0.010392, test_acc: 0.50\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011027, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.013470, train_acc: 0.45\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011119, train_acc: 0.68\n",
      "alexnet1d, trial.13:\n",
      "Epoch 12, avg test_loss: 0.010620, test_acc: 0.50\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011815, train_acc: 0.52\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011011, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011521, train_acc: 0.57\n",
      "alexnet1d, trial.13:\n",
      "Epoch 13, avg test_loss: 0.010850, test_acc: 0.50\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010941, train_acc: 0.64\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010614, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011567, train_acc: 0.61\n",
      "alexnet1d, trial.13:\n",
      "Epoch 14, avg test_loss: 0.010887, test_acc: 0.49\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.011142, train_acc: 0.59\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.012205, train_acc: 0.57\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011837, train_acc: 0.59\n",
      "alexnet1d, trial.13:\n",
      "Epoch 15, avg test_loss: 0.011107, test_acc: 0.50\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.011391, train_acc: 0.62\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.011839, train_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010895, train_acc: 0.59\n",
      "alexnet1d, trial.13:\n",
      "Epoch 16, avg test_loss: 0.010482, test_acc: 0.57\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.012082, train_acc: 0.61\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010410, train_acc: 0.68\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.011157, train_acc: 0.59\n",
      "alexnet1d, trial.13:\n",
      "Epoch 17, avg test_loss: 0.011475, test_acc: 0.57\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.011024, train_acc: 0.66\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.010264, train_acc: 0.70\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.011833, train_acc: 0.64\n",
      "alexnet1d, trial.13:\n",
      "Epoch 18, avg test_loss: 0.011365, test_acc: 0.57\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008133, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009013, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.012215, train_acc: 0.66\n",
      "alexnet1d, trial.13:\n",
      "Epoch 19, avg test_loss: 0.012745, test_acc: 0.50\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.010798, train_acc: 0.73\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.011413, train_acc: 0.73\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009964, train_acc: 0.70\n",
      "alexnet1d, trial.13:\n",
      "Epoch 20, avg test_loss: 0.011068, test_acc: 0.47\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008919, train_acc: 0.77\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.010028, train_acc: 0.71\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.009766, train_acc: 0.66\n",
      "alexnet1d, trial.13:\n",
      "Epoch 21, avg test_loss: 0.010774, test_acc: 0.50\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.010482, train_acc: 0.71\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.011500, train_acc: 0.66\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008295, train_acc: 0.84\n",
      "alexnet1d, trial.13:\n",
      "Epoch 22, avg test_loss: 0.011792, test_acc: 0.49\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.008746, train_acc: 0.75\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.009389, train_acc: 0.77\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.009396, train_acc: 0.71\n",
      "alexnet1d, trial.13:\n",
      "Epoch 23, avg test_loss: 0.012519, test_acc: 0.50\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.008372, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.009691, train_acc: 0.71\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.007481, train_acc: 0.82\n",
      "alexnet1d, trial.13:\n",
      "Epoch 24, avg test_loss: 0.013126, test_acc: 0.50\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.008244, train_acc: 0.82\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.010898, train_acc: 0.64\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.008598, train_acc: 0.79\n",
      "alexnet1d, trial.13:\n",
      "Epoch 25, avg test_loss: 0.012690, test_acc: 0.53\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.007567, train_acc: 0.86\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.009589, train_acc: 0.73\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.007849, train_acc: 0.77\n",
      "alexnet1d, trial.13:\n",
      "Epoch 26, avg test_loss: 0.015170, test_acc: 0.49\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.007221, train_acc: 0.82\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.007152, train_acc: 0.82\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.008666, train_acc: 0.79\n",
      "alexnet1d, trial.13:\n",
      "Epoch 27, avg test_loss: 0.013046, test_acc: 0.60\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.006166, train_acc: 0.82\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.007565, train_acc: 0.77\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.007487, train_acc: 0.80\n",
      "alexnet1d, trial.13:\n",
      "Epoch 28, avg test_loss: 0.016055, test_acc: 0.50\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.004213, train_acc: 0.91\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.008023, train_acc: 0.75\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.005057, train_acc: 0.89\n",
      "alexnet1d, trial.13:\n",
      "Epoch 29, avg test_loss: 0.013670, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.004668, train_acc: 0.93\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.006985, train_acc: 0.80\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.006029, train_acc: 0.89\n",
      "alexnet1d, trial.13:\n",
      "Epoch 30, avg test_loss: 0.015852, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.005960, train_acc: 0.88\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.004498, train_acc: 0.89\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.002954, train_acc: 0.95\n",
      "alexnet1d, trial.13:\n",
      "Epoch 31, avg test_loss: 0.018819, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.004406, train_acc: 0.88\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.003237, train_acc: 0.95\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.003430, train_acc: 0.95\n",
      "alexnet1d, trial.13:\n",
      "Epoch 32, avg test_loss: 0.019593, test_acc: 0.49\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号35个\n",
      "错误信号35个\n",
      "信号正确并预测正确的概率为0.243\n",
      "信号错误并预测正确的概率为0.243\n",
      "总正确率为0.49\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012423, train_acc: 0.43\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012377, train_acc: 0.64\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012503, train_acc: 0.54\n",
      "alexnet1d, trial.14:\n",
      "Epoch 0, avg test_loss: 0.009713, test_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012372, train_acc: 0.46\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012284, train_acc: 0.52\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012190, train_acc: 0.62\n",
      "alexnet1d, trial.14:\n",
      "Epoch 1, avg test_loss: 0.009361, test_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012512, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012094, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012287, train_acc: 0.55\n",
      "alexnet1d, trial.14:\n",
      "Epoch 2, avg test_loss: 0.009358, test_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012520, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012509, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012007, train_acc: 0.59\n",
      "alexnet1d, trial.14:\n",
      "Epoch 3, avg test_loss: 0.009449, test_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012012, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012214, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012572, train_acc: 0.48\n",
      "alexnet1d, trial.14:\n",
      "Epoch 4, avg test_loss: 0.009370, test_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012093, train_acc: 0.52\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012266, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012333, train_acc: 0.48\n",
      "alexnet1d, trial.14:\n",
      "Epoch 5, avg test_loss: 0.009776, test_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011415, train_acc: 0.68\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012189, train_acc: 0.50\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012756, train_acc: 0.50\n",
      "alexnet1d, trial.14:\n",
      "Epoch 6, avg test_loss: 0.009073, test_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011097, train_acc: 0.68\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011917, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011744, train_acc: 0.57\n",
      "alexnet1d, trial.14:\n",
      "Epoch 7, avg test_loss: 0.009155, test_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010837, train_acc: 0.71\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.013590, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012084, train_acc: 0.57\n",
      "alexnet1d, trial.14:\n",
      "Epoch 8, avg test_loss: 0.008945, test_acc: 0.67\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011822, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011657, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011841, train_acc: 0.61\n",
      "alexnet1d, trial.14:\n",
      "Epoch 9, avg test_loss: 0.009171, test_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011655, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011509, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011981, train_acc: 0.55\n",
      "alexnet1d, trial.14:\n",
      "Epoch 10, avg test_loss: 0.009710, test_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.012071, train_acc: 0.54\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011297, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011066, train_acc: 0.70\n",
      "alexnet1d, trial.14:\n",
      "Epoch 11, avg test_loss: 0.009287, test_acc: 0.63\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011942, train_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010612, train_acc: 0.77\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011551, train_acc: 0.66\n",
      "alexnet1d, trial.14:\n",
      "Epoch 12, avg test_loss: 0.009462, test_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010980, train_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011079, train_acc: 0.79\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010234, train_acc: 0.71\n",
      "alexnet1d, trial.14:\n",
      "Epoch 13, avg test_loss: 0.009362, test_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010045, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010008, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009495, train_acc: 0.70\n",
      "alexnet1d, trial.14:\n",
      "Epoch 14, avg test_loss: 0.009960, test_acc: 0.57\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008984, train_acc: 0.82\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009219, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009204, train_acc: 0.75\n",
      "alexnet1d, trial.14:\n",
      "Epoch 15, avg test_loss: 0.009789, test_acc: 0.63\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008871, train_acc: 0.82\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009334, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009627, train_acc: 0.68\n",
      "alexnet1d, trial.14:\n",
      "Epoch 16, avg test_loss: 0.011918, test_acc: 0.64\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008663, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009327, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010628, train_acc: 0.79\n",
      "alexnet1d, trial.14:\n",
      "Epoch 17, avg test_loss: 0.010593, test_acc: 0.54\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009069, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007896, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.011205, train_acc: 0.61\n",
      "alexnet1d, trial.14:\n",
      "Epoch 18, avg test_loss: 0.010073, test_acc: 0.64\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007662, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007402, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.010994, train_acc: 0.68\n",
      "alexnet1d, trial.14:\n",
      "Epoch 19, avg test_loss: 0.012329, test_acc: 0.57\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008336, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007432, train_acc: 0.75\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008960, train_acc: 0.80\n",
      "alexnet1d, trial.14:\n",
      "Epoch 20, avg test_loss: 0.014984, test_acc: 0.60\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006693, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007641, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.010532, train_acc: 0.73\n",
      "alexnet1d, trial.14:\n",
      "Epoch 21, avg test_loss: 0.013304, test_acc: 0.50\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006535, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.009405, train_acc: 0.77\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008697, train_acc: 0.77\n",
      "alexnet1d, trial.14:\n",
      "Epoch 22, avg test_loss: 0.013990, test_acc: 0.64\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.009096, train_acc: 0.77\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006601, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006615, train_acc: 0.84\n",
      "alexnet1d, trial.14:\n",
      "Epoch 23, avg test_loss: 0.014803, test_acc: 0.56\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006777, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.007003, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005677, train_acc: 0.89\n",
      "alexnet1d, trial.14:\n",
      "Epoch 24, avg test_loss: 0.016082, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.006555, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005277, train_acc: 0.82\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004465, train_acc: 0.89\n",
      "alexnet1d, trial.14:\n",
      "Epoch 25, avg test_loss: 0.017221, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005754, train_acc: 0.86\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004653, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.007475, train_acc: 0.84\n",
      "alexnet1d, trial.14:\n",
      "Epoch 26, avg test_loss: 0.018829, test_acc: 0.56\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004147, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.006414, train_acc: 0.80\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.006767, train_acc: 0.80\n",
      "alexnet1d, trial.14:\n",
      "Epoch 27, avg test_loss: 0.019144, test_acc: 0.59\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.002597, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.003816, train_acc: 0.89\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.004233, train_acc: 0.89\n",
      "alexnet1d, trial.14:\n",
      "Epoch 28, avg test_loss: 0.023203, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.002461, train_acc: 0.96\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.004558, train_acc: 0.91\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.002448, train_acc: 0.96\n",
      "alexnet1d, trial.14:\n",
      "Epoch 29, avg test_loss: 0.027893, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号45个\n",
      "错误信号25个\n",
      "信号正确并预测正确的概率为0.414\n",
      "信号错误并预测正确的概率为0.143\n",
      "总正确率为0.56\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012350, train_acc: 0.50\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012392, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.013077, train_acc: 0.55\n",
      "alexnet1d, trial.15:\n",
      "Epoch 0, avg test_loss: 0.009797, test_acc: 0.53\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012195, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012585, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012017, train_acc: 0.64\n",
      "alexnet1d, trial.15:\n",
      "Epoch 1, avg test_loss: 0.009856, test_acc: 0.53\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011942, train_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011871, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011951, train_acc: 0.61\n",
      "alexnet1d, trial.15:\n",
      "Epoch 2, avg test_loss: 0.009925, test_acc: 0.53\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012234, train_acc: 0.55\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012418, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.013288, train_acc: 0.54\n",
      "alexnet1d, trial.15:\n",
      "Epoch 3, avg test_loss: 0.009847, test_acc: 0.53\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011472, train_acc: 0.66\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.013073, train_acc: 0.46\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012467, train_acc: 0.52\n",
      "alexnet1d, trial.15:\n",
      "Epoch 4, avg test_loss: 0.009837, test_acc: 0.53\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012513, train_acc: 0.50\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011829, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012003, train_acc: 0.57\n",
      "alexnet1d, trial.15:\n",
      "Epoch 5, avg test_loss: 0.009757, test_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012099, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011089, train_acc: 0.73\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011642, train_acc: 0.64\n",
      "alexnet1d, trial.15:\n",
      "Epoch 6, avg test_loss: 0.009753, test_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012072, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.013359, train_acc: 0.50\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.010926, train_acc: 0.71\n",
      "alexnet1d, trial.15:\n",
      "Epoch 7, avg test_loss: 0.009574, test_acc: 0.60\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011384, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.009390, train_acc: 0.80\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.013031, train_acc: 0.57\n",
      "alexnet1d, trial.15:\n",
      "Epoch 8, avg test_loss: 0.011402, test_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011745, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.009829, train_acc: 0.73\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012336, train_acc: 0.54\n",
      "alexnet1d, trial.15:\n",
      "Epoch 9, avg test_loss: 0.009274, test_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.009829, train_acc: 0.75\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011036, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011491, train_acc: 0.64\n",
      "alexnet1d, trial.15:\n",
      "Epoch 10, avg test_loss: 0.010242, test_acc: 0.60\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011723, train_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011274, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011280, train_acc: 0.59\n",
      "alexnet1d, trial.15:\n",
      "Epoch 11, avg test_loss: 0.010718, test_acc: 0.56\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011184, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011577, train_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.009196, train_acc: 0.70\n",
      "alexnet1d, trial.15:\n",
      "Epoch 12, avg test_loss: 0.010738, test_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010387, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011610, train_acc: 0.62\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009988, train_acc: 0.66\n",
      "alexnet1d, trial.15:\n",
      "Epoch 13, avg test_loss: 0.011183, test_acc: 0.63\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010185, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010762, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.008552, train_acc: 0.79\n",
      "alexnet1d, trial.15:\n",
      "Epoch 14, avg test_loss: 0.012476, test_acc: 0.61\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008962, train_acc: 0.80\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008791, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.006980, train_acc: 0.91\n",
      "alexnet1d, trial.15:\n",
      "Epoch 15, avg test_loss: 0.011963, test_acc: 0.66\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008943, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.005944, train_acc: 0.91\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009056, train_acc: 0.77\n",
      "alexnet1d, trial.15:\n",
      "Epoch 16, avg test_loss: 0.017249, test_acc: 0.64\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.006523, train_acc: 0.84\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009125, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007742, train_acc: 0.84\n",
      "alexnet1d, trial.15:\n",
      "Epoch 17, avg test_loss: 0.017080, test_acc: 0.54\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.010482, train_acc: 0.71\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.006512, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.011240, train_acc: 0.79\n",
      "alexnet1d, trial.15:\n",
      "Epoch 18, avg test_loss: 0.017612, test_acc: 0.61\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008810, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007112, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007424, train_acc: 0.80\n",
      "alexnet1d, trial.15:\n",
      "Epoch 19, avg test_loss: 0.015250, test_acc: 0.54\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007067, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008139, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006994, train_acc: 0.80\n",
      "alexnet1d, trial.15:\n",
      "Epoch 20, avg test_loss: 0.017268, test_acc: 0.61\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.005415, train_acc: 0.93\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.004638, train_acc: 0.95\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006631, train_acc: 0.86\n",
      "alexnet1d, trial.15:\n",
      "Epoch 21, avg test_loss: 0.017429, test_acc: 0.57\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.004614, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005555, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.005180, train_acc: 0.86\n",
      "alexnet1d, trial.15:\n",
      "Epoch 22, avg test_loss: 0.023642, test_acc: 0.60\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005632, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005864, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.004022, train_acc: 0.91\n",
      "alexnet1d, trial.15:\n",
      "Epoch 23, avg test_loss: 0.026907, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.002623, train_acc: 0.96\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006426, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005582, train_acc: 0.82\n",
      "alexnet1d, trial.15:\n",
      "Epoch 24, avg test_loss: 0.027809, test_acc: 0.60\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.001350, train_acc: 0.98\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006682, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.002440, train_acc: 0.96\n",
      "alexnet1d, trial.15:\n",
      "Epoch 25, avg test_loss: 0.030900, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.002014, train_acc: 0.96\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.006322, train_acc: 0.82\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.003148, train_acc: 0.93\n",
      "alexnet1d, trial.15:\n",
      "Epoch 26, avg test_loss: 0.026590, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号37个\n",
      "错误信号33个\n",
      "信号正确并预测正确的概率为0.414\n",
      "信号错误并预测正确的概率为0.129\n",
      "总正确率为0.54\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012359, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012375, train_acc: 0.50\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.013723, train_acc: 0.57\n",
      "alexnet1d, trial.16:\n",
      "Epoch 0, avg test_loss: 0.009568, test_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012733, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012405, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012358, train_acc: 0.52\n",
      "alexnet1d, trial.16:\n",
      "Epoch 1, avg test_loss: 0.009879, test_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012296, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012481, train_acc: 0.50\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012103, train_acc: 0.64\n",
      "alexnet1d, trial.16:\n",
      "Epoch 2, avg test_loss: 0.009631, test_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012727, train_acc: 0.48\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011845, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012617, train_acc: 0.59\n",
      "alexnet1d, trial.16:\n",
      "Epoch 3, avg test_loss: 0.009577, test_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012715, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011797, train_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011857, train_acc: 0.66\n",
      "alexnet1d, trial.16:\n",
      "Epoch 4, avg test_loss: 0.009654, test_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012035, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012354, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012278, train_acc: 0.54\n",
      "alexnet1d, trial.16:\n",
      "Epoch 5, avg test_loss: 0.009600, test_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012706, train_acc: 0.45\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012382, train_acc: 0.52\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011510, train_acc: 0.66\n",
      "alexnet1d, trial.16:\n",
      "Epoch 6, avg test_loss: 0.009503, test_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011332, train_acc: 0.68\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012960, train_acc: 0.48\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012069, train_acc: 0.50\n",
      "alexnet1d, trial.16:\n",
      "Epoch 7, avg test_loss: 0.009405, test_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010741, train_acc: 0.73\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012148, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011877, train_acc: 0.64\n",
      "alexnet1d, trial.16:\n",
      "Epoch 8, avg test_loss: 0.009799, test_acc: 0.50\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011411, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010774, train_acc: 0.73\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011896, train_acc: 0.66\n",
      "alexnet1d, trial.16:\n",
      "Epoch 9, avg test_loss: 0.009635, test_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010698, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012658, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012051, train_acc: 0.59\n",
      "alexnet1d, trial.16:\n",
      "Epoch 10, avg test_loss: 0.010002, test_acc: 0.50\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010765, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011714, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010029, train_acc: 0.70\n",
      "alexnet1d, trial.16:\n",
      "Epoch 11, avg test_loss: 0.010105, test_acc: 0.53\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010657, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010387, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010977, train_acc: 0.73\n",
      "alexnet1d, trial.16:\n",
      "Epoch 12, avg test_loss: 0.010332, test_acc: 0.57\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009617, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.008487, train_acc: 0.77\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.012034, train_acc: 0.68\n",
      "alexnet1d, trial.16:\n",
      "Epoch 13, avg test_loss: 0.011126, test_acc: 0.53\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009598, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.008783, train_acc: 0.80\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010558, train_acc: 0.62\n",
      "alexnet1d, trial.16:\n",
      "Epoch 14, avg test_loss: 0.010040, test_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.007688, train_acc: 0.80\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.007745, train_acc: 0.88\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010700, train_acc: 0.70\n",
      "alexnet1d, trial.16:\n",
      "Epoch 15, avg test_loss: 0.011311, test_acc: 0.57\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008615, train_acc: 0.84\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.011111, train_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008338, train_acc: 0.82\n",
      "alexnet1d, trial.16:\n",
      "Epoch 16, avg test_loss: 0.010616, test_acc: 0.61\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008556, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007741, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008125, train_acc: 0.84\n",
      "alexnet1d, trial.16:\n",
      "Epoch 17, avg test_loss: 0.010941, test_acc: 0.63\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007901, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007594, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.012351, train_acc: 0.66\n",
      "alexnet1d, trial.16:\n",
      "Epoch 18, avg test_loss: 0.011288, test_acc: 0.63\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.005866, train_acc: 0.88\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007008, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007711, train_acc: 0.80\n",
      "alexnet1d, trial.16:\n",
      "Epoch 19, avg test_loss: 0.012307, test_acc: 0.63\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.004996, train_acc: 0.91\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007304, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007149, train_acc: 0.82\n",
      "alexnet1d, trial.16:\n",
      "Epoch 20, avg test_loss: 0.013746, test_acc: 0.63\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.004836, train_acc: 0.89\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007567, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008638, train_acc: 0.77\n",
      "alexnet1d, trial.16:\n",
      "Epoch 21, avg test_loss: 0.013788, test_acc: 0.56\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007641, train_acc: 0.77\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.004355, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006178, train_acc: 0.82\n",
      "alexnet1d, trial.16:\n",
      "Epoch 22, avg test_loss: 0.015538, test_acc: 0.61\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.003749, train_acc: 0.96\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.004995, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.005621, train_acc: 0.91\n",
      "alexnet1d, trial.16:\n",
      "Epoch 23, avg test_loss: 0.017776, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004246, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005279, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.003534, train_acc: 0.95\n",
      "alexnet1d, trial.16:\n",
      "Epoch 24, avg test_loss: 0.017313, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004563, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006553, train_acc: 0.82\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.003941, train_acc: 0.93\n",
      "alexnet1d, trial.16:\n",
      "Epoch 25, avg test_loss: 0.013695, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.002739, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004304, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.003971, train_acc: 0.88\n",
      "alexnet1d, trial.16:\n",
      "Epoch 26, avg test_loss: 0.019255, test_acc: 0.57\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.002099, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004749, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.001261, train_acc: 0.98\n",
      "alexnet1d, trial.16:\n",
      "Epoch 27, avg test_loss: 0.029058, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号43个\n",
      "错误信号27个\n",
      "信号正确并预测正确的概率为0.414\n",
      "信号错误并预测正确的概率为0.129\n",
      "总正确率为0.54\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012383, train_acc: 0.48\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.013023, train_acc: 0.52\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012109, train_acc: 0.61\n",
      "alexnet1d, trial.17:\n",
      "Epoch 0, avg test_loss: 0.009607, test_acc: 0.63\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012172, train_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012637, train_acc: 0.52\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012276, train_acc: 0.62\n",
      "alexnet1d, trial.17:\n",
      "Epoch 1, avg test_loss: 0.009807, test_acc: 0.63\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012346, train_acc: 0.50\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012366, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012364, train_acc: 0.50\n",
      "alexnet1d, trial.17:\n",
      "Epoch 2, avg test_loss: 0.009772, test_acc: 0.63\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012247, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012107, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012207, train_acc: 0.59\n",
      "alexnet1d, trial.17:\n",
      "Epoch 3, avg test_loss: 0.009516, test_acc: 0.63\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011973, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012387, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011656, train_acc: 0.62\n",
      "alexnet1d, trial.17:\n",
      "Epoch 4, avg test_loss: 0.009423, test_acc: 0.63\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012037, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011616, train_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012082, train_acc: 0.54\n",
      "alexnet1d, trial.17:\n",
      "Epoch 5, avg test_loss: 0.009503, test_acc: 0.63\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011808, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011894, train_acc: 0.71\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011981, train_acc: 0.57\n",
      "alexnet1d, trial.17:\n",
      "Epoch 6, avg test_loss: 0.009822, test_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011751, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011092, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011458, train_acc: 0.62\n",
      "alexnet1d, trial.17:\n",
      "Epoch 7, avg test_loss: 0.009725, test_acc: 0.60\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010703, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.013053, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.014270, train_acc: 0.59\n",
      "alexnet1d, trial.17:\n",
      "Epoch 8, avg test_loss: 0.009959, test_acc: 0.60\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010800, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011129, train_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012185, train_acc: 0.57\n",
      "alexnet1d, trial.17:\n",
      "Epoch 9, avg test_loss: 0.009515, test_acc: 0.60\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011789, train_acc: 0.55\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012045, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010705, train_acc: 0.75\n",
      "alexnet1d, trial.17:\n",
      "Epoch 10, avg test_loss: 0.009490, test_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011019, train_acc: 0.55\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010833, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010206, train_acc: 0.75\n",
      "alexnet1d, trial.17:\n",
      "Epoch 11, avg test_loss: 0.009854, test_acc: 0.60\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011306, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010746, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.009613, train_acc: 0.77\n",
      "alexnet1d, trial.17:\n",
      "Epoch 12, avg test_loss: 0.010033, test_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010716, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010849, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010082, train_acc: 0.71\n",
      "alexnet1d, trial.17:\n",
      "Epoch 13, avg test_loss: 0.009915, test_acc: 0.60\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009384, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009481, train_acc: 0.79\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009154, train_acc: 0.82\n",
      "alexnet1d, trial.17:\n",
      "Epoch 14, avg test_loss: 0.011416, test_acc: 0.56\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008601, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010485, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011114, train_acc: 0.64\n",
      "alexnet1d, trial.17:\n",
      "Epoch 15, avg test_loss: 0.011793, test_acc: 0.54\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009877, train_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008311, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009042, train_acc: 0.80\n",
      "alexnet1d, trial.17:\n",
      "Epoch 16, avg test_loss: 0.010662, test_acc: 0.63\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007946, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009280, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009020, train_acc: 0.79\n",
      "alexnet1d, trial.17:\n",
      "Epoch 17, avg test_loss: 0.009545, test_acc: 0.61\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007688, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.006773, train_acc: 0.86\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007671, train_acc: 0.80\n",
      "alexnet1d, trial.17:\n",
      "Epoch 18, avg test_loss: 0.011089, test_acc: 0.59\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006626, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007071, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007998, train_acc: 0.82\n",
      "alexnet1d, trial.17:\n",
      "Epoch 19, avg test_loss: 0.012474, test_acc: 0.64\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.005587, train_acc: 0.89\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.004765, train_acc: 0.89\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009401, train_acc: 0.73\n",
      "alexnet1d, trial.17:\n",
      "Epoch 20, avg test_loss: 0.012788, test_acc: 0.64\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.009135, train_acc: 0.77\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.003681, train_acc: 0.95\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.003383, train_acc: 0.96\n",
      "alexnet1d, trial.17:\n",
      "Epoch 21, avg test_loss: 0.014520, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005609, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.004595, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006247, train_acc: 0.84\n",
      "alexnet1d, trial.17:\n",
      "Epoch 22, avg test_loss: 0.013762, test_acc: 0.67\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005741, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.004029, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006906, train_acc: 0.82\n",
      "alexnet1d, trial.17:\n",
      "Epoch 23, avg test_loss: 0.013878, test_acc: 0.70\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004288, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004214, train_acc: 0.95\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.004018, train_acc: 0.93\n",
      "alexnet1d, trial.17:\n",
      "Epoch 24, avg test_loss: 0.014718, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004139, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.007618, train_acc: 0.82\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004556, train_acc: 0.89\n",
      "alexnet1d, trial.17:\n",
      "Epoch 25, avg test_loss: 0.019526, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005476, train_acc: 0.84\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005119, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004858, train_acc: 0.93\n",
      "alexnet1d, trial.17:\n",
      "Epoch 26, avg test_loss: 0.015920, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号44个\n",
      "错误信号26个\n",
      "信号正确并预测正确的概率为0.4\n",
      "信号错误并预测正确的概率为0.2\n",
      "总正确率为0.60\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012370, train_acc: 0.52\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012573, train_acc: 0.36\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.013250, train_acc: 0.48\n",
      "alexnet1d, trial.18:\n",
      "Epoch 0, avg test_loss: 0.009634, test_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012512, train_acc: 0.52\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012368, train_acc: 0.52\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012439, train_acc: 0.52\n",
      "alexnet1d, trial.18:\n",
      "Epoch 1, avg test_loss: 0.009594, test_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012172, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012256, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012153, train_acc: 0.59\n",
      "alexnet1d, trial.18:\n",
      "Epoch 2, avg test_loss: 0.009523, test_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012711, train_acc: 0.46\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012055, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011897, train_acc: 0.62\n",
      "alexnet1d, trial.18:\n",
      "Epoch 3, avg test_loss: 0.009572, test_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011957, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011496, train_acc: 0.68\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012206, train_acc: 0.48\n",
      "alexnet1d, trial.18:\n",
      "Epoch 4, avg test_loss: 0.009858, test_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011908, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012338, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011817, train_acc: 0.57\n",
      "alexnet1d, trial.18:\n",
      "Epoch 5, avg test_loss: 0.009893, test_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011721, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011839, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011615, train_acc: 0.59\n",
      "alexnet1d, trial.18:\n",
      "Epoch 6, avg test_loss: 0.010334, test_acc: 0.60\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.010196, train_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011834, train_acc: 0.68\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011569, train_acc: 0.66\n",
      "alexnet1d, trial.18:\n",
      "Epoch 7, avg test_loss: 0.010053, test_acc: 0.56\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011103, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012541, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011159, train_acc: 0.68\n",
      "alexnet1d, trial.18:\n",
      "Epoch 8, avg test_loss: 0.010083, test_acc: 0.60\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010666, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010998, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012991, train_acc: 0.54\n",
      "alexnet1d, trial.18:\n",
      "Epoch 9, avg test_loss: 0.009777, test_acc: 0.69\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011294, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011720, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010175, train_acc: 0.75\n",
      "alexnet1d, trial.18:\n",
      "Epoch 10, avg test_loss: 0.009902, test_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010957, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.009508, train_acc: 0.80\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.013735, train_acc: 0.57\n",
      "alexnet1d, trial.18:\n",
      "Epoch 11, avg test_loss: 0.011989, test_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011065, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.008817, train_acc: 0.75\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011352, train_acc: 0.61\n",
      "alexnet1d, trial.18:\n",
      "Epoch 12, avg test_loss: 0.009323, test_acc: 0.67\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010520, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010757, train_acc: 0.62\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009970, train_acc: 0.75\n",
      "alexnet1d, trial.18:\n",
      "Epoch 13, avg test_loss: 0.011966, test_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.008529, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011722, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010148, train_acc: 0.71\n",
      "alexnet1d, trial.18:\n",
      "Epoch 14, avg test_loss: 0.010861, test_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009972, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.007693, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008821, train_acc: 0.79\n",
      "alexnet1d, trial.18:\n",
      "Epoch 15, avg test_loss: 0.014271, test_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008367, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009354, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.011175, train_acc: 0.62\n",
      "alexnet1d, trial.18:\n",
      "Epoch 16, avg test_loss: 0.012383, test_acc: 0.67\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008132, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007489, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007477, train_acc: 0.80\n",
      "alexnet1d, trial.18:\n",
      "Epoch 17, avg test_loss: 0.012958, test_acc: 0.69\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.006763, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007331, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008625, train_acc: 0.79\n",
      "alexnet1d, trial.18:\n",
      "Epoch 18, avg test_loss: 0.014829, test_acc: 0.70\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006376, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007417, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.006536, train_acc: 0.86\n",
      "alexnet1d, trial.18:\n",
      "Epoch 19, avg test_loss: 0.020883, test_acc: 0.63\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006049, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.005782, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008387, train_acc: 0.80\n",
      "alexnet1d, trial.18:\n",
      "Epoch 20, avg test_loss: 0.015652, test_acc: 0.67\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006848, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.008897, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006062, train_acc: 0.89\n",
      "alexnet1d, trial.18:\n",
      "Epoch 21, avg test_loss: 0.015447, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006061, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005451, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.005052, train_acc: 0.89\n",
      "alexnet1d, trial.18:\n",
      "Epoch 22, avg test_loss: 0.015600, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.004622, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005563, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.003191, train_acc: 0.98\n",
      "alexnet1d, trial.18:\n",
      "Epoch 23, avg test_loss: 0.015490, test_acc: 0.66\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.003676, train_acc: 0.93\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005138, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.004051, train_acc: 0.89\n",
      "alexnet1d, trial.18:\n",
      "Epoch 24, avg test_loss: 0.020912, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号45个\n",
      "错误信号25个\n",
      "信号正确并预测正确的概率为0.414\n",
      "信号错误并预测正确的概率为0.2\n",
      "总正确率为0.61\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012412, train_acc: 0.46\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012813, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012368, train_acc: 0.50\n",
      "alexnet1d, trial.19:\n",
      "Epoch 0, avg test_loss: 0.009674, test_acc: 0.67\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012354, train_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012335, train_acc: 0.46\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012275, train_acc: 0.61\n",
      "alexnet1d, trial.19:\n",
      "Epoch 1, avg test_loss: 0.009668, test_acc: 0.67\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012044, train_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012242, train_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012658, train_acc: 0.45\n",
      "alexnet1d, trial.19:\n",
      "Epoch 2, avg test_loss: 0.009722, test_acc: 0.70\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012236, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012298, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012385, train_acc: 0.46\n",
      "alexnet1d, trial.19:\n",
      "Epoch 3, avg test_loss: 0.009530, test_acc: 0.67\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012181, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011682, train_acc: 0.66\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011974, train_acc: 0.55\n",
      "alexnet1d, trial.19:\n",
      "Epoch 4, avg test_loss: 0.009311, test_acc: 0.69\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011341, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011825, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011380, train_acc: 0.68\n",
      "alexnet1d, trial.19:\n",
      "Epoch 5, avg test_loss: 0.010746, test_acc: 0.49\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012320, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011459, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011737, train_acc: 0.64\n",
      "alexnet1d, trial.19:\n",
      "Epoch 6, avg test_loss: 0.009225, test_acc: 0.67\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011838, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012264, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011602, train_acc: 0.61\n",
      "alexnet1d, trial.19:\n",
      "Epoch 7, avg test_loss: 0.009138, test_acc: 0.70\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011275, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011437, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012565, train_acc: 0.48\n",
      "alexnet1d, trial.19:\n",
      "Epoch 8, avg test_loss: 0.010148, test_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.009974, train_acc: 0.75\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011825, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010593, train_acc: 0.68\n",
      "alexnet1d, trial.19:\n",
      "Epoch 9, avg test_loss: 0.009229, test_acc: 0.69\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011217, train_acc: 0.70\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011170, train_acc: 0.73\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010476, train_acc: 0.66\n",
      "alexnet1d, trial.19:\n",
      "Epoch 10, avg test_loss: 0.010158, test_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010329, train_acc: 0.73\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010509, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010934, train_acc: 0.68\n",
      "alexnet1d, trial.19:\n",
      "Epoch 11, avg test_loss: 0.009459, test_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010395, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010657, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.009684, train_acc: 0.77\n",
      "alexnet1d, trial.19:\n",
      "Epoch 12, avg test_loss: 0.009841, test_acc: 0.67\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009076, train_acc: 0.80\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009061, train_acc: 0.75\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010073, train_acc: 0.71\n",
      "alexnet1d, trial.19:\n",
      "Epoch 13, avg test_loss: 0.010157, test_acc: 0.64\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010051, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.012178, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009275, train_acc: 0.75\n",
      "alexnet1d, trial.19:\n",
      "Epoch 14, avg test_loss: 0.009185, test_acc: 0.67\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009754, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009817, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009253, train_acc: 0.71\n",
      "alexnet1d, trial.19:\n",
      "Epoch 15, avg test_loss: 0.012207, test_acc: 0.56\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008582, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010605, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009249, train_acc: 0.70\n",
      "alexnet1d, trial.19:\n",
      "Epoch 16, avg test_loss: 0.010730, test_acc: 0.67\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007261, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008827, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007516, train_acc: 0.80\n",
      "alexnet1d, trial.19:\n",
      "Epoch 17, avg test_loss: 0.011601, test_acc: 0.64\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009367, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.006285, train_acc: 0.86\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.006387, train_acc: 0.82\n",
      "alexnet1d, trial.19:\n",
      "Epoch 18, avg test_loss: 0.013068, test_acc: 0.59\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006580, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.005732, train_acc: 0.88\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008089, train_acc: 0.80\n",
      "alexnet1d, trial.19:\n",
      "Epoch 19, avg test_loss: 0.012713, test_acc: 0.69\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006237, train_acc: 0.89\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007325, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008885, train_acc: 0.79\n",
      "alexnet1d, trial.19:\n",
      "Epoch 20, avg test_loss: 0.013829, test_acc: 0.64\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.003798, train_acc: 0.91\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006245, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006741, train_acc: 0.82\n",
      "alexnet1d, trial.19:\n",
      "Epoch 21, avg test_loss: 0.012428, test_acc: 0.67\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.004561, train_acc: 0.93\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007133, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.004758, train_acc: 0.91\n",
      "alexnet1d, trial.19:\n",
      "Epoch 22, avg test_loss: 0.019735, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005039, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.003584, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.005676, train_acc: 0.86\n",
      "alexnet1d, trial.19:\n",
      "Epoch 23, avg test_loss: 0.015628, test_acc: 0.69\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005067, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.002191, train_acc: 0.96\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006219, train_acc: 0.84\n",
      "alexnet1d, trial.19:\n",
      "Epoch 24, avg test_loss: 0.018803, test_acc: 0.66\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.003316, train_acc: 0.95\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.007919, train_acc: 0.77\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.002287, train_acc: 0.95\n",
      "alexnet1d, trial.19:\n",
      "Epoch 25, avg test_loss: 0.016584, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004648, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.003337, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.003166, train_acc: 0.95\n",
      "alexnet1d, trial.19:\n",
      "Epoch 26, avg test_loss: 0.023180, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003201, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.002001, train_acc: 0.98\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.002102, train_acc: 0.95\n",
      "alexnet1d, trial.19:\n",
      "Epoch 27, avg test_loss: 0.021799, test_acc: 0.67\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号47个\n",
      "错误信号23个\n",
      "信号正确并预测正确的概率为0.5\n",
      "信号错误并预测正确的概率为0.171\n",
      "总正确率为0.67\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012449, train_acc: 0.48\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.011840, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.011924, train_acc: 0.66\n",
      "alexnet1d, trial.20:\n",
      "Epoch 0, avg test_loss: 0.009888, test_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012282, train_acc: 0.52\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012512, train_acc: 0.45\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012488, train_acc: 0.38\n",
      "alexnet1d, trial.20:\n",
      "Epoch 1, avg test_loss: 0.009852, test_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012304, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012284, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012124, train_acc: 0.61\n",
      "alexnet1d, trial.20:\n",
      "Epoch 2, avg test_loss: 0.009558, test_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011593, train_acc: 0.71\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011926, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012512, train_acc: 0.55\n",
      "alexnet1d, trial.20:\n",
      "Epoch 3, avg test_loss: 0.009400, test_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012817, train_acc: 0.48\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012429, train_acc: 0.52\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012154, train_acc: 0.57\n",
      "alexnet1d, trial.20:\n",
      "Epoch 4, avg test_loss: 0.009523, test_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012454, train_acc: 0.50\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012353, train_acc: 0.52\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011927, train_acc: 0.64\n",
      "alexnet1d, trial.20:\n",
      "Epoch 5, avg test_loss: 0.009525, test_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012225, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012371, train_acc: 0.52\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012355, train_acc: 0.57\n",
      "alexnet1d, trial.20:\n",
      "Epoch 6, avg test_loss: 0.009439, test_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012356, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012419, train_acc: 0.52\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012209, train_acc: 0.55\n",
      "alexnet1d, trial.20:\n",
      "Epoch 7, avg test_loss: 0.009435, test_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012536, train_acc: 0.45\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011787, train_acc: 0.68\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011672, train_acc: 0.68\n",
      "alexnet1d, trial.20:\n",
      "Epoch 8, avg test_loss: 0.009421, test_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011544, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012575, train_acc: 0.50\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012069, train_acc: 0.57\n",
      "alexnet1d, trial.20:\n",
      "Epoch 9, avg test_loss: 0.009222, test_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011637, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011839, train_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012031, train_acc: 0.55\n",
      "alexnet1d, trial.20:\n",
      "Epoch 10, avg test_loss: 0.009138, test_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011203, train_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.012363, train_acc: 0.55\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011476, train_acc: 0.68\n",
      "alexnet1d, trial.20:\n",
      "Epoch 11, avg test_loss: 0.009280, test_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011407, train_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011626, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.012019, train_acc: 0.50\n",
      "alexnet1d, trial.20:\n",
      "Epoch 12, avg test_loss: 0.008969, test_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011133, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.012348, train_acc: 0.54\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011116, train_acc: 0.64\n",
      "alexnet1d, trial.20:\n",
      "Epoch 13, avg test_loss: 0.009092, test_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010674, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010853, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011253, train_acc: 0.59\n",
      "alexnet1d, trial.20:\n",
      "Epoch 14, avg test_loss: 0.009560, test_acc: 0.61\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010836, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009944, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011153, train_acc: 0.61\n",
      "alexnet1d, trial.20:\n",
      "Epoch 15, avg test_loss: 0.009019, test_acc: 0.67\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.011235, train_acc: 0.68\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010927, train_acc: 0.61\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.012879, train_acc: 0.59\n",
      "alexnet1d, trial.20:\n",
      "Epoch 16, avg test_loss: 0.008608, test_acc: 0.67\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009652, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009381, train_acc: 0.68\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.012164, train_acc: 0.59\n",
      "alexnet1d, trial.20:\n",
      "Epoch 17, avg test_loss: 0.010384, test_acc: 0.54\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.010709, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008793, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.011719, train_acc: 0.64\n",
      "alexnet1d, trial.20:\n",
      "Epoch 18, avg test_loss: 0.009250, test_acc: 0.60\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008854, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.010494, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.010287, train_acc: 0.79\n",
      "alexnet1d, trial.20:\n",
      "Epoch 19, avg test_loss: 0.009298, test_acc: 0.70\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.009205, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008329, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.010183, train_acc: 0.64\n",
      "alexnet1d, trial.20:\n",
      "Epoch 20, avg test_loss: 0.011316, test_acc: 0.60\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008566, train_acc: 0.75\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007480, train_acc: 0.89\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008081, train_acc: 0.75\n",
      "alexnet1d, trial.20:\n",
      "Epoch 21, avg test_loss: 0.009186, test_acc: 0.67\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007388, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.008567, train_acc: 0.73\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008474, train_acc: 0.77\n",
      "alexnet1d, trial.20:\n",
      "Epoch 22, avg test_loss: 0.010969, test_acc: 0.57\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.008211, train_acc: 0.79\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007254, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.007993, train_acc: 0.77\n",
      "alexnet1d, trial.20:\n",
      "Epoch 23, avg test_loss: 0.010810, test_acc: 0.67\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006653, train_acc: 0.82\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.008000, train_acc: 0.79\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.007835, train_acc: 0.82\n",
      "alexnet1d, trial.20:\n",
      "Epoch 24, avg test_loss: 0.010689, test_acc: 0.66\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005228, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005718, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005971, train_acc: 0.86\n",
      "alexnet1d, trial.20:\n",
      "Epoch 25, avg test_loss: 0.013119, test_acc: 0.57\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.007024, train_acc: 0.79\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005185, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.005349, train_acc: 0.88\n",
      "alexnet1d, trial.20:\n",
      "Epoch 26, avg test_loss: 0.015078, test_acc: 0.56\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004514, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.005659, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004566, train_acc: 0.91\n",
      "alexnet1d, trial.20:\n",
      "Epoch 27, avg test_loss: 0.018901, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004727, train_acc: 0.88\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.006654, train_acc: 0.82\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.006349, train_acc: 0.86\n",
      "alexnet1d, trial.20:\n",
      "Epoch 28, avg test_loss: 0.014316, test_acc: 0.59\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.004988, train_acc: 0.84\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.005198, train_acc: 0.88\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.009204, train_acc: 0.75\n",
      "alexnet1d, trial.20:\n",
      "Epoch 29, avg test_loss: 0.013889, test_acc: 0.61\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.003441, train_acc: 0.93\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.005895, train_acc: 0.82\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.005419, train_acc: 0.88\n",
      "alexnet1d, trial.20:\n",
      "Epoch 30, avg test_loss: 0.017566, test_acc: 0.56\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.005259, train_acc: 0.88\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.005048, train_acc: 0.91\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.005886, train_acc: 0.89\n",
      "alexnet1d, trial.20:\n",
      "Epoch 31, avg test_loss: 0.016427, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.003242, train_acc: 0.93\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.001756, train_acc: 0.98\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.004701, train_acc: 0.89\n",
      "alexnet1d, trial.20:\n",
      "Epoch 32, avg test_loss: 0.018778, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 33, lr: 0.000377, 0/280, avg loss: 0.002427, train_acc: 0.95\n",
      "Train Epoch 33, lr: 0.000377, 112/280, avg loss: 0.005308, train_acc: 0.93\n",
      "Train Epoch 33, lr: 0.000377, 224/280, avg loss: 0.001839, train_acc: 0.98\n",
      "alexnet1d, trial.20:\n",
      "Epoch 33, avg test_loss: 0.018577, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号45个\n",
      "错误信号25个\n",
      "信号正确并预测正确的概率为0.457\n",
      "信号错误并预测正确的概率为0.114\n",
      "总正确率为0.57\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012380, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012250, train_acc: 0.61\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012142, train_acc: 0.59\n",
      "alexnet1d, trial.21:\n",
      "Epoch 0, avg test_loss: 0.009645, test_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012361, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.011537, train_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012287, train_acc: 0.55\n",
      "alexnet1d, trial.21:\n",
      "Epoch 1, avg test_loss: 0.009650, test_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012041, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012236, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011979, train_acc: 0.62\n",
      "alexnet1d, trial.21:\n",
      "Epoch 2, avg test_loss: 0.009702, test_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012152, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012371, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012553, train_acc: 0.55\n",
      "alexnet1d, trial.21:\n",
      "Epoch 3, avg test_loss: 0.009616, test_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012405, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011768, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012089, train_acc: 0.59\n",
      "alexnet1d, trial.21:\n",
      "Epoch 4, avg test_loss: 0.009651, test_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012434, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012043, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012384, train_acc: 0.54\n",
      "alexnet1d, trial.21:\n",
      "Epoch 5, avg test_loss: 0.009557, test_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011846, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012548, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011410, train_acc: 0.64\n",
      "alexnet1d, trial.21:\n",
      "Epoch 6, avg test_loss: 0.009540, test_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011587, train_acc: 0.55\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011424, train_acc: 0.73\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011887, train_acc: 0.57\n",
      "alexnet1d, trial.21:\n",
      "Epoch 7, avg test_loss: 0.009635, test_acc: 0.56\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011343, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011344, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012864, train_acc: 0.55\n",
      "alexnet1d, trial.21:\n",
      "Epoch 8, avg test_loss: 0.009764, test_acc: 0.60\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011826, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011504, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011096, train_acc: 0.68\n",
      "alexnet1d, trial.21:\n",
      "Epoch 9, avg test_loss: 0.009314, test_acc: 0.63\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011616, train_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010898, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011701, train_acc: 0.59\n",
      "alexnet1d, trial.21:\n",
      "Epoch 10, avg test_loss: 0.009334, test_acc: 0.60\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010069, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011490, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010678, train_acc: 0.68\n",
      "alexnet1d, trial.21:\n",
      "Epoch 11, avg test_loss: 0.010103, test_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011499, train_acc: 0.54\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011494, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.009688, train_acc: 0.73\n",
      "alexnet1d, trial.21:\n",
      "Epoch 12, avg test_loss: 0.010040, test_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010938, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010156, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010947, train_acc: 0.71\n",
      "alexnet1d, trial.21:\n",
      "Epoch 13, avg test_loss: 0.009486, test_acc: 0.63\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010525, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011397, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.012038, train_acc: 0.57\n",
      "alexnet1d, trial.21:\n",
      "Epoch 14, avg test_loss: 0.010453, test_acc: 0.61\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010420, train_acc: 0.68\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.011303, train_acc: 0.61\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009546, train_acc: 0.77\n",
      "alexnet1d, trial.21:\n",
      "Epoch 15, avg test_loss: 0.009524, test_acc: 0.64\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009656, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009848, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010401, train_acc: 0.68\n",
      "alexnet1d, trial.21:\n",
      "Epoch 16, avg test_loss: 0.009533, test_acc: 0.64\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008149, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009722, train_acc: 0.68\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010555, train_acc: 0.70\n",
      "alexnet1d, trial.21:\n",
      "Epoch 17, avg test_loss: 0.011420, test_acc: 0.61\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007885, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008701, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008203, train_acc: 0.82\n",
      "alexnet1d, trial.21:\n",
      "Epoch 18, avg test_loss: 0.009889, test_acc: 0.66\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009150, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.010330, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008759, train_acc: 0.77\n",
      "alexnet1d, trial.21:\n",
      "Epoch 19, avg test_loss: 0.011161, test_acc: 0.64\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008416, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.009712, train_acc: 0.73\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009570, train_acc: 0.82\n",
      "alexnet1d, trial.21:\n",
      "Epoch 20, avg test_loss: 0.011117, test_acc: 0.63\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007205, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007830, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007937, train_acc: 0.86\n",
      "alexnet1d, trial.21:\n",
      "Epoch 21, avg test_loss: 0.011488, test_acc: 0.64\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005582, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.008021, train_acc: 0.77\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006868, train_acc: 0.84\n",
      "alexnet1d, trial.21:\n",
      "Epoch 22, avg test_loss: 0.011162, test_acc: 0.63\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006639, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006070, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.005755, train_acc: 0.88\n",
      "alexnet1d, trial.21:\n",
      "Epoch 23, avg test_loss: 0.014672, test_acc: 0.63\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004344, train_acc: 0.93\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004113, train_acc: 0.95\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.004376, train_acc: 0.89\n",
      "alexnet1d, trial.21:\n",
      "Epoch 24, avg test_loss: 0.015348, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004463, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005065, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006131, train_acc: 0.91\n",
      "alexnet1d, trial.21:\n",
      "Epoch 25, avg test_loss: 0.019176, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.007553, train_acc: 0.86\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.003006, train_acc: 0.96\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.005142, train_acc: 0.88\n",
      "alexnet1d, trial.21:\n",
      "Epoch 26, avg test_loss: 0.018030, test_acc: 0.60\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003975, train_acc: 0.86\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.006179, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.007722, train_acc: 0.80\n",
      "alexnet1d, trial.21:\n",
      "Epoch 27, avg test_loss: 0.018176, test_acc: 0.63\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003469, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.003758, train_acc: 0.89\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.006179, train_acc: 0.84\n",
      "alexnet1d, trial.21:\n",
      "Epoch 28, avg test_loss: 0.014654, test_acc: 0.60\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.004227, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.006978, train_acc: 0.84\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.004077, train_acc: 0.91\n",
      "alexnet1d, trial.21:\n",
      "Epoch 29, avg test_loss: 0.016744, test_acc: 0.67\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.006677, train_acc: 0.84\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.004831, train_acc: 0.89\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.003903, train_acc: 0.93\n",
      "alexnet1d, trial.21:\n",
      "Epoch 30, avg test_loss: 0.019256, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号41个\n",
      "错误信号29个\n",
      "信号正确并预测正确的概率为0.329\n",
      "信号错误并预测正确的概率为0.186\n",
      "总正确率为0.51\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012480, train_acc: 0.30\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012898, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012416, train_acc: 0.50\n",
      "alexnet1d, trial.22:\n",
      "Epoch 0, avg test_loss: 0.009981, test_acc: 0.51\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012164, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.011648, train_acc: 0.70\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012671, train_acc: 0.57\n",
      "alexnet1d, trial.22:\n",
      "Epoch 1, avg test_loss: 0.010833, test_acc: 0.51\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.010983, train_acc: 0.73\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.013472, train_acc: 0.52\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012502, train_acc: 0.48\n",
      "alexnet1d, trial.22:\n",
      "Epoch 2, avg test_loss: 0.010001, test_acc: 0.51\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011739, train_acc: 0.62\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012116, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012227, train_acc: 0.54\n",
      "alexnet1d, trial.22:\n",
      "Epoch 3, avg test_loss: 0.010045, test_acc: 0.51\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011741, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011997, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011103, train_acc: 0.71\n",
      "alexnet1d, trial.22:\n",
      "Epoch 4, avg test_loss: 0.010824, test_acc: 0.51\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011896, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011394, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012110, train_acc: 0.54\n",
      "alexnet1d, trial.22:\n",
      "Epoch 5, avg test_loss: 0.011085, test_acc: 0.49\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.010397, train_acc: 0.68\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012016, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011942, train_acc: 0.66\n",
      "alexnet1d, trial.22:\n",
      "Epoch 6, avg test_loss: 0.011003, test_acc: 0.51\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011113, train_acc: 0.70\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011288, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011396, train_acc: 0.66\n",
      "alexnet1d, trial.22:\n",
      "Epoch 7, avg test_loss: 0.011838, test_acc: 0.50\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010036, train_acc: 0.70\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.010847, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011457, train_acc: 0.62\n",
      "alexnet1d, trial.22:\n",
      "Epoch 8, avg test_loss: 0.012512, test_acc: 0.53\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010919, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.008104, train_acc: 0.79\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011594, train_acc: 0.68\n",
      "alexnet1d, trial.22:\n",
      "Epoch 9, avg test_loss: 0.014257, test_acc: 0.49\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.009376, train_acc: 0.75\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010564, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010774, train_acc: 0.64\n",
      "alexnet1d, trial.22:\n",
      "Epoch 10, avg test_loss: 0.011714, test_acc: 0.54\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.008166, train_acc: 0.80\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.009470, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.008311, train_acc: 0.80\n",
      "alexnet1d, trial.22:\n",
      "Epoch 11, avg test_loss: 0.012682, test_acc: 0.51\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.008990, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011920, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.008613, train_acc: 0.86\n",
      "alexnet1d, trial.22:\n",
      "Epoch 12, avg test_loss: 0.013051, test_acc: 0.50\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009071, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.008548, train_acc: 0.86\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009602, train_acc: 0.80\n",
      "alexnet1d, trial.22:\n",
      "Epoch 13, avg test_loss: 0.014787, test_acc: 0.53\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.008304, train_acc: 0.79\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010700, train_acc: 0.79\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011461, train_acc: 0.75\n",
      "alexnet1d, trial.22:\n",
      "Epoch 14, avg test_loss: 0.015091, test_acc: 0.51\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.007590, train_acc: 0.84\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008152, train_acc: 0.84\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008791, train_acc: 0.79\n",
      "alexnet1d, trial.22:\n",
      "Epoch 15, avg test_loss: 0.014558, test_acc: 0.56\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008326, train_acc: 0.82\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.006584, train_acc: 0.86\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.006577, train_acc: 0.88\n",
      "alexnet1d, trial.22:\n",
      "Epoch 16, avg test_loss: 0.017511, test_acc: 0.51\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007067, train_acc: 0.88\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.005471, train_acc: 0.86\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007093, train_acc: 0.82\n",
      "alexnet1d, trial.22:\n",
      "Epoch 17, avg test_loss: 0.019994, test_acc: 0.50\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007015, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.006395, train_acc: 0.91\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.006501, train_acc: 0.82\n",
      "alexnet1d, trial.22:\n",
      "Epoch 18, avg test_loss: 0.017612, test_acc: 0.56\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.005371, train_acc: 0.88\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007174, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007167, train_acc: 0.86\n",
      "alexnet1d, trial.22:\n",
      "Epoch 19, avg test_loss: 0.020204, test_acc: 0.54\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006385, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.004791, train_acc: 0.91\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008872, train_acc: 0.82\n",
      "alexnet1d, trial.22:\n",
      "Epoch 20, avg test_loss: 0.020937, test_acc: 0.54\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.005846, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.005085, train_acc: 0.91\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007638, train_acc: 0.80\n",
      "alexnet1d, trial.22:\n",
      "Epoch 21, avg test_loss: 0.021056, test_acc: 0.51\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.002877, train_acc: 0.93\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.004376, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008590, train_acc: 0.82\n",
      "alexnet1d, trial.22:\n",
      "Epoch 22, avg test_loss: 0.019661, test_acc: 0.56\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.003855, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005706, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006427, train_acc: 0.88\n",
      "alexnet1d, trial.22:\n",
      "Epoch 23, avg test_loss: 0.022017, test_acc: 0.56\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.003432, train_acc: 0.95\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.002842, train_acc: 0.95\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.002755, train_acc: 0.93\n",
      "alexnet1d, trial.22:\n",
      "Epoch 24, avg test_loss: 0.028016, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.003379, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.002187, train_acc: 0.96\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.001528, train_acc: 0.98\n",
      "alexnet1d, trial.22:\n",
      "Epoch 25, avg test_loss: 0.028217, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.002591, train_acc: 0.96\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.003452, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.001453, train_acc: 0.98\n",
      "alexnet1d, trial.22:\n",
      "Epoch 26, avg test_loss: 0.033936, test_acc: 0.50\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.002469, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.000734, train_acc: 1.00\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003348, train_acc: 0.95\n",
      "alexnet1d, trial.22:\n",
      "Epoch 27, avg test_loss: 0.036735, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号36个\n",
      "错误信号34个\n",
      "信号正确并预测正确的概率为0.286\n",
      "信号错误并预测正确的概率为0.257\n",
      "总正确率为0.54\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012303, train_acc: 0.62\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012383, train_acc: 0.50\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012172, train_acc: 0.66\n",
      "alexnet1d, trial.23:\n",
      "Epoch 0, avg test_loss: 0.009318, test_acc: 0.70\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012361, train_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012451, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012208, train_acc: 0.57\n",
      "alexnet1d, trial.23:\n",
      "Epoch 1, avg test_loss: 0.009421, test_acc: 0.70\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012192, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012543, train_acc: 0.48\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011981, train_acc: 0.61\n",
      "alexnet1d, trial.23:\n",
      "Epoch 2, avg test_loss: 0.009312, test_acc: 0.70\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011776, train_acc: 0.62\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.013143, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012319, train_acc: 0.55\n",
      "alexnet1d, trial.23:\n",
      "Epoch 3, avg test_loss: 0.009510, test_acc: 0.70\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012471, train_acc: 0.48\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012197, train_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012378, train_acc: 0.54\n",
      "alexnet1d, trial.23:\n",
      "Epoch 4, avg test_loss: 0.009655, test_acc: 0.70\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011941, train_acc: 0.68\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012865, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012251, train_acc: 0.55\n",
      "alexnet1d, trial.23:\n",
      "Epoch 5, avg test_loss: 0.009517, test_acc: 0.71\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012275, train_acc: 0.48\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012207, train_acc: 0.50\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012330, train_acc: 0.57\n",
      "alexnet1d, trial.23:\n",
      "Epoch 6, avg test_loss: 0.009565, test_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012568, train_acc: 0.55\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011942, train_acc: 0.52\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011893, train_acc: 0.61\n",
      "alexnet1d, trial.23:\n",
      "Epoch 7, avg test_loss: 0.009450, test_acc: 0.67\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011752, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012073, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012163, train_acc: 0.62\n",
      "alexnet1d, trial.23:\n",
      "Epoch 8, avg test_loss: 0.009694, test_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011316, train_acc: 0.71\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012520, train_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012088, train_acc: 0.48\n",
      "alexnet1d, trial.23:\n",
      "Epoch 9, avg test_loss: 0.009571, test_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011636, train_acc: 0.55\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011585, train_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011673, train_acc: 0.62\n",
      "alexnet1d, trial.23:\n",
      "Epoch 10, avg test_loss: 0.009277, test_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011262, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011577, train_acc: 0.54\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.012320, train_acc: 0.62\n",
      "alexnet1d, trial.23:\n",
      "Epoch 11, avg test_loss: 0.009765, test_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011159, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.012168, train_acc: 0.54\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.012452, train_acc: 0.55\n",
      "alexnet1d, trial.23:\n",
      "Epoch 12, avg test_loss: 0.008885, test_acc: 0.76\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010824, train_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011181, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011982, train_acc: 0.57\n",
      "alexnet1d, trial.23:\n",
      "Epoch 13, avg test_loss: 0.009909, test_acc: 0.59\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.012401, train_acc: 0.54\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011439, train_acc: 0.57\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.012408, train_acc: 0.52\n",
      "alexnet1d, trial.23:\n",
      "Epoch 14, avg test_loss: 0.010318, test_acc: 0.60\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010452, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.011512, train_acc: 0.57\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011255, train_acc: 0.66\n",
      "alexnet1d, trial.23:\n",
      "Epoch 15, avg test_loss: 0.009461, test_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010077, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010658, train_acc: 0.68\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.012722, train_acc: 0.57\n",
      "alexnet1d, trial.23:\n",
      "Epoch 16, avg test_loss: 0.009879, test_acc: 0.67\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.010390, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009658, train_acc: 0.70\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008709, train_acc: 0.82\n",
      "alexnet1d, trial.23:\n",
      "Epoch 17, avg test_loss: 0.009700, test_acc: 0.67\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009418, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009843, train_acc: 0.70\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008493, train_acc: 0.75\n",
      "alexnet1d, trial.23:\n",
      "Epoch 18, avg test_loss: 0.011053, test_acc: 0.63\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.011308, train_acc: 0.68\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009332, train_acc: 0.68\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007478, train_acc: 0.84\n",
      "alexnet1d, trial.23:\n",
      "Epoch 19, avg test_loss: 0.011539, test_acc: 0.66\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008238, train_acc: 0.75\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008733, train_acc: 0.77\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008571, train_acc: 0.77\n",
      "alexnet1d, trial.23:\n",
      "Epoch 20, avg test_loss: 0.009984, test_acc: 0.69\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006447, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.009523, train_acc: 0.71\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008566, train_acc: 0.70\n",
      "alexnet1d, trial.23:\n",
      "Epoch 21, avg test_loss: 0.012676, test_acc: 0.64\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006223, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006481, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007361, train_acc: 0.80\n",
      "alexnet1d, trial.23:\n",
      "Epoch 22, avg test_loss: 0.013367, test_acc: 0.63\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006355, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007175, train_acc: 0.82\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.009838, train_acc: 0.70\n",
      "alexnet1d, trial.23:\n",
      "Epoch 23, avg test_loss: 0.012213, test_acc: 0.66\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005340, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.007208, train_acc: 0.82\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.008608, train_acc: 0.84\n",
      "alexnet1d, trial.23:\n",
      "Epoch 24, avg test_loss: 0.012850, test_acc: 0.67\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.007801, train_acc: 0.80\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005303, train_acc: 0.95\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005080, train_acc: 0.89\n",
      "alexnet1d, trial.23:\n",
      "Epoch 25, avg test_loss: 0.016546, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005354, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005645, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.005758, train_acc: 0.89\n",
      "alexnet1d, trial.23:\n",
      "Epoch 26, avg test_loss: 0.015746, test_acc: 0.67\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.005585, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.003403, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.002724, train_acc: 0.93\n",
      "alexnet1d, trial.23:\n",
      "Epoch 27, avg test_loss: 0.021853, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.002671, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.003032, train_acc: 0.96\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.006514, train_acc: 0.88\n",
      "alexnet1d, trial.23:\n",
      "Epoch 28, avg test_loss: 0.026912, test_acc: 0.60\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.002799, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.002650, train_acc: 0.89\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.003075, train_acc: 0.98\n",
      "alexnet1d, trial.23:\n",
      "Epoch 29, avg test_loss: 0.025468, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号49个\n",
      "错误信号21个\n",
      "信号正确并预测正确的概率为0.471\n",
      "信号错误并预测正确的概率为0.129\n",
      "总正确率为0.60\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012428, train_acc: 0.46\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.013543, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012296, train_acc: 0.55\n",
      "alexnet1d, trial.24:\n",
      "Epoch 0, avg test_loss: 0.009850, test_acc: 0.60\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012238, train_acc: 0.62\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012371, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011867, train_acc: 0.64\n",
      "alexnet1d, trial.24:\n",
      "Epoch 1, avg test_loss: 0.009929, test_acc: 0.60\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011993, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012222, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012197, train_acc: 0.59\n",
      "alexnet1d, trial.24:\n",
      "Epoch 2, avg test_loss: 0.010008, test_acc: 0.60\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012485, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011801, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011816, train_acc: 0.59\n",
      "alexnet1d, trial.24:\n",
      "Epoch 3, avg test_loss: 0.010795, test_acc: 0.60\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012162, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012682, train_acc: 0.52\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011658, train_acc: 0.66\n",
      "alexnet1d, trial.24:\n",
      "Epoch 4, avg test_loss: 0.009954, test_acc: 0.60\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011610, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011597, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012768, train_acc: 0.52\n",
      "alexnet1d, trial.24:\n",
      "Epoch 5, avg test_loss: 0.010564, test_acc: 0.60\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011485, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011338, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.013034, train_acc: 0.54\n",
      "alexnet1d, trial.24:\n",
      "Epoch 6, avg test_loss: 0.011237, test_acc: 0.63\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.010880, train_acc: 0.73\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.010974, train_acc: 0.68\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011930, train_acc: 0.52\n",
      "alexnet1d, trial.24:\n",
      "Epoch 7, avg test_loss: 0.010098, test_acc: 0.53\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012079, train_acc: 0.55\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.010910, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.013439, train_acc: 0.46\n",
      "alexnet1d, trial.24:\n",
      "Epoch 8, avg test_loss: 0.011236, test_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011578, train_acc: 0.50\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012159, train_acc: 0.50\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011144, train_acc: 0.64\n",
      "alexnet1d, trial.24:\n",
      "Epoch 9, avg test_loss: 0.011158, test_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011779, train_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011558, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010636, train_acc: 0.68\n",
      "alexnet1d, trial.24:\n",
      "Epoch 10, avg test_loss: 0.011279, test_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011044, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.012449, train_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010507, train_acc: 0.59\n",
      "alexnet1d, trial.24:\n",
      "Epoch 11, avg test_loss: 0.010763, test_acc: 0.54\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010285, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010376, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010966, train_acc: 0.75\n",
      "alexnet1d, trial.24:\n",
      "Epoch 12, avg test_loss: 0.011528, test_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010252, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011540, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009744, train_acc: 0.59\n",
      "alexnet1d, trial.24:\n",
      "Epoch 13, avg test_loss: 0.012062, test_acc: 0.60\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010158, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.008435, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009036, train_acc: 0.75\n",
      "alexnet1d, trial.24:\n",
      "Epoch 14, avg test_loss: 0.012412, test_acc: 0.57\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009261, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010424, train_acc: 0.68\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009751, train_acc: 0.73\n",
      "alexnet1d, trial.24:\n",
      "Epoch 15, avg test_loss: 0.012420, test_acc: 0.61\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008923, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008596, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009174, train_acc: 0.77\n",
      "alexnet1d, trial.24:\n",
      "Epoch 16, avg test_loss: 0.014224, test_acc: 0.64\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008320, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010177, train_acc: 0.70\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007784, train_acc: 0.80\n",
      "alexnet1d, trial.24:\n",
      "Epoch 17, avg test_loss: 0.014314, test_acc: 0.54\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008071, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.010845, train_acc: 0.59\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009795, train_acc: 0.73\n",
      "alexnet1d, trial.24:\n",
      "Epoch 18, avg test_loss: 0.014968, test_acc: 0.60\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009022, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.010783, train_acc: 0.75\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008124, train_acc: 0.79\n",
      "alexnet1d, trial.24:\n",
      "Epoch 19, avg test_loss: 0.012062, test_acc: 0.60\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008463, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.009939, train_acc: 0.73\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007627, train_acc: 0.77\n",
      "alexnet1d, trial.24:\n",
      "Epoch 20, avg test_loss: 0.014832, test_acc: 0.54\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007103, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006297, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.005774, train_acc: 0.86\n",
      "alexnet1d, trial.24:\n",
      "Epoch 21, avg test_loss: 0.015270, test_acc: 0.61\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005909, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007938, train_acc: 0.79\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007756, train_acc: 0.82\n",
      "alexnet1d, trial.24:\n",
      "Epoch 22, avg test_loss: 0.017323, test_acc: 0.57\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006076, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006234, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006675, train_acc: 0.84\n",
      "alexnet1d, trial.24:\n",
      "Epoch 23, avg test_loss: 0.015372, test_acc: 0.63\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004529, train_acc: 0.93\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.007625, train_acc: 0.77\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.008186, train_acc: 0.80\n",
      "alexnet1d, trial.24:\n",
      "Epoch 24, avg test_loss: 0.017161, test_acc: 0.61\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005917, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006113, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005398, train_acc: 0.88\n",
      "alexnet1d, trial.24:\n",
      "Epoch 25, avg test_loss: 0.016678, test_acc: 0.50\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004346, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005622, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.006262, train_acc: 0.89\n",
      "alexnet1d, trial.24:\n",
      "Epoch 26, avg test_loss: 0.019696, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004242, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.003935, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003034, train_acc: 0.93\n",
      "alexnet1d, trial.24:\n",
      "Epoch 27, avg test_loss: 0.022217, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.001872, train_acc: 0.96\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.003103, train_acc: 0.96\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.001835, train_acc: 0.98\n",
      "alexnet1d, trial.24:\n",
      "Epoch 28, avg test_loss: 0.024349, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.002477, train_acc: 0.96\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.001335, train_acc: 0.98\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.002852, train_acc: 0.96\n",
      "alexnet1d, trial.24:\n",
      "Epoch 29, avg test_loss: 0.029296, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号42个\n",
      "错误信号28个\n",
      "信号正确并预测正确的概率为0.314\n",
      "信号错误并预测正确的概率为0.229\n",
      "总正确率为0.54\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012279, train_acc: 0.61\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012452, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.011932, train_acc: 0.66\n",
      "alexnet1d, trial.25:\n",
      "Epoch 0, avg test_loss: 0.009922, test_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012015, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.013743, train_acc: 0.52\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012350, train_acc: 0.55\n",
      "alexnet1d, trial.25:\n",
      "Epoch 1, avg test_loss: 0.009865, test_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012404, train_acc: 0.52\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012420, train_acc: 0.48\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012127, train_acc: 0.57\n",
      "alexnet1d, trial.25:\n",
      "Epoch 2, avg test_loss: 0.009805, test_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012103, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011742, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.014013, train_acc: 0.52\n",
      "alexnet1d, trial.25:\n",
      "Epoch 3, avg test_loss: 0.010184, test_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.013170, train_acc: 0.50\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012341, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011926, train_acc: 0.64\n",
      "alexnet1d, trial.25:\n",
      "Epoch 4, avg test_loss: 0.009775, test_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012160, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012038, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012229, train_acc: 0.55\n",
      "alexnet1d, trial.25:\n",
      "Epoch 5, avg test_loss: 0.009745, test_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012126, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012483, train_acc: 0.52\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012055, train_acc: 0.59\n",
      "alexnet1d, trial.25:\n",
      "Epoch 6, avg test_loss: 0.009835, test_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011874, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012059, train_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012862, train_acc: 0.48\n",
      "alexnet1d, trial.25:\n",
      "Epoch 7, avg test_loss: 0.009889, test_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012200, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011618, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012223, train_acc: 0.54\n",
      "alexnet1d, trial.25:\n",
      "Epoch 8, avg test_loss: 0.009771, test_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012183, train_acc: 0.48\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012005, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011582, train_acc: 0.66\n",
      "alexnet1d, trial.25:\n",
      "Epoch 9, avg test_loss: 0.009995, test_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011395, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012196, train_acc: 0.52\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011403, train_acc: 0.62\n",
      "alexnet1d, trial.25:\n",
      "Epoch 10, avg test_loss: 0.009810, test_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011819, train_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011542, train_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.013267, train_acc: 0.52\n",
      "alexnet1d, trial.25:\n",
      "Epoch 11, avg test_loss: 0.010177, test_acc: 0.63\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011959, train_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011608, train_acc: 0.50\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011598, train_acc: 0.57\n",
      "alexnet1d, trial.25:\n",
      "Epoch 12, avg test_loss: 0.009837, test_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011334, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011449, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011213, train_acc: 0.68\n",
      "alexnet1d, trial.25:\n",
      "Epoch 13, avg test_loss: 0.010069, test_acc: 0.54\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010595, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011818, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009941, train_acc: 0.70\n",
      "alexnet1d, trial.25:\n",
      "Epoch 14, avg test_loss: 0.009772, test_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009984, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010785, train_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011732, train_acc: 0.62\n",
      "alexnet1d, trial.25:\n",
      "Epoch 15, avg test_loss: 0.011006, test_acc: 0.64\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.011293, train_acc: 0.54\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010156, train_acc: 0.68\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.011205, train_acc: 0.68\n",
      "alexnet1d, trial.25:\n",
      "Epoch 16, avg test_loss: 0.009796, test_acc: 0.60\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009965, train_acc: 0.62\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.012230, train_acc: 0.61\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009920, train_acc: 0.70\n",
      "alexnet1d, trial.25:\n",
      "Epoch 17, avg test_loss: 0.010644, test_acc: 0.51\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.010919, train_acc: 0.68\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008996, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.011321, train_acc: 0.66\n",
      "alexnet1d, trial.25:\n",
      "Epoch 18, avg test_loss: 0.011114, test_acc: 0.54\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008585, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.011056, train_acc: 0.66\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008195, train_acc: 0.86\n",
      "alexnet1d, trial.25:\n",
      "Epoch 19, avg test_loss: 0.009777, test_acc: 0.67\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008301, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008590, train_acc: 0.77\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009810, train_acc: 0.71\n",
      "alexnet1d, trial.25:\n",
      "Epoch 20, avg test_loss: 0.011294, test_acc: 0.59\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008742, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.005479, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007990, train_acc: 0.82\n",
      "alexnet1d, trial.25:\n",
      "Epoch 21, avg test_loss: 0.013259, test_acc: 0.61\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006764, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007637, train_acc: 0.77\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007936, train_acc: 0.84\n",
      "alexnet1d, trial.25:\n",
      "Epoch 22, avg test_loss: 0.013179, test_acc: 0.60\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007130, train_acc: 0.82\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.008986, train_acc: 0.77\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.007144, train_acc: 0.80\n",
      "alexnet1d, trial.25:\n",
      "Epoch 23, avg test_loss: 0.012381, test_acc: 0.59\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.008034, train_acc: 0.80\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006508, train_acc: 0.82\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.007459, train_acc: 0.79\n",
      "alexnet1d, trial.25:\n",
      "Epoch 24, avg test_loss: 0.013192, test_acc: 0.59\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.007801, train_acc: 0.75\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005058, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005173, train_acc: 0.88\n",
      "alexnet1d, trial.25:\n",
      "Epoch 25, avg test_loss: 0.014314, test_acc: 0.64\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.006257, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004855, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004862, train_acc: 0.88\n",
      "alexnet1d, trial.25:\n",
      "Epoch 26, avg test_loss: 0.015996, test_acc: 0.59\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004127, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.002342, train_acc: 0.98\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.005456, train_acc: 0.89\n",
      "alexnet1d, trial.25:\n",
      "Epoch 27, avg test_loss: 0.019397, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004147, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004363, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.005274, train_acc: 0.84\n",
      "alexnet1d, trial.25:\n",
      "Epoch 28, avg test_loss: 0.019245, test_acc: 0.66\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.002330, train_acc: 0.98\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.005653, train_acc: 0.89\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.004284, train_acc: 0.93\n",
      "alexnet1d, trial.25:\n",
      "Epoch 29, avg test_loss: 0.019743, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.002706, train_acc: 0.95\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.003557, train_acc: 0.96\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.002873, train_acc: 0.96\n",
      "alexnet1d, trial.25:\n",
      "Epoch 30, avg test_loss: 0.026559, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.001589, train_acc: 0.98\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.003078, train_acc: 0.93\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.002608, train_acc: 0.98\n",
      "alexnet1d, trial.25:\n",
      "Epoch 31, avg test_loss: 0.021640, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号41个\n",
      "错误信号29个\n",
      "信号正确并预测正确的概率为0.371\n",
      "信号错误并预测正确的概率为0.186\n",
      "总正确率为0.56\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012402, train_acc: 0.48\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.010981, train_acc: 0.66\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.014124, train_acc: 0.39\n",
      "alexnet1d, trial.26:\n",
      "Epoch 0, avg test_loss: 0.009903, test_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012347, train_acc: 0.52\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012417, train_acc: 0.43\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012435, train_acc: 0.43\n",
      "alexnet1d, trial.26:\n",
      "Epoch 1, avg test_loss: 0.009905, test_acc: 0.44\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012360, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012259, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012293, train_acc: 0.55\n",
      "alexnet1d, trial.26:\n",
      "Epoch 2, avg test_loss: 0.009940, test_acc: 0.50\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011863, train_acc: 0.73\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011655, train_acc: 0.68\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012611, train_acc: 0.50\n",
      "alexnet1d, trial.26:\n",
      "Epoch 3, avg test_loss: 0.010101, test_acc: 0.50\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012107, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011979, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011418, train_acc: 0.70\n",
      "alexnet1d, trial.26:\n",
      "Epoch 4, avg test_loss: 0.010120, test_acc: 0.50\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011769, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011768, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011467, train_acc: 0.70\n",
      "alexnet1d, trial.26:\n",
      "Epoch 5, avg test_loss: 0.010243, test_acc: 0.50\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012401, train_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011894, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011923, train_acc: 0.59\n",
      "alexnet1d, trial.26:\n",
      "Epoch 6, avg test_loss: 0.010324, test_acc: 0.50\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011403, train_acc: 0.68\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012418, train_acc: 0.55\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012145, train_acc: 0.59\n",
      "alexnet1d, trial.26:\n",
      "Epoch 7, avg test_loss: 0.010248, test_acc: 0.50\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012209, train_acc: 0.55\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011808, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012172, train_acc: 0.57\n",
      "alexnet1d, trial.26:\n",
      "Epoch 8, avg test_loss: 0.010110, test_acc: 0.50\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011585, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011973, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011560, train_acc: 0.62\n",
      "alexnet1d, trial.26:\n",
      "Epoch 9, avg test_loss: 0.010203, test_acc: 0.50\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011884, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011109, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010878, train_acc: 0.68\n",
      "alexnet1d, trial.26:\n",
      "Epoch 10, avg test_loss: 0.010468, test_acc: 0.50\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.012578, train_acc: 0.50\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011818, train_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.012432, train_acc: 0.62\n",
      "alexnet1d, trial.26:\n",
      "Epoch 11, avg test_loss: 0.011139, test_acc: 0.50\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011585, train_acc: 0.55\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011510, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010588, train_acc: 0.71\n",
      "alexnet1d, trial.26:\n",
      "Epoch 12, avg test_loss: 0.010693, test_acc: 0.47\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010952, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.012540, train_acc: 0.55\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011219, train_acc: 0.54\n",
      "alexnet1d, trial.26:\n",
      "Epoch 13, avg test_loss: 0.011062, test_acc: 0.49\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010579, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010907, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011902, train_acc: 0.70\n",
      "alexnet1d, trial.26:\n",
      "Epoch 14, avg test_loss: 0.012373, test_acc: 0.53\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.011375, train_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009701, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009588, train_acc: 0.73\n",
      "alexnet1d, trial.26:\n",
      "Epoch 15, avg test_loss: 0.011558, test_acc: 0.50\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010207, train_acc: 0.68\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010191, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010424, train_acc: 0.68\n",
      "alexnet1d, trial.26:\n",
      "Epoch 16, avg test_loss: 0.012314, test_acc: 0.50\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.011262, train_acc: 0.61\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010942, train_acc: 0.64\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009804, train_acc: 0.73\n",
      "alexnet1d, trial.26:\n",
      "Epoch 17, avg test_loss: 0.013523, test_acc: 0.51\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007968, train_acc: 0.86\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.011184, train_acc: 0.64\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.012176, train_acc: 0.66\n",
      "alexnet1d, trial.26:\n",
      "Epoch 18, avg test_loss: 0.013323, test_acc: 0.43\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009471, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.010001, train_acc: 0.68\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.010130, train_acc: 0.62\n",
      "alexnet1d, trial.26:\n",
      "Epoch 19, avg test_loss: 0.012416, test_acc: 0.50\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.009855, train_acc: 0.70\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.011906, train_acc: 0.59\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008646, train_acc: 0.77\n",
      "alexnet1d, trial.26:\n",
      "Epoch 20, avg test_loss: 0.012189, test_acc: 0.44\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.009195, train_acc: 0.73\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.008790, train_acc: 0.73\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008562, train_acc: 0.88\n",
      "alexnet1d, trial.26:\n",
      "Epoch 21, avg test_loss: 0.013258, test_acc: 0.41\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.011264, train_acc: 0.62\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.008002, train_acc: 0.77\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008541, train_acc: 0.82\n",
      "alexnet1d, trial.26:\n",
      "Epoch 22, avg test_loss: 0.016413, test_acc: 0.40\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006200, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.009780, train_acc: 0.71\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.007662, train_acc: 0.70\n",
      "alexnet1d, trial.26:\n",
      "Epoch 23, avg test_loss: 0.016223, test_acc: 0.41\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.009542, train_acc: 0.73\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.008127, train_acc: 0.80\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.008256, train_acc: 0.77\n",
      "alexnet1d, trial.26:\n",
      "Epoch 24, avg test_loss: 0.016721, test_acc: 0.47\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.008083, train_acc: 0.79\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.007483, train_acc: 0.75\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.007860, train_acc: 0.79\n",
      "alexnet1d, trial.26:\n",
      "Epoch 25, avg test_loss: 0.015881, test_acc: 0.40\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.007042, train_acc: 0.82\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.006919, train_acc: 0.82\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.006919, train_acc: 0.88\n",
      "alexnet1d, trial.26:\n",
      "Epoch 26, avg test_loss: 0.017971, test_acc: 0.36\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.008695, train_acc: 0.79\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.006083, train_acc: 0.84\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.007578, train_acc: 0.79\n",
      "alexnet1d, trial.26:\n",
      "Epoch 27, avg test_loss: 0.020053, test_acc: 0.41\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004732, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.010575, train_acc: 0.77\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.008938, train_acc: 0.71\n",
      "alexnet1d, trial.26:\n",
      "Epoch 28, avg test_loss: 0.022296, test_acc: 0.47\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.007979, train_acc: 0.73\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.009405, train_acc: 0.80\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.010486, train_acc: 0.70\n",
      "alexnet1d, trial.26:\n",
      "Epoch 29, avg test_loss: 0.017216, test_acc: 0.51\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.009773, train_acc: 0.71\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.006491, train_acc: 0.84\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.004677, train_acc: 0.93\n",
      "alexnet1d, trial.26:\n",
      "Epoch 30, avg test_loss: 0.020119, test_acc: 0.41\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.007910, train_acc: 0.80\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.007392, train_acc: 0.86\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.006269, train_acc: 0.84\n",
      "alexnet1d, trial.26:\n",
      "Epoch 31, avg test_loss: 0.021591, test_acc: 0.40\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.006049, train_acc: 0.84\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.006629, train_acc: 0.84\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.005813, train_acc: 0.88\n",
      "alexnet1d, trial.26:\n",
      "Epoch 32, avg test_loss: 0.020169, test_acc: 0.37\n",
      "Train Epoch 33, lr: 0.000377, 0/280, avg loss: 0.005533, train_acc: 0.91\n",
      "Train Epoch 33, lr: 0.000377, 112/280, avg loss: 0.004503, train_acc: 0.89\n",
      "Train Epoch 33, lr: 0.000377, 224/280, avg loss: 0.006920, train_acc: 0.86\n",
      "alexnet1d, trial.26:\n",
      "Epoch 33, avg test_loss: 0.021883, test_acc: 0.41\n",
      "Train Epoch 34, lr: 0.000377, 0/280, avg loss: 0.005187, train_acc: 0.91\n",
      "Train Epoch 34, lr: 0.000377, 112/280, avg loss: 0.007151, train_acc: 0.82\n",
      "Train Epoch 34, lr: 0.000377, 224/280, avg loss: 0.003040, train_acc: 0.95\n",
      "alexnet1d, trial.26:\n",
      "Epoch 34, avg test_loss: 0.024409, test_acc: 0.41\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 35, lr: 0.000321, 0/280, avg loss: 0.004029, train_acc: 0.93\n",
      "Train Epoch 35, lr: 0.000321, 112/280, avg loss: 0.002680, train_acc: 0.98\n",
      "Train Epoch 35, lr: 0.000321, 224/280, avg loss: 0.004965, train_acc: 0.88\n",
      "alexnet1d, trial.26:\n",
      "Epoch 35, avg test_loss: 0.027924, test_acc: 0.40\n",
      "Train Epoch 36, lr: 0.000321, 0/280, avg loss: 0.002915, train_acc: 0.95\n",
      "Train Epoch 36, lr: 0.000321, 112/280, avg loss: 0.002750, train_acc: 0.95\n",
      "Train Epoch 36, lr: 0.000321, 224/280, avg loss: 0.002630, train_acc: 0.95\n",
      "alexnet1d, trial.26:\n",
      "Epoch 36, avg test_loss: 0.031032, test_acc: 0.40\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 37, lr: 0.000321, 0/280, avg loss: 0.002978, train_acc: 0.93\n",
      "Train Epoch 37, lr: 0.000321, 112/280, avg loss: 0.001864, train_acc: 0.98\n",
      "Train Epoch 37, lr: 0.000321, 224/280, avg loss: 0.004136, train_acc: 0.93\n",
      "alexnet1d, trial.26:\n",
      "Epoch 37, avg test_loss: 0.032344, test_acc: 0.46\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号35个\n",
      "错误信号35个\n",
      "信号正确并预测正确的概率为0.3\n",
      "信号错误并预测正确的概率为0.157\n",
      "总正确率为0.46\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012260, train_acc: 0.62\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012344, train_acc: 0.50\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012352, train_acc: 0.54\n",
      "alexnet1d, trial.27:\n",
      "Epoch 0, avg test_loss: 0.009721, test_acc: 0.63\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012161, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012461, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011939, train_acc: 0.64\n",
      "alexnet1d, trial.27:\n",
      "Epoch 1, avg test_loss: 0.009669, test_acc: 0.63\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012628, train_acc: 0.52\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012108, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011849, train_acc: 0.66\n",
      "alexnet1d, trial.27:\n",
      "Epoch 2, avg test_loss: 0.009666, test_acc: 0.63\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012456, train_acc: 0.46\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012499, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.013151, train_acc: 0.48\n",
      "alexnet1d, trial.27:\n",
      "Epoch 3, avg test_loss: 0.009716, test_acc: 0.63\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012326, train_acc: 0.50\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012173, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012150, train_acc: 0.64\n",
      "alexnet1d, trial.27:\n",
      "Epoch 4, avg test_loss: 0.009777, test_acc: 0.63\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012094, train_acc: 0.68\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012235, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012209, train_acc: 0.61\n",
      "alexnet1d, trial.27:\n",
      "Epoch 5, avg test_loss: 0.009667, test_acc: 0.63\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012262, train_acc: 0.52\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011980, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012575, train_acc: 0.50\n",
      "alexnet1d, trial.27:\n",
      "Epoch 6, avg test_loss: 0.009575, test_acc: 0.63\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012477, train_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011899, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011754, train_acc: 0.66\n",
      "alexnet1d, trial.27:\n",
      "Epoch 7, avg test_loss: 0.009602, test_acc: 0.63\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011727, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012237, train_acc: 0.50\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012071, train_acc: 0.55\n",
      "alexnet1d, trial.27:\n",
      "Epoch 8, avg test_loss: 0.009765, test_acc: 0.63\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012048, train_acc: 0.55\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012613, train_acc: 0.46\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011709, train_acc: 0.66\n",
      "alexnet1d, trial.27:\n",
      "Epoch 9, avg test_loss: 0.009773, test_acc: 0.63\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.012229, train_acc: 0.50\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011823, train_acc: 0.52\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011325, train_acc: 0.62\n",
      "alexnet1d, trial.27:\n",
      "Epoch 10, avg test_loss: 0.010021, test_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.012417, train_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.012563, train_acc: 0.50\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.012028, train_acc: 0.48\n",
      "alexnet1d, trial.27:\n",
      "Epoch 11, avg test_loss: 0.009705, test_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011594, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.012481, train_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010975, train_acc: 0.75\n",
      "alexnet1d, trial.27:\n",
      "Epoch 12, avg test_loss: 0.009696, test_acc: 0.57\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010995, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011416, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010975, train_acc: 0.64\n",
      "alexnet1d, trial.27:\n",
      "Epoch 13, avg test_loss: 0.009694, test_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.011357, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010298, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010735, train_acc: 0.59\n",
      "alexnet1d, trial.27:\n",
      "Epoch 14, avg test_loss: 0.010227, test_acc: 0.61\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010526, train_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010898, train_acc: 0.62\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010802, train_acc: 0.68\n",
      "alexnet1d, trial.27:\n",
      "Epoch 15, avg test_loss: 0.010087, test_acc: 0.59\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009900, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009870, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.011488, train_acc: 0.64\n",
      "alexnet1d, trial.27:\n",
      "Epoch 16, avg test_loss: 0.010692, test_acc: 0.64\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008967, train_acc: 0.68\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010645, train_acc: 0.68\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008560, train_acc: 0.84\n",
      "alexnet1d, trial.27:\n",
      "Epoch 17, avg test_loss: 0.010054, test_acc: 0.63\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009168, train_acc: 0.71\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008889, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008617, train_acc: 0.75\n",
      "alexnet1d, trial.27:\n",
      "Epoch 18, avg test_loss: 0.012336, test_acc: 0.64\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007969, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007194, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.010353, train_acc: 0.77\n",
      "alexnet1d, trial.27:\n",
      "Epoch 19, avg test_loss: 0.015789, test_acc: 0.57\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006626, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.005718, train_acc: 0.88\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009198, train_acc: 0.75\n",
      "alexnet1d, trial.27:\n",
      "Epoch 20, avg test_loss: 0.011752, test_acc: 0.64\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007297, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007522, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008291, train_acc: 0.82\n",
      "alexnet1d, trial.27:\n",
      "Epoch 21, avg test_loss: 0.013388, test_acc: 0.64\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006589, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007505, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008219, train_acc: 0.79\n",
      "alexnet1d, trial.27:\n",
      "Epoch 22, avg test_loss: 0.015009, test_acc: 0.63\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005046, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006416, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.009012, train_acc: 0.73\n",
      "alexnet1d, trial.27:\n",
      "Epoch 23, avg test_loss: 0.019463, test_acc: 0.56\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.008392, train_acc: 0.75\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005992, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.008114, train_acc: 0.79\n",
      "alexnet1d, trial.27:\n",
      "Epoch 24, avg test_loss: 0.014081, test_acc: 0.63\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.007189, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006506, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.008372, train_acc: 0.80\n",
      "alexnet1d, trial.27:\n",
      "Epoch 25, avg test_loss: 0.016925, test_acc: 0.61\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005598, train_acc: 0.84\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004803, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.003049, train_acc: 0.93\n",
      "alexnet1d, trial.27:\n",
      "Epoch 26, avg test_loss: 0.016233, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004737, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.002950, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.006099, train_acc: 0.82\n",
      "alexnet1d, trial.27:\n",
      "Epoch 27, avg test_loss: 0.020307, test_acc: 0.64\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.002223, train_acc: 0.96\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004113, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.004592, train_acc: 0.91\n",
      "alexnet1d, trial.27:\n",
      "Epoch 28, avg test_loss: 0.019961, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.002789, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.002597, train_acc: 0.96\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.003973, train_acc: 0.93\n",
      "alexnet1d, trial.27:\n",
      "Epoch 29, avg test_loss: 0.025198, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.001719, train_acc: 0.96\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.002482, train_acc: 0.93\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.002604, train_acc: 0.96\n",
      "alexnet1d, trial.27:\n",
      "Epoch 30, avg test_loss: 0.029702, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号44个\n",
      "错误信号26个\n",
      "信号正确并预测正确的概率为0.429\n",
      "信号错误并预测正确的概率为0.214\n",
      "总正确率为0.64\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012293, train_acc: 0.61\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.011486, train_acc: 0.68\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012197, train_acc: 0.59\n",
      "alexnet1d, trial.28:\n",
      "Epoch 0, avg test_loss: 0.010227, test_acc: 0.44\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012888, train_acc: 0.38\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012397, train_acc: 0.46\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012380, train_acc: 0.50\n",
      "alexnet1d, trial.28:\n",
      "Epoch 1, avg test_loss: 0.009800, test_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012291, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012399, train_acc: 0.48\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012180, train_acc: 0.62\n",
      "alexnet1d, trial.28:\n",
      "Epoch 2, avg test_loss: 0.009623, test_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012413, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012197, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012437, train_acc: 0.55\n",
      "alexnet1d, trial.28:\n",
      "Epoch 3, avg test_loss: 0.009478, test_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012990, train_acc: 0.48\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011893, train_acc: 0.66\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012132, train_acc: 0.59\n",
      "alexnet1d, trial.28:\n",
      "Epoch 4, avg test_loss: 0.009640, test_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012476, train_acc: 0.52\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012215, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012128, train_acc: 0.57\n",
      "alexnet1d, trial.28:\n",
      "Epoch 5, avg test_loss: 0.009626, test_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012535, train_acc: 0.48\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012118, train_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011733, train_acc: 0.66\n",
      "alexnet1d, trial.28:\n",
      "Epoch 6, avg test_loss: 0.009545, test_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012003, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012441, train_acc: 0.52\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012735, train_acc: 0.50\n",
      "alexnet1d, trial.28:\n",
      "Epoch 7, avg test_loss: 0.009408, test_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012301, train_acc: 0.55\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011899, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012023, train_acc: 0.59\n",
      "alexnet1d, trial.28:\n",
      "Epoch 8, avg test_loss: 0.009569, test_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011295, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012559, train_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011884, train_acc: 0.64\n",
      "alexnet1d, trial.28:\n",
      "Epoch 9, avg test_loss: 0.009537, test_acc: 0.63\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011579, train_acc: 0.70\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011595, train_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012299, train_acc: 0.57\n",
      "alexnet1d, trial.28:\n",
      "Epoch 10, avg test_loss: 0.009652, test_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011368, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011719, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.012582, train_acc: 0.50\n",
      "alexnet1d, trial.28:\n",
      "Epoch 11, avg test_loss: 0.009524, test_acc: 0.63\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010834, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.012260, train_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011077, train_acc: 0.68\n",
      "alexnet1d, trial.28:\n",
      "Epoch 12, avg test_loss: 0.009536, test_acc: 0.63\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010644, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011712, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010250, train_acc: 0.70\n",
      "alexnet1d, trial.28:\n",
      "Epoch 13, avg test_loss: 0.009690, test_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010741, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010949, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009981, train_acc: 0.77\n",
      "alexnet1d, trial.28:\n",
      "Epoch 14, avg test_loss: 0.009628, test_acc: 0.61\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010209, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009199, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.012208, train_acc: 0.62\n",
      "alexnet1d, trial.28:\n",
      "Epoch 15, avg test_loss: 0.009374, test_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010061, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.011134, train_acc: 0.55\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009470, train_acc: 0.77\n",
      "alexnet1d, trial.28:\n",
      "Epoch 16, avg test_loss: 0.009794, test_acc: 0.64\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008701, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009352, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.012432, train_acc: 0.64\n",
      "alexnet1d, trial.28:\n",
      "Epoch 17, avg test_loss: 0.009246, test_acc: 0.66\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.010586, train_acc: 0.70\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.010243, train_acc: 0.66\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009506, train_acc: 0.77\n",
      "alexnet1d, trial.28:\n",
      "Epoch 18, avg test_loss: 0.009827, test_acc: 0.64\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.010691, train_acc: 0.70\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009541, train_acc: 0.71\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009185, train_acc: 0.82\n",
      "alexnet1d, trial.28:\n",
      "Epoch 19, avg test_loss: 0.010140, test_acc: 0.66\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008455, train_acc: 0.75\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.009125, train_acc: 0.77\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008846, train_acc: 0.75\n",
      "alexnet1d, trial.28:\n",
      "Epoch 20, avg test_loss: 0.009914, test_acc: 0.61\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007790, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.009496, train_acc: 0.75\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.009396, train_acc: 0.75\n",
      "alexnet1d, trial.28:\n",
      "Epoch 21, avg test_loss: 0.010533, test_acc: 0.61\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.009206, train_acc: 0.79\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.008460, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008268, train_acc: 0.71\n",
      "alexnet1d, trial.28:\n",
      "Epoch 22, avg test_loss: 0.010885, test_acc: 0.66\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007280, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.008883, train_acc: 0.77\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.012967, train_acc: 0.68\n",
      "alexnet1d, trial.28:\n",
      "Epoch 23, avg test_loss: 0.013261, test_acc: 0.54\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.010861, train_acc: 0.71\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006775, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006137, train_acc: 0.89\n",
      "alexnet1d, trial.28:\n",
      "Epoch 24, avg test_loss: 0.010263, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.008403, train_acc: 0.80\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.007685, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006760, train_acc: 0.79\n",
      "alexnet1d, trial.28:\n",
      "Epoch 25, avg test_loss: 0.012644, test_acc: 0.56\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.008737, train_acc: 0.77\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.007926, train_acc: 0.77\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.005799, train_acc: 0.86\n",
      "alexnet1d, trial.28:\n",
      "Epoch 26, avg test_loss: 0.011284, test_acc: 0.60\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.006835, train_acc: 0.82\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.005415, train_acc: 0.88\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.006575, train_acc: 0.84\n",
      "alexnet1d, trial.28:\n",
      "Epoch 27, avg test_loss: 0.013675, test_acc: 0.59\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.006464, train_acc: 0.86\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004698, train_acc: 0.86\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.009835, train_acc: 0.77\n",
      "alexnet1d, trial.28:\n",
      "Epoch 28, avg test_loss: 0.014288, test_acc: 0.60\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.007333, train_acc: 0.84\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.007558, train_acc: 0.82\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.009425, train_acc: 0.79\n",
      "alexnet1d, trial.28:\n",
      "Epoch 29, avg test_loss: 0.014489, test_acc: 0.59\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.006252, train_acc: 0.88\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.006500, train_acc: 0.84\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.005433, train_acc: 0.88\n",
      "alexnet1d, trial.28:\n",
      "Epoch 30, avg test_loss: 0.013357, test_acc: 0.59\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.006772, train_acc: 0.82\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.003907, train_acc: 0.93\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.004368, train_acc: 0.93\n",
      "alexnet1d, trial.28:\n",
      "Epoch 31, avg test_loss: 0.015697, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.004615, train_acc: 0.93\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.004656, train_acc: 0.89\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.003580, train_acc: 0.95\n",
      "alexnet1d, trial.28:\n",
      "Epoch 32, avg test_loss: 0.013234, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 33, lr: 0.000377, 0/280, avg loss: 0.004048, train_acc: 0.88\n",
      "Train Epoch 33, lr: 0.000377, 112/280, avg loss: 0.002830, train_acc: 0.93\n",
      "Train Epoch 33, lr: 0.000377, 224/280, avg loss: 0.005872, train_acc: 0.86\n",
      "alexnet1d, trial.28:\n",
      "Epoch 33, avg test_loss: 0.016091, test_acc: 0.57\n",
      "Train Epoch 34, lr: 0.000377, 0/280, avg loss: 0.001840, train_acc: 1.00\n",
      "Train Epoch 34, lr: 0.000377, 112/280, avg loss: 0.004109, train_acc: 0.93\n",
      "Train Epoch 34, lr: 0.000377, 224/280, avg loss: 0.003510, train_acc: 0.93\n",
      "alexnet1d, trial.28:\n",
      "Epoch 34, avg test_loss: 0.017179, test_acc: 0.69\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号43个\n",
      "错误信号27个\n",
      "信号正确并预测正确的概率为0.514\n",
      "信号错误并预测正确的概率为0.171\n",
      "总正确率为0.69\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012438, train_acc: 0.41\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012716, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012338, train_acc: 0.54\n",
      "alexnet1d, trial.29:\n",
      "Epoch 0, avg test_loss: 0.009817, test_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012416, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012420, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012132, train_acc: 0.61\n",
      "alexnet1d, trial.29:\n",
      "Epoch 1, avg test_loss: 0.009523, test_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011951, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012648, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012426, train_acc: 0.55\n",
      "alexnet1d, trial.29:\n",
      "Epoch 2, avg test_loss: 0.009540, test_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012514, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012499, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012281, train_acc: 0.55\n",
      "alexnet1d, trial.29:\n",
      "Epoch 3, avg test_loss: 0.009681, test_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012084, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012151, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011719, train_acc: 0.70\n",
      "alexnet1d, trial.29:\n",
      "Epoch 4, avg test_loss: 0.009635, test_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012380, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012492, train_acc: 0.45\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012585, train_acc: 0.55\n",
      "alexnet1d, trial.29:\n",
      "Epoch 5, avg test_loss: 0.009528, test_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012654, train_acc: 0.50\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011756, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011482, train_acc: 0.62\n",
      "alexnet1d, trial.29:\n",
      "Epoch 6, avg test_loss: 0.009577, test_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011777, train_acc: 0.55\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011215, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012115, train_acc: 0.55\n",
      "alexnet1d, trial.29:\n",
      "Epoch 7, avg test_loss: 0.009382, test_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011978, train_acc: 0.55\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012032, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011515, train_acc: 0.70\n",
      "alexnet1d, trial.29:\n",
      "Epoch 8, avg test_loss: 0.009101, test_acc: 0.60\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011363, train_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012130, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012150, train_acc: 0.62\n",
      "alexnet1d, trial.29:\n",
      "Epoch 9, avg test_loss: 0.008970, test_acc: 0.69\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011151, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010948, train_acc: 0.75\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012035, train_acc: 0.62\n",
      "alexnet1d, trial.29:\n",
      "Epoch 10, avg test_loss: 0.009288, test_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011649, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011326, train_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011081, train_acc: 0.57\n",
      "alexnet1d, trial.29:\n",
      "Epoch 11, avg test_loss: 0.009400, test_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011081, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011513, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010653, train_acc: 0.61\n",
      "alexnet1d, trial.29:\n",
      "Epoch 12, avg test_loss: 0.009121, test_acc: 0.67\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010576, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010254, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010500, train_acc: 0.73\n",
      "alexnet1d, trial.29:\n",
      "Epoch 13, avg test_loss: 0.009000, test_acc: 0.67\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009749, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010468, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009266, train_acc: 0.75\n",
      "alexnet1d, trial.29:\n",
      "Epoch 14, avg test_loss: 0.010337, test_acc: 0.61\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.007994, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008577, train_acc: 0.80\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.013575, train_acc: 0.66\n",
      "alexnet1d, trial.29:\n",
      "Epoch 15, avg test_loss: 0.010176, test_acc: 0.63\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009303, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009243, train_acc: 0.64\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008892, train_acc: 0.80\n",
      "alexnet1d, trial.29:\n",
      "Epoch 16, avg test_loss: 0.009417, test_acc: 0.63\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008866, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009622, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009569, train_acc: 0.71\n",
      "alexnet1d, trial.29:\n",
      "Epoch 17, avg test_loss: 0.011083, test_acc: 0.57\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008463, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009445, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009337, train_acc: 0.70\n",
      "alexnet1d, trial.29:\n",
      "Epoch 18, avg test_loss: 0.010498, test_acc: 0.59\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009776, train_acc: 0.75\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009785, train_acc: 0.70\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009376, train_acc: 0.77\n",
      "alexnet1d, trial.29:\n",
      "Epoch 19, avg test_loss: 0.011815, test_acc: 0.63\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006690, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.005153, train_acc: 0.93\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008885, train_acc: 0.75\n",
      "alexnet1d, trial.29:\n",
      "Epoch 20, avg test_loss: 0.012426, test_acc: 0.61\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.005283, train_acc: 0.93\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.005949, train_acc: 0.89\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006605, train_acc: 0.84\n",
      "alexnet1d, trial.29:\n",
      "Epoch 21, avg test_loss: 0.011181, test_acc: 0.70\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.004395, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007799, train_acc: 0.75\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006790, train_acc: 0.84\n",
      "alexnet1d, trial.29:\n",
      "Epoch 22, avg test_loss: 0.012931, test_acc: 0.60\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.004534, train_acc: 0.95\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006427, train_acc: 0.82\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.005505, train_acc: 0.91\n",
      "alexnet1d, trial.29:\n",
      "Epoch 23, avg test_loss: 0.012743, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005290, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006402, train_acc: 0.80\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005358, train_acc: 0.89\n",
      "alexnet1d, trial.29:\n",
      "Epoch 24, avg test_loss: 0.012406, test_acc: 0.70\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.006075, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004183, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.003437, train_acc: 0.96\n",
      "alexnet1d, trial.29:\n",
      "Epoch 25, avg test_loss: 0.012496, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005344, train_acc: 0.86\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.003737, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.003034, train_acc: 0.93\n",
      "alexnet1d, trial.29:\n",
      "Epoch 26, avg test_loss: 0.015038, test_acc: 0.66\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号43个\n",
      "错误信号27个\n",
      "信号正确并预测正确的概率为0.414\n",
      "信号错误并预测正确的概率为0.243\n",
      "总正确率为0.66\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012379, train_acc: 0.50\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012017, train_acc: 0.62\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.013332, train_acc: 0.50\n",
      "alexnet1d, trial.30:\n",
      "Epoch 0, avg test_loss: 0.009957, test_acc: 0.51\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012050, train_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012251, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012308, train_acc: 0.54\n",
      "alexnet1d, trial.30:\n",
      "Epoch 1, avg test_loss: 0.009940, test_acc: 0.51\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012017, train_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012490, train_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012515, train_acc: 0.55\n",
      "alexnet1d, trial.30:\n",
      "Epoch 2, avg test_loss: 0.010393, test_acc: 0.51\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012055, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012153, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012283, train_acc: 0.57\n",
      "alexnet1d, trial.30:\n",
      "Epoch 3, avg test_loss: 0.010460, test_acc: 0.51\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012619, train_acc: 0.52\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012039, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012015, train_acc: 0.61\n",
      "alexnet1d, trial.30:\n",
      "Epoch 4, avg test_loss: 0.010411, test_acc: 0.51\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012067, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011373, train_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012234, train_acc: 0.48\n",
      "alexnet1d, trial.30:\n",
      "Epoch 5, avg test_loss: 0.011264, test_acc: 0.51\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.010648, train_acc: 0.75\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011291, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012666, train_acc: 0.54\n",
      "alexnet1d, trial.30:\n",
      "Epoch 6, avg test_loss: 0.010479, test_acc: 0.51\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012158, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012092, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011937, train_acc: 0.55\n",
      "alexnet1d, trial.30:\n",
      "Epoch 7, avg test_loss: 0.010056, test_acc: 0.51\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011698, train_acc: 0.68\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011614, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012476, train_acc: 0.59\n",
      "alexnet1d, trial.30:\n",
      "Epoch 8, avg test_loss: 0.010882, test_acc: 0.51\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010607, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011277, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.013387, train_acc: 0.54\n",
      "alexnet1d, trial.30:\n",
      "Epoch 9, avg test_loss: 0.010365, test_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011503, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011568, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011127, train_acc: 0.66\n",
      "alexnet1d, trial.30:\n",
      "Epoch 10, avg test_loss: 0.010183, test_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010611, train_acc: 0.75\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011055, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.013147, train_acc: 0.55\n",
      "alexnet1d, trial.30:\n",
      "Epoch 11, avg test_loss: 0.010809, test_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010595, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.009900, train_acc: 0.75\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011659, train_acc: 0.66\n",
      "alexnet1d, trial.30:\n",
      "Epoch 12, avg test_loss: 0.010285, test_acc: 0.56\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011056, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010975, train_acc: 0.62\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011814, train_acc: 0.64\n",
      "alexnet1d, trial.30:\n",
      "Epoch 13, avg test_loss: 0.010353, test_acc: 0.54\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009452, train_acc: 0.82\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011369, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011711, train_acc: 0.50\n",
      "alexnet1d, trial.30:\n",
      "Epoch 14, avg test_loss: 0.011402, test_acc: 0.56\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010328, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009042, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010558, train_acc: 0.66\n",
      "alexnet1d, trial.30:\n",
      "Epoch 15, avg test_loss: 0.010936, test_acc: 0.56\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010967, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008971, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008744, train_acc: 0.71\n",
      "alexnet1d, trial.30:\n",
      "Epoch 16, avg test_loss: 0.011067, test_acc: 0.56\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.010105, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008985, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009909, train_acc: 0.75\n",
      "alexnet1d, trial.30:\n",
      "Epoch 17, avg test_loss: 0.013291, test_acc: 0.57\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007858, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.010014, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009291, train_acc: 0.77\n",
      "alexnet1d, trial.30:\n",
      "Epoch 18, avg test_loss: 0.013707, test_acc: 0.60\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008531, train_acc: 0.75\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008222, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009808, train_acc: 0.79\n",
      "alexnet1d, trial.30:\n",
      "Epoch 19, avg test_loss: 0.015028, test_acc: 0.59\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007605, train_acc: 0.77\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007896, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.010703, train_acc: 0.71\n",
      "alexnet1d, trial.30:\n",
      "Epoch 20, avg test_loss: 0.013395, test_acc: 0.59\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008648, train_acc: 0.73\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007893, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008256, train_acc: 0.82\n",
      "alexnet1d, trial.30:\n",
      "Epoch 21, avg test_loss: 0.013050, test_acc: 0.54\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006491, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006988, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006314, train_acc: 0.84\n",
      "alexnet1d, trial.30:\n",
      "Epoch 22, avg test_loss: 0.015955, test_acc: 0.59\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007759, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006877, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006501, train_acc: 0.86\n",
      "alexnet1d, trial.30:\n",
      "Epoch 23, avg test_loss: 0.017058, test_acc: 0.59\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004858, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.008201, train_acc: 0.80\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005154, train_acc: 0.88\n",
      "alexnet1d, trial.30:\n",
      "Epoch 24, avg test_loss: 0.012802, test_acc: 0.64\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005498, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.012104, train_acc: 0.71\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005485, train_acc: 0.88\n",
      "alexnet1d, trial.30:\n",
      "Epoch 25, avg test_loss: 0.021090, test_acc: 0.57\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005725, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.007369, train_acc: 0.82\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.007304, train_acc: 0.80\n",
      "alexnet1d, trial.30:\n",
      "Epoch 26, avg test_loss: 0.019427, test_acc: 0.57\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.005331, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.003930, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.007112, train_acc: 0.82\n",
      "alexnet1d, trial.30:\n",
      "Epoch 27, avg test_loss: 0.017519, test_acc: 0.54\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.006118, train_acc: 0.88\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004277, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.003355, train_acc: 0.96\n",
      "alexnet1d, trial.30:\n",
      "Epoch 28, avg test_loss: 0.019196, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.004409, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.003390, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.002849, train_acc: 0.96\n",
      "alexnet1d, trial.30:\n",
      "Epoch 29, avg test_loss: 0.024837, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.003147, train_acc: 0.95\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.001443, train_acc: 0.98\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.005755, train_acc: 0.89\n",
      "alexnet1d, trial.30:\n",
      "Epoch 30, avg test_loss: 0.028401, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.002318, train_acc: 0.96\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.004951, train_acc: 0.93\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.004550, train_acc: 0.88\n",
      "alexnet1d, trial.30:\n",
      "Epoch 31, avg test_loss: 0.025354, test_acc: 0.53\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.005309, train_acc: 0.88\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.006272, train_acc: 0.89\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.003613, train_acc: 0.91\n",
      "alexnet1d, trial.30:\n",
      "Epoch 32, avg test_loss: 0.023582, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号36个\n",
      "错误信号34个\n",
      "信号正确并预测正确的概率为0.357\n",
      "信号错误并预测正确的概率为0.214\n",
      "总正确率为0.57\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012262, train_acc: 0.62\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012085, train_acc: 0.66\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012488, train_acc: 0.54\n",
      "alexnet1d, trial.31:\n",
      "Epoch 0, avg test_loss: 0.009916, test_acc: 0.56\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011609, train_acc: 0.66\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.011704, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012525, train_acc: 0.55\n",
      "alexnet1d, trial.31:\n",
      "Epoch 1, avg test_loss: 0.009856, test_acc: 0.56\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012291, train_acc: 0.50\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012039, train_acc: 0.75\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012193, train_acc: 0.61\n",
      "alexnet1d, trial.31:\n",
      "Epoch 2, avg test_loss: 0.009826, test_acc: 0.56\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012014, train_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011938, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012921, train_acc: 0.50\n",
      "alexnet1d, trial.31:\n",
      "Epoch 3, avg test_loss: 0.010005, test_acc: 0.56\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011458, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012057, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012491, train_acc: 0.52\n",
      "alexnet1d, trial.31:\n",
      "Epoch 4, avg test_loss: 0.009846, test_acc: 0.56\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012199, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012138, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012132, train_acc: 0.52\n",
      "alexnet1d, trial.31:\n",
      "Epoch 5, avg test_loss: 0.009829, test_acc: 0.56\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011406, train_acc: 0.68\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012152, train_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.013334, train_acc: 0.48\n",
      "alexnet1d, trial.31:\n",
      "Epoch 6, avg test_loss: 0.009797, test_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012086, train_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011794, train_acc: 0.70\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011752, train_acc: 0.64\n",
      "alexnet1d, trial.31:\n",
      "Epoch 7, avg test_loss: 0.009827, test_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011587, train_acc: 0.55\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012278, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012282, train_acc: 0.52\n",
      "alexnet1d, trial.31:\n",
      "Epoch 8, avg test_loss: 0.009956, test_acc: 0.56\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011146, train_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010910, train_acc: 0.71\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011059, train_acc: 0.71\n",
      "alexnet1d, trial.31:\n",
      "Epoch 9, avg test_loss: 0.010041, test_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011976, train_acc: 0.71\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011647, train_acc: 0.70\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011864, train_acc: 0.61\n",
      "alexnet1d, trial.31:\n",
      "Epoch 10, avg test_loss: 0.009883, test_acc: 0.54\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010520, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010668, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011372, train_acc: 0.59\n",
      "alexnet1d, trial.31:\n",
      "Epoch 11, avg test_loss: 0.010209, test_acc: 0.54\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010355, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011682, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010287, train_acc: 0.75\n",
      "alexnet1d, trial.31:\n",
      "Epoch 12, avg test_loss: 0.010823, test_acc: 0.51\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010080, train_acc: 0.79\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011741, train_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010375, train_acc: 0.71\n",
      "alexnet1d, trial.31:\n",
      "Epoch 13, avg test_loss: 0.010212, test_acc: 0.53\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010745, train_acc: 0.64\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009919, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009590, train_acc: 0.79\n",
      "alexnet1d, trial.31:\n",
      "Epoch 14, avg test_loss: 0.010527, test_acc: 0.53\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008841, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009603, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011859, train_acc: 0.66\n",
      "alexnet1d, trial.31:\n",
      "Epoch 15, avg test_loss: 0.010790, test_acc: 0.51\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008572, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010064, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009771, train_acc: 0.75\n",
      "alexnet1d, trial.31:\n",
      "Epoch 16, avg test_loss: 0.009115, test_acc: 0.61\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007931, train_acc: 0.84\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008950, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007285, train_acc: 0.88\n",
      "alexnet1d, trial.31:\n",
      "Epoch 17, avg test_loss: 0.011070, test_acc: 0.53\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.010368, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009106, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009956, train_acc: 0.73\n",
      "alexnet1d, trial.31:\n",
      "Epoch 18, avg test_loss: 0.012054, test_acc: 0.59\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007865, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007917, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.010189, train_acc: 0.77\n",
      "alexnet1d, trial.31:\n",
      "Epoch 19, avg test_loss: 0.010651, test_acc: 0.56\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006876, train_acc: 0.88\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.009151, train_acc: 0.71\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008958, train_acc: 0.73\n",
      "alexnet1d, trial.31:\n",
      "Epoch 20, avg test_loss: 0.012005, test_acc: 0.51\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006795, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007085, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008547, train_acc: 0.77\n",
      "alexnet1d, trial.31:\n",
      "Epoch 21, avg test_loss: 0.012139, test_acc: 0.53\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007958, train_acc: 0.79\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005510, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006900, train_acc: 0.86\n",
      "alexnet1d, trial.31:\n",
      "Epoch 22, avg test_loss: 0.013738, test_acc: 0.57\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.004472, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007722, train_acc: 0.82\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006750, train_acc: 0.82\n",
      "alexnet1d, trial.31:\n",
      "Epoch 23, avg test_loss: 0.013660, test_acc: 0.59\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005050, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006573, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.004770, train_acc: 0.91\n",
      "alexnet1d, trial.31:\n",
      "Epoch 24, avg test_loss: 0.016172, test_acc: 0.47\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004625, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.003907, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005519, train_acc: 0.88\n",
      "alexnet1d, trial.31:\n",
      "Epoch 25, avg test_loss: 0.016257, test_acc: 0.56\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004658, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.003044, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004251, train_acc: 0.91\n",
      "alexnet1d, trial.31:\n",
      "Epoch 26, avg test_loss: 0.018274, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003849, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004328, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.005472, train_acc: 0.89\n",
      "alexnet1d, trial.31:\n",
      "Epoch 27, avg test_loss: 0.019812, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003904, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.003739, train_acc: 0.89\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.002976, train_acc: 0.96\n",
      "alexnet1d, trial.31:\n",
      "Epoch 28, avg test_loss: 0.020555, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号39个\n",
      "错误信号31个\n",
      "信号正确并预测正确的概率为0.314\n",
      "信号错误并预测正确的概率为0.271\n",
      "总正确率为0.59\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012401, train_acc: 0.39\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012298, train_acc: 0.50\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012317, train_acc: 0.57\n",
      "alexnet1d, trial.32:\n",
      "Epoch 0, avg test_loss: 0.009638, test_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012276, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.011741, train_acc: 0.66\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012608, train_acc: 0.46\n",
      "alexnet1d, trial.32:\n",
      "Epoch 1, avg test_loss: 0.009519, test_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012409, train_acc: 0.52\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012461, train_acc: 0.48\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012211, train_acc: 0.62\n",
      "alexnet1d, trial.32:\n",
      "Epoch 2, avg test_loss: 0.009780, test_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012361, train_acc: 0.55\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012427, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012110, train_acc: 0.59\n",
      "alexnet1d, trial.32:\n",
      "Epoch 3, avg test_loss: 0.009584, test_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012247, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012242, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.013530, train_acc: 0.39\n",
      "alexnet1d, trial.32:\n",
      "Epoch 4, avg test_loss: 0.009398, test_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012212, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012076, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012488, train_acc: 0.48\n",
      "alexnet1d, trial.32:\n",
      "Epoch 5, avg test_loss: 0.009468, test_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012044, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011637, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012380, train_acc: 0.54\n",
      "alexnet1d, trial.32:\n",
      "Epoch 6, avg test_loss: 0.009636, test_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011797, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012292, train_acc: 0.45\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011181, train_acc: 0.62\n",
      "alexnet1d, trial.32:\n",
      "Epoch 7, avg test_loss: 0.009860, test_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011664, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011756, train_acc: 0.55\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011693, train_acc: 0.66\n",
      "alexnet1d, trial.32:\n",
      "Epoch 8, avg test_loss: 0.010587, test_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011369, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011127, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010736, train_acc: 0.71\n",
      "alexnet1d, trial.32:\n",
      "Epoch 9, avg test_loss: 0.010051, test_acc: 0.63\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010508, train_acc: 0.77\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010276, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011745, train_acc: 0.62\n",
      "alexnet1d, trial.32:\n",
      "Epoch 10, avg test_loss: 0.010732, test_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.009551, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.009489, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.009984, train_acc: 0.73\n",
      "alexnet1d, trial.32:\n",
      "Epoch 11, avg test_loss: 0.010274, test_acc: 0.63\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011736, train_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010352, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.009951, train_acc: 0.73\n",
      "alexnet1d, trial.32:\n",
      "Epoch 12, avg test_loss: 0.011861, test_acc: 0.56\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010542, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010376, train_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010692, train_acc: 0.68\n",
      "alexnet1d, trial.32:\n",
      "Epoch 13, avg test_loss: 0.011978, test_acc: 0.51\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.008248, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.008733, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010622, train_acc: 0.62\n",
      "alexnet1d, trial.32:\n",
      "Epoch 14, avg test_loss: 0.013479, test_acc: 0.60\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009325, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010419, train_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009145, train_acc: 0.71\n",
      "alexnet1d, trial.32:\n",
      "Epoch 15, avg test_loss: 0.011997, test_acc: 0.53\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009197, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009038, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008667, train_acc: 0.79\n",
      "alexnet1d, trial.32:\n",
      "Epoch 16, avg test_loss: 0.012930, test_acc: 0.63\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007183, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009754, train_acc: 0.70\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010252, train_acc: 0.68\n",
      "alexnet1d, trial.32:\n",
      "Epoch 17, avg test_loss: 0.013836, test_acc: 0.53\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007314, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008132, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008079, train_acc: 0.80\n",
      "alexnet1d, trial.32:\n",
      "Epoch 18, avg test_loss: 0.013621, test_acc: 0.56\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006828, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008555, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007905, train_acc: 0.79\n",
      "alexnet1d, trial.32:\n",
      "Epoch 19, avg test_loss: 0.015940, test_acc: 0.53\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007415, train_acc: 0.77\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.006361, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007022, train_acc: 0.80\n",
      "alexnet1d, trial.32:\n",
      "Epoch 20, avg test_loss: 0.016835, test_acc: 0.53\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006762, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.008333, train_acc: 0.75\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007514, train_acc: 0.79\n",
      "alexnet1d, trial.32:\n",
      "Epoch 21, avg test_loss: 0.016550, test_acc: 0.57\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005663, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007186, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006219, train_acc: 0.86\n",
      "alexnet1d, trial.32:\n",
      "Epoch 22, avg test_loss: 0.018972, test_acc: 0.57\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.004361, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007132, train_acc: 0.82\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.005889, train_acc: 0.86\n",
      "alexnet1d, trial.32:\n",
      "Epoch 23, avg test_loss: 0.021079, test_acc: 0.59\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005227, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005671, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.003745, train_acc: 0.95\n",
      "alexnet1d, trial.32:\n",
      "Epoch 24, avg test_loss: 0.022427, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005174, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.003007, train_acc: 0.95\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005797, train_acc: 0.88\n",
      "alexnet1d, trial.32:\n",
      "Epoch 25, avg test_loss: 0.026017, test_acc: 0.50\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004655, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004895, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.005625, train_acc: 0.86\n",
      "alexnet1d, trial.32:\n",
      "Epoch 26, avg test_loss: 0.019579, test_acc: 0.60\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004130, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.003566, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003857, train_acc: 0.91\n",
      "alexnet1d, trial.32:\n",
      "Epoch 27, avg test_loss: 0.022372, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004645, train_acc: 0.86\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.003075, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.003829, train_acc: 0.91\n",
      "alexnet1d, trial.32:\n",
      "Epoch 28, avg test_loss: 0.024015, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.002287, train_acc: 0.96\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.003654, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.002398, train_acc: 0.95\n",
      "alexnet1d, trial.32:\n",
      "Epoch 29, avg test_loss: 0.029065, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号45个\n",
      "错误信号25个\n",
      "信号正确并预测正确的概率为0.414\n",
      "信号错误并预测正确的概率为0.157\n",
      "总正确率为0.57\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012399, train_acc: 0.46\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012577, train_acc: 0.45\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.013132, train_acc: 0.62\n",
      "alexnet1d, trial.33:\n",
      "Epoch 0, avg test_loss: 0.009480, test_acc: 0.66\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012394, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012627, train_acc: 0.46\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012263, train_acc: 0.61\n",
      "alexnet1d, trial.33:\n",
      "Epoch 1, avg test_loss: 0.009811, test_acc: 0.66\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012256, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012092, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012320, train_acc: 0.55\n",
      "alexnet1d, trial.33:\n",
      "Epoch 2, avg test_loss: 0.009499, test_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012420, train_acc: 0.46\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011865, train_acc: 0.55\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012703, train_acc: 0.52\n",
      "alexnet1d, trial.33:\n",
      "Epoch 3, avg test_loss: 0.009549, test_acc: 0.66\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011913, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011903, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012295, train_acc: 0.61\n",
      "alexnet1d, trial.33:\n",
      "Epoch 4, avg test_loss: 0.009681, test_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011504, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012210, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011845, train_acc: 0.62\n",
      "alexnet1d, trial.33:\n",
      "Epoch 5, avg test_loss: 0.009626, test_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011955, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012099, train_acc: 0.48\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011240, train_acc: 0.66\n",
      "alexnet1d, trial.33:\n",
      "Epoch 6, avg test_loss: 0.010130, test_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011890, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011687, train_acc: 0.68\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.010229, train_acc: 0.70\n",
      "alexnet1d, trial.33:\n",
      "Epoch 7, avg test_loss: 0.010550, test_acc: 0.53\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010527, train_acc: 0.73\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011572, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.009950, train_acc: 0.73\n",
      "alexnet1d, trial.33:\n",
      "Epoch 8, avg test_loss: 0.009578, test_acc: 0.60\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011740, train_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010403, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011823, train_acc: 0.61\n",
      "alexnet1d, trial.33:\n",
      "Epoch 9, avg test_loss: 0.010077, test_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011120, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010016, train_acc: 0.79\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010772, train_acc: 0.75\n",
      "alexnet1d, trial.33:\n",
      "Epoch 10, avg test_loss: 0.010015, test_acc: 0.56\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.009109, train_acc: 0.75\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010726, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010464, train_acc: 0.64\n",
      "alexnet1d, trial.33:\n",
      "Epoch 11, avg test_loss: 0.010473, test_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011047, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010428, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.009601, train_acc: 0.71\n",
      "alexnet1d, trial.33:\n",
      "Epoch 12, avg test_loss: 0.010248, test_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.008115, train_acc: 0.79\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.008747, train_acc: 0.80\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011234, train_acc: 0.64\n",
      "alexnet1d, trial.33:\n",
      "Epoch 13, avg test_loss: 0.011187, test_acc: 0.57\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.008614, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.008878, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009451, train_acc: 0.77\n",
      "alexnet1d, trial.33:\n",
      "Epoch 14, avg test_loss: 0.010870, test_acc: 0.60\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008581, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010566, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009209, train_acc: 0.73\n",
      "alexnet1d, trial.33:\n",
      "Epoch 15, avg test_loss: 0.010748, test_acc: 0.60\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.011253, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009044, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008540, train_acc: 0.80\n",
      "alexnet1d, trial.33:\n",
      "Epoch 16, avg test_loss: 0.012304, test_acc: 0.54\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008327, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007920, train_acc: 0.86\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008960, train_acc: 0.75\n",
      "alexnet1d, trial.33:\n",
      "Epoch 17, avg test_loss: 0.012717, test_acc: 0.57\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007598, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009061, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008982, train_acc: 0.75\n",
      "alexnet1d, trial.33:\n",
      "Epoch 18, avg test_loss: 0.013390, test_acc: 0.60\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007006, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.006752, train_acc: 0.88\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007596, train_acc: 0.80\n",
      "alexnet1d, trial.33:\n",
      "Epoch 19, avg test_loss: 0.013396, test_acc: 0.59\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.005978, train_acc: 0.91\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007510, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007381, train_acc: 0.82\n",
      "alexnet1d, trial.33:\n",
      "Epoch 20, avg test_loss: 0.016449, test_acc: 0.59\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.005558, train_acc: 0.89\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006142, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.004496, train_acc: 0.93\n",
      "alexnet1d, trial.33:\n",
      "Epoch 21, avg test_loss: 0.018474, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005470, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005895, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007334, train_acc: 0.86\n",
      "alexnet1d, trial.33:\n",
      "Epoch 22, avg test_loss: 0.016073, test_acc: 0.59\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005251, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005794, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.005667, train_acc: 0.88\n",
      "alexnet1d, trial.33:\n",
      "Epoch 23, avg test_loss: 0.022926, test_acc: 0.53\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.003538, train_acc: 0.93\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004464, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006806, train_acc: 0.84\n",
      "alexnet1d, trial.33:\n",
      "Epoch 24, avg test_loss: 0.017300, test_acc: 0.57\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.006926, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005116, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.002812, train_acc: 0.95\n",
      "alexnet1d, trial.33:\n",
      "Epoch 25, avg test_loss: 0.027465, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.003148, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004732, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.003095, train_acc: 0.93\n",
      "alexnet1d, trial.33:\n",
      "Epoch 26, avg test_loss: 0.026180, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.002243, train_acc: 0.96\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004101, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003879, train_acc: 0.95\n",
      "alexnet1d, trial.33:\n",
      "Epoch 27, avg test_loss: 0.022618, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号46个\n",
      "错误信号24个\n",
      "信号正确并预测正确的概率为0.529\n",
      "信号错误并预测正确的概率为0.114\n",
      "总正确率为0.64\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012492, train_acc: 0.34\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.014635, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012566, train_acc: 0.45\n",
      "alexnet1d, trial.34:\n",
      "Epoch 0, avg test_loss: 0.009846, test_acc: 0.56\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012217, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012120, train_acc: 0.66\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012262, train_acc: 0.57\n",
      "alexnet1d, trial.34:\n",
      "Epoch 1, avg test_loss: 0.009810, test_acc: 0.56\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012246, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012182, train_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011958, train_acc: 0.59\n",
      "alexnet1d, trial.34:\n",
      "Epoch 2, avg test_loss: 0.010084, test_acc: 0.56\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011982, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012250, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012063, train_acc: 0.59\n",
      "alexnet1d, trial.34:\n",
      "Epoch 3, avg test_loss: 0.009830, test_acc: 0.56\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011821, train_acc: 0.66\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012189, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012185, train_acc: 0.55\n",
      "alexnet1d, trial.34:\n",
      "Epoch 4, avg test_loss: 0.009828, test_acc: 0.56\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011928, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012449, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012096, train_acc: 0.54\n",
      "alexnet1d, trial.34:\n",
      "Epoch 5, avg test_loss: 0.009832, test_acc: 0.56\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011775, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012219, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011429, train_acc: 0.64\n",
      "alexnet1d, trial.34:\n",
      "Epoch 6, avg test_loss: 0.010043, test_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.010999, train_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012031, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011737, train_acc: 0.59\n",
      "alexnet1d, trial.34:\n",
      "Epoch 7, avg test_loss: 0.010000, test_acc: 0.51\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011865, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.010650, train_acc: 0.73\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011224, train_acc: 0.64\n",
      "alexnet1d, trial.34:\n",
      "Epoch 8, avg test_loss: 0.010315, test_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010956, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011343, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011616, train_acc: 0.61\n",
      "alexnet1d, trial.34:\n",
      "Epoch 9, avg test_loss: 0.009949, test_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010482, train_acc: 0.70\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011602, train_acc: 0.52\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012886, train_acc: 0.57\n",
      "alexnet1d, trial.34:\n",
      "Epoch 10, avg test_loss: 0.010901, test_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010888, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011478, train_acc: 0.73\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011328, train_acc: 0.66\n",
      "alexnet1d, trial.34:\n",
      "Epoch 11, avg test_loss: 0.009784, test_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010930, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010639, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010803, train_acc: 0.71\n",
      "alexnet1d, trial.34:\n",
      "Epoch 12, avg test_loss: 0.010417, test_acc: 0.60\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009508, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010847, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010586, train_acc: 0.68\n",
      "alexnet1d, trial.34:\n",
      "Epoch 13, avg test_loss: 0.011375, test_acc: 0.54\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010672, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009084, train_acc: 0.79\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009521, train_acc: 0.70\n",
      "alexnet1d, trial.34:\n",
      "Epoch 14, avg test_loss: 0.011386, test_acc: 0.56\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009494, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.007851, train_acc: 0.82\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009838, train_acc: 0.75\n",
      "alexnet1d, trial.34:\n",
      "Epoch 15, avg test_loss: 0.010650, test_acc: 0.56\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008516, train_acc: 0.82\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009147, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009974, train_acc: 0.77\n",
      "alexnet1d, trial.34:\n",
      "Epoch 16, avg test_loss: 0.012181, test_acc: 0.53\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009229, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007274, train_acc: 0.84\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010696, train_acc: 0.77\n",
      "alexnet1d, trial.34:\n",
      "Epoch 17, avg test_loss: 0.013191, test_acc: 0.56\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009012, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.005828, train_acc: 0.89\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008281, train_acc: 0.75\n",
      "alexnet1d, trial.34:\n",
      "Epoch 18, avg test_loss: 0.013011, test_acc: 0.53\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007395, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.006727, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007787, train_acc: 0.80\n",
      "alexnet1d, trial.34:\n",
      "Epoch 19, avg test_loss: 0.014747, test_acc: 0.57\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006505, train_acc: 0.88\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007995, train_acc: 0.77\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006373, train_acc: 0.89\n",
      "alexnet1d, trial.34:\n",
      "Epoch 20, avg test_loss: 0.015303, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.005399, train_acc: 0.91\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007979, train_acc: 0.77\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.003368, train_acc: 0.93\n",
      "alexnet1d, trial.34:\n",
      "Epoch 21, avg test_loss: 0.016916, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005066, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007018, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007060, train_acc: 0.86\n",
      "alexnet1d, trial.34:\n",
      "Epoch 22, avg test_loss: 0.016985, test_acc: 0.57\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005013, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006194, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.003940, train_acc: 0.93\n",
      "alexnet1d, trial.34:\n",
      "Epoch 23, avg test_loss: 0.020322, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.003363, train_acc: 0.93\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005051, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.004761, train_acc: 0.91\n",
      "alexnet1d, trial.34:\n",
      "Epoch 24, avg test_loss: 0.020465, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号39个\n",
      "错误信号31个\n",
      "信号正确并预测正确的概率为0.4\n",
      "信号错误并预测正确的概率为0.143\n",
      "总正确率为0.54\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012368, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012631, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012110, train_acc: 0.71\n",
      "alexnet1d, trial.35:\n",
      "Epoch 0, avg test_loss: 0.009928, test_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012197, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012887, train_acc: 0.45\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011549, train_acc: 0.70\n",
      "alexnet1d, trial.35:\n",
      "Epoch 1, avg test_loss: 0.010020, test_acc: 0.50\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012782, train_acc: 0.46\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012102, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012118, train_acc: 0.59\n",
      "alexnet1d, trial.35:\n",
      "Epoch 2, avg test_loss: 0.010194, test_acc: 0.50\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.013358, train_acc: 0.41\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011962, train_acc: 0.62\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011886, train_acc: 0.62\n",
      "alexnet1d, trial.35:\n",
      "Epoch 3, avg test_loss: 0.010206, test_acc: 0.50\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012728, train_acc: 0.52\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012309, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012351, train_acc: 0.55\n",
      "alexnet1d, trial.35:\n",
      "Epoch 4, avg test_loss: 0.010296, test_acc: 0.50\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.013006, train_acc: 0.52\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011801, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011494, train_acc: 0.68\n",
      "alexnet1d, trial.35:\n",
      "Epoch 5, avg test_loss: 0.009994, test_acc: 0.50\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011830, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011736, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012565, train_acc: 0.52\n",
      "alexnet1d, trial.35:\n",
      "Epoch 6, avg test_loss: 0.010090, test_acc: 0.50\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012426, train_acc: 0.52\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.010695, train_acc: 0.73\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011638, train_acc: 0.62\n",
      "alexnet1d, trial.35:\n",
      "Epoch 7, avg test_loss: 0.009886, test_acc: 0.50\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011777, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011408, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012055, train_acc: 0.52\n",
      "alexnet1d, trial.35:\n",
      "Epoch 8, avg test_loss: 0.009669, test_acc: 0.50\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010928, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011707, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012609, train_acc: 0.54\n",
      "alexnet1d, trial.35:\n",
      "Epoch 9, avg test_loss: 0.009199, test_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011053, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010447, train_acc: 0.73\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012564, train_acc: 0.64\n",
      "alexnet1d, trial.35:\n",
      "Epoch 10, avg test_loss: 0.009338, test_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010547, train_acc: 0.73\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011204, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.012115, train_acc: 0.55\n",
      "alexnet1d, trial.35:\n",
      "Epoch 11, avg test_loss: 0.008716, test_acc: 0.60\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011866, train_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.009859, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.009142, train_acc: 0.79\n",
      "alexnet1d, trial.35:\n",
      "Epoch 12, avg test_loss: 0.008642, test_acc: 0.63\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.008606, train_acc: 0.80\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010159, train_acc: 0.77\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010441, train_acc: 0.73\n",
      "alexnet1d, trial.35:\n",
      "Epoch 13, avg test_loss: 0.008570, test_acc: 0.64\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009677, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010864, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009946, train_acc: 0.73\n",
      "alexnet1d, trial.35:\n",
      "Epoch 14, avg test_loss: 0.009103, test_acc: 0.63\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.011058, train_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009871, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010131, train_acc: 0.77\n",
      "alexnet1d, trial.35:\n",
      "Epoch 15, avg test_loss: 0.008877, test_acc: 0.61\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.007695, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008621, train_acc: 0.80\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009465, train_acc: 0.75\n",
      "alexnet1d, trial.35:\n",
      "Epoch 16, avg test_loss: 0.008578, test_acc: 0.66\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008152, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008208, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.011944, train_acc: 0.71\n",
      "alexnet1d, trial.35:\n",
      "Epoch 17, avg test_loss: 0.009092, test_acc: 0.70\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008690, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.006896, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008837, train_acc: 0.77\n",
      "alexnet1d, trial.35:\n",
      "Epoch 18, avg test_loss: 0.008766, test_acc: 0.59\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008009, train_acc: 0.75\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.004416, train_acc: 0.96\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007322, train_acc: 0.84\n",
      "alexnet1d, trial.35:\n",
      "Epoch 19, avg test_loss: 0.009972, test_acc: 0.61\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007337, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.006138, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007605, train_acc: 0.79\n",
      "alexnet1d, trial.35:\n",
      "Epoch 20, avg test_loss: 0.010738, test_acc: 0.64\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006434, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006329, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.004561, train_acc: 0.91\n",
      "alexnet1d, trial.35:\n",
      "Epoch 21, avg test_loss: 0.010535, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.003695, train_acc: 0.96\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.004798, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008143, train_acc: 0.86\n",
      "alexnet1d, trial.35:\n",
      "Epoch 22, avg test_loss: 0.011555, test_acc: 0.60\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.003903, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006384, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006563, train_acc: 0.84\n",
      "alexnet1d, trial.35:\n",
      "Epoch 23, avg test_loss: 0.012620, test_acc: 0.60\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.003763, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005180, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.003427, train_acc: 0.95\n",
      "alexnet1d, trial.35:\n",
      "Epoch 24, avg test_loss: 0.013536, test_acc: 0.67\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.003312, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005518, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.002463, train_acc: 0.96\n",
      "alexnet1d, trial.35:\n",
      "Epoch 25, avg test_loss: 0.016068, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.003877, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.002517, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.002397, train_acc: 0.93\n",
      "alexnet1d, trial.35:\n",
      "Epoch 26, avg test_loss: 0.017071, test_acc: 0.66\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号35个\n",
      "错误信号35个\n",
      "信号正确并预测正确的概率为0.357\n",
      "信号错误并预测正确的概率为0.3\n",
      "总正确率为0.66\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012291, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.011999, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012694, train_acc: 0.61\n",
      "alexnet1d, trial.36:\n",
      "Epoch 0, avg test_loss: 0.009832, test_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012056, train_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012005, train_acc: 0.62\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012832, train_acc: 0.46\n",
      "alexnet1d, trial.36:\n",
      "Epoch 1, avg test_loss: 0.009848, test_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012160, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012349, train_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011973, train_acc: 0.68\n",
      "alexnet1d, trial.36:\n",
      "Epoch 2, avg test_loss: 0.009837, test_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012253, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011970, train_acc: 0.62\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012150, train_acc: 0.57\n",
      "alexnet1d, trial.36:\n",
      "Epoch 3, avg test_loss: 0.009903, test_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011681, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011648, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011763, train_acc: 0.61\n",
      "alexnet1d, trial.36:\n",
      "Epoch 4, avg test_loss: 0.009970, test_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012319, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012412, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011395, train_acc: 0.70\n",
      "alexnet1d, trial.36:\n",
      "Epoch 5, avg test_loss: 0.009893, test_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011497, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012170, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012274, train_acc: 0.50\n",
      "alexnet1d, trial.36:\n",
      "Epoch 6, avg test_loss: 0.009872, test_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011919, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011381, train_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011922, train_acc: 0.61\n",
      "alexnet1d, trial.36:\n",
      "Epoch 7, avg test_loss: 0.009414, test_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011927, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.010514, train_acc: 0.70\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011975, train_acc: 0.62\n",
      "alexnet1d, trial.36:\n",
      "Epoch 8, avg test_loss: 0.009714, test_acc: 0.53\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011519, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011223, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012024, train_acc: 0.55\n",
      "alexnet1d, trial.36:\n",
      "Epoch 9, avg test_loss: 0.009399, test_acc: 0.67\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011986, train_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011011, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012407, train_acc: 0.50\n",
      "alexnet1d, trial.36:\n",
      "Epoch 10, avg test_loss: 0.009954, test_acc: 0.60\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011032, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010472, train_acc: 0.77\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011562, train_acc: 0.62\n",
      "alexnet1d, trial.36:\n",
      "Epoch 11, avg test_loss: 0.009331, test_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011257, train_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010808, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011232, train_acc: 0.64\n",
      "alexnet1d, trial.36:\n",
      "Epoch 12, avg test_loss: 0.009996, test_acc: 0.57\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009585, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010184, train_acc: 0.79\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009864, train_acc: 0.71\n",
      "alexnet1d, trial.36:\n",
      "Epoch 13, avg test_loss: 0.010042, test_acc: 0.64\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.011702, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010188, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009243, train_acc: 0.79\n",
      "alexnet1d, trial.36:\n",
      "Epoch 14, avg test_loss: 0.009716, test_acc: 0.61\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009086, train_acc: 0.80\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008597, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.012066, train_acc: 0.64\n",
      "alexnet1d, trial.36:\n",
      "Epoch 15, avg test_loss: 0.010513, test_acc: 0.60\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010039, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009024, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008806, train_acc: 0.80\n",
      "alexnet1d, trial.36:\n",
      "Epoch 16, avg test_loss: 0.010579, test_acc: 0.59\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009371, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009104, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008687, train_acc: 0.79\n",
      "alexnet1d, trial.36:\n",
      "Epoch 17, avg test_loss: 0.010359, test_acc: 0.64\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007265, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008428, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.011602, train_acc: 0.70\n",
      "alexnet1d, trial.36:\n",
      "Epoch 18, avg test_loss: 0.014789, test_acc: 0.43\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008666, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009889, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.011207, train_acc: 0.68\n",
      "alexnet1d, trial.36:\n",
      "Epoch 19, avg test_loss: 0.010159, test_acc: 0.57\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008883, train_acc: 0.71\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.009235, train_acc: 0.73\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007347, train_acc: 0.86\n",
      "alexnet1d, trial.36:\n",
      "Epoch 20, avg test_loss: 0.009413, test_acc: 0.69\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007797, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006906, train_acc: 0.77\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008082, train_acc: 0.79\n",
      "alexnet1d, trial.36:\n",
      "Epoch 21, avg test_loss: 0.010763, test_acc: 0.61\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005460, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.010182, train_acc: 0.79\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006860, train_acc: 0.80\n",
      "alexnet1d, trial.36:\n",
      "Epoch 22, avg test_loss: 0.013211, test_acc: 0.54\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007458, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007473, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006357, train_acc: 0.86\n",
      "alexnet1d, trial.36:\n",
      "Epoch 23, avg test_loss: 0.010496, test_acc: 0.64\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.007131, train_acc: 0.80\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005118, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006647, train_acc: 0.88\n",
      "alexnet1d, trial.36:\n",
      "Epoch 24, avg test_loss: 0.013619, test_acc: 0.63\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004724, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006279, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.003908, train_acc: 0.89\n",
      "alexnet1d, trial.36:\n",
      "Epoch 25, avg test_loss: 0.014437, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.003678, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.006357, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.002638, train_acc: 0.96\n",
      "alexnet1d, trial.36:\n",
      "Epoch 26, avg test_loss: 0.014739, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003608, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.006149, train_acc: 0.84\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004518, train_acc: 0.91\n",
      "alexnet1d, trial.36:\n",
      "Epoch 27, avg test_loss: 0.011446, test_acc: 0.67\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.002804, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.002868, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.004417, train_acc: 0.91\n",
      "alexnet1d, trial.36:\n",
      "Epoch 28, avg test_loss: 0.015764, test_acc: 0.69\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号38个\n",
      "错误信号32个\n",
      "信号正确并预测正确的概率为0.386\n",
      "信号错误并预测正确的概率为0.3\n",
      "总正确率为0.69\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012388, train_acc: 0.52\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012444, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012197, train_acc: 0.59\n",
      "alexnet1d, trial.37:\n",
      "Epoch 0, avg test_loss: 0.009434, test_acc: 0.66\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011928, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.011933, train_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011905, train_acc: 0.55\n",
      "alexnet1d, trial.37:\n",
      "Epoch 1, avg test_loss: 0.009540, test_acc: 0.66\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011870, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011451, train_acc: 0.66\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011986, train_acc: 0.61\n",
      "alexnet1d, trial.37:\n",
      "Epoch 2, avg test_loss: 0.010266, test_acc: 0.34\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012617, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012739, train_acc: 0.43\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012457, train_acc: 0.41\n",
      "alexnet1d, trial.37:\n",
      "Epoch 3, avg test_loss: 0.010069, test_acc: 0.34\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012458, train_acc: 0.43\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012356, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012239, train_acc: 0.61\n",
      "alexnet1d, trial.37:\n",
      "Epoch 4, avg test_loss: 0.009664, test_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012274, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012392, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012443, train_acc: 0.52\n",
      "alexnet1d, trial.37:\n",
      "Epoch 5, avg test_loss: 0.009419, test_acc: 0.66\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011845, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012493, train_acc: 0.52\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012351, train_acc: 0.54\n",
      "alexnet1d, trial.37:\n",
      "Epoch 6, avg test_loss: 0.009396, test_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012372, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012628, train_acc: 0.46\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012116, train_acc: 0.59\n",
      "alexnet1d, trial.37:\n",
      "Epoch 7, avg test_loss: 0.009498, test_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011986, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012194, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012331, train_acc: 0.52\n",
      "alexnet1d, trial.37:\n",
      "Epoch 8, avg test_loss: 0.009442, test_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011840, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012141, train_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012569, train_acc: 0.48\n",
      "alexnet1d, trial.37:\n",
      "Epoch 9, avg test_loss: 0.009441, test_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011489, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012410, train_acc: 0.55\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012211, train_acc: 0.57\n",
      "alexnet1d, trial.37:\n",
      "Epoch 10, avg test_loss: 0.009620, test_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011902, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010697, train_acc: 0.75\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011957, train_acc: 0.48\n",
      "alexnet1d, trial.37:\n",
      "Epoch 11, avg test_loss: 0.009431, test_acc: 0.67\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011180, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011867, train_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011865, train_acc: 0.61\n",
      "alexnet1d, trial.37:\n",
      "Epoch 12, avg test_loss: 0.009492, test_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011796, train_acc: 0.55\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011130, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011914, train_acc: 0.57\n",
      "alexnet1d, trial.37:\n",
      "Epoch 13, avg test_loss: 0.009372, test_acc: 0.63\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.012032, train_acc: 0.62\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011638, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011950, train_acc: 0.59\n",
      "alexnet1d, trial.37:\n",
      "Epoch 14, avg test_loss: 0.009346, test_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.011737, train_acc: 0.59\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.012610, train_acc: 0.55\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010513, train_acc: 0.75\n",
      "alexnet1d, trial.37:\n",
      "Epoch 15, avg test_loss: 0.009579, test_acc: 0.60\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010896, train_acc: 0.61\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.011496, train_acc: 0.68\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.011425, train_acc: 0.66\n",
      "alexnet1d, trial.37:\n",
      "Epoch 16, avg test_loss: 0.009696, test_acc: 0.56\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.011481, train_acc: 0.66\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.011629, train_acc: 0.66\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010334, train_acc: 0.70\n",
      "alexnet1d, trial.37:\n",
      "Epoch 17, avg test_loss: 0.009570, test_acc: 0.70\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009978, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.011029, train_acc: 0.66\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.011656, train_acc: 0.54\n",
      "alexnet1d, trial.37:\n",
      "Epoch 18, avg test_loss: 0.009675, test_acc: 0.67\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009986, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.010082, train_acc: 0.71\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.011401, train_acc: 0.68\n",
      "alexnet1d, trial.37:\n",
      "Epoch 19, avg test_loss: 0.009834, test_acc: 0.63\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.009372, train_acc: 0.70\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.011166, train_acc: 0.62\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.011164, train_acc: 0.64\n",
      "alexnet1d, trial.37:\n",
      "Epoch 20, avg test_loss: 0.010091, test_acc: 0.60\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.010549, train_acc: 0.66\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.008969, train_acc: 0.75\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.010041, train_acc: 0.64\n",
      "alexnet1d, trial.37:\n",
      "Epoch 21, avg test_loss: 0.009883, test_acc: 0.69\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.009590, train_acc: 0.75\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.009212, train_acc: 0.75\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.010201, train_acc: 0.71\n",
      "alexnet1d, trial.37:\n",
      "Epoch 22, avg test_loss: 0.009892, test_acc: 0.61\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.009895, train_acc: 0.73\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.009394, train_acc: 0.73\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.009587, train_acc: 0.75\n",
      "alexnet1d, trial.37:\n",
      "Epoch 23, avg test_loss: 0.010732, test_acc: 0.54\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.009854, train_acc: 0.71\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.009239, train_acc: 0.71\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.010700, train_acc: 0.70\n",
      "alexnet1d, trial.37:\n",
      "Epoch 24, avg test_loss: 0.010726, test_acc: 0.64\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.009978, train_acc: 0.70\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.008316, train_acc: 0.79\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.008458, train_acc: 0.79\n",
      "alexnet1d, trial.37:\n",
      "Epoch 25, avg test_loss: 0.010336, test_acc: 0.56\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.008521, train_acc: 0.77\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.009703, train_acc: 0.73\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.007454, train_acc: 0.77\n",
      "alexnet1d, trial.37:\n",
      "Epoch 26, avg test_loss: 0.010479, test_acc: 0.66\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.007245, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.010105, train_acc: 0.62\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.008433, train_acc: 0.86\n",
      "alexnet1d, trial.37:\n",
      "Epoch 27, avg test_loss: 0.011226, test_acc: 0.63\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.007349, train_acc: 0.79\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.008912, train_acc: 0.80\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.008970, train_acc: 0.75\n",
      "alexnet1d, trial.37:\n",
      "Epoch 28, avg test_loss: 0.010915, test_acc: 0.63\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.006748, train_acc: 0.82\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.008566, train_acc: 0.73\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.009308, train_acc: 0.71\n",
      "alexnet1d, trial.37:\n",
      "Epoch 29, avg test_loss: 0.012431, test_acc: 0.54\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.007342, train_acc: 0.82\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.006328, train_acc: 0.84\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.007628, train_acc: 0.79\n",
      "alexnet1d, trial.37:\n",
      "Epoch 30, avg test_loss: 0.012315, test_acc: 0.64\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.006860, train_acc: 0.82\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.006054, train_acc: 0.86\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.007725, train_acc: 0.77\n",
      "alexnet1d, trial.37:\n",
      "Epoch 31, avg test_loss: 0.013236, test_acc: 0.54\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.006734, train_acc: 0.77\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.005629, train_acc: 0.84\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.008031, train_acc: 0.86\n",
      "alexnet1d, trial.37:\n",
      "Epoch 32, avg test_loss: 0.013225, test_acc: 0.64\n",
      "Train Epoch 33, lr: 0.000377, 0/280, avg loss: 0.006405, train_acc: 0.89\n",
      "Train Epoch 33, lr: 0.000377, 112/280, avg loss: 0.007308, train_acc: 0.77\n",
      "Train Epoch 33, lr: 0.000377, 224/280, avg loss: 0.006890, train_acc: 0.84\n",
      "alexnet1d, trial.37:\n",
      "Epoch 33, avg test_loss: 0.014934, test_acc: 0.59\n",
      "Train Epoch 34, lr: 0.000377, 0/280, avg loss: 0.005135, train_acc: 0.88\n",
      "Train Epoch 34, lr: 0.000377, 112/280, avg loss: 0.005975, train_acc: 0.89\n",
      "Train Epoch 34, lr: 0.000377, 224/280, avg loss: 0.006488, train_acc: 0.86\n",
      "alexnet1d, trial.37:\n",
      "Epoch 34, avg test_loss: 0.015075, test_acc: 0.59\n",
      "Train Epoch 35, lr: 0.000321, 0/280, avg loss: 0.004403, train_acc: 0.88\n",
      "Train Epoch 35, lr: 0.000321, 112/280, avg loss: 0.005608, train_acc: 0.86\n",
      "Train Epoch 35, lr: 0.000321, 224/280, avg loss: 0.004177, train_acc: 0.96\n",
      "alexnet1d, trial.37:\n",
      "Epoch 35, avg test_loss: 0.016783, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 36, lr: 0.000321, 0/280, avg loss: 0.003976, train_acc: 0.91\n",
      "Train Epoch 36, lr: 0.000321, 112/280, avg loss: 0.005216, train_acc: 0.86\n",
      "Train Epoch 36, lr: 0.000321, 224/280, avg loss: 0.004155, train_acc: 0.89\n",
      "alexnet1d, trial.37:\n",
      "Epoch 36, avg test_loss: 0.018622, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 37, lr: 0.000321, 0/280, avg loss: 0.003682, train_acc: 0.88\n",
      "Train Epoch 37, lr: 0.000321, 112/280, avg loss: 0.003897, train_acc: 0.95\n",
      "Train Epoch 37, lr: 0.000321, 224/280, avg loss: 0.005342, train_acc: 0.84\n",
      "alexnet1d, trial.37:\n",
      "Epoch 37, avg test_loss: 0.017226, test_acc: 0.61\n",
      "Train Epoch 38, lr: 0.000321, 0/280, avg loss: 0.002663, train_acc: 0.96\n",
      "Train Epoch 38, lr: 0.000321, 112/280, avg loss: 0.004225, train_acc: 0.91\n",
      "Train Epoch 38, lr: 0.000321, 224/280, avg loss: 0.004756, train_acc: 0.93\n",
      "alexnet1d, trial.37:\n",
      "Epoch 38, avg test_loss: 0.019980, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 39, lr: 0.000321, 0/280, avg loss: 0.003483, train_acc: 0.93\n",
      "Train Epoch 39, lr: 0.000321, 112/280, avg loss: 0.001942, train_acc: 0.98\n",
      "Train Epoch 39, lr: 0.000321, 224/280, avg loss: 0.003796, train_acc: 0.93\n",
      "alexnet1d, trial.37:\n",
      "Epoch 39, avg test_loss: 0.022791, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号46个\n",
      "错误信号24个\n",
      "信号正确并预测正确的概率为0.457\n",
      "信号错误并预测正确的概率为0.129\n",
      "总正确率为0.59\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012401, train_acc: 0.38\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012260, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.014419, train_acc: 0.54\n",
      "alexnet1d, trial.38:\n",
      "Epoch 0, avg test_loss: 0.009698, test_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012164, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012420, train_acc: 0.43\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012398, train_acc: 0.43\n",
      "alexnet1d, trial.38:\n",
      "Epoch 1, avg test_loss: 0.009840, test_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012431, train_acc: 0.50\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012303, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012148, train_acc: 0.59\n",
      "alexnet1d, trial.38:\n",
      "Epoch 2, avg test_loss: 0.009615, test_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012469, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012164, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012294, train_acc: 0.52\n",
      "alexnet1d, trial.38:\n",
      "Epoch 3, avg test_loss: 0.009583, test_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011848, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012490, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012323, train_acc: 0.52\n",
      "alexnet1d, trial.38:\n",
      "Epoch 4, avg test_loss: 0.009662, test_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012016, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011937, train_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012297, train_acc: 0.52\n",
      "alexnet1d, trial.38:\n",
      "Epoch 5, avg test_loss: 0.009754, test_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012082, train_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011477, train_acc: 0.66\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012238, train_acc: 0.64\n",
      "alexnet1d, trial.38:\n",
      "Epoch 6, avg test_loss: 0.010084, test_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011062, train_acc: 0.68\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011974, train_acc: 0.55\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011868, train_acc: 0.52\n",
      "alexnet1d, trial.38:\n",
      "Epoch 7, avg test_loss: 0.009931, test_acc: 0.63\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011688, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012132, train_acc: 0.50\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011904, train_acc: 0.52\n",
      "alexnet1d, trial.38:\n",
      "Epoch 8, avg test_loss: 0.010351, test_acc: 0.49\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012231, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011076, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011074, train_acc: 0.59\n",
      "alexnet1d, trial.38:\n",
      "Epoch 9, avg test_loss: 0.010224, test_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011876, train_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011556, train_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012269, train_acc: 0.62\n",
      "alexnet1d, trial.38:\n",
      "Epoch 10, avg test_loss: 0.010987, test_acc: 0.47\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011258, train_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011063, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011227, train_acc: 0.70\n",
      "alexnet1d, trial.38:\n",
      "Epoch 11, avg test_loss: 0.010410, test_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010343, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011774, train_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010678, train_acc: 0.61\n",
      "alexnet1d, trial.38:\n",
      "Epoch 12, avg test_loss: 0.010478, test_acc: 0.49\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010146, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011801, train_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010252, train_acc: 0.75\n",
      "alexnet1d, trial.38:\n",
      "Epoch 13, avg test_loss: 0.012180, test_acc: 0.57\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.011595, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010423, train_acc: 0.62\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011056, train_acc: 0.77\n",
      "alexnet1d, trial.38:\n",
      "Epoch 14, avg test_loss: 0.010223, test_acc: 0.59\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009812, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010032, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010144, train_acc: 0.71\n",
      "alexnet1d, trial.38:\n",
      "Epoch 15, avg test_loss: 0.011767, test_acc: 0.53\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009359, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009370, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009285, train_acc: 0.70\n",
      "alexnet1d, trial.38:\n",
      "Epoch 16, avg test_loss: 0.011839, test_acc: 0.53\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009809, train_acc: 0.71\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009680, train_acc: 0.71\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007549, train_acc: 0.79\n",
      "alexnet1d, trial.38:\n",
      "Epoch 17, avg test_loss: 0.015537, test_acc: 0.53\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008535, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008280, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007765, train_acc: 0.84\n",
      "alexnet1d, trial.38:\n",
      "Epoch 18, avg test_loss: 0.013218, test_acc: 0.59\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007952, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007634, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008838, train_acc: 0.73\n",
      "alexnet1d, trial.38:\n",
      "Epoch 19, avg test_loss: 0.014458, test_acc: 0.53\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006786, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.006832, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008470, train_acc: 0.77\n",
      "alexnet1d, trial.38:\n",
      "Epoch 20, avg test_loss: 0.018780, test_acc: 0.53\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007671, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.005680, train_acc: 0.89\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007579, train_acc: 0.79\n",
      "alexnet1d, trial.38:\n",
      "Epoch 21, avg test_loss: 0.015481, test_acc: 0.51\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.004567, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005711, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.005858, train_acc: 0.86\n",
      "alexnet1d, trial.38:\n",
      "Epoch 22, avg test_loss: 0.017165, test_acc: 0.56\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.004803, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006445, train_acc: 0.82\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.007598, train_acc: 0.82\n",
      "alexnet1d, trial.38:\n",
      "Epoch 23, avg test_loss: 0.021552, test_acc: 0.49\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.007808, train_acc: 0.82\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004499, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.011123, train_acc: 0.80\n",
      "alexnet1d, trial.38:\n",
      "Epoch 24, avg test_loss: 0.018218, test_acc: 0.57\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.003326, train_acc: 0.95\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004813, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.009578, train_acc: 0.68\n",
      "alexnet1d, trial.38:\n",
      "Epoch 25, avg test_loss: 0.017785, test_acc: 0.50\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.003978, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004786, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.005422, train_acc: 0.86\n",
      "alexnet1d, trial.38:\n",
      "Epoch 26, avg test_loss: 0.019878, test_acc: 0.53\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004837, train_acc: 0.86\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.002845, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003235, train_acc: 0.95\n",
      "alexnet1d, trial.38:\n",
      "Epoch 27, avg test_loss: 0.022861, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004928, train_acc: 0.86\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.002459, train_acc: 0.96\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.002073, train_acc: 0.95\n",
      "alexnet1d, trial.38:\n",
      "Epoch 28, avg test_loss: 0.028165, test_acc: 0.49\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.003232, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.001476, train_acc: 0.96\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.002326, train_acc: 0.95\n",
      "alexnet1d, trial.38:\n",
      "Epoch 29, avg test_loss: 0.029939, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.002376, train_acc: 0.95\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.002613, train_acc: 0.96\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.001033, train_acc: 0.98\n",
      "alexnet1d, trial.38:\n",
      "Epoch 30, avg test_loss: 0.030937, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号45个\n",
      "错误信号25个\n",
      "信号正确并预测正确的概率为0.443\n",
      "信号错误并预测正确的概率为0.129\n",
      "总正确率为0.57\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012137, train_acc: 0.68\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012268, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.011970, train_acc: 0.57\n",
      "alexnet1d, trial.39:\n",
      "Epoch 0, avg test_loss: 0.009798, test_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011666, train_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.014556, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012321, train_acc: 0.54\n",
      "alexnet1d, trial.39:\n",
      "Epoch 1, avg test_loss: 0.009814, test_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012214, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012404, train_acc: 0.45\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012336, train_acc: 0.55\n",
      "alexnet1d, trial.39:\n",
      "Epoch 2, avg test_loss: 0.009848, test_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012319, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012195, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012496, train_acc: 0.50\n",
      "alexnet1d, trial.39:\n",
      "Epoch 3, avg test_loss: 0.009743, test_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011959, train_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012261, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012476, train_acc: 0.59\n",
      "alexnet1d, trial.39:\n",
      "Epoch 4, avg test_loss: 0.009795, test_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012366, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012166, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012272, train_acc: 0.57\n",
      "alexnet1d, trial.39:\n",
      "Epoch 5, avg test_loss: 0.009716, test_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012196, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011758, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012108, train_acc: 0.55\n",
      "alexnet1d, trial.39:\n",
      "Epoch 6, avg test_loss: 0.009742, test_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011821, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011891, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011565, train_acc: 0.59\n",
      "alexnet1d, trial.39:\n",
      "Epoch 7, avg test_loss: 0.009934, test_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011418, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.010803, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012115, train_acc: 0.48\n",
      "alexnet1d, trial.39:\n",
      "Epoch 8, avg test_loss: 0.009645, test_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012482, train_acc: 0.55\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011291, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011986, train_acc: 0.64\n",
      "alexnet1d, trial.39:\n",
      "Epoch 9, avg test_loss: 0.011108, test_acc: 0.60\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011857, train_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011163, train_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011766, train_acc: 0.50\n",
      "alexnet1d, trial.39:\n",
      "Epoch 10, avg test_loss: 0.009658, test_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011546, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010598, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011429, train_acc: 0.66\n",
      "alexnet1d, trial.39:\n",
      "Epoch 11, avg test_loss: 0.011129, test_acc: 0.54\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010754, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010590, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011099, train_acc: 0.64\n",
      "alexnet1d, trial.39:\n",
      "Epoch 12, avg test_loss: 0.010051, test_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010826, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010440, train_acc: 0.77\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010088, train_acc: 0.75\n",
      "alexnet1d, trial.39:\n",
      "Epoch 13, avg test_loss: 0.011075, test_acc: 0.56\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010613, train_acc: 0.62\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009180, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.012678, train_acc: 0.62\n",
      "alexnet1d, trial.39:\n",
      "Epoch 14, avg test_loss: 0.013125, test_acc: 0.49\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.011543, train_acc: 0.59\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.011165, train_acc: 0.59\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011375, train_acc: 0.62\n",
      "alexnet1d, trial.39:\n",
      "Epoch 15, avg test_loss: 0.011548, test_acc: 0.57\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009106, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010107, train_acc: 0.64\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010351, train_acc: 0.64\n",
      "alexnet1d, trial.39:\n",
      "Epoch 16, avg test_loss: 0.012124, test_acc: 0.61\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009919, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009343, train_acc: 0.71\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008810, train_acc: 0.70\n",
      "alexnet1d, trial.39:\n",
      "Epoch 17, avg test_loss: 0.015118, test_acc: 0.54\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008403, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008468, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009011, train_acc: 0.70\n",
      "alexnet1d, trial.39:\n",
      "Epoch 18, avg test_loss: 0.016000, test_acc: 0.57\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008831, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008106, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007798, train_acc: 0.84\n",
      "alexnet1d, trial.39:\n",
      "Epoch 19, avg test_loss: 0.016653, test_acc: 0.57\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008044, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008856, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008049, train_acc: 0.77\n",
      "alexnet1d, trial.39:\n",
      "Epoch 20, avg test_loss: 0.019244, test_acc: 0.67\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008245, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.009674, train_acc: 0.61\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007434, train_acc: 0.82\n",
      "alexnet1d, trial.39:\n",
      "Epoch 21, avg test_loss: 0.013401, test_acc: 0.60\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007772, train_acc: 0.79\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.008399, train_acc: 0.73\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008005, train_acc: 0.79\n",
      "alexnet1d, trial.39:\n",
      "Epoch 22, avg test_loss: 0.014926, test_acc: 0.63\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007566, train_acc: 0.77\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006953, train_acc: 0.80\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006254, train_acc: 0.82\n",
      "alexnet1d, trial.39:\n",
      "Epoch 23, avg test_loss: 0.018478, test_acc: 0.63\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.007187, train_acc: 0.79\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006362, train_acc: 0.82\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006319, train_acc: 0.88\n",
      "alexnet1d, trial.39:\n",
      "Epoch 24, avg test_loss: 0.019989, test_acc: 0.60\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005085, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004693, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.008841, train_acc: 0.80\n",
      "alexnet1d, trial.39:\n",
      "Epoch 25, avg test_loss: 0.023539, test_acc: 0.66\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004653, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005737, train_acc: 0.84\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.005674, train_acc: 0.84\n",
      "alexnet1d, trial.39:\n",
      "Epoch 26, avg test_loss: 0.020870, test_acc: 0.66\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004393, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.006684, train_acc: 0.80\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.005317, train_acc: 0.84\n",
      "alexnet1d, trial.39:\n",
      "Epoch 27, avg test_loss: 0.025600, test_acc: 0.66\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.002859, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004227, train_acc: 0.89\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.003560, train_acc: 0.91\n",
      "alexnet1d, trial.39:\n",
      "Epoch 28, avg test_loss: 0.027411, test_acc: 0.66\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.002699, train_acc: 0.96\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.002662, train_acc: 0.96\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.003086, train_acc: 0.95\n",
      "alexnet1d, trial.39:\n",
      "Epoch 29, avg test_loss: 0.032379, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.002488, train_acc: 0.98\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.003111, train_acc: 0.91\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.002718, train_acc: 0.95\n",
      "alexnet1d, trial.39:\n",
      "Epoch 30, avg test_loss: 0.032976, test_acc: 0.69\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.003727, train_acc: 0.91\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.003212, train_acc: 0.95\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.003960, train_acc: 0.91\n",
      "alexnet1d, trial.39:\n",
      "Epoch 31, avg test_loss: 0.040359, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号41个\n",
      "错误信号29个\n",
      "信号正确并预测正确的概率为0.371\n",
      "信号错误并预测正确的概率为0.229\n",
      "总正确率为0.60\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012441, train_acc: 0.39\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012341, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012350, train_acc: 0.55\n",
      "alexnet1d, trial.40:\n",
      "Epoch 0, avg test_loss: 0.009882, test_acc: 0.53\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012498, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012373, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011864, train_acc: 0.70\n",
      "alexnet1d, trial.40:\n",
      "Epoch 1, avg test_loss: 0.009942, test_acc: 0.53\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011769, train_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.013428, train_acc: 0.48\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012374, train_acc: 0.57\n",
      "alexnet1d, trial.40:\n",
      "Epoch 2, avg test_loss: 0.010052, test_acc: 0.53\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011999, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012011, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012264, train_acc: 0.57\n",
      "alexnet1d, trial.40:\n",
      "Epoch 3, avg test_loss: 0.010030, test_acc: 0.53\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012255, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012261, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012620, train_acc: 0.52\n",
      "alexnet1d, trial.40:\n",
      "Epoch 4, avg test_loss: 0.010336, test_acc: 0.53\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012150, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012207, train_acc: 0.52\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011397, train_acc: 0.68\n",
      "alexnet1d, trial.40:\n",
      "Epoch 5, avg test_loss: 0.010039, test_acc: 0.53\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012265, train_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011835, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012631, train_acc: 0.46\n",
      "alexnet1d, trial.40:\n",
      "Epoch 6, avg test_loss: 0.010364, test_acc: 0.53\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011804, train_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011315, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.013773, train_acc: 0.48\n",
      "alexnet1d, trial.40:\n",
      "Epoch 7, avg test_loss: 0.010085, test_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011629, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011272, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011602, train_acc: 0.59\n",
      "alexnet1d, trial.40:\n",
      "Epoch 8, avg test_loss: 0.009932, test_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011487, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011409, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011485, train_acc: 0.57\n",
      "alexnet1d, trial.40:\n",
      "Epoch 9, avg test_loss: 0.010633, test_acc: 0.53\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011388, train_acc: 0.55\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010496, train_acc: 0.75\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010745, train_acc: 0.73\n",
      "alexnet1d, trial.40:\n",
      "Epoch 10, avg test_loss: 0.010216, test_acc: 0.54\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010175, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011358, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011147, train_acc: 0.68\n",
      "alexnet1d, trial.40:\n",
      "Epoch 11, avg test_loss: 0.011578, test_acc: 0.51\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009594, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010835, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010606, train_acc: 0.68\n",
      "alexnet1d, trial.40:\n",
      "Epoch 12, avg test_loss: 0.010545, test_acc: 0.51\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009782, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010196, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011866, train_acc: 0.75\n",
      "alexnet1d, trial.40:\n",
      "Epoch 13, avg test_loss: 0.011924, test_acc: 0.54\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.008510, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010320, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011696, train_acc: 0.62\n",
      "alexnet1d, trial.40:\n",
      "Epoch 14, avg test_loss: 0.012034, test_acc: 0.49\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010205, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009669, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010111, train_acc: 0.73\n",
      "alexnet1d, trial.40:\n",
      "Epoch 15, avg test_loss: 0.011474, test_acc: 0.59\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009723, train_acc: 0.68\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010240, train_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008236, train_acc: 0.79\n",
      "alexnet1d, trial.40:\n",
      "Epoch 16, avg test_loss: 0.012740, test_acc: 0.57\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007266, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007357, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007241, train_acc: 0.82\n",
      "alexnet1d, trial.40:\n",
      "Epoch 17, avg test_loss: 0.014191, test_acc: 0.60\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009180, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008776, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008738, train_acc: 0.77\n",
      "alexnet1d, trial.40:\n",
      "Epoch 18, avg test_loss: 0.014049, test_acc: 0.56\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.005769, train_acc: 0.88\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.006907, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009365, train_acc: 0.79\n",
      "alexnet1d, trial.40:\n",
      "Epoch 19, avg test_loss: 0.012496, test_acc: 0.60\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.005416, train_acc: 0.91\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.011085, train_acc: 0.71\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009075, train_acc: 0.70\n",
      "alexnet1d, trial.40:\n",
      "Epoch 20, avg test_loss: 0.013506, test_acc: 0.60\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006996, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006099, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.010098, train_acc: 0.75\n",
      "alexnet1d, trial.40:\n",
      "Epoch 21, avg test_loss: 0.015807, test_acc: 0.47\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007958, train_acc: 0.77\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.008323, train_acc: 0.77\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008271, train_acc: 0.82\n",
      "alexnet1d, trial.40:\n",
      "Epoch 22, avg test_loss: 0.012255, test_acc: 0.57\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006507, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007056, train_acc: 0.80\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.007597, train_acc: 0.82\n",
      "alexnet1d, trial.40:\n",
      "Epoch 23, avg test_loss: 0.014191, test_acc: 0.59\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.007377, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005311, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005640, train_acc: 0.84\n",
      "alexnet1d, trial.40:\n",
      "Epoch 24, avg test_loss: 0.016619, test_acc: 0.56\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004339, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006325, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.003745, train_acc: 0.89\n",
      "alexnet1d, trial.40:\n",
      "Epoch 25, avg test_loss: 0.019521, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004599, train_acc: 0.86\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005407, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004125, train_acc: 0.91\n",
      "alexnet1d, trial.40:\n",
      "Epoch 26, avg test_loss: 0.017725, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.002194, train_acc: 0.98\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.002195, train_acc: 1.00\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.005750, train_acc: 0.86\n",
      "alexnet1d, trial.40:\n",
      "Epoch 27, avg test_loss: 0.021161, test_acc: 0.54\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003033, train_acc: 0.96\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.001711, train_acc: 0.98\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.004691, train_acc: 0.88\n",
      "alexnet1d, trial.40:\n",
      "Epoch 28, avg test_loss: 0.024786, test_acc: 0.57\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.001097, train_acc: 0.98\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.003839, train_acc: 0.91\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.003153, train_acc: 0.91\n",
      "alexnet1d, trial.40:\n",
      "Epoch 29, avg test_loss: 0.025364, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.001862, train_acc: 0.95\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.005467, train_acc: 0.88\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.002638, train_acc: 0.93\n",
      "alexnet1d, trial.40:\n",
      "Epoch 30, avg test_loss: 0.029274, test_acc: 0.49\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号37个\n",
      "错误信号33个\n",
      "信号正确并预测正确的概率为0.343\n",
      "信号错误并预测正确的概率为0.143\n",
      "总正确率为0.49\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012314, train_acc: 0.61\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.016227, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012343, train_acc: 0.39\n",
      "alexnet1d, trial.41:\n",
      "Epoch 0, avg test_loss: 0.009904, test_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012339, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012320, train_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012138, train_acc: 0.62\n",
      "alexnet1d, trial.41:\n",
      "Epoch 1, avg test_loss: 0.010005, test_acc: 0.50\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012924, train_acc: 0.39\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012367, train_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011503, train_acc: 0.68\n",
      "alexnet1d, trial.41:\n",
      "Epoch 2, avg test_loss: 0.010345, test_acc: 0.50\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012182, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.013434, train_acc: 0.50\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012000, train_acc: 0.57\n",
      "alexnet1d, trial.41:\n",
      "Epoch 3, avg test_loss: 0.010189, test_acc: 0.50\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012337, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012809, train_acc: 0.48\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012343, train_acc: 0.55\n",
      "alexnet1d, trial.41:\n",
      "Epoch 4, avg test_loss: 0.010167, test_acc: 0.50\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011936, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011427, train_acc: 0.70\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012367, train_acc: 0.54\n",
      "alexnet1d, trial.41:\n",
      "Epoch 5, avg test_loss: 0.010235, test_acc: 0.50\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011892, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011403, train_acc: 0.68\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012314, train_acc: 0.54\n",
      "alexnet1d, trial.41:\n",
      "Epoch 6, avg test_loss: 0.010296, test_acc: 0.50\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011731, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011402, train_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011523, train_acc: 0.64\n",
      "alexnet1d, trial.41:\n",
      "Epoch 7, avg test_loss: 0.010299, test_acc: 0.50\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012419, train_acc: 0.52\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011576, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011234, train_acc: 0.61\n",
      "alexnet1d, trial.41:\n",
      "Epoch 8, avg test_loss: 0.010280, test_acc: 0.50\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011574, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011362, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011666, train_acc: 0.55\n",
      "alexnet1d, trial.41:\n",
      "Epoch 9, avg test_loss: 0.010093, test_acc: 0.50\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011814, train_acc: 0.55\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011452, train_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011533, train_acc: 0.59\n",
      "alexnet1d, trial.41:\n",
      "Epoch 10, avg test_loss: 0.010325, test_acc: 0.56\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011293, train_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010922, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011097, train_acc: 0.61\n",
      "alexnet1d, trial.41:\n",
      "Epoch 11, avg test_loss: 0.010382, test_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010506, train_acc: 0.75\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011582, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011540, train_acc: 0.75\n",
      "alexnet1d, trial.41:\n",
      "Epoch 12, avg test_loss: 0.010662, test_acc: 0.63\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009917, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010874, train_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011325, train_acc: 0.73\n",
      "alexnet1d, trial.41:\n",
      "Epoch 13, avg test_loss: 0.010609, test_acc: 0.57\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.011066, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011682, train_acc: 0.57\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009709, train_acc: 0.62\n",
      "alexnet1d, trial.41:\n",
      "Epoch 14, avg test_loss: 0.010086, test_acc: 0.59\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010474, train_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009865, train_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009144, train_acc: 0.80\n",
      "alexnet1d, trial.41:\n",
      "Epoch 15, avg test_loss: 0.010298, test_acc: 0.64\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009206, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009465, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.011105, train_acc: 0.62\n",
      "alexnet1d, trial.41:\n",
      "Epoch 16, avg test_loss: 0.011488, test_acc: 0.53\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007936, train_acc: 0.84\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007773, train_acc: 0.86\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009794, train_acc: 0.73\n",
      "alexnet1d, trial.41:\n",
      "Epoch 17, avg test_loss: 0.011994, test_acc: 0.57\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009283, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008040, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.011025, train_acc: 0.70\n",
      "alexnet1d, trial.41:\n",
      "Epoch 18, avg test_loss: 0.011604, test_acc: 0.53\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008433, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008461, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008214, train_acc: 0.88\n",
      "alexnet1d, trial.41:\n",
      "Epoch 19, avg test_loss: 0.011005, test_acc: 0.57\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008561, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.006136, train_acc: 0.88\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007510, train_acc: 0.86\n",
      "alexnet1d, trial.41:\n",
      "Epoch 20, avg test_loss: 0.012302, test_acc: 0.60\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006447, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006386, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007432, train_acc: 0.86\n",
      "alexnet1d, trial.41:\n",
      "Epoch 21, avg test_loss: 0.016641, test_acc: 0.59\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007265, train_acc: 0.79\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005767, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006917, train_acc: 0.84\n",
      "alexnet1d, trial.41:\n",
      "Epoch 22, avg test_loss: 0.015945, test_acc: 0.61\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005074, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.004610, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.004835, train_acc: 0.86\n",
      "alexnet1d, trial.41:\n",
      "Epoch 23, avg test_loss: 0.020629, test_acc: 0.56\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006165, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004578, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.003146, train_acc: 0.95\n",
      "alexnet1d, trial.41:\n",
      "Epoch 24, avg test_loss: 0.022674, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004253, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.002450, train_acc: 0.96\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.003558, train_acc: 0.91\n",
      "alexnet1d, trial.41:\n",
      "Epoch 25, avg test_loss: 0.025024, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005077, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004223, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.002924, train_acc: 0.93\n",
      "alexnet1d, trial.41:\n",
      "Epoch 26, avg test_loss: 0.021932, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004191, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.005442, train_acc: 0.88\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.001180, train_acc: 1.00\n",
      "alexnet1d, trial.41:\n",
      "Epoch 27, avg test_loss: 0.028604, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号35个\n",
      "错误信号35个\n",
      "信号正确并预测正确的概率为0.386\n",
      "信号错误并预测正确的概率为0.129\n",
      "总正确率为0.51\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012423, train_acc: 0.48\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012977, train_acc: 0.50\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.011986, train_acc: 0.62\n",
      "alexnet1d, trial.42:\n",
      "Epoch 0, avg test_loss: 0.009851, test_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012468, train_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.011840, train_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012104, train_acc: 0.59\n",
      "alexnet1d, trial.42:\n",
      "Epoch 1, avg test_loss: 0.009808, test_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011891, train_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011980, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012100, train_acc: 0.54\n",
      "alexnet1d, trial.42:\n",
      "Epoch 2, avg test_loss: 0.009825, test_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012449, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012302, train_acc: 0.55\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011344, train_acc: 0.68\n",
      "alexnet1d, trial.42:\n",
      "Epoch 3, avg test_loss: 0.009982, test_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012222, train_acc: 0.48\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011822, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.010867, train_acc: 0.68\n",
      "alexnet1d, trial.42:\n",
      "Epoch 4, avg test_loss: 0.010549, test_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012067, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011732, train_acc: 0.52\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012519, train_acc: 0.52\n",
      "alexnet1d, trial.42:\n",
      "Epoch 5, avg test_loss: 0.010398, test_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011036, train_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011772, train_acc: 0.52\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011337, train_acc: 0.59\n",
      "alexnet1d, trial.42:\n",
      "Epoch 6, avg test_loss: 0.010655, test_acc: 0.56\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.010747, train_acc: 0.68\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.010292, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012009, train_acc: 0.59\n",
      "alexnet1d, trial.42:\n",
      "Epoch 7, avg test_loss: 0.010981, test_acc: 0.53\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010657, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011690, train_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012196, train_acc: 0.54\n",
      "alexnet1d, trial.42:\n",
      "Epoch 8, avg test_loss: 0.010393, test_acc: 0.50\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011488, train_acc: 0.55\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011555, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010859, train_acc: 0.71\n",
      "alexnet1d, trial.42:\n",
      "Epoch 9, avg test_loss: 0.011588, test_acc: 0.60\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010687, train_acc: 0.79\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011768, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010628, train_acc: 0.71\n",
      "alexnet1d, trial.42:\n",
      "Epoch 10, avg test_loss: 0.014764, test_acc: 0.60\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.008587, train_acc: 0.77\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.009767, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010191, train_acc: 0.70\n",
      "alexnet1d, trial.42:\n",
      "Epoch 11, avg test_loss: 0.012302, test_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009481, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010673, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010863, train_acc: 0.70\n",
      "alexnet1d, trial.42:\n",
      "Epoch 12, avg test_loss: 0.010771, test_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010854, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009731, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011628, train_acc: 0.70\n",
      "alexnet1d, trial.42:\n",
      "Epoch 13, avg test_loss: 0.013375, test_acc: 0.63\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.008929, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009897, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010125, train_acc: 0.70\n",
      "alexnet1d, trial.42:\n",
      "Epoch 14, avg test_loss: 0.013088, test_acc: 0.56\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009313, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008803, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008645, train_acc: 0.77\n",
      "alexnet1d, trial.42:\n",
      "Epoch 15, avg test_loss: 0.013700, test_acc: 0.60\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008309, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.007894, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009735, train_acc: 0.75\n",
      "alexnet1d, trial.42:\n",
      "Epoch 16, avg test_loss: 0.014057, test_acc: 0.67\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007765, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010176, train_acc: 0.68\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009651, train_acc: 0.77\n",
      "alexnet1d, trial.42:\n",
      "Epoch 17, avg test_loss: 0.020554, test_acc: 0.61\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007228, train_acc: 0.88\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.010744, train_acc: 0.70\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008408, train_acc: 0.79\n",
      "alexnet1d, trial.42:\n",
      "Epoch 18, avg test_loss: 0.011999, test_acc: 0.64\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009687, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009661, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007720, train_acc: 0.80\n",
      "alexnet1d, trial.42:\n",
      "Epoch 19, avg test_loss: 0.011931, test_acc: 0.59\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008780, train_acc: 0.73\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008128, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009682, train_acc: 0.70\n",
      "alexnet1d, trial.42:\n",
      "Epoch 20, avg test_loss: 0.013528, test_acc: 0.64\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007540, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006073, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008661, train_acc: 0.75\n",
      "alexnet1d, trial.42:\n",
      "Epoch 21, avg test_loss: 0.016221, test_acc: 0.60\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.003895, train_acc: 0.95\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005881, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.005602, train_acc: 0.91\n",
      "alexnet1d, trial.42:\n",
      "Epoch 22, avg test_loss: 0.014457, test_acc: 0.66\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005859, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006307, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.005900, train_acc: 0.86\n",
      "alexnet1d, trial.42:\n",
      "Epoch 23, avg test_loss: 0.014593, test_acc: 0.69\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004392, train_acc: 0.98\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.007605, train_acc: 0.79\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.004057, train_acc: 0.91\n",
      "alexnet1d, trial.42:\n",
      "Epoch 24, avg test_loss: 0.018374, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005953, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004256, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004500, train_acc: 0.93\n",
      "alexnet1d, trial.42:\n",
      "Epoch 25, avg test_loss: 0.022012, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004136, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004816, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004066, train_acc: 0.91\n",
      "alexnet1d, trial.42:\n",
      "Epoch 26, avg test_loss: 0.024450, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号43个\n",
      "错误信号27个\n",
      "信号正确并预测正确的概率为0.429\n",
      "信号错误并预测正确的概率为0.171\n",
      "总正确率为0.60\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012344, train_acc: 0.62\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.011600, train_acc: 0.62\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012341, train_acc: 0.55\n",
      "alexnet1d, trial.43:\n",
      "Epoch 0, avg test_loss: 0.009984, test_acc: 0.41\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012310, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012460, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012280, train_acc: 0.57\n",
      "alexnet1d, trial.43:\n",
      "Epoch 1, avg test_loss: 0.009841, test_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012308, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012100, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012036, train_acc: 0.61\n",
      "alexnet1d, trial.43:\n",
      "Epoch 2, avg test_loss: 0.009627, test_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011921, train_acc: 0.62\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012346, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012780, train_acc: 0.48\n",
      "alexnet1d, trial.43:\n",
      "Epoch 3, avg test_loss: 0.009727, test_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011466, train_acc: 0.68\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012514, train_acc: 0.52\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012779, train_acc: 0.52\n",
      "alexnet1d, trial.43:\n",
      "Epoch 4, avg test_loss: 0.009766, test_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012098, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012260, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012085, train_acc: 0.57\n",
      "alexnet1d, trial.43:\n",
      "Epoch 5, avg test_loss: 0.009746, test_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012401, train_acc: 0.48\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011666, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011691, train_acc: 0.62\n",
      "alexnet1d, trial.43:\n",
      "Epoch 6, avg test_loss: 0.009856, test_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011700, train_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012082, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011858, train_acc: 0.61\n",
      "alexnet1d, trial.43:\n",
      "Epoch 7, avg test_loss: 0.009856, test_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.013243, train_acc: 0.46\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011001, train_acc: 0.70\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011535, train_acc: 0.62\n",
      "alexnet1d, trial.43:\n",
      "Epoch 8, avg test_loss: 0.010008, test_acc: 0.60\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010731, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010111, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010462, train_acc: 0.71\n",
      "alexnet1d, trial.43:\n",
      "Epoch 9, avg test_loss: 0.010098, test_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010017, train_acc: 0.70\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011583, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010044, train_acc: 0.70\n",
      "alexnet1d, trial.43:\n",
      "Epoch 10, avg test_loss: 0.011816, test_acc: 0.49\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011311, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010598, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.009485, train_acc: 0.77\n",
      "alexnet1d, trial.43:\n",
      "Epoch 11, avg test_loss: 0.010910, test_acc: 0.54\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010697, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.009306, train_acc: 0.77\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.012666, train_acc: 0.61\n",
      "alexnet1d, trial.43:\n",
      "Epoch 12, avg test_loss: 0.010415, test_acc: 0.56\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009669, train_acc: 0.79\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010678, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009569, train_acc: 0.70\n",
      "alexnet1d, trial.43:\n",
      "Epoch 13, avg test_loss: 0.010404, test_acc: 0.51\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.008782, train_acc: 0.82\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010440, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010795, train_acc: 0.62\n",
      "alexnet1d, trial.43:\n",
      "Epoch 14, avg test_loss: 0.010546, test_acc: 0.57\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009927, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008666, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008142, train_acc: 0.84\n",
      "alexnet1d, trial.43:\n",
      "Epoch 15, avg test_loss: 0.012211, test_acc: 0.51\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008001, train_acc: 0.82\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008621, train_acc: 0.82\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009235, train_acc: 0.73\n",
      "alexnet1d, trial.43:\n",
      "Epoch 16, avg test_loss: 0.011406, test_acc: 0.57\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.006823, train_acc: 0.86\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007785, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009882, train_acc: 0.70\n",
      "alexnet1d, trial.43:\n",
      "Epoch 17, avg test_loss: 0.012181, test_acc: 0.53\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.006892, train_acc: 0.88\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009682, train_acc: 0.68\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007190, train_acc: 0.80\n",
      "alexnet1d, trial.43:\n",
      "Epoch 18, avg test_loss: 0.014473, test_acc: 0.57\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006589, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009292, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007692, train_acc: 0.82\n",
      "alexnet1d, trial.43:\n",
      "Epoch 19, avg test_loss: 0.012326, test_acc: 0.59\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007476, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.006372, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008881, train_acc: 0.79\n",
      "alexnet1d, trial.43:\n",
      "Epoch 20, avg test_loss: 0.014994, test_acc: 0.53\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.004438, train_acc: 0.93\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006171, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008797, train_acc: 0.77\n",
      "alexnet1d, trial.43:\n",
      "Epoch 21, avg test_loss: 0.013816, test_acc: 0.59\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007005, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006285, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007895, train_acc: 0.80\n",
      "alexnet1d, trial.43:\n",
      "Epoch 22, avg test_loss: 0.013717, test_acc: 0.54\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007283, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.004986, train_acc: 0.95\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.004435, train_acc: 0.89\n",
      "alexnet1d, trial.43:\n",
      "Epoch 23, avg test_loss: 0.013446, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.003160, train_acc: 0.98\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005151, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006127, train_acc: 0.88\n",
      "alexnet1d, trial.43:\n",
      "Epoch 24, avg test_loss: 0.015416, test_acc: 0.63\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.006169, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005691, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004454, train_acc: 0.91\n",
      "alexnet1d, trial.43:\n",
      "Epoch 25, avg test_loss: 0.016262, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.003163, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.009391, train_acc: 0.77\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.006655, train_acc: 0.86\n",
      "alexnet1d, trial.43:\n",
      "Epoch 26, avg test_loss: 0.015506, test_acc: 0.57\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004819, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.006413, train_acc: 0.86\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004939, train_acc: 0.91\n",
      "alexnet1d, trial.43:\n",
      "Epoch 27, avg test_loss: 0.013264, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003635, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004129, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.006350, train_acc: 0.80\n",
      "alexnet1d, trial.43:\n",
      "Epoch 28, avg test_loss: 0.018830, test_acc: 0.54\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.006013, train_acc: 0.88\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.004630, train_acc: 0.91\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.006264, train_acc: 0.91\n",
      "alexnet1d, trial.43:\n",
      "Epoch 29, avg test_loss: 0.015133, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号41个\n",
      "错误信号29个\n",
      "信号正确并预测正确的概率为0.486\n",
      "信号错误并预测正确的概率为0.157\n",
      "总正确率为0.64\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012395, train_acc: 0.45\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012195, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012109, train_acc: 0.64\n",
      "alexnet1d, trial.44:\n",
      "Epoch 0, avg test_loss: 0.009707, test_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011983, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.011920, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012988, train_acc: 0.48\n",
      "alexnet1d, trial.44:\n",
      "Epoch 1, avg test_loss: 0.009583, test_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012454, train_acc: 0.48\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012250, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012322, train_acc: 0.52\n",
      "alexnet1d, trial.44:\n",
      "Epoch 2, avg test_loss: 0.009648, test_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012197, train_acc: 0.48\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011950, train_acc: 0.62\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.013198, train_acc: 0.59\n",
      "alexnet1d, trial.44:\n",
      "Epoch 3, avg test_loss: 0.009475, test_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012305, train_acc: 0.52\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011768, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012125, train_acc: 0.54\n",
      "alexnet1d, trial.44:\n",
      "Epoch 4, avg test_loss: 0.009428, test_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011441, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011728, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012094, train_acc: 0.52\n",
      "alexnet1d, trial.44:\n",
      "Epoch 5, avg test_loss: 0.009430, test_acc: 0.69\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011536, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012350, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012112, train_acc: 0.48\n",
      "alexnet1d, trial.44:\n",
      "Epoch 6, avg test_loss: 0.009530, test_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012009, train_acc: 0.55\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012022, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.010501, train_acc: 0.79\n",
      "alexnet1d, trial.44:\n",
      "Epoch 7, avg test_loss: 0.009854, test_acc: 0.63\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010741, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012780, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.013114, train_acc: 0.57\n",
      "alexnet1d, trial.44:\n",
      "Epoch 8, avg test_loss: 0.010163, test_acc: 0.46\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011448, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011632, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012132, train_acc: 0.52\n",
      "alexnet1d, trial.44:\n",
      "Epoch 9, avg test_loss: 0.009542, test_acc: 0.69\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010609, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010157, train_acc: 0.71\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011315, train_acc: 0.68\n",
      "alexnet1d, trial.44:\n",
      "Epoch 10, avg test_loss: 0.009568, test_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010988, train_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010758, train_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011301, train_acc: 0.57\n",
      "alexnet1d, trial.44:\n",
      "Epoch 11, avg test_loss: 0.010319, test_acc: 0.63\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009421, train_acc: 0.75\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010747, train_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011113, train_acc: 0.62\n",
      "alexnet1d, trial.44:\n",
      "Epoch 12, avg test_loss: 0.010198, test_acc: 0.63\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009937, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009596, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011231, train_acc: 0.62\n",
      "alexnet1d, trial.44:\n",
      "Epoch 13, avg test_loss: 0.010065, test_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009769, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009592, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.007666, train_acc: 0.86\n",
      "alexnet1d, trial.44:\n",
      "Epoch 14, avg test_loss: 0.012330, test_acc: 0.51\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008950, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010315, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011327, train_acc: 0.62\n",
      "alexnet1d, trial.44:\n",
      "Epoch 15, avg test_loss: 0.014769, test_acc: 0.54\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.007553, train_acc: 0.82\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008875, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008068, train_acc: 0.84\n",
      "alexnet1d, trial.44:\n",
      "Epoch 16, avg test_loss: 0.012772, test_acc: 0.54\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008688, train_acc: 0.86\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.006774, train_acc: 0.86\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009214, train_acc: 0.77\n",
      "alexnet1d, trial.44:\n",
      "Epoch 17, avg test_loss: 0.016001, test_acc: 0.51\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.005149, train_acc: 0.93\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.005981, train_acc: 0.88\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009960, train_acc: 0.71\n",
      "alexnet1d, trial.44:\n",
      "Epoch 18, avg test_loss: 0.018466, test_acc: 0.60\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007611, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009934, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007795, train_acc: 0.80\n",
      "alexnet1d, trial.44:\n",
      "Epoch 19, avg test_loss: 0.017800, test_acc: 0.53\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007424, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.009052, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006611, train_acc: 0.88\n",
      "alexnet1d, trial.44:\n",
      "Epoch 20, avg test_loss: 0.016242, test_acc: 0.56\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007299, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006611, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.005693, train_acc: 0.93\n",
      "alexnet1d, trial.44:\n",
      "Epoch 21, avg test_loss: 0.015503, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005991, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005340, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.005711, train_acc: 0.86\n",
      "alexnet1d, trial.44:\n",
      "Epoch 22, avg test_loss: 0.020755, test_acc: 0.54\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.004170, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005363, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.004120, train_acc: 0.93\n",
      "alexnet1d, trial.44:\n",
      "Epoch 23, avg test_loss: 0.025428, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.003576, train_acc: 0.95\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.003382, train_acc: 0.96\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.003159, train_acc: 0.93\n",
      "alexnet1d, trial.44:\n",
      "Epoch 24, avg test_loss: 0.029325, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.003343, train_acc: 0.96\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.003213, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005992, train_acc: 0.82\n",
      "alexnet1d, trial.44:\n",
      "Epoch 25, avg test_loss: 0.035985, test_acc: 0.57\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004330, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005450, train_acc: 0.86\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.005703, train_acc: 0.86\n",
      "alexnet1d, trial.44:\n",
      "Epoch 26, avg test_loss: 0.029625, test_acc: 0.53\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.002471, train_acc: 0.96\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.002413, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.002406, train_acc: 0.98\n",
      "alexnet1d, trial.44:\n",
      "Epoch 27, avg test_loss: 0.031747, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号45个\n",
      "错误信号25个\n",
      "信号正确并预测正确的概率为0.4\n",
      "信号错误并预测正确的概率为0.157\n",
      "总正确率为0.56\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012522, train_acc: 0.38\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012116, train_acc: 0.61\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.011856, train_acc: 0.61\n",
      "alexnet1d, trial.45:\n",
      "Epoch 0, avg test_loss: 0.009900, test_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012113, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012842, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.013315, train_acc: 0.46\n",
      "alexnet1d, trial.45:\n",
      "Epoch 1, avg test_loss: 0.009843, test_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012165, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012279, train_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012121, train_acc: 0.64\n",
      "alexnet1d, trial.45:\n",
      "Epoch 2, avg test_loss: 0.009849, test_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012548, train_acc: 0.46\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012221, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011931, train_acc: 0.61\n",
      "alexnet1d, trial.45:\n",
      "Epoch 3, avg test_loss: 0.009977, test_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011736, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.013201, train_acc: 0.50\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012041, train_acc: 0.59\n",
      "alexnet1d, trial.45:\n",
      "Epoch 4, avg test_loss: 0.009971, test_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011557, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012440, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012523, train_acc: 0.52\n",
      "alexnet1d, trial.45:\n",
      "Epoch 5, avg test_loss: 0.009924, test_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011369, train_acc: 0.70\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012294, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012554, train_acc: 0.46\n",
      "alexnet1d, trial.45:\n",
      "Epoch 6, avg test_loss: 0.009887, test_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012137, train_acc: 0.52\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012068, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011670, train_acc: 0.66\n",
      "alexnet1d, trial.45:\n",
      "Epoch 7, avg test_loss: 0.009849, test_acc: 0.56\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011455, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012101, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011310, train_acc: 0.68\n",
      "alexnet1d, trial.45:\n",
      "Epoch 8, avg test_loss: 0.010060, test_acc: 0.51\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011036, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012562, train_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010395, train_acc: 0.73\n",
      "alexnet1d, trial.45:\n",
      "Epoch 9, avg test_loss: 0.010156, test_acc: 0.47\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.012299, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011264, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011158, train_acc: 0.64\n",
      "alexnet1d, trial.45:\n",
      "Epoch 10, avg test_loss: 0.009610, test_acc: 0.56\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010783, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010268, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010717, train_acc: 0.71\n",
      "alexnet1d, trial.45:\n",
      "Epoch 11, avg test_loss: 0.010283, test_acc: 0.53\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.012367, train_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010197, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010846, train_acc: 0.66\n",
      "alexnet1d, trial.45:\n",
      "Epoch 12, avg test_loss: 0.009999, test_acc: 0.60\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009802, train_acc: 0.75\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009972, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010554, train_acc: 0.77\n",
      "alexnet1d, trial.45:\n",
      "Epoch 13, avg test_loss: 0.011103, test_acc: 0.51\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.008804, train_acc: 0.80\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011230, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011555, train_acc: 0.68\n",
      "alexnet1d, trial.45:\n",
      "Epoch 14, avg test_loss: 0.010882, test_acc: 0.53\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009847, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010022, train_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009050, train_acc: 0.77\n",
      "alexnet1d, trial.45:\n",
      "Epoch 15, avg test_loss: 0.010337, test_acc: 0.57\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008092, train_acc: 0.80\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010712, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009795, train_acc: 0.70\n",
      "alexnet1d, trial.45:\n",
      "Epoch 16, avg test_loss: 0.012645, test_acc: 0.56\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008159, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008761, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009282, train_acc: 0.77\n",
      "alexnet1d, trial.45:\n",
      "Epoch 17, avg test_loss: 0.011162, test_acc: 0.60\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007762, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009490, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009008, train_acc: 0.73\n",
      "alexnet1d, trial.45:\n",
      "Epoch 18, avg test_loss: 0.011863, test_acc: 0.57\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008703, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007956, train_acc: 0.75\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.006963, train_acc: 0.80\n",
      "alexnet1d, trial.45:\n",
      "Epoch 19, avg test_loss: 0.013569, test_acc: 0.61\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.005236, train_acc: 0.93\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007935, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006065, train_acc: 0.86\n",
      "alexnet1d, trial.45:\n",
      "Epoch 20, avg test_loss: 0.014619, test_acc: 0.60\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007537, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.005284, train_acc: 0.89\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.004030, train_acc: 0.93\n",
      "alexnet1d, trial.45:\n",
      "Epoch 21, avg test_loss: 0.016909, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.003740, train_acc: 0.93\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.008216, train_acc: 0.79\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006547, train_acc: 0.86\n",
      "alexnet1d, trial.45:\n",
      "Epoch 22, avg test_loss: 0.016675, test_acc: 0.57\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.004753, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.004311, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.008099, train_acc: 0.86\n",
      "alexnet1d, trial.45:\n",
      "Epoch 23, avg test_loss: 0.017833, test_acc: 0.50\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006277, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005146, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.004931, train_acc: 0.86\n",
      "alexnet1d, trial.45:\n",
      "Epoch 24, avg test_loss: 0.023781, test_acc: 0.56\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.003525, train_acc: 0.95\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005259, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004711, train_acc: 0.89\n",
      "alexnet1d, trial.45:\n",
      "Epoch 25, avg test_loss: 0.019394, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.003468, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004100, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.002014, train_acc: 0.98\n",
      "alexnet1d, trial.45:\n",
      "Epoch 26, avg test_loss: 0.022750, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.002534, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004540, train_acc: 0.86\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.002418, train_acc: 0.93\n",
      "alexnet1d, trial.45:\n",
      "Epoch 27, avg test_loss: 0.028121, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号38个\n",
      "错误信号32个\n",
      "信号正确并预测正确的概率为0.343\n",
      "信号错误并预测正确的概率为0.229\n",
      "总正确率为0.57\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012389, train_acc: 0.43\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.013153, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012070, train_acc: 0.66\n",
      "alexnet1d, trial.46:\n",
      "Epoch 0, avg test_loss: 0.009889, test_acc: 0.53\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011808, train_acc: 0.70\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.013143, train_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012621, train_acc: 0.52\n",
      "alexnet1d, trial.46:\n",
      "Epoch 1, avg test_loss: 0.009908, test_acc: 0.53\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012119, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012359, train_acc: 0.52\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012205, train_acc: 0.57\n",
      "alexnet1d, trial.46:\n",
      "Epoch 2, avg test_loss: 0.009908, test_acc: 0.53\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011773, train_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012487, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011374, train_acc: 0.71\n",
      "alexnet1d, trial.46:\n",
      "Epoch 3, avg test_loss: 0.009919, test_acc: 0.53\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012234, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012327, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011935, train_acc: 0.61\n",
      "alexnet1d, trial.46:\n",
      "Epoch 4, avg test_loss: 0.009995, test_acc: 0.53\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.010727, train_acc: 0.73\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012584, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012751, train_acc: 0.52\n",
      "alexnet1d, trial.46:\n",
      "Epoch 5, avg test_loss: 0.009952, test_acc: 0.53\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011600, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012141, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011956, train_acc: 0.55\n",
      "alexnet1d, trial.46:\n",
      "Epoch 6, avg test_loss: 0.009841, test_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011471, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011925, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011166, train_acc: 0.66\n",
      "alexnet1d, trial.46:\n",
      "Epoch 7, avg test_loss: 0.009783, test_acc: 0.51\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011342, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012066, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011091, train_acc: 0.68\n",
      "alexnet1d, trial.46:\n",
      "Epoch 8, avg test_loss: 0.009741, test_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011685, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010126, train_acc: 0.71\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012144, train_acc: 0.62\n",
      "alexnet1d, trial.46:\n",
      "Epoch 9, avg test_loss: 0.010781, test_acc: 0.53\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011499, train_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010303, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010505, train_acc: 0.59\n",
      "alexnet1d, trial.46:\n",
      "Epoch 10, avg test_loss: 0.011075, test_acc: 0.63\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010854, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.009976, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010403, train_acc: 0.62\n",
      "alexnet1d, trial.46:\n",
      "Epoch 11, avg test_loss: 0.011514, test_acc: 0.53\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009683, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010162, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010742, train_acc: 0.62\n",
      "alexnet1d, trial.46:\n",
      "Epoch 12, avg test_loss: 0.014306, test_acc: 0.57\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009903, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009584, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010411, train_acc: 0.68\n",
      "alexnet1d, trial.46:\n",
      "Epoch 13, avg test_loss: 0.014953, test_acc: 0.57\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.008081, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010044, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009054, train_acc: 0.75\n",
      "alexnet1d, trial.46:\n",
      "Epoch 14, avg test_loss: 0.012148, test_acc: 0.57\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008405, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009614, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009481, train_acc: 0.64\n",
      "alexnet1d, trial.46:\n",
      "Epoch 15, avg test_loss: 0.013052, test_acc: 0.57\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008926, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.007135, train_acc: 0.88\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.007234, train_acc: 0.79\n",
      "alexnet1d, trial.46:\n",
      "Epoch 16, avg test_loss: 0.016282, test_acc: 0.54\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007573, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.006071, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.006292, train_acc: 0.82\n",
      "alexnet1d, trial.46:\n",
      "Epoch 17, avg test_loss: 0.017614, test_acc: 0.54\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.004526, train_acc: 0.89\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.006415, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.004138, train_acc: 0.88\n",
      "alexnet1d, trial.46:\n",
      "Epoch 18, avg test_loss: 0.024476, test_acc: 0.57\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.004284, train_acc: 0.93\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.006504, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.005367, train_acc: 0.86\n",
      "alexnet1d, trial.46:\n",
      "Epoch 19, avg test_loss: 0.022789, test_acc: 0.57\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.004321, train_acc: 0.91\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.004446, train_acc: 0.89\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.005877, train_acc: 0.82\n",
      "alexnet1d, trial.46:\n",
      "Epoch 20, avg test_loss: 0.019730, test_acc: 0.51\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.003975, train_acc: 0.93\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007447, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.004380, train_acc: 0.91\n",
      "alexnet1d, trial.46:\n",
      "Epoch 21, avg test_loss: 0.023297, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.004263, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.004002, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.005991, train_acc: 0.86\n",
      "alexnet1d, trial.46:\n",
      "Epoch 22, avg test_loss: 0.027049, test_acc: 0.53\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.002492, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006723, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.002639, train_acc: 0.93\n",
      "alexnet1d, trial.46:\n",
      "Epoch 23, avg test_loss: 0.032491, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.002639, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004331, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.001345, train_acc: 1.00\n",
      "alexnet1d, trial.46:\n",
      "Epoch 24, avg test_loss: 0.031467, test_acc: 0.47\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.003274, train_acc: 0.95\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004036, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.002022, train_acc: 0.96\n",
      "alexnet1d, trial.46:\n",
      "Epoch 25, avg test_loss: 0.026454, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号37个\n",
      "错误信号33个\n",
      "信号正确并预测正确的概率为0.314\n",
      "信号错误并预测正确的概率为0.214\n",
      "总正确率为0.53\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012380, train_acc: 0.46\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.011857, train_acc: 0.66\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012946, train_acc: 0.52\n",
      "alexnet1d, trial.47:\n",
      "Epoch 0, avg test_loss: 0.009881, test_acc: 0.60\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012350, train_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012326, train_acc: 0.48\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012302, train_acc: 0.52\n",
      "alexnet1d, trial.47:\n",
      "Epoch 1, avg test_loss: 0.009805, test_acc: 0.60\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012209, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012087, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012915, train_acc: 0.54\n",
      "alexnet1d, trial.47:\n",
      "Epoch 2, avg test_loss: 0.009754, test_acc: 0.60\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011978, train_acc: 0.62\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012184, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012147, train_acc: 0.55\n",
      "alexnet1d, trial.47:\n",
      "Epoch 3, avg test_loss: 0.009733, test_acc: 0.60\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012629, train_acc: 0.48\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011885, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012174, train_acc: 0.55\n",
      "alexnet1d, trial.47:\n",
      "Epoch 4, avg test_loss: 0.009700, test_acc: 0.60\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012439, train_acc: 0.50\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011777, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011457, train_acc: 0.66\n",
      "alexnet1d, trial.47:\n",
      "Epoch 5, avg test_loss: 0.009771, test_acc: 0.60\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012136, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012171, train_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012598, train_acc: 0.57\n",
      "alexnet1d, trial.47:\n",
      "Epoch 6, avg test_loss: 0.009763, test_acc: 0.60\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012272, train_acc: 0.48\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012127, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011872, train_acc: 0.57\n",
      "alexnet1d, trial.47:\n",
      "Epoch 7, avg test_loss: 0.009808, test_acc: 0.60\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012243, train_acc: 0.52\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011274, train_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012911, train_acc: 0.66\n",
      "alexnet1d, trial.47:\n",
      "Epoch 8, avg test_loss: 0.009926, test_acc: 0.56\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011275, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011014, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.013210, train_acc: 0.57\n",
      "alexnet1d, trial.47:\n",
      "Epoch 9, avg test_loss: 0.009954, test_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011224, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011795, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011118, train_acc: 0.62\n",
      "alexnet1d, trial.47:\n",
      "Epoch 10, avg test_loss: 0.010081, test_acc: 0.47\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011123, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011806, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011263, train_acc: 0.64\n",
      "alexnet1d, trial.47:\n",
      "Epoch 11, avg test_loss: 0.010408, test_acc: 0.54\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011416, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010378, train_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010823, train_acc: 0.71\n",
      "alexnet1d, trial.47:\n",
      "Epoch 12, avg test_loss: 0.010652, test_acc: 0.57\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010246, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011879, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009902, train_acc: 0.71\n",
      "alexnet1d, trial.47:\n",
      "Epoch 13, avg test_loss: 0.010475, test_acc: 0.57\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.008409, train_acc: 0.86\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011291, train_acc: 0.64\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.013636, train_acc: 0.59\n",
      "alexnet1d, trial.47:\n",
      "Epoch 14, avg test_loss: 0.011205, test_acc: 0.51\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010601, train_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009637, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011378, train_acc: 0.62\n",
      "alexnet1d, trial.47:\n",
      "Epoch 15, avg test_loss: 0.010596, test_acc: 0.54\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010568, train_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009496, train_acc: 0.82\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008874, train_acc: 0.82\n",
      "alexnet1d, trial.47:\n",
      "Epoch 16, avg test_loss: 0.011030, test_acc: 0.54\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009823, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009798, train_acc: 0.68\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008609, train_acc: 0.75\n",
      "alexnet1d, trial.47:\n",
      "Epoch 17, avg test_loss: 0.012192, test_acc: 0.49\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009079, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009317, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008146, train_acc: 0.88\n",
      "alexnet1d, trial.47:\n",
      "Epoch 18, avg test_loss: 0.013602, test_acc: 0.50\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009995, train_acc: 0.71\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008170, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007699, train_acc: 0.73\n",
      "alexnet1d, trial.47:\n",
      "Epoch 19, avg test_loss: 0.014407, test_acc: 0.41\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008962, train_acc: 0.73\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.009020, train_acc: 0.73\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007465, train_acc: 0.84\n",
      "alexnet1d, trial.47:\n",
      "Epoch 20, avg test_loss: 0.013319, test_acc: 0.56\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008726, train_acc: 0.75\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007060, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008640, train_acc: 0.79\n",
      "alexnet1d, trial.47:\n",
      "Epoch 21, avg test_loss: 0.014292, test_acc: 0.57\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006879, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006957, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006852, train_acc: 0.79\n",
      "alexnet1d, trial.47:\n",
      "Epoch 22, avg test_loss: 0.016899, test_acc: 0.56\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006834, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007628, train_acc: 0.80\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.004075, train_acc: 0.91\n",
      "alexnet1d, trial.47:\n",
      "Epoch 23, avg test_loss: 0.019302, test_acc: 0.40\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005961, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.007133, train_acc: 0.82\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006883, train_acc: 0.89\n",
      "alexnet1d, trial.47:\n",
      "Epoch 24, avg test_loss: 0.019587, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.007377, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.010150, train_acc: 0.75\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.007769, train_acc: 0.77\n",
      "alexnet1d, trial.47:\n",
      "Epoch 25, avg test_loss: 0.017529, test_acc: 0.46\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004283, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.007560, train_acc: 0.82\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.007133, train_acc: 0.80\n",
      "alexnet1d, trial.47:\n",
      "Epoch 26, avg test_loss: 0.017228, test_acc: 0.46\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004744, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.005135, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.005584, train_acc: 0.91\n",
      "alexnet1d, trial.47:\n",
      "Epoch 27, avg test_loss: 0.020264, test_acc: 0.43\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003773, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.007671, train_acc: 0.84\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.005325, train_acc: 0.88\n",
      "alexnet1d, trial.47:\n",
      "Epoch 28, avg test_loss: 0.020403, test_acc: 0.44\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.004513, train_acc: 0.89\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.007344, train_acc: 0.86\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.002969, train_acc: 0.96\n",
      "alexnet1d, trial.47:\n",
      "Epoch 29, avg test_loss: 0.023401, test_acc: 0.41\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号42个\n",
      "错误信号28个\n",
      "信号正确并预测正确的概率为0.286\n",
      "信号错误并预测正确的概率为0.129\n",
      "总正确率为0.41\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012367, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.013009, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012165, train_acc: 0.59\n",
      "alexnet1d, trial.48:\n",
      "Epoch 0, avg test_loss: 0.009927, test_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011777, train_acc: 0.70\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012494, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011776, train_acc: 0.61\n",
      "alexnet1d, trial.48:\n",
      "Epoch 1, avg test_loss: 0.009864, test_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012057, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012167, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012406, train_acc: 0.54\n",
      "alexnet1d, trial.48:\n",
      "Epoch 2, avg test_loss: 0.009854, test_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012344, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012120, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012140, train_acc: 0.55\n",
      "alexnet1d, trial.48:\n",
      "Epoch 3, avg test_loss: 0.009891, test_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011990, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012640, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011621, train_acc: 0.64\n",
      "alexnet1d, trial.48:\n",
      "Epoch 4, avg test_loss: 0.010001, test_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012546, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011757, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011643, train_acc: 0.62\n",
      "alexnet1d, trial.48:\n",
      "Epoch 5, avg test_loss: 0.009915, test_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011812, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012627, train_acc: 0.48\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012245, train_acc: 0.55\n",
      "alexnet1d, trial.48:\n",
      "Epoch 6, avg test_loss: 0.010111, test_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012036, train_acc: 0.55\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011983, train_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011718, train_acc: 0.54\n",
      "alexnet1d, trial.48:\n",
      "Epoch 7, avg test_loss: 0.009998, test_acc: 0.53\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012342, train_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011983, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.010511, train_acc: 0.75\n",
      "alexnet1d, trial.48:\n",
      "Epoch 8, avg test_loss: 0.010111, test_acc: 0.56\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011067, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012335, train_acc: 0.55\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011699, train_acc: 0.61\n",
      "alexnet1d, trial.48:\n",
      "Epoch 9, avg test_loss: 0.009891, test_acc: 0.63\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.012637, train_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012245, train_acc: 0.50\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011776, train_acc: 0.66\n",
      "alexnet1d, trial.48:\n",
      "Epoch 10, avg test_loss: 0.009844, test_acc: 0.60\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011247, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011105, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011470, train_acc: 0.66\n",
      "alexnet1d, trial.48:\n",
      "Epoch 11, avg test_loss: 0.010636, test_acc: 0.54\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010433, train_acc: 0.75\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010696, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011694, train_acc: 0.64\n",
      "alexnet1d, trial.48:\n",
      "Epoch 12, avg test_loss: 0.009844, test_acc: 0.56\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011156, train_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011095, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011589, train_acc: 0.61\n",
      "alexnet1d, trial.48:\n",
      "Epoch 13, avg test_loss: 0.009950, test_acc: 0.60\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010430, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011810, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010099, train_acc: 0.68\n",
      "alexnet1d, trial.48:\n",
      "Epoch 14, avg test_loss: 0.009987, test_acc: 0.54\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010188, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009404, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011467, train_acc: 0.75\n",
      "alexnet1d, trial.48:\n",
      "Epoch 15, avg test_loss: 0.011199, test_acc: 0.54\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009398, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010495, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009247, train_acc: 0.73\n",
      "alexnet1d, trial.48:\n",
      "Epoch 16, avg test_loss: 0.010064, test_acc: 0.57\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009385, train_acc: 0.71\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010151, train_acc: 0.70\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010129, train_acc: 0.77\n",
      "alexnet1d, trial.48:\n",
      "Epoch 17, avg test_loss: 0.011314, test_acc: 0.56\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009445, train_acc: 0.71\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.010012, train_acc: 0.70\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009644, train_acc: 0.73\n",
      "alexnet1d, trial.48:\n",
      "Epoch 18, avg test_loss: 0.012852, test_acc: 0.50\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007449, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009303, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008436, train_acc: 0.77\n",
      "alexnet1d, trial.48:\n",
      "Epoch 19, avg test_loss: 0.012661, test_acc: 0.53\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008622, train_acc: 0.71\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.009093, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.010285, train_acc: 0.73\n",
      "alexnet1d, trial.48:\n",
      "Epoch 20, avg test_loss: 0.012569, test_acc: 0.59\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008864, train_acc: 0.75\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.008433, train_acc: 0.73\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.009208, train_acc: 0.73\n",
      "alexnet1d, trial.48:\n",
      "Epoch 21, avg test_loss: 0.014314, test_acc: 0.56\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.008315, train_acc: 0.79\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007453, train_acc: 0.79\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006361, train_acc: 0.88\n",
      "alexnet1d, trial.48:\n",
      "Epoch 22, avg test_loss: 0.014095, test_acc: 0.57\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006790, train_acc: 0.80\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.008040, train_acc: 0.82\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006904, train_acc: 0.82\n",
      "alexnet1d, trial.48:\n",
      "Epoch 23, avg test_loss: 0.014780, test_acc: 0.56\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.007062, train_acc: 0.82\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.009332, train_acc: 0.71\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006277, train_acc: 0.84\n",
      "alexnet1d, trial.48:\n",
      "Epoch 24, avg test_loss: 0.017259, test_acc: 0.54\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.006257, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.007714, train_acc: 0.80\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.007739, train_acc: 0.80\n",
      "alexnet1d, trial.48:\n",
      "Epoch 25, avg test_loss: 0.016989, test_acc: 0.57\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005793, train_acc: 0.86\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004722, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.006726, train_acc: 0.80\n",
      "alexnet1d, trial.48:\n",
      "Epoch 26, avg test_loss: 0.016517, test_acc: 0.57\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.005704, train_acc: 0.84\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.005613, train_acc: 0.88\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004816, train_acc: 0.91\n",
      "alexnet1d, trial.48:\n",
      "Epoch 27, avg test_loss: 0.019387, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.005204, train_acc: 0.88\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004519, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.005131, train_acc: 0.88\n",
      "alexnet1d, trial.48:\n",
      "Epoch 28, avg test_loss: 0.022596, test_acc: 0.59\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.004027, train_acc: 0.91\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.002987, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.004431, train_acc: 0.91\n",
      "alexnet1d, trial.48:\n",
      "Epoch 29, avg test_loss: 0.024570, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.004099, train_acc: 0.88\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.003620, train_acc: 0.91\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.003679, train_acc: 0.91\n",
      "alexnet1d, trial.48:\n",
      "Epoch 30, avg test_loss: 0.029122, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.004592, train_acc: 0.91\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.004845, train_acc: 0.86\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.005383, train_acc: 0.82\n",
      "alexnet1d, trial.48:\n",
      "Epoch 31, avg test_loss: 0.026910, test_acc: 0.50\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.005234, train_acc: 0.86\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.001647, train_acc: 1.00\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.004119, train_acc: 0.86\n",
      "alexnet1d, trial.48:\n",
      "Epoch 32, avg test_loss: 0.031579, test_acc: 0.50\n",
      "Train Epoch 33, lr: 0.000377, 0/280, avg loss: 0.006573, train_acc: 0.80\n",
      "Train Epoch 33, lr: 0.000377, 112/280, avg loss: 0.002900, train_acc: 0.95\n",
      "Train Epoch 33, lr: 0.000377, 224/280, avg loss: 0.003883, train_acc: 0.91\n",
      "alexnet1d, trial.48:\n",
      "Epoch 33, avg test_loss: 0.028333, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号38个\n",
      "错误信号32个\n",
      "信号正确并预测正确的概率为0.314\n",
      "信号错误并预测正确的概率为0.271\n",
      "总正确率为0.59\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012428, train_acc: 0.41\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.011829, train_acc: 0.66\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012336, train_acc: 0.57\n",
      "alexnet1d, trial.49:\n",
      "Epoch 0, avg test_loss: 0.010001, test_acc: 0.46\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012486, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012191, train_acc: 0.62\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011755, train_acc: 0.66\n",
      "alexnet1d, trial.49:\n",
      "Epoch 1, avg test_loss: 0.010390, test_acc: 0.46\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012851, train_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011551, train_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012374, train_acc: 0.55\n",
      "alexnet1d, trial.49:\n",
      "Epoch 2, avg test_loss: 0.010482, test_acc: 0.46\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012027, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011500, train_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011712, train_acc: 0.66\n",
      "alexnet1d, trial.49:\n",
      "Epoch 3, avg test_loss: 0.010304, test_acc: 0.46\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011958, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011883, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011848, train_acc: 0.64\n",
      "alexnet1d, trial.49:\n",
      "Epoch 4, avg test_loss: 0.010470, test_acc: 0.46\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011897, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012163, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012133, train_acc: 0.59\n",
      "alexnet1d, trial.49:\n",
      "Epoch 5, avg test_loss: 0.010441, test_acc: 0.46\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011520, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012054, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011854, train_acc: 0.59\n",
      "alexnet1d, trial.49:\n",
      "Epoch 6, avg test_loss: 0.010460, test_acc: 0.46\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012058, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011507, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012317, train_acc: 0.55\n",
      "alexnet1d, trial.49:\n",
      "Epoch 7, avg test_loss: 0.010363, test_acc: 0.46\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011590, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011284, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011837, train_acc: 0.57\n",
      "alexnet1d, trial.49:\n",
      "Epoch 8, avg test_loss: 0.010000, test_acc: 0.49\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012783, train_acc: 0.48\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011244, train_acc: 0.71\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012217, train_acc: 0.61\n",
      "alexnet1d, trial.49:\n",
      "Epoch 9, avg test_loss: 0.009914, test_acc: 0.47\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011566, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011383, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010730, train_acc: 0.66\n",
      "alexnet1d, trial.49:\n",
      "Epoch 10, avg test_loss: 0.009744, test_acc: 0.54\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011862, train_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010362, train_acc: 0.80\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010625, train_acc: 0.66\n",
      "alexnet1d, trial.49:\n",
      "Epoch 11, avg test_loss: 0.009679, test_acc: 0.56\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010192, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010677, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.012756, train_acc: 0.66\n",
      "alexnet1d, trial.49:\n",
      "Epoch 12, avg test_loss: 0.009290, test_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011001, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011098, train_acc: 0.62\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.008932, train_acc: 0.79\n",
      "alexnet1d, trial.49:\n",
      "Epoch 13, avg test_loss: 0.011614, test_acc: 0.47\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009824, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009271, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009080, train_acc: 0.75\n",
      "alexnet1d, trial.49:\n",
      "Epoch 14, avg test_loss: 0.010078, test_acc: 0.61\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009333, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008438, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009791, train_acc: 0.70\n",
      "alexnet1d, trial.49:\n",
      "Epoch 15, avg test_loss: 0.010163, test_acc: 0.63\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009396, train_acc: 0.84\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009677, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008532, train_acc: 0.79\n",
      "alexnet1d, trial.49:\n",
      "Epoch 16, avg test_loss: 0.012072, test_acc: 0.50\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007364, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008082, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.012031, train_acc: 0.71\n",
      "alexnet1d, trial.49:\n",
      "Epoch 17, avg test_loss: 0.010352, test_acc: 0.57\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007017, train_acc: 0.88\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009338, train_acc: 0.68\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.012080, train_acc: 0.64\n",
      "alexnet1d, trial.49:\n",
      "Epoch 18, avg test_loss: 0.010593, test_acc: 0.59\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006884, train_acc: 0.88\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009901, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009452, train_acc: 0.71\n",
      "alexnet1d, trial.49:\n",
      "Epoch 19, avg test_loss: 0.011676, test_acc: 0.50\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008933, train_acc: 0.73\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008177, train_acc: 0.75\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007357, train_acc: 0.82\n",
      "alexnet1d, trial.49:\n",
      "Epoch 20, avg test_loss: 0.010522, test_acc: 0.66\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006937, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007586, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008176, train_acc: 0.82\n",
      "alexnet1d, trial.49:\n",
      "Epoch 21, avg test_loss: 0.014773, test_acc: 0.54\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005023, train_acc: 0.93\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007685, train_acc: 0.79\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006033, train_acc: 0.86\n",
      "alexnet1d, trial.49:\n",
      "Epoch 22, avg test_loss: 0.014467, test_acc: 0.51\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.004889, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.004637, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.005390, train_acc: 0.86\n",
      "alexnet1d, trial.49:\n",
      "Epoch 23, avg test_loss: 0.015551, test_acc: 0.56\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004185, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005341, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006129, train_acc: 0.84\n",
      "alexnet1d, trial.49:\n",
      "Epoch 24, avg test_loss: 0.017947, test_acc: 0.54\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.002413, train_acc: 1.00\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.003297, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005068, train_acc: 0.91\n",
      "alexnet1d, trial.49:\n",
      "Epoch 25, avg test_loss: 0.018780, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.002521, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.001586, train_acc: 0.96\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.003925, train_acc: 0.91\n",
      "alexnet1d, trial.49:\n",
      "Epoch 26, avg test_loss: 0.021349, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003318, train_acc: 0.96\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.002969, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.002325, train_acc: 0.95\n",
      "alexnet1d, trial.49:\n",
      "Epoch 27, avg test_loss: 0.026173, test_acc: 0.50\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.001768, train_acc: 0.96\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.001968, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.003308, train_acc: 0.93\n",
      "alexnet1d, trial.49:\n",
      "Epoch 28, avg test_loss: 0.026058, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号32个\n",
      "错误信号38个\n",
      "信号正确并预测正确的概率为0.329\n",
      "信号错误并预测正确的概率为0.243\n",
      "总正确率为0.57\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012366, train_acc: 0.48\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012325, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.013888, train_acc: 0.64\n",
      "alexnet1d, trial.50:\n",
      "Epoch 0, avg test_loss: 0.010062, test_acc: 0.56\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012165, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012264, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012257, train_acc: 0.59\n",
      "alexnet1d, trial.50:\n",
      "Epoch 1, avg test_loss: 0.009841, test_acc: 0.56\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012313, train_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012475, train_acc: 0.48\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011675, train_acc: 0.70\n",
      "alexnet1d, trial.50:\n",
      "Epoch 2, avg test_loss: 0.009892, test_acc: 0.56\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012339, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011875, train_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011850, train_acc: 0.59\n",
      "alexnet1d, trial.50:\n",
      "Epoch 3, avg test_loss: 0.009942, test_acc: 0.56\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011917, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012170, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012212, train_acc: 0.57\n",
      "alexnet1d, trial.50:\n",
      "Epoch 4, avg test_loss: 0.009876, test_acc: 0.56\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012106, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011755, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011761, train_acc: 0.64\n",
      "alexnet1d, trial.50:\n",
      "Epoch 5, avg test_loss: 0.009816, test_acc: 0.56\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012370, train_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012030, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012028, train_acc: 0.57\n",
      "alexnet1d, trial.50:\n",
      "Epoch 6, avg test_loss: 0.009855, test_acc: 0.56\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012461, train_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012380, train_acc: 0.52\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012749, train_acc: 0.55\n",
      "alexnet1d, trial.50:\n",
      "Epoch 7, avg test_loss: 0.009890, test_acc: 0.56\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012151, train_acc: 0.55\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011772, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012168, train_acc: 0.62\n",
      "alexnet1d, trial.50:\n",
      "Epoch 8, avg test_loss: 0.009692, test_acc: 0.56\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012190, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012325, train_acc: 0.50\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011379, train_acc: 0.62\n",
      "alexnet1d, trial.50:\n",
      "Epoch 9, avg test_loss: 0.009647, test_acc: 0.56\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011745, train_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011611, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012339, train_acc: 0.54\n",
      "alexnet1d, trial.50:\n",
      "Epoch 10, avg test_loss: 0.009379, test_acc: 0.63\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.012491, train_acc: 0.52\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011988, train_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011159, train_acc: 0.73\n",
      "alexnet1d, trial.50:\n",
      "Epoch 11, avg test_loss: 0.009427, test_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010690, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.013530, train_acc: 0.55\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.012141, train_acc: 0.52\n",
      "alexnet1d, trial.50:\n",
      "Epoch 12, avg test_loss: 0.009578, test_acc: 0.57\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011969, train_acc: 0.55\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011652, train_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011596, train_acc: 0.62\n",
      "alexnet1d, trial.50:\n",
      "Epoch 13, avg test_loss: 0.009258, test_acc: 0.63\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010994, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010141, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011401, train_acc: 0.66\n",
      "alexnet1d, trial.50:\n",
      "Epoch 14, avg test_loss: 0.009275, test_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.011333, train_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010319, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009497, train_acc: 0.73\n",
      "alexnet1d, trial.50:\n",
      "Epoch 15, avg test_loss: 0.009241, test_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009773, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009753, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010658, train_acc: 0.73\n",
      "alexnet1d, trial.50:\n",
      "Epoch 16, avg test_loss: 0.009073, test_acc: 0.64\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.010622, train_acc: 0.68\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010341, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009711, train_acc: 0.70\n",
      "alexnet1d, trial.50:\n",
      "Epoch 17, avg test_loss: 0.009207, test_acc: 0.61\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.010230, train_acc: 0.71\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009957, train_acc: 0.68\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009499, train_acc: 0.71\n",
      "alexnet1d, trial.50:\n",
      "Epoch 18, avg test_loss: 0.009422, test_acc: 0.59\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.010031, train_acc: 0.70\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007953, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008797, train_acc: 0.80\n",
      "alexnet1d, trial.50:\n",
      "Epoch 19, avg test_loss: 0.009816, test_acc: 0.66\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.009981, train_acc: 0.70\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008031, train_acc: 0.75\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008201, train_acc: 0.80\n",
      "alexnet1d, trial.50:\n",
      "Epoch 20, avg test_loss: 0.009748, test_acc: 0.64\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007115, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007995, train_acc: 0.77\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.009447, train_acc: 0.82\n",
      "alexnet1d, trial.50:\n",
      "Epoch 21, avg test_loss: 0.009848, test_acc: 0.67\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007448, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007072, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.009420, train_acc: 0.75\n",
      "alexnet1d, trial.50:\n",
      "Epoch 22, avg test_loss: 0.011934, test_acc: 0.60\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.008636, train_acc: 0.80\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.009737, train_acc: 0.75\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006781, train_acc: 0.86\n",
      "alexnet1d, trial.50:\n",
      "Epoch 23, avg test_loss: 0.010335, test_acc: 0.63\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.007654, train_acc: 0.80\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006163, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.010345, train_acc: 0.68\n",
      "alexnet1d, trial.50:\n",
      "Epoch 24, avg test_loss: 0.009811, test_acc: 0.67\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.006119, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.007601, train_acc: 0.80\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006784, train_acc: 0.82\n",
      "alexnet1d, trial.50:\n",
      "Epoch 25, avg test_loss: 0.009889, test_acc: 0.64\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005744, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.006722, train_acc: 0.80\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.006538, train_acc: 0.88\n",
      "alexnet1d, trial.50:\n",
      "Epoch 26, avg test_loss: 0.011132, test_acc: 0.63\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.006011, train_acc: 0.88\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004248, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.006756, train_acc: 0.82\n",
      "alexnet1d, trial.50:\n",
      "Epoch 27, avg test_loss: 0.013696, test_acc: 0.60\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.005374, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004886, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.004113, train_acc: 0.91\n",
      "alexnet1d, trial.50:\n",
      "Epoch 28, avg test_loss: 0.013670, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.002539, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.003730, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.005141, train_acc: 0.88\n",
      "alexnet1d, trial.50:\n",
      "Epoch 29, avg test_loss: 0.014179, test_acc: 0.64\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.001806, train_acc: 0.98\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.003114, train_acc: 0.91\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.003216, train_acc: 0.95\n",
      "alexnet1d, trial.50:\n",
      "Epoch 30, avg test_loss: 0.014684, test_acc: 0.66\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.003521, train_acc: 0.95\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.004718, train_acc: 0.88\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.004279, train_acc: 0.91\n",
      "alexnet1d, trial.50:\n",
      "Epoch 31, avg test_loss: 0.017741, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.001593, train_acc: 1.00\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.001972, train_acc: 0.95\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.003735, train_acc: 0.89\n",
      "alexnet1d, trial.50:\n",
      "Epoch 32, avg test_loss: 0.018628, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号39个\n",
      "错误信号31个\n",
      "信号正确并预测正确的概率为0.4\n",
      "信号错误并预测正确的概率为0.229\n",
      "总正确率为0.63\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012394, train_acc: 0.43\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012818, train_acc: 0.61\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012161, train_acc: 0.59\n",
      "alexnet1d, trial.51:\n",
      "Epoch 0, avg test_loss: 0.009854, test_acc: 0.56\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012009, train_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.011593, train_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.014277, train_acc: 0.55\n",
      "alexnet1d, trial.51:\n",
      "Epoch 1, avg test_loss: 0.010127, test_acc: 0.56\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012049, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012401, train_acc: 0.52\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012221, train_acc: 0.57\n",
      "alexnet1d, trial.51:\n",
      "Epoch 2, avg test_loss: 0.009852, test_acc: 0.56\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012255, train_acc: 0.55\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012011, train_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011985, train_acc: 0.59\n",
      "alexnet1d, trial.51:\n",
      "Epoch 3, avg test_loss: 0.009946, test_acc: 0.56\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012196, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011920, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012351, train_acc: 0.55\n",
      "alexnet1d, trial.51:\n",
      "Epoch 4, avg test_loss: 0.010458, test_acc: 0.56\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012329, train_acc: 0.52\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012384, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011680, train_acc: 0.64\n",
      "alexnet1d, trial.51:\n",
      "Epoch 5, avg test_loss: 0.009932, test_acc: 0.56\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012113, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011777, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011992, train_acc: 0.57\n",
      "alexnet1d, trial.51:\n",
      "Epoch 6, avg test_loss: 0.010487, test_acc: 0.56\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011696, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011203, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012117, train_acc: 0.62\n",
      "alexnet1d, trial.51:\n",
      "Epoch 7, avg test_loss: 0.010144, test_acc: 0.56\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010951, train_acc: 0.77\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011294, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.013003, train_acc: 0.50\n",
      "alexnet1d, trial.51:\n",
      "Epoch 8, avg test_loss: 0.010041, test_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011506, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011555, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011571, train_acc: 0.61\n",
      "alexnet1d, trial.51:\n",
      "Epoch 9, avg test_loss: 0.009990, test_acc: 0.60\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011765, train_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011590, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012176, train_acc: 0.57\n",
      "alexnet1d, trial.51:\n",
      "Epoch 10, avg test_loss: 0.010257, test_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011026, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011180, train_acc: 0.75\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011676, train_acc: 0.64\n",
      "alexnet1d, trial.51:\n",
      "Epoch 11, avg test_loss: 0.009570, test_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010979, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011307, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010809, train_acc: 0.70\n",
      "alexnet1d, trial.51:\n",
      "Epoch 12, avg test_loss: 0.010001, test_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010882, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010486, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.008748, train_acc: 0.80\n",
      "alexnet1d, trial.51:\n",
      "Epoch 13, avg test_loss: 0.009766, test_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010872, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009663, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009979, train_acc: 0.77\n",
      "alexnet1d, trial.51:\n",
      "Epoch 14, avg test_loss: 0.009604, test_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010777, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009012, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009341, train_acc: 0.70\n",
      "alexnet1d, trial.51:\n",
      "Epoch 15, avg test_loss: 0.009810, test_acc: 0.67\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008500, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008465, train_acc: 0.80\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.007013, train_acc: 0.89\n",
      "alexnet1d, trial.51:\n",
      "Epoch 16, avg test_loss: 0.010431, test_acc: 0.66\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008254, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008878, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009230, train_acc: 0.77\n",
      "alexnet1d, trial.51:\n",
      "Epoch 17, avg test_loss: 0.011799, test_acc: 0.66\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.010322, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007470, train_acc: 0.86\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007158, train_acc: 0.82\n",
      "alexnet1d, trial.51:\n",
      "Epoch 18, avg test_loss: 0.010786, test_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006558, train_acc: 0.88\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008266, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007221, train_acc: 0.82\n",
      "alexnet1d, trial.51:\n",
      "Epoch 19, avg test_loss: 0.011426, test_acc: 0.67\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007218, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007003, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007190, train_acc: 0.80\n",
      "alexnet1d, trial.51:\n",
      "Epoch 20, avg test_loss: 0.014894, test_acc: 0.63\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.005558, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.010629, train_acc: 0.75\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008155, train_acc: 0.80\n",
      "alexnet1d, trial.51:\n",
      "Epoch 21, avg test_loss: 0.013117, test_acc: 0.63\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.008136, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005420, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007053, train_acc: 0.77\n",
      "alexnet1d, trial.51:\n",
      "Epoch 22, avg test_loss: 0.012265, test_acc: 0.64\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.004240, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006968, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.004430, train_acc: 0.89\n",
      "alexnet1d, trial.51:\n",
      "Epoch 23, avg test_loss: 0.018928, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005648, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004637, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005130, train_acc: 0.95\n",
      "alexnet1d, trial.51:\n",
      "Epoch 24, avg test_loss: 0.016419, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.003983, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004862, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004365, train_acc: 0.93\n",
      "alexnet1d, trial.51:\n",
      "Epoch 25, avg test_loss: 0.019028, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号39个\n",
      "错误信号31个\n",
      "信号正确并预测正确的概率为0.4\n",
      "信号错误并预测正确的概率为0.186\n",
      "总正确率为0.59\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012446, train_acc: 0.41\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012311, train_acc: 0.61\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012271, train_acc: 0.55\n",
      "alexnet1d, trial.52:\n",
      "Epoch 0, avg test_loss: 0.009710, test_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.010949, train_acc: 0.73\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.013792, train_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012317, train_acc: 0.48\n",
      "alexnet1d, trial.52:\n",
      "Epoch 1, avg test_loss: 0.009932, test_acc: 0.47\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012426, train_acc: 0.46\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012419, train_acc: 0.46\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012412, train_acc: 0.43\n",
      "alexnet1d, trial.52:\n",
      "Epoch 2, avg test_loss: 0.009885, test_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012321, train_acc: 0.48\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012256, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012314, train_acc: 0.52\n",
      "alexnet1d, trial.52:\n",
      "Epoch 3, avg test_loss: 0.009768, test_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012002, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012739, train_acc: 0.46\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011656, train_acc: 0.68\n",
      "alexnet1d, trial.52:\n",
      "Epoch 4, avg test_loss: 0.009740, test_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012306, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012337, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012433, train_acc: 0.57\n",
      "alexnet1d, trial.52:\n",
      "Epoch 5, avg test_loss: 0.009725, test_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011867, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012114, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012293, train_acc: 0.59\n",
      "alexnet1d, trial.52:\n",
      "Epoch 6, avg test_loss: 0.009689, test_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011697, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.013008, train_acc: 0.45\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011533, train_acc: 0.61\n",
      "alexnet1d, trial.52:\n",
      "Epoch 7, avg test_loss: 0.009691, test_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011660, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011936, train_acc: 0.52\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011221, train_acc: 0.66\n",
      "alexnet1d, trial.52:\n",
      "Epoch 8, avg test_loss: 0.009975, test_acc: 0.63\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010480, train_acc: 0.75\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012025, train_acc: 0.55\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011063, train_acc: 0.64\n",
      "alexnet1d, trial.52:\n",
      "Epoch 9, avg test_loss: 0.009828, test_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011533, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011022, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010478, train_acc: 0.73\n",
      "alexnet1d, trial.52:\n",
      "Epoch 10, avg test_loss: 0.010312, test_acc: 0.56\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.009408, train_acc: 0.73\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011599, train_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011809, train_acc: 0.66\n",
      "alexnet1d, trial.52:\n",
      "Epoch 11, avg test_loss: 0.010389, test_acc: 0.54\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011475, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010830, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010311, train_acc: 0.73\n",
      "alexnet1d, trial.52:\n",
      "Epoch 12, avg test_loss: 0.010292, test_acc: 0.54\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010779, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009915, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010217, train_acc: 0.70\n",
      "alexnet1d, trial.52:\n",
      "Epoch 13, avg test_loss: 0.011937, test_acc: 0.54\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010797, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009645, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011552, train_acc: 0.68\n",
      "alexnet1d, trial.52:\n",
      "Epoch 14, avg test_loss: 0.010309, test_acc: 0.61\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009727, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009327, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010664, train_acc: 0.66\n",
      "alexnet1d, trial.52:\n",
      "Epoch 15, avg test_loss: 0.011586, test_acc: 0.50\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008768, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010198, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009531, train_acc: 0.66\n",
      "alexnet1d, trial.52:\n",
      "Epoch 16, avg test_loss: 0.011906, test_acc: 0.54\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007960, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008998, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009311, train_acc: 0.71\n",
      "alexnet1d, trial.52:\n",
      "Epoch 17, avg test_loss: 0.013587, test_acc: 0.57\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.006730, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.006690, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007848, train_acc: 0.80\n",
      "alexnet1d, trial.52:\n",
      "Epoch 18, avg test_loss: 0.013308, test_acc: 0.56\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006246, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008401, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.011139, train_acc: 0.71\n",
      "alexnet1d, trial.52:\n",
      "Epoch 19, avg test_loss: 0.016588, test_acc: 0.53\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006284, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007764, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007410, train_acc: 0.79\n",
      "alexnet1d, trial.52:\n",
      "Epoch 20, avg test_loss: 0.015528, test_acc: 0.51\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.004610, train_acc: 0.89\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.004213, train_acc: 0.89\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.005204, train_acc: 0.88\n",
      "alexnet1d, trial.52:\n",
      "Epoch 21, avg test_loss: 0.018155, test_acc: 0.56\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005993, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.004982, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.004644, train_acc: 0.88\n",
      "alexnet1d, trial.52:\n",
      "Epoch 22, avg test_loss: 0.020960, test_acc: 0.61\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005100, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006995, train_acc: 0.82\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006422, train_acc: 0.80\n",
      "alexnet1d, trial.52:\n",
      "Epoch 23, avg test_loss: 0.019043, test_acc: 0.47\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005154, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005733, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006107, train_acc: 0.84\n",
      "alexnet1d, trial.52:\n",
      "Epoch 24, avg test_loss: 0.013710, test_acc: 0.53\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.003855, train_acc: 0.95\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006461, train_acc: 0.82\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006501, train_acc: 0.82\n",
      "alexnet1d, trial.52:\n",
      "Epoch 25, avg test_loss: 0.016406, test_acc: 0.57\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004021, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004114, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.002553, train_acc: 0.95\n",
      "alexnet1d, trial.52:\n",
      "Epoch 26, avg test_loss: 0.023232, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004394, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.002963, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003407, train_acc: 0.93\n",
      "alexnet1d, trial.52:\n",
      "Epoch 27, avg test_loss: 0.025245, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.000868, train_acc: 1.00\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.002647, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.002448, train_acc: 0.95\n",
      "alexnet1d, trial.52:\n",
      "Epoch 28, avg test_loss: 0.024760, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.002551, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.004170, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.001315, train_acc: 0.98\n",
      "alexnet1d, trial.52:\n",
      "Epoch 29, avg test_loss: 0.030464, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号40个\n",
      "错误信号30个\n",
      "信号正确并预测正确的概率为0.343\n",
      "信号错误并预测正确的概率为0.214\n",
      "总正确率为0.56\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012339, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012095, train_acc: 0.61\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012792, train_acc: 0.55\n",
      "alexnet1d, trial.53:\n",
      "Epoch 0, avg test_loss: 0.009617, test_acc: 0.63\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011903, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.013144, train_acc: 0.46\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012442, train_acc: 0.48\n",
      "alexnet1d, trial.53:\n",
      "Epoch 1, avg test_loss: 0.009728, test_acc: 0.63\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012034, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012292, train_acc: 0.50\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012125, train_acc: 0.64\n",
      "alexnet1d, trial.53:\n",
      "Epoch 2, avg test_loss: 0.009664, test_acc: 0.63\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011972, train_acc: 0.55\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011666, train_acc: 0.68\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012665, train_acc: 0.52\n",
      "alexnet1d, trial.53:\n",
      "Epoch 3, avg test_loss: 0.009495, test_acc: 0.63\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012104, train_acc: 0.50\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011798, train_acc: 0.66\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012249, train_acc: 0.57\n",
      "alexnet1d, trial.53:\n",
      "Epoch 4, avg test_loss: 0.009547, test_acc: 0.63\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011972, train_acc: 0.46\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011180, train_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011912, train_acc: 0.57\n",
      "alexnet1d, trial.53:\n",
      "Epoch 5, avg test_loss: 0.009463, test_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.010734, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012524, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011079, train_acc: 0.61\n",
      "alexnet1d, trial.53:\n",
      "Epoch 6, avg test_loss: 0.009749, test_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.010357, train_acc: 0.68\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011948, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011175, train_acc: 0.62\n",
      "alexnet1d, trial.53:\n",
      "Epoch 7, avg test_loss: 0.009276, test_acc: 0.63\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012166, train_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011531, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012574, train_acc: 0.57\n",
      "alexnet1d, trial.53:\n",
      "Epoch 8, avg test_loss: 0.009788, test_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010593, train_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010055, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011437, train_acc: 0.64\n",
      "alexnet1d, trial.53:\n",
      "Epoch 9, avg test_loss: 0.009805, test_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010507, train_acc: 0.70\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.009826, train_acc: 0.71\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010936, train_acc: 0.57\n",
      "alexnet1d, trial.53:\n",
      "Epoch 10, avg test_loss: 0.009566, test_acc: 0.67\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011729, train_acc: 0.55\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010360, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011008, train_acc: 0.70\n",
      "alexnet1d, trial.53:\n",
      "Epoch 11, avg test_loss: 0.010471, test_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009268, train_acc: 0.77\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.009981, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011224, train_acc: 0.66\n",
      "alexnet1d, trial.53:\n",
      "Epoch 12, avg test_loss: 0.009543, test_acc: 0.67\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009998, train_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009371, train_acc: 0.77\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009729, train_acc: 0.70\n",
      "alexnet1d, trial.53:\n",
      "Epoch 13, avg test_loss: 0.010006, test_acc: 0.59\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009931, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009437, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.007699, train_acc: 0.82\n",
      "alexnet1d, trial.53:\n",
      "Epoch 14, avg test_loss: 0.010384, test_acc: 0.67\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010753, train_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010240, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010883, train_acc: 0.68\n",
      "alexnet1d, trial.53:\n",
      "Epoch 15, avg test_loss: 0.011301, test_acc: 0.61\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.007191, train_acc: 0.86\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008852, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009148, train_acc: 0.71\n",
      "alexnet1d, trial.53:\n",
      "Epoch 16, avg test_loss: 0.009924, test_acc: 0.61\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008732, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.006949, train_acc: 0.84\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009804, train_acc: 0.66\n",
      "alexnet1d, trial.53:\n",
      "Epoch 17, avg test_loss: 0.011608, test_acc: 0.56\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007380, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008609, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009175, train_acc: 0.68\n",
      "alexnet1d, trial.53:\n",
      "Epoch 18, avg test_loss: 0.012320, test_acc: 0.60\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.005352, train_acc: 0.95\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008034, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.011671, train_acc: 0.59\n",
      "alexnet1d, trial.53:\n",
      "Epoch 19, avg test_loss: 0.010854, test_acc: 0.67\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007008, train_acc: 0.88\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.006079, train_acc: 0.88\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007892, train_acc: 0.75\n",
      "alexnet1d, trial.53:\n",
      "Epoch 20, avg test_loss: 0.012269, test_acc: 0.57\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007003, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006556, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007476, train_acc: 0.77\n",
      "alexnet1d, trial.53:\n",
      "Epoch 21, avg test_loss: 0.011343, test_acc: 0.66\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006664, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005579, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006614, train_acc: 0.84\n",
      "alexnet1d, trial.53:\n",
      "Epoch 22, avg test_loss: 0.015210, test_acc: 0.59\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.003297, train_acc: 0.95\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005343, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.008974, train_acc: 0.84\n",
      "alexnet1d, trial.53:\n",
      "Epoch 23, avg test_loss: 0.015296, test_acc: 0.63\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005060, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005377, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.004890, train_acc: 0.93\n",
      "alexnet1d, trial.53:\n",
      "Epoch 24, avg test_loss: 0.017062, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004238, train_acc: 0.95\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.003731, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004817, train_acc: 0.88\n",
      "alexnet1d, trial.53:\n",
      "Epoch 25, avg test_loss: 0.017188, test_acc: 0.64\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004768, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.003051, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004064, train_acc: 0.88\n",
      "alexnet1d, trial.53:\n",
      "Epoch 26, avg test_loss: 0.018353, test_acc: 0.60\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003838, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.002535, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003250, train_acc: 0.93\n",
      "alexnet1d, trial.53:\n",
      "Epoch 27, avg test_loss: 0.017246, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.002323, train_acc: 0.96\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.002835, train_acc: 0.96\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.001282, train_acc: 1.00\n",
      "alexnet1d, trial.53:\n",
      "Epoch 28, avg test_loss: 0.025415, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.002185, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.002336, train_acc: 0.96\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.002024, train_acc: 0.98\n",
      "alexnet1d, trial.53:\n",
      "Epoch 29, avg test_loss: 0.030135, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号44个\n",
      "错误信号26个\n",
      "信号正确并预测正确的概率为0.357\n",
      "信号错误并预测正确的概率为0.257\n",
      "总正确率为0.61\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012499, train_acc: 0.39\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.014136, train_acc: 0.52\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012482, train_acc: 0.46\n",
      "alexnet1d, trial.54:\n",
      "Epoch 0, avg test_loss: 0.009866, test_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012364, train_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012391, train_acc: 0.45\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012330, train_acc: 0.54\n",
      "alexnet1d, trial.54:\n",
      "Epoch 1, avg test_loss: 0.009782, test_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012278, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012165, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012289, train_acc: 0.57\n",
      "alexnet1d, trial.54:\n",
      "Epoch 2, avg test_loss: 0.009381, test_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011969, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011837, train_acc: 0.62\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012170, train_acc: 0.57\n",
      "alexnet1d, trial.54:\n",
      "Epoch 3, avg test_loss: 0.009519, test_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012821, train_acc: 0.41\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011994, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012094, train_acc: 0.57\n",
      "alexnet1d, trial.54:\n",
      "Epoch 4, avg test_loss: 0.009610, test_acc: 0.56\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012202, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011549, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011550, train_acc: 0.64\n",
      "alexnet1d, trial.54:\n",
      "Epoch 5, avg test_loss: 0.010198, test_acc: 0.53\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011742, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011671, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012145, train_acc: 0.61\n",
      "alexnet1d, trial.54:\n",
      "Epoch 6, avg test_loss: 0.010346, test_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011833, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012346, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012155, train_acc: 0.61\n",
      "alexnet1d, trial.54:\n",
      "Epoch 7, avg test_loss: 0.010130, test_acc: 0.53\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011712, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011922, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011035, train_acc: 0.68\n",
      "alexnet1d, trial.54:\n",
      "Epoch 8, avg test_loss: 0.011016, test_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011168, train_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010971, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011371, train_acc: 0.64\n",
      "alexnet1d, trial.54:\n",
      "Epoch 9, avg test_loss: 0.011540, test_acc: 0.53\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.009471, train_acc: 0.71\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.013639, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011895, train_acc: 0.59\n",
      "alexnet1d, trial.54:\n",
      "Epoch 10, avg test_loss: 0.011611, test_acc: 0.51\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011404, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.009937, train_acc: 0.79\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010977, train_acc: 0.64\n",
      "alexnet1d, trial.54:\n",
      "Epoch 11, avg test_loss: 0.010759, test_acc: 0.53\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010396, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010073, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011532, train_acc: 0.71\n",
      "alexnet1d, trial.54:\n",
      "Epoch 12, avg test_loss: 0.012019, test_acc: 0.50\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011158, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009461, train_acc: 0.75\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009502, train_acc: 0.70\n",
      "alexnet1d, trial.54:\n",
      "Epoch 13, avg test_loss: 0.011819, test_acc: 0.56\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009854, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011911, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009805, train_acc: 0.75\n",
      "alexnet1d, trial.54:\n",
      "Epoch 14, avg test_loss: 0.013216, test_acc: 0.47\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009836, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009937, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.012169, train_acc: 0.59\n",
      "alexnet1d, trial.54:\n",
      "Epoch 15, avg test_loss: 0.011408, test_acc: 0.56\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010176, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010149, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008393, train_acc: 0.80\n",
      "alexnet1d, trial.54:\n",
      "Epoch 16, avg test_loss: 0.012810, test_acc: 0.53\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008020, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007749, train_acc: 0.84\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007728, train_acc: 0.86\n",
      "alexnet1d, trial.54:\n",
      "Epoch 17, avg test_loss: 0.011608, test_acc: 0.57\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.006920, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.010067, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008795, train_acc: 0.77\n",
      "alexnet1d, trial.54:\n",
      "Epoch 18, avg test_loss: 0.014045, test_acc: 0.53\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008154, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009243, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007433, train_acc: 0.84\n",
      "alexnet1d, trial.54:\n",
      "Epoch 19, avg test_loss: 0.015330, test_acc: 0.53\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007263, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.006446, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009430, train_acc: 0.73\n",
      "alexnet1d, trial.54:\n",
      "Epoch 20, avg test_loss: 0.013521, test_acc: 0.53\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006386, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007774, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006698, train_acc: 0.86\n",
      "alexnet1d, trial.54:\n",
      "Epoch 21, avg test_loss: 0.014325, test_acc: 0.57\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007881, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.004814, train_acc: 0.93\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006507, train_acc: 0.89\n",
      "alexnet1d, trial.54:\n",
      "Epoch 22, avg test_loss: 0.014260, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006585, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005904, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.007462, train_acc: 0.80\n",
      "alexnet1d, trial.54:\n",
      "Epoch 23, avg test_loss: 0.015344, test_acc: 0.60\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.007791, train_acc: 0.80\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006460, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005584, train_acc: 0.88\n",
      "alexnet1d, trial.54:\n",
      "Epoch 24, avg test_loss: 0.013869, test_acc: 0.60\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005765, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006801, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005743, train_acc: 0.88\n",
      "alexnet1d, trial.54:\n",
      "Epoch 25, avg test_loss: 0.017485, test_acc: 0.57\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005256, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005061, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004590, train_acc: 0.91\n",
      "alexnet1d, trial.54:\n",
      "Epoch 26, avg test_loss: 0.015269, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.002840, train_acc: 0.96\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.003822, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.005501, train_acc: 0.91\n",
      "alexnet1d, trial.54:\n",
      "Epoch 27, avg test_loss: 0.018737, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004535, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.005069, train_acc: 0.88\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.002633, train_acc: 0.96\n",
      "alexnet1d, trial.54:\n",
      "Epoch 28, avg test_loss: 0.023707, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号45个\n",
      "错误信号25个\n",
      "信号正确并预测正确的概率为0.357\n",
      "信号错误并预测正确的概率为0.229\n",
      "总正确率为0.59\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012372, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012754, train_acc: 0.52\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012276, train_acc: 0.57\n",
      "alexnet1d, trial.55:\n",
      "Epoch 0, avg test_loss: 0.009546, test_acc: 0.63\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011784, train_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.011986, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012174, train_acc: 0.59\n",
      "alexnet1d, trial.55:\n",
      "Epoch 1, avg test_loss: 0.009675, test_acc: 0.63\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012347, train_acc: 0.50\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011928, train_acc: 0.70\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.013608, train_acc: 0.39\n",
      "alexnet1d, trial.55:\n",
      "Epoch 2, avg test_loss: 0.009540, test_acc: 0.63\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012439, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011730, train_acc: 0.68\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012200, train_acc: 0.54\n",
      "alexnet1d, trial.55:\n",
      "Epoch 3, avg test_loss: 0.009601, test_acc: 0.63\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011934, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012453, train_acc: 0.50\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012211, train_acc: 0.55\n",
      "alexnet1d, trial.55:\n",
      "Epoch 4, avg test_loss: 0.009568, test_acc: 0.63\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011601, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012472, train_acc: 0.48\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011878, train_acc: 0.62\n",
      "alexnet1d, trial.55:\n",
      "Epoch 5, avg test_loss: 0.009833, test_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011975, train_acc: 0.70\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011543, train_acc: 0.70\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012935, train_acc: 0.48\n",
      "alexnet1d, trial.55:\n",
      "Epoch 6, avg test_loss: 0.009453, test_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011745, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011539, train_acc: 0.73\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012862, train_acc: 0.46\n",
      "alexnet1d, trial.55:\n",
      "Epoch 7, avg test_loss: 0.010759, test_acc: 0.47\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011319, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011088, train_acc: 0.70\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011021, train_acc: 0.64\n",
      "alexnet1d, trial.55:\n",
      "Epoch 8, avg test_loss: 0.009753, test_acc: 0.63\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010899, train_acc: 0.73\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011200, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011541, train_acc: 0.61\n",
      "alexnet1d, trial.55:\n",
      "Epoch 9, avg test_loss: 0.009841, test_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010655, train_acc: 0.71\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011438, train_acc: 0.55\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011716, train_acc: 0.64\n",
      "alexnet1d, trial.55:\n",
      "Epoch 10, avg test_loss: 0.010378, test_acc: 0.53\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010711, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010727, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011954, train_acc: 0.64\n",
      "alexnet1d, trial.55:\n",
      "Epoch 11, avg test_loss: 0.010144, test_acc: 0.54\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010744, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010542, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010469, train_acc: 0.64\n",
      "alexnet1d, trial.55:\n",
      "Epoch 12, avg test_loss: 0.010975, test_acc: 0.51\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011475, train_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.008331, train_acc: 0.88\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.012484, train_acc: 0.57\n",
      "alexnet1d, trial.55:\n",
      "Epoch 13, avg test_loss: 0.009930, test_acc: 0.57\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009643, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009999, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010505, train_acc: 0.64\n",
      "alexnet1d, trial.55:\n",
      "Epoch 14, avg test_loss: 0.011150, test_acc: 0.51\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.007251, train_acc: 0.86\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008400, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.012655, train_acc: 0.71\n",
      "alexnet1d, trial.55:\n",
      "Epoch 15, avg test_loss: 0.012053, test_acc: 0.54\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010836, train_acc: 0.68\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010345, train_acc: 0.64\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010144, train_acc: 0.70\n",
      "alexnet1d, trial.55:\n",
      "Epoch 16, avg test_loss: 0.010634, test_acc: 0.49\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008563, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008118, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008223, train_acc: 0.82\n",
      "alexnet1d, trial.55:\n",
      "Epoch 17, avg test_loss: 0.011848, test_acc: 0.53\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008631, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009485, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009473, train_acc: 0.77\n",
      "alexnet1d, trial.55:\n",
      "Epoch 18, avg test_loss: 0.011901, test_acc: 0.53\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008437, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009160, train_acc: 0.75\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.006482, train_acc: 0.91\n",
      "alexnet1d, trial.55:\n",
      "Epoch 19, avg test_loss: 0.012407, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006984, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.009862, train_acc: 0.71\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006644, train_acc: 0.84\n",
      "alexnet1d, trial.55:\n",
      "Epoch 20, avg test_loss: 0.013428, test_acc: 0.56\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006205, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006108, train_acc: 0.89\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006314, train_acc: 0.88\n",
      "alexnet1d, trial.55:\n",
      "Epoch 21, avg test_loss: 0.012525, test_acc: 0.57\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.004988, train_acc: 0.93\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005909, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008002, train_acc: 0.80\n",
      "alexnet1d, trial.55:\n",
      "Epoch 22, avg test_loss: 0.019282, test_acc: 0.51\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.004697, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.003791, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.005695, train_acc: 0.84\n",
      "alexnet1d, trial.55:\n",
      "Epoch 23, avg test_loss: 0.017261, test_acc: 0.47\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004740, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004188, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.004345, train_acc: 0.91\n",
      "alexnet1d, trial.55:\n",
      "Epoch 24, avg test_loss: 0.025143, test_acc: 0.50\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005700, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.002912, train_acc: 0.95\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006269, train_acc: 0.84\n",
      "alexnet1d, trial.55:\n",
      "Epoch 25, avg test_loss: 0.020885, test_acc: 0.56\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.002160, train_acc: 0.98\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.003505, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004094, train_acc: 0.93\n",
      "alexnet1d, trial.55:\n",
      "Epoch 26, avg test_loss: 0.019746, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004559, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.003940, train_acc: 0.96\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.002873, train_acc: 0.93\n",
      "alexnet1d, trial.55:\n",
      "Epoch 27, avg test_loss: 0.031823, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号44个\n",
      "错误信号26个\n",
      "信号正确并预测正确的概率为0.3\n",
      "信号错误并预测正确的概率为0.214\n",
      "总正确率为0.51\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012365, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012467, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.011641, train_acc: 0.66\n",
      "alexnet1d, trial.56:\n",
      "Epoch 0, avg test_loss: 0.011131, test_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011991, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012368, train_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012187, train_acc: 0.62\n",
      "alexnet1d, trial.56:\n",
      "Epoch 1, avg test_loss: 0.009950, test_acc: 0.50\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012394, train_acc: 0.43\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012205, train_acc: 0.52\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012423, train_acc: 0.48\n",
      "alexnet1d, trial.56:\n",
      "Epoch 2, avg test_loss: 0.009872, test_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011849, train_acc: 0.62\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012642, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012001, train_acc: 0.55\n",
      "alexnet1d, trial.56:\n",
      "Epoch 3, avg test_loss: 0.010384, test_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012045, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011884, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012098, train_acc: 0.59\n",
      "alexnet1d, trial.56:\n",
      "Epoch 4, avg test_loss: 0.010176, test_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011807, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011980, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012466, train_acc: 0.50\n",
      "alexnet1d, trial.56:\n",
      "Epoch 5, avg test_loss: 0.011054, test_acc: 0.56\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012548, train_acc: 0.52\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011627, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012199, train_acc: 0.66\n",
      "alexnet1d, trial.56:\n",
      "Epoch 6, avg test_loss: 0.009814, test_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012095, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011631, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012398, train_acc: 0.52\n",
      "alexnet1d, trial.56:\n",
      "Epoch 7, avg test_loss: 0.009844, test_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011878, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.010705, train_acc: 0.71\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.010950, train_acc: 0.66\n",
      "alexnet1d, trial.56:\n",
      "Epoch 8, avg test_loss: 0.012213, test_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011235, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011384, train_acc: 0.55\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011290, train_acc: 0.62\n",
      "alexnet1d, trial.56:\n",
      "Epoch 9, avg test_loss: 0.010386, test_acc: 0.56\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011561, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010693, train_acc: 0.70\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010620, train_acc: 0.64\n",
      "alexnet1d, trial.56:\n",
      "Epoch 10, avg test_loss: 0.011259, test_acc: 0.54\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.009937, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.009897, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011330, train_acc: 0.68\n",
      "alexnet1d, trial.56:\n",
      "Epoch 11, avg test_loss: 0.011894, test_acc: 0.56\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009448, train_acc: 0.77\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.009991, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010753, train_acc: 0.64\n",
      "alexnet1d, trial.56:\n",
      "Epoch 12, avg test_loss: 0.009855, test_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010627, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010131, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.008114, train_acc: 0.79\n",
      "alexnet1d, trial.56:\n",
      "Epoch 13, avg test_loss: 0.013407, test_acc: 0.53\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.007948, train_acc: 0.88\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011434, train_acc: 0.62\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010700, train_acc: 0.66\n",
      "alexnet1d, trial.56:\n",
      "Epoch 14, avg test_loss: 0.011119, test_acc: 0.60\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009158, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010098, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009604, train_acc: 0.75\n",
      "alexnet1d, trial.56:\n",
      "Epoch 15, avg test_loss: 0.010876, test_acc: 0.56\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.007535, train_acc: 0.89\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.011300, train_acc: 0.62\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009216, train_acc: 0.79\n",
      "alexnet1d, trial.56:\n",
      "Epoch 16, avg test_loss: 0.011956, test_acc: 0.60\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.006890, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007202, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009380, train_acc: 0.77\n",
      "alexnet1d, trial.56:\n",
      "Epoch 17, avg test_loss: 0.015099, test_acc: 0.49\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007138, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.005684, train_acc: 0.89\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.015431, train_acc: 0.62\n",
      "alexnet1d, trial.56:\n",
      "Epoch 18, avg test_loss: 0.013338, test_acc: 0.64\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007317, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007634, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008273, train_acc: 0.77\n",
      "alexnet1d, trial.56:\n",
      "Epoch 19, avg test_loss: 0.013882, test_acc: 0.61\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007630, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007996, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006807, train_acc: 0.82\n",
      "alexnet1d, trial.56:\n",
      "Epoch 20, avg test_loss: 0.013850, test_acc: 0.57\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007481, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.004879, train_acc: 0.91\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006134, train_acc: 0.80\n",
      "alexnet1d, trial.56:\n",
      "Epoch 21, avg test_loss: 0.015794, test_acc: 0.60\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.004959, train_acc: 0.95\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006920, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006061, train_acc: 0.82\n",
      "alexnet1d, trial.56:\n",
      "Epoch 22, avg test_loss: 0.016602, test_acc: 0.60\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006664, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.009891, train_acc: 0.75\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.007714, train_acc: 0.79\n",
      "alexnet1d, trial.56:\n",
      "Epoch 23, avg test_loss: 0.026384, test_acc: 0.60\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005907, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004187, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005849, train_acc: 0.86\n",
      "alexnet1d, trial.56:\n",
      "Epoch 24, avg test_loss: 0.011153, test_acc: 0.64\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005497, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006772, train_acc: 0.82\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005045, train_acc: 0.95\n",
      "alexnet1d, trial.56:\n",
      "Epoch 25, avg test_loss: 0.014468, test_acc: 0.66\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004043, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004943, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004553, train_acc: 0.91\n",
      "alexnet1d, trial.56:\n",
      "Epoch 26, avg test_loss: 0.016900, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003910, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004448, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003675, train_acc: 0.89\n",
      "alexnet1d, trial.56:\n",
      "Epoch 27, avg test_loss: 0.016088, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004005, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.002882, train_acc: 0.96\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.001967, train_acc: 0.98\n",
      "alexnet1d, trial.56:\n",
      "Epoch 28, avg test_loss: 0.021616, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号40个\n",
      "错误信号30个\n",
      "信号正确并预测正确的概率为0.371\n",
      "信号错误并预测正确的概率为0.243\n",
      "总正确率为0.61\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012447, train_acc: 0.45\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012398, train_acc: 0.68\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012473, train_acc: 0.46\n",
      "alexnet1d, trial.57:\n",
      "Epoch 0, avg test_loss: 0.009837, test_acc: 0.66\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012278, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012384, train_acc: 0.48\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012340, train_acc: 0.48\n",
      "alexnet1d, trial.57:\n",
      "Epoch 1, avg test_loss: 0.009744, test_acc: 0.66\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012413, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012234, train_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012135, train_acc: 0.55\n",
      "alexnet1d, trial.57:\n",
      "Epoch 2, avg test_loss: 0.009314, test_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.014218, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012077, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011955, train_acc: 0.64\n",
      "alexnet1d, trial.57:\n",
      "Epoch 3, avg test_loss: 0.009568, test_acc: 0.66\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011993, train_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012018, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012320, train_acc: 0.55\n",
      "alexnet1d, trial.57:\n",
      "Epoch 4, avg test_loss: 0.009461, test_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012514, train_acc: 0.52\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011941, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012251, train_acc: 0.55\n",
      "alexnet1d, trial.57:\n",
      "Epoch 5, avg test_loss: 0.009476, test_acc: 0.66\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012104, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012032, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012414, train_acc: 0.50\n",
      "alexnet1d, trial.57:\n",
      "Epoch 6, avg test_loss: 0.009417, test_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012442, train_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012352, train_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011879, train_acc: 0.61\n",
      "alexnet1d, trial.57:\n",
      "Epoch 7, avg test_loss: 0.009383, test_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011960, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012183, train_acc: 0.55\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.013029, train_acc: 0.50\n",
      "alexnet1d, trial.57:\n",
      "Epoch 8, avg test_loss: 0.009277, test_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012438, train_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012033, train_acc: 0.55\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012094, train_acc: 0.61\n",
      "alexnet1d, trial.57:\n",
      "Epoch 9, avg test_loss: 0.009304, test_acc: 0.67\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011680, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011329, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012219, train_acc: 0.57\n",
      "alexnet1d, trial.57:\n",
      "Epoch 10, avg test_loss: 0.009220, test_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011067, train_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011385, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011933, train_acc: 0.66\n",
      "alexnet1d, trial.57:\n",
      "Epoch 11, avg test_loss: 0.009127, test_acc: 0.63\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010136, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011250, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011489, train_acc: 0.62\n",
      "alexnet1d, trial.57:\n",
      "Epoch 12, avg test_loss: 0.010190, test_acc: 0.60\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010666, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010408, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010435, train_acc: 0.77\n",
      "alexnet1d, trial.57:\n",
      "Epoch 13, avg test_loss: 0.010355, test_acc: 0.56\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010667, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010076, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011501, train_acc: 0.66\n",
      "alexnet1d, trial.57:\n",
      "Epoch 14, avg test_loss: 0.010069, test_acc: 0.56\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.007846, train_acc: 0.80\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010049, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009780, train_acc: 0.71\n",
      "alexnet1d, trial.57:\n",
      "Epoch 15, avg test_loss: 0.010604, test_acc: 0.59\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010154, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009915, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009076, train_acc: 0.80\n",
      "alexnet1d, trial.57:\n",
      "Epoch 16, avg test_loss: 0.010810, test_acc: 0.56\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007636, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008676, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010472, train_acc: 0.70\n",
      "alexnet1d, trial.57:\n",
      "Epoch 17, avg test_loss: 0.012984, test_acc: 0.51\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008787, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008652, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008528, train_acc: 0.80\n",
      "alexnet1d, trial.57:\n",
      "Epoch 18, avg test_loss: 0.011373, test_acc: 0.56\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008635, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007315, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008194, train_acc: 0.88\n",
      "alexnet1d, trial.57:\n",
      "Epoch 19, avg test_loss: 0.014018, test_acc: 0.56\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006545, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007386, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006283, train_acc: 0.89\n",
      "alexnet1d, trial.57:\n",
      "Epoch 20, avg test_loss: 0.015125, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.005627, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007617, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006534, train_acc: 0.80\n",
      "alexnet1d, trial.57:\n",
      "Epoch 21, avg test_loss: 0.014257, test_acc: 0.60\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005169, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005514, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.004911, train_acc: 0.93\n",
      "alexnet1d, trial.57:\n",
      "Epoch 22, avg test_loss: 0.019721, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005132, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.008606, train_acc: 0.79\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.004227, train_acc: 0.93\n",
      "alexnet1d, trial.57:\n",
      "Epoch 23, avg test_loss: 0.019491, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.009706, train_acc: 0.80\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005253, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.004947, train_acc: 0.91\n",
      "alexnet1d, trial.57:\n",
      "Epoch 24, avg test_loss: 0.019011, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号46个\n",
      "错误信号24个\n",
      "信号正确并预测正确的概率为0.4\n",
      "信号错误并预测正确的概率为0.114\n",
      "总正确率为0.51\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012368, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012299, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.014257, train_acc: 0.50\n",
      "alexnet1d, trial.58:\n",
      "Epoch 0, avg test_loss: 0.009672, test_acc: 0.66\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012061, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012476, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012385, train_acc: 0.46\n",
      "alexnet1d, trial.58:\n",
      "Epoch 1, avg test_loss: 0.009529, test_acc: 0.66\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012154, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012868, train_acc: 0.48\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012417, train_acc: 0.54\n",
      "alexnet1d, trial.58:\n",
      "Epoch 2, avg test_loss: 0.009439, test_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011753, train_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012506, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011995, train_acc: 0.61\n",
      "alexnet1d, trial.58:\n",
      "Epoch 3, avg test_loss: 0.009395, test_acc: 0.66\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012289, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012865, train_acc: 0.48\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011821, train_acc: 0.62\n",
      "alexnet1d, trial.58:\n",
      "Epoch 4, avg test_loss: 0.009412, test_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012311, train_acc: 0.52\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011820, train_acc: 0.68\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011944, train_acc: 0.54\n",
      "alexnet1d, trial.58:\n",
      "Epoch 5, avg test_loss: 0.009164, test_acc: 0.66\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012195, train_acc: 0.50\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.014973, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011617, train_acc: 0.62\n",
      "alexnet1d, trial.58:\n",
      "Epoch 6, avg test_loss: 0.009417, test_acc: 0.67\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012128, train_acc: 0.55\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012268, train_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011619, train_acc: 0.66\n",
      "alexnet1d, trial.58:\n",
      "Epoch 7, avg test_loss: 0.009168, test_acc: 0.70\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012128, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011221, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012868, train_acc: 0.62\n",
      "alexnet1d, trial.58:\n",
      "Epoch 8, avg test_loss: 0.008791, test_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011916, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011277, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011890, train_acc: 0.64\n",
      "alexnet1d, trial.58:\n",
      "Epoch 9, avg test_loss: 0.009042, test_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010981, train_acc: 0.71\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010943, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012580, train_acc: 0.54\n",
      "alexnet1d, trial.58:\n",
      "Epoch 10, avg test_loss: 0.008992, test_acc: 0.63\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011012, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011696, train_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010519, train_acc: 0.68\n",
      "alexnet1d, trial.58:\n",
      "Epoch 11, avg test_loss: 0.009184, test_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010666, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010556, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011466, train_acc: 0.55\n",
      "alexnet1d, trial.58:\n",
      "Epoch 12, avg test_loss: 0.008784, test_acc: 0.67\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.012281, train_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011402, train_acc: 0.57\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010753, train_acc: 0.68\n",
      "alexnet1d, trial.58:\n",
      "Epoch 13, avg test_loss: 0.010040, test_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010411, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009801, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011889, train_acc: 0.68\n",
      "alexnet1d, trial.58:\n",
      "Epoch 14, avg test_loss: 0.009799, test_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009266, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009686, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009808, train_acc: 0.66\n",
      "alexnet1d, trial.58:\n",
      "Epoch 15, avg test_loss: 0.009537, test_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.007222, train_acc: 0.84\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009588, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.011315, train_acc: 0.66\n",
      "alexnet1d, trial.58:\n",
      "Epoch 16, avg test_loss: 0.009286, test_acc: 0.67\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008331, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009362, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008999, train_acc: 0.73\n",
      "alexnet1d, trial.58:\n",
      "Epoch 17, avg test_loss: 0.010635, test_acc: 0.59\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009447, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009477, train_acc: 0.71\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008808, train_acc: 0.70\n",
      "alexnet1d, trial.58:\n",
      "Epoch 18, avg test_loss: 0.008901, test_acc: 0.70\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007842, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008209, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009385, train_acc: 0.71\n",
      "alexnet1d, trial.58:\n",
      "Epoch 19, avg test_loss: 0.010887, test_acc: 0.61\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007064, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007640, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007641, train_acc: 0.77\n",
      "alexnet1d, trial.58:\n",
      "Epoch 20, avg test_loss: 0.009976, test_acc: 0.60\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006112, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.009934, train_acc: 0.77\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007517, train_acc: 0.75\n",
      "alexnet1d, trial.58:\n",
      "Epoch 21, avg test_loss: 0.010695, test_acc: 0.63\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007127, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005535, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.009007, train_acc: 0.75\n",
      "alexnet1d, trial.58:\n",
      "Epoch 22, avg test_loss: 0.010609, test_acc: 0.64\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005164, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.004652, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.008847, train_acc: 0.79\n",
      "alexnet1d, trial.58:\n",
      "Epoch 23, avg test_loss: 0.010630, test_acc: 0.70\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004928, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005342, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005479, train_acc: 0.86\n",
      "alexnet1d, trial.58:\n",
      "Epoch 24, avg test_loss: 0.012637, test_acc: 0.56\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.003928, train_acc: 0.96\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005395, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005445, train_acc: 0.89\n",
      "alexnet1d, trial.58:\n",
      "Epoch 25, avg test_loss: 0.010499, test_acc: 0.70\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004508, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.003233, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.005751, train_acc: 0.86\n",
      "alexnet1d, trial.58:\n",
      "Epoch 26, avg test_loss: 0.012283, test_acc: 0.66\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003039, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.003899, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.002579, train_acc: 0.95\n",
      "alexnet1d, trial.58:\n",
      "Epoch 27, avg test_loss: 0.015780, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003928, train_acc: 0.84\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004228, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.003697, train_acc: 0.89\n",
      "alexnet1d, trial.58:\n",
      "Epoch 28, avg test_loss: 0.011819, test_acc: 0.71\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.002391, train_acc: 0.96\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.006088, train_acc: 0.89\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.005517, train_acc: 0.86\n",
      "alexnet1d, trial.58:\n",
      "Epoch 29, avg test_loss: 0.013971, test_acc: 0.71\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.002448, train_acc: 0.96\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.002675, train_acc: 0.91\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.004479, train_acc: 0.91\n",
      "alexnet1d, trial.58:\n",
      "Epoch 30, avg test_loss: 0.012403, test_acc: 0.67\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号46个\n",
      "错误信号24个\n",
      "信号正确并预测正确的概率为0.443\n",
      "信号错误并预测正确的概率为0.229\n",
      "总正确率为0.67\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012406, train_acc: 0.50\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.011602, train_acc: 0.64\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012479, train_acc: 0.48\n",
      "alexnet1d, trial.59:\n",
      "Epoch 0, avg test_loss: 0.010098, test_acc: 0.34\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012463, train_acc: 0.45\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012606, train_acc: 0.41\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012475, train_acc: 0.38\n",
      "alexnet1d, trial.59:\n",
      "Epoch 1, avg test_loss: 0.009894, test_acc: 0.63\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012341, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012446, train_acc: 0.46\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012116, train_acc: 0.62\n",
      "alexnet1d, trial.59:\n",
      "Epoch 2, avg test_loss: 0.009531, test_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012412, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012014, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012743, train_acc: 0.54\n",
      "alexnet1d, trial.59:\n",
      "Epoch 3, avg test_loss: 0.009375, test_acc: 0.66\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012175, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012011, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012099, train_acc: 0.61\n",
      "alexnet1d, trial.59:\n",
      "Epoch 4, avg test_loss: 0.009582, test_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012546, train_acc: 0.48\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012284, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012156, train_acc: 0.59\n",
      "alexnet1d, trial.59:\n",
      "Epoch 5, avg test_loss: 0.009605, test_acc: 0.66\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012181, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012357, train_acc: 0.50\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012262, train_acc: 0.52\n",
      "alexnet1d, trial.59:\n",
      "Epoch 6, avg test_loss: 0.009495, test_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011455, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011385, train_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012208, train_acc: 0.52\n",
      "alexnet1d, trial.59:\n",
      "Epoch 7, avg test_loss: 0.009452, test_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011943, train_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012247, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012794, train_acc: 0.48\n",
      "alexnet1d, trial.59:\n",
      "Epoch 8, avg test_loss: 0.009515, test_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012111, train_acc: 0.55\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011591, train_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011743, train_acc: 0.59\n",
      "alexnet1d, trial.59:\n",
      "Epoch 9, avg test_loss: 0.009541, test_acc: 0.60\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011642, train_acc: 0.70\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011652, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012565, train_acc: 0.55\n",
      "alexnet1d, trial.59:\n",
      "Epoch 10, avg test_loss: 0.009806, test_acc: 0.51\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011348, train_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010664, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010850, train_acc: 0.62\n",
      "alexnet1d, trial.59:\n",
      "Epoch 11, avg test_loss: 0.009625, test_acc: 0.56\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010639, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.012046, train_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.013589, train_acc: 0.66\n",
      "alexnet1d, trial.59:\n",
      "Epoch 12, avg test_loss: 0.010012, test_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011308, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011202, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011661, train_acc: 0.68\n",
      "alexnet1d, trial.59:\n",
      "Epoch 13, avg test_loss: 0.009719, test_acc: 0.56\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010644, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011377, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010192, train_acc: 0.68\n",
      "alexnet1d, trial.59:\n",
      "Epoch 14, avg test_loss: 0.009798, test_acc: 0.61\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008334, train_acc: 0.80\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010969, train_acc: 0.61\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010006, train_acc: 0.77\n",
      "alexnet1d, trial.59:\n",
      "Epoch 15, avg test_loss: 0.011067, test_acc: 0.56\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008583, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010703, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.011552, train_acc: 0.66\n",
      "alexnet1d, trial.59:\n",
      "Epoch 16, avg test_loss: 0.010323, test_acc: 0.51\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009246, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010130, train_acc: 0.71\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010486, train_acc: 0.75\n",
      "alexnet1d, trial.59:\n",
      "Epoch 17, avg test_loss: 0.009760, test_acc: 0.64\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.010911, train_acc: 0.68\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008511, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007546, train_acc: 0.89\n",
      "alexnet1d, trial.59:\n",
      "Epoch 18, avg test_loss: 0.010353, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009292, train_acc: 0.70\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007965, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007639, train_acc: 0.82\n",
      "alexnet1d, trial.59:\n",
      "Epoch 19, avg test_loss: 0.012990, test_acc: 0.59\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.009385, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.006236, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007858, train_acc: 0.84\n",
      "alexnet1d, trial.59:\n",
      "Epoch 20, avg test_loss: 0.013531, test_acc: 0.59\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.009068, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.005284, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007259, train_acc: 0.77\n",
      "alexnet1d, trial.59:\n",
      "Epoch 21, avg test_loss: 0.015248, test_acc: 0.64\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005471, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.008071, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006038, train_acc: 0.84\n",
      "alexnet1d, trial.59:\n",
      "Epoch 22, avg test_loss: 0.015904, test_acc: 0.60\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006041, train_acc: 0.82\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006273, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.009432, train_acc: 0.77\n",
      "alexnet1d, trial.59:\n",
      "Epoch 23, avg test_loss: 0.013640, test_acc: 0.64\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005897, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.007409, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005462, train_acc: 0.88\n",
      "alexnet1d, trial.59:\n",
      "Epoch 24, avg test_loss: 0.014539, test_acc: 0.57\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004593, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005407, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004202, train_acc: 0.91\n",
      "alexnet1d, trial.59:\n",
      "Epoch 25, avg test_loss: 0.012866, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.003910, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.002949, train_acc: 0.98\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004663, train_acc: 0.88\n",
      "alexnet1d, trial.59:\n",
      "Epoch 26, avg test_loss: 0.016633, test_acc: 0.60\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004410, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.006646, train_acc: 0.86\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003823, train_acc: 0.91\n",
      "alexnet1d, trial.59:\n",
      "Epoch 27, avg test_loss: 0.017146, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.002752, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.002983, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.005855, train_acc: 0.86\n",
      "alexnet1d, trial.59:\n",
      "Epoch 28, avg test_loss: 0.022144, test_acc: 0.51\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.003553, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.002635, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.004365, train_acc: 0.89\n",
      "alexnet1d, trial.59:\n",
      "Epoch 29, avg test_loss: 0.021204, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号46个\n",
      "错误信号24个\n",
      "信号正确并预测正确的概率为0.443\n",
      "信号错误并预测正确的概率为0.129\n",
      "总正确率为0.57\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012477, train_acc: 0.34\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.011819, train_acc: 0.66\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.013937, train_acc: 0.50\n",
      "alexnet1d, trial.60:\n",
      "Epoch 0, avg test_loss: 0.009827, test_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012142, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012329, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012226, train_acc: 0.55\n",
      "alexnet1d, trial.60:\n",
      "Epoch 1, avg test_loss: 0.009839, test_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012067, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011564, train_acc: 0.68\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012875, train_acc: 0.50\n",
      "alexnet1d, trial.60:\n",
      "Epoch 2, avg test_loss: 0.009876, test_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012338, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011530, train_acc: 0.73\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012555, train_acc: 0.50\n",
      "alexnet1d, trial.60:\n",
      "Epoch 3, avg test_loss: 0.009854, test_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012346, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012309, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011871, train_acc: 0.61\n",
      "alexnet1d, trial.60:\n",
      "Epoch 4, avg test_loss: 0.009875, test_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012721, train_acc: 0.48\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011769, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011822, train_acc: 0.61\n",
      "alexnet1d, trial.60:\n",
      "Epoch 5, avg test_loss: 0.009949, test_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011370, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011591, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011935, train_acc: 0.57\n",
      "alexnet1d, trial.60:\n",
      "Epoch 6, avg test_loss: 0.009859, test_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011701, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011362, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012982, train_acc: 0.50\n",
      "alexnet1d, trial.60:\n",
      "Epoch 7, avg test_loss: 0.009879, test_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011567, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.013876, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.010936, train_acc: 0.66\n",
      "alexnet1d, trial.60:\n",
      "Epoch 8, avg test_loss: 0.009913, test_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011697, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010646, train_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011033, train_acc: 0.75\n",
      "alexnet1d, trial.60:\n",
      "Epoch 9, avg test_loss: 0.010056, test_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011118, train_acc: 0.73\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011449, train_acc: 0.70\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011981, train_acc: 0.66\n",
      "alexnet1d, trial.60:\n",
      "Epoch 10, avg test_loss: 0.010012, test_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011214, train_acc: 0.73\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010205, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.012001, train_acc: 0.64\n",
      "alexnet1d, trial.60:\n",
      "Epoch 11, avg test_loss: 0.010189, test_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010078, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011562, train_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011266, train_acc: 0.64\n",
      "alexnet1d, trial.60:\n",
      "Epoch 12, avg test_loss: 0.010951, test_acc: 0.53\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009004, train_acc: 0.79\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011081, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009581, train_acc: 0.80\n",
      "alexnet1d, trial.60:\n",
      "Epoch 13, avg test_loss: 0.011642, test_acc: 0.50\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.007390, train_acc: 0.84\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010965, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009776, train_acc: 0.80\n",
      "alexnet1d, trial.60:\n",
      "Epoch 14, avg test_loss: 0.011812, test_acc: 0.47\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009749, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.007844, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009253, train_acc: 0.66\n",
      "alexnet1d, trial.60:\n",
      "Epoch 15, avg test_loss: 0.012487, test_acc: 0.50\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009182, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.007733, train_acc: 0.84\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008789, train_acc: 0.71\n",
      "alexnet1d, trial.60:\n",
      "Epoch 16, avg test_loss: 0.012600, test_acc: 0.53\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008265, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007207, train_acc: 0.84\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007926, train_acc: 0.77\n",
      "alexnet1d, trial.60:\n",
      "Epoch 17, avg test_loss: 0.015238, test_acc: 0.50\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008311, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007653, train_acc: 0.86\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.006909, train_acc: 0.86\n",
      "alexnet1d, trial.60:\n",
      "Epoch 18, avg test_loss: 0.016626, test_acc: 0.49\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007114, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007189, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.006663, train_acc: 0.86\n",
      "alexnet1d, trial.60:\n",
      "Epoch 19, avg test_loss: 0.017346, test_acc: 0.50\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006658, train_acc: 0.89\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.005310, train_acc: 0.88\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007118, train_acc: 0.80\n",
      "alexnet1d, trial.60:\n",
      "Epoch 20, avg test_loss: 0.017712, test_acc: 0.60\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.005399, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.005076, train_acc: 0.89\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.004404, train_acc: 0.91\n",
      "alexnet1d, trial.60:\n",
      "Epoch 21, avg test_loss: 0.022874, test_acc: 0.46\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006837, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.004270, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.003470, train_acc: 0.95\n",
      "alexnet1d, trial.60:\n",
      "Epoch 22, avg test_loss: 0.022143, test_acc: 0.50\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.003821, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005440, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.001652, train_acc: 0.98\n",
      "alexnet1d, trial.60:\n",
      "Epoch 23, avg test_loss: 0.025840, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.003424, train_acc: 0.93\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.002725, train_acc: 0.96\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.002063, train_acc: 0.95\n",
      "alexnet1d, trial.60:\n",
      "Epoch 24, avg test_loss: 0.027018, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号38个\n",
      "错误信号32个\n",
      "信号正确并预测正确的概率为0.286\n",
      "信号错误并预测正确的概率为0.229\n",
      "总正确率为0.51\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012354, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.013368, train_acc: 0.48\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012321, train_acc: 0.59\n",
      "alexnet1d, trial.61:\n",
      "Epoch 0, avg test_loss: 0.009630, test_acc: 0.60\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012492, train_acc: 0.52\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.011914, train_acc: 0.71\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012419, train_acc: 0.52\n",
      "alexnet1d, trial.61:\n",
      "Epoch 1, avg test_loss: 0.009556, test_acc: 0.60\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011647, train_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012237, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011926, train_acc: 0.64\n",
      "alexnet1d, trial.61:\n",
      "Epoch 2, avg test_loss: 0.009670, test_acc: 0.60\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012009, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012075, train_acc: 0.62\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012370, train_acc: 0.55\n",
      "alexnet1d, trial.61:\n",
      "Epoch 3, avg test_loss: 0.009611, test_acc: 0.60\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012753, train_acc: 0.41\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012313, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012014, train_acc: 0.62\n",
      "alexnet1d, trial.61:\n",
      "Epoch 4, avg test_loss: 0.009578, test_acc: 0.60\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011679, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012003, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012439, train_acc: 0.55\n",
      "alexnet1d, trial.61:\n",
      "Epoch 5, avg test_loss: 0.009487, test_acc: 0.60\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012418, train_acc: 0.50\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011548, train_acc: 0.71\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011978, train_acc: 0.52\n",
      "alexnet1d, trial.61:\n",
      "Epoch 6, avg test_loss: 0.009590, test_acc: 0.60\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012249, train_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011578, train_acc: 0.68\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011383, train_acc: 0.66\n",
      "alexnet1d, trial.61:\n",
      "Epoch 7, avg test_loss: 0.009481, test_acc: 0.63\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012104, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011132, train_acc: 0.70\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.013616, train_acc: 0.57\n",
      "alexnet1d, trial.61:\n",
      "Epoch 8, avg test_loss: 0.009637, test_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.013757, train_acc: 0.50\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011308, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011855, train_acc: 0.59\n",
      "alexnet1d, trial.61:\n",
      "Epoch 9, avg test_loss: 0.009538, test_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011700, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011807, train_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011841, train_acc: 0.62\n",
      "alexnet1d, trial.61:\n",
      "Epoch 10, avg test_loss: 0.009645, test_acc: 0.60\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011335, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011507, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.012237, train_acc: 0.50\n",
      "alexnet1d, trial.61:\n",
      "Epoch 11, avg test_loss: 0.009599, test_acc: 0.63\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.012770, train_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011493, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010614, train_acc: 0.61\n",
      "alexnet1d, trial.61:\n",
      "Epoch 12, avg test_loss: 0.009371, test_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010739, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010802, train_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010731, train_acc: 0.70\n",
      "alexnet1d, trial.61:\n",
      "Epoch 13, avg test_loss: 0.009916, test_acc: 0.60\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010130, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009219, train_acc: 0.79\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010589, train_acc: 0.71\n",
      "alexnet1d, trial.61:\n",
      "Epoch 14, avg test_loss: 0.009713, test_acc: 0.59\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009664, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.011158, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011862, train_acc: 0.61\n",
      "alexnet1d, trial.61:\n",
      "Epoch 15, avg test_loss: 0.010185, test_acc: 0.61\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.011311, train_acc: 0.68\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008469, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.011730, train_acc: 0.62\n",
      "alexnet1d, trial.61:\n",
      "Epoch 16, avg test_loss: 0.009775, test_acc: 0.60\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009618, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009484, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009063, train_acc: 0.80\n",
      "alexnet1d, trial.61:\n",
      "Epoch 17, avg test_loss: 0.010796, test_acc: 0.59\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008921, train_acc: 0.71\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007221, train_acc: 0.86\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008727, train_acc: 0.77\n",
      "alexnet1d, trial.61:\n",
      "Epoch 18, avg test_loss: 0.010906, test_acc: 0.53\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007958, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008545, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009528, train_acc: 0.77\n",
      "alexnet1d, trial.61:\n",
      "Epoch 19, avg test_loss: 0.011125, test_acc: 0.63\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008511, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007718, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008218, train_acc: 0.79\n",
      "alexnet1d, trial.61:\n",
      "Epoch 20, avg test_loss: 0.009847, test_acc: 0.64\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008020, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007426, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.009348, train_acc: 0.75\n",
      "alexnet1d, trial.61:\n",
      "Epoch 21, avg test_loss: 0.010183, test_acc: 0.66\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006364, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006496, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008096, train_acc: 0.82\n",
      "alexnet1d, trial.61:\n",
      "Epoch 22, avg test_loss: 0.011392, test_acc: 0.63\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007885, train_acc: 0.82\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.004992, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006845, train_acc: 0.88\n",
      "alexnet1d, trial.61:\n",
      "Epoch 23, avg test_loss: 0.012838, test_acc: 0.69\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005171, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.007164, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006481, train_acc: 0.82\n",
      "alexnet1d, trial.61:\n",
      "Epoch 24, avg test_loss: 0.014950, test_acc: 0.63\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004056, train_acc: 0.95\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006513, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.003496, train_acc: 0.93\n",
      "alexnet1d, trial.61:\n",
      "Epoch 25, avg test_loss: 0.015585, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.003190, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.006043, train_acc: 0.84\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.003707, train_acc: 0.93\n",
      "alexnet1d, trial.61:\n",
      "Epoch 26, avg test_loss: 0.016608, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003464, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.003965, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004329, train_acc: 0.91\n",
      "alexnet1d, trial.61:\n",
      "Epoch 27, avg test_loss: 0.016576, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003315, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.002143, train_acc: 0.96\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.004381, train_acc: 0.91\n",
      "alexnet1d, trial.61:\n",
      "Epoch 28, avg test_loss: 0.017470, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号42个\n",
      "错误信号28个\n",
      "信号正确并预测正确的概率为0.429\n",
      "信号错误并预测正确的概率为0.214\n",
      "总正确率为0.64\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012350, train_acc: 0.50\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012707, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012328, train_acc: 0.52\n",
      "alexnet1d, trial.62:\n",
      "Epoch 0, avg test_loss: 0.010026, test_acc: 0.56\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011361, train_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.013093, train_acc: 0.48\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012014, train_acc: 0.64\n",
      "alexnet1d, trial.62:\n",
      "Epoch 1, avg test_loss: 0.009833, test_acc: 0.56\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012098, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012106, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012159, train_acc: 0.59\n",
      "alexnet1d, trial.62:\n",
      "Epoch 2, avg test_loss: 0.009990, test_acc: 0.56\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012633, train_acc: 0.50\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011128, train_acc: 0.68\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012536, train_acc: 0.50\n",
      "alexnet1d, trial.62:\n",
      "Epoch 3, avg test_loss: 0.010198, test_acc: 0.56\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012773, train_acc: 0.52\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012195, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011903, train_acc: 0.62\n",
      "alexnet1d, trial.62:\n",
      "Epoch 4, avg test_loss: 0.009883, test_acc: 0.56\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012026, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011787, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012391, train_acc: 0.57\n",
      "alexnet1d, trial.62:\n",
      "Epoch 5, avg test_loss: 0.010185, test_acc: 0.56\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011776, train_acc: 0.66\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012397, train_acc: 0.52\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011164, train_acc: 0.68\n",
      "alexnet1d, trial.62:\n",
      "Epoch 6, avg test_loss: 0.009961, test_acc: 0.56\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011236, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011286, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011809, train_acc: 0.54\n",
      "alexnet1d, trial.62:\n",
      "Epoch 7, avg test_loss: 0.010093, test_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010811, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011418, train_acc: 0.73\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011706, train_acc: 0.59\n",
      "alexnet1d, trial.62:\n",
      "Epoch 8, avg test_loss: 0.010121, test_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010781, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011746, train_acc: 0.55\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011682, train_acc: 0.64\n",
      "alexnet1d, trial.62:\n",
      "Epoch 9, avg test_loss: 0.010277, test_acc: 0.60\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010670, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011513, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011276, train_acc: 0.68\n",
      "alexnet1d, trial.62:\n",
      "Epoch 10, avg test_loss: 0.010084, test_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010152, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.012115, train_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010219, train_acc: 0.71\n",
      "alexnet1d, trial.62:\n",
      "Epoch 11, avg test_loss: 0.010162, test_acc: 0.67\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009616, train_acc: 0.79\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.014220, train_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.009920, train_acc: 0.79\n",
      "alexnet1d, trial.62:\n",
      "Epoch 12, avg test_loss: 0.009452, test_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010253, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.012310, train_acc: 0.62\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009768, train_acc: 0.66\n",
      "alexnet1d, trial.62:\n",
      "Epoch 13, avg test_loss: 0.010234, test_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010205, train_acc: 0.79\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010615, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010935, train_acc: 0.61\n",
      "alexnet1d, trial.62:\n",
      "Epoch 14, avg test_loss: 0.010117, test_acc: 0.63\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009952, train_acc: 0.62\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010860, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008497, train_acc: 0.75\n",
      "alexnet1d, trial.62:\n",
      "Epoch 15, avg test_loss: 0.010581, test_acc: 0.61\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010755, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.007858, train_acc: 0.82\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008639, train_acc: 0.80\n",
      "alexnet1d, trial.62:\n",
      "Epoch 16, avg test_loss: 0.011525, test_acc: 0.63\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008538, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007964, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009032, train_acc: 0.79\n",
      "alexnet1d, trial.62:\n",
      "Epoch 17, avg test_loss: 0.010574, test_acc: 0.64\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008679, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008393, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007768, train_acc: 0.79\n",
      "alexnet1d, trial.62:\n",
      "Epoch 18, avg test_loss: 0.010804, test_acc: 0.54\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007332, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007213, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008056, train_acc: 0.84\n",
      "alexnet1d, trial.62:\n",
      "Epoch 19, avg test_loss: 0.012930, test_acc: 0.63\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008152, train_acc: 0.77\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.005262, train_acc: 0.89\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008393, train_acc: 0.77\n",
      "alexnet1d, trial.62:\n",
      "Epoch 20, avg test_loss: 0.012376, test_acc: 0.63\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.005269, train_acc: 0.89\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.004574, train_acc: 0.93\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.010780, train_acc: 0.73\n",
      "alexnet1d, trial.62:\n",
      "Epoch 21, avg test_loss: 0.014715, test_acc: 0.60\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006403, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007544, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007774, train_acc: 0.80\n",
      "alexnet1d, trial.62:\n",
      "Epoch 22, avg test_loss: 0.013866, test_acc: 0.63\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.004601, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006531, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.007290, train_acc: 0.82\n",
      "alexnet1d, trial.62:\n",
      "Epoch 23, avg test_loss: 0.015640, test_acc: 0.61\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004127, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006734, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.004014, train_acc: 0.95\n",
      "alexnet1d, trial.62:\n",
      "Epoch 24, avg test_loss: 0.016336, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004171, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005891, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004296, train_acc: 0.89\n",
      "alexnet1d, trial.62:\n",
      "Epoch 25, avg test_loss: 0.018970, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.003155, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004702, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.005081, train_acc: 0.86\n",
      "alexnet1d, trial.62:\n",
      "Epoch 26, avg test_loss: 0.018404, test_acc: 0.64\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003922, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.003500, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.002434, train_acc: 0.98\n",
      "alexnet1d, trial.62:\n",
      "Epoch 27, avg test_loss: 0.020686, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003001, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.002613, train_acc: 0.98\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.003320, train_acc: 0.93\n",
      "alexnet1d, trial.62:\n",
      "Epoch 28, avg test_loss: 0.021578, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号39个\n",
      "错误信号31个\n",
      "信号正确并预测正确的概率为0.371\n",
      "信号错误并预测正确的概率为0.257\n",
      "总正确率为0.63\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012501, train_acc: 0.38\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012185, train_acc: 0.70\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012474, train_acc: 0.54\n",
      "alexnet1d, trial.63:\n",
      "Epoch 0, avg test_loss: 0.010038, test_acc: 0.51\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011780, train_acc: 0.62\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012021, train_acc: 0.62\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012608, train_acc: 0.52\n",
      "alexnet1d, trial.63:\n",
      "Epoch 1, avg test_loss: 0.009991, test_acc: 0.51\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012064, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011339, train_acc: 0.73\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012472, train_acc: 0.54\n",
      "alexnet1d, trial.63:\n",
      "Epoch 2, avg test_loss: 0.009981, test_acc: 0.51\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011827, train_acc: 0.62\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012329, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012055, train_acc: 0.61\n",
      "alexnet1d, trial.63:\n",
      "Epoch 3, avg test_loss: 0.010024, test_acc: 0.51\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011715, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012079, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012366, train_acc: 0.55\n",
      "alexnet1d, trial.63:\n",
      "Epoch 4, avg test_loss: 0.010131, test_acc: 0.51\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012887, train_acc: 0.48\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011353, train_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011649, train_acc: 0.64\n",
      "alexnet1d, trial.63:\n",
      "Epoch 5, avg test_loss: 0.010050, test_acc: 0.51\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011021, train_acc: 0.70\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012678, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012746, train_acc: 0.54\n",
      "alexnet1d, trial.63:\n",
      "Epoch 6, avg test_loss: 0.010004, test_acc: 0.51\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012059, train_acc: 0.50\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012256, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011962, train_acc: 0.57\n",
      "alexnet1d, trial.63:\n",
      "Epoch 7, avg test_loss: 0.009914, test_acc: 0.49\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011532, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012087, train_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011267, train_acc: 0.68\n",
      "alexnet1d, trial.63:\n",
      "Epoch 8, avg test_loss: 0.009837, test_acc: 0.53\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012375, train_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011385, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010589, train_acc: 0.66\n",
      "alexnet1d, trial.63:\n",
      "Epoch 9, avg test_loss: 0.010062, test_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010900, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011067, train_acc: 0.52\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.013654, train_acc: 0.57\n",
      "alexnet1d, trial.63:\n",
      "Epoch 10, avg test_loss: 0.009571, test_acc: 0.63\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010440, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010982, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011160, train_acc: 0.64\n",
      "alexnet1d, trial.63:\n",
      "Epoch 11, avg test_loss: 0.009442, test_acc: 0.63\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011680, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010232, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.012950, train_acc: 0.54\n",
      "alexnet1d, trial.63:\n",
      "Epoch 12, avg test_loss: 0.010858, test_acc: 0.51\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.012489, train_acc: 0.52\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011335, train_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009410, train_acc: 0.77\n",
      "alexnet1d, trial.63:\n",
      "Epoch 13, avg test_loss: 0.009214, test_acc: 0.64\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010251, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009880, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009806, train_acc: 0.66\n",
      "alexnet1d, trial.63:\n",
      "Epoch 14, avg test_loss: 0.009688, test_acc: 0.57\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009424, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008738, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008521, train_acc: 0.75\n",
      "alexnet1d, trial.63:\n",
      "Epoch 15, avg test_loss: 0.009689, test_acc: 0.60\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008908, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.006971, train_acc: 0.86\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008890, train_acc: 0.77\n",
      "alexnet1d, trial.63:\n",
      "Epoch 16, avg test_loss: 0.010763, test_acc: 0.60\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008029, train_acc: 0.84\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009361, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008963, train_acc: 0.70\n",
      "alexnet1d, trial.63:\n",
      "Epoch 17, avg test_loss: 0.009984, test_acc: 0.64\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007556, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007135, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.011142, train_acc: 0.64\n",
      "alexnet1d, trial.63:\n",
      "Epoch 18, avg test_loss: 0.011740, test_acc: 0.56\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007071, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007463, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009652, train_acc: 0.75\n",
      "alexnet1d, trial.63:\n",
      "Epoch 19, avg test_loss: 0.010895, test_acc: 0.59\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006297, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.009013, train_acc: 0.77\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008815, train_acc: 0.73\n",
      "alexnet1d, trial.63:\n",
      "Epoch 20, avg test_loss: 0.012602, test_acc: 0.54\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006381, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007241, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008052, train_acc: 0.79\n",
      "alexnet1d, trial.63:\n",
      "Epoch 21, avg test_loss: 0.013664, test_acc: 0.60\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.003062, train_acc: 0.98\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.011119, train_acc: 0.68\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007167, train_acc: 0.82\n",
      "alexnet1d, trial.63:\n",
      "Epoch 22, avg test_loss: 0.012903, test_acc: 0.60\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005778, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007693, train_acc: 0.82\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006859, train_acc: 0.84\n",
      "alexnet1d, trial.63:\n",
      "Epoch 23, avg test_loss: 0.016249, test_acc: 0.57\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005865, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004686, train_acc: 0.93\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.004056, train_acc: 0.93\n",
      "alexnet1d, trial.63:\n",
      "Epoch 24, avg test_loss: 0.013819, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.003997, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005333, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004681, train_acc: 0.89\n",
      "alexnet1d, trial.63:\n",
      "Epoch 25, avg test_loss: 0.016942, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004040, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004489, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.001896, train_acc: 0.96\n",
      "alexnet1d, trial.63:\n",
      "Epoch 26, avg test_loss: 0.019983, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003144, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.003746, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.002598, train_acc: 0.95\n",
      "alexnet1d, trial.63:\n",
      "Epoch 27, avg test_loss: 0.019308, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号36个\n",
      "错误信号34个\n",
      "信号正确并预测正确的概率为0.286\n",
      "信号错误并预测正确的概率为0.271\n",
      "总正确率为0.56\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012347, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012280, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012024, train_acc: 0.66\n",
      "alexnet1d, trial.64:\n",
      "Epoch 0, avg test_loss: 0.009578, test_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012330, train_acc: 0.48\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012029, train_acc: 0.62\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012000, train_acc: 0.66\n",
      "alexnet1d, trial.64:\n",
      "Epoch 1, avg test_loss: 0.009464, test_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012133, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012473, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011988, train_acc: 0.64\n",
      "alexnet1d, trial.64:\n",
      "Epoch 2, avg test_loss: 0.009557, test_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012194, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012352, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012363, train_acc: 0.52\n",
      "alexnet1d, trial.64:\n",
      "Epoch 3, avg test_loss: 0.009654, test_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012144, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012547, train_acc: 0.45\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012167, train_acc: 0.59\n",
      "alexnet1d, trial.64:\n",
      "Epoch 4, avg test_loss: 0.009513, test_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011806, train_acc: 0.68\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012721, train_acc: 0.46\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011766, train_acc: 0.68\n",
      "alexnet1d, trial.64:\n",
      "Epoch 5, avg test_loss: 0.009499, test_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012409, train_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011640, train_acc: 0.68\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011934, train_acc: 0.62\n",
      "alexnet1d, trial.64:\n",
      "Epoch 6, avg test_loss: 0.009458, test_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012460, train_acc: 0.50\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012156, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012245, train_acc: 0.57\n",
      "alexnet1d, trial.64:\n",
      "Epoch 7, avg test_loss: 0.009452, test_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012252, train_acc: 0.52\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012259, train_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011955, train_acc: 0.59\n",
      "alexnet1d, trial.64:\n",
      "Epoch 8, avg test_loss: 0.009272, test_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012317, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012130, train_acc: 0.52\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012230, train_acc: 0.54\n",
      "alexnet1d, trial.64:\n",
      "Epoch 9, avg test_loss: 0.009033, test_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011333, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012193, train_acc: 0.48\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011830, train_acc: 0.62\n",
      "alexnet1d, trial.64:\n",
      "Epoch 10, avg test_loss: 0.009103, test_acc: 0.73\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.012225, train_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011928, train_acc: 0.52\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011658, train_acc: 0.59\n",
      "alexnet1d, trial.64:\n",
      "Epoch 11, avg test_loss: 0.009099, test_acc: 0.69\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011318, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011397, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011761, train_acc: 0.52\n",
      "alexnet1d, trial.64:\n",
      "Epoch 12, avg test_loss: 0.008329, test_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011009, train_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.012184, train_acc: 0.55\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011955, train_acc: 0.70\n",
      "alexnet1d, trial.64:\n",
      "Epoch 13, avg test_loss: 0.008606, test_acc: 0.67\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010692, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011654, train_acc: 0.55\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011138, train_acc: 0.64\n",
      "alexnet1d, trial.64:\n",
      "Epoch 14, avg test_loss: 0.009244, test_acc: 0.61\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.011422, train_acc: 0.68\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010107, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011683, train_acc: 0.57\n",
      "alexnet1d, trial.64:\n",
      "Epoch 15, avg test_loss: 0.008672, test_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.011694, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010028, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010850, train_acc: 0.61\n",
      "alexnet1d, trial.64:\n",
      "Epoch 16, avg test_loss: 0.009370, test_acc: 0.70\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009698, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010080, train_acc: 0.66\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010419, train_acc: 0.70\n",
      "alexnet1d, trial.64:\n",
      "Epoch 17, avg test_loss: 0.008860, test_acc: 0.71\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009988, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009977, train_acc: 0.66\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008961, train_acc: 0.82\n",
      "alexnet1d, trial.64:\n",
      "Epoch 18, avg test_loss: 0.010321, test_acc: 0.67\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009608, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008179, train_acc: 0.75\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.010623, train_acc: 0.73\n",
      "alexnet1d, trial.64:\n",
      "Epoch 19, avg test_loss: 0.010848, test_acc: 0.70\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007532, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007809, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.010938, train_acc: 0.80\n",
      "alexnet1d, trial.64:\n",
      "Epoch 20, avg test_loss: 0.012095, test_acc: 0.66\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006991, train_acc: 0.89\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.008043, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.009197, train_acc: 0.77\n",
      "alexnet1d, trial.64:\n",
      "Epoch 21, avg test_loss: 0.010371, test_acc: 0.63\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007275, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007748, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.009277, train_acc: 0.80\n",
      "alexnet1d, trial.64:\n",
      "Epoch 22, avg test_loss: 0.009453, test_acc: 0.67\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007615, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.008501, train_acc: 0.77\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.008500, train_acc: 0.79\n",
      "alexnet1d, trial.64:\n",
      "Epoch 23, avg test_loss: 0.010283, test_acc: 0.69\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.007775, train_acc: 0.77\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006356, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.008160, train_acc: 0.80\n",
      "alexnet1d, trial.64:\n",
      "Epoch 24, avg test_loss: 0.010608, test_acc: 0.67\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.007035, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.007435, train_acc: 0.77\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005810, train_acc: 0.88\n",
      "alexnet1d, trial.64:\n",
      "Epoch 25, avg test_loss: 0.013444, test_acc: 0.69\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.006006, train_acc: 0.84\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005749, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.006236, train_acc: 0.86\n",
      "alexnet1d, trial.64:\n",
      "Epoch 26, avg test_loss: 0.015160, test_acc: 0.57\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.006353, train_acc: 0.84\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.003356, train_acc: 0.96\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004813, train_acc: 0.89\n",
      "alexnet1d, trial.64:\n",
      "Epoch 27, avg test_loss: 0.014116, test_acc: 0.66\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.006389, train_acc: 0.88\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004800, train_acc: 0.88\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.005658, train_acc: 0.91\n",
      "alexnet1d, trial.64:\n",
      "Epoch 28, avg test_loss: 0.019610, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.003623, train_acc: 0.89\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.003974, train_acc: 0.88\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.004549, train_acc: 0.91\n",
      "alexnet1d, trial.64:\n",
      "Epoch 29, avg test_loss: 0.018193, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.005733, train_acc: 0.89\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.004406, train_acc: 0.88\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.004943, train_acc: 0.91\n",
      "alexnet1d, trial.64:\n",
      "Epoch 30, avg test_loss: 0.017389, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号45个\n",
      "错误信号25个\n",
      "信号正确并预测正确的概率为0.4\n",
      "信号错误并预测正确的概率为0.229\n",
      "总正确率为0.63\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012358, train_acc: 0.62\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.011330, train_acc: 0.66\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012019, train_acc: 0.64\n",
      "alexnet1d, trial.65:\n",
      "Epoch 0, avg test_loss: 0.009890, test_acc: 0.53\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012054, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012288, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011985, train_acc: 0.70\n",
      "alexnet1d, trial.65:\n",
      "Epoch 1, avg test_loss: 0.010029, test_acc: 0.53\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012278, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012328, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012116, train_acc: 0.55\n",
      "alexnet1d, trial.65:\n",
      "Epoch 2, avg test_loss: 0.010189, test_acc: 0.53\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011587, train_acc: 0.62\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012631, train_acc: 0.46\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012395, train_acc: 0.57\n",
      "alexnet1d, trial.65:\n",
      "Epoch 3, avg test_loss: 0.009926, test_acc: 0.53\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012042, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012168, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012116, train_acc: 0.55\n",
      "alexnet1d, trial.65:\n",
      "Epoch 4, avg test_loss: 0.010049, test_acc: 0.53\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011318, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011977, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011910, train_acc: 0.64\n",
      "alexnet1d, trial.65:\n",
      "Epoch 5, avg test_loss: 0.010096, test_acc: 0.53\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012447, train_acc: 0.50\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011973, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012114, train_acc: 0.59\n",
      "alexnet1d, trial.65:\n",
      "Epoch 6, avg test_loss: 0.010028, test_acc: 0.53\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012117, train_acc: 0.50\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012210, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011610, train_acc: 0.61\n",
      "alexnet1d, trial.65:\n",
      "Epoch 7, avg test_loss: 0.009950, test_acc: 0.53\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011915, train_acc: 0.55\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012072, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011143, train_acc: 0.64\n",
      "alexnet1d, trial.65:\n",
      "Epoch 8, avg test_loss: 0.010298, test_acc: 0.49\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010397, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.013300, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011878, train_acc: 0.68\n",
      "alexnet1d, trial.65:\n",
      "Epoch 9, avg test_loss: 0.009812, test_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011080, train_acc: 0.70\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010246, train_acc: 0.77\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.013813, train_acc: 0.50\n",
      "alexnet1d, trial.65:\n",
      "Epoch 10, avg test_loss: 0.010594, test_acc: 0.51\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010878, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011034, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010704, train_acc: 0.75\n",
      "alexnet1d, trial.65:\n",
      "Epoch 11, avg test_loss: 0.009997, test_acc: 0.60\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011289, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010159, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011758, train_acc: 0.62\n",
      "alexnet1d, trial.65:\n",
      "Epoch 12, avg test_loss: 0.011361, test_acc: 0.50\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.008663, train_acc: 0.77\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010559, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011567, train_acc: 0.66\n",
      "alexnet1d, trial.65:\n",
      "Epoch 13, avg test_loss: 0.009826, test_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009009, train_acc: 0.80\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.012310, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010162, train_acc: 0.75\n",
      "alexnet1d, trial.65:\n",
      "Epoch 14, avg test_loss: 0.011317, test_acc: 0.44\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010396, train_acc: 0.68\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009599, train_acc: 0.80\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009725, train_acc: 0.66\n",
      "alexnet1d, trial.65:\n",
      "Epoch 15, avg test_loss: 0.009968, test_acc: 0.57\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009970, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009541, train_acc: 0.68\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009529, train_acc: 0.71\n",
      "alexnet1d, trial.65:\n",
      "Epoch 16, avg test_loss: 0.011191, test_acc: 0.51\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008242, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007559, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010241, train_acc: 0.71\n",
      "alexnet1d, trial.65:\n",
      "Epoch 17, avg test_loss: 0.012449, test_acc: 0.60\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008493, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.010945, train_acc: 0.68\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008130, train_acc: 0.79\n",
      "alexnet1d, trial.65:\n",
      "Epoch 18, avg test_loss: 0.011524, test_acc: 0.54\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008505, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.006757, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007070, train_acc: 0.88\n",
      "alexnet1d, trial.65:\n",
      "Epoch 19, avg test_loss: 0.011330, test_acc: 0.50\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008926, train_acc: 0.73\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.005931, train_acc: 0.88\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007265, train_acc: 0.86\n",
      "alexnet1d, trial.65:\n",
      "Epoch 20, avg test_loss: 0.013593, test_acc: 0.50\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008245, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007734, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007372, train_acc: 0.77\n",
      "alexnet1d, trial.65:\n",
      "Epoch 21, avg test_loss: 0.013062, test_acc: 0.57\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.009174, train_acc: 0.75\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005851, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007351, train_acc: 0.82\n",
      "alexnet1d, trial.65:\n",
      "Epoch 22, avg test_loss: 0.012722, test_acc: 0.56\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007557, train_acc: 0.80\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005264, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006749, train_acc: 0.82\n",
      "alexnet1d, trial.65:\n",
      "Epoch 23, avg test_loss: 0.013419, test_acc: 0.53\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006158, train_acc: 0.80\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.007714, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005151, train_acc: 0.86\n",
      "alexnet1d, trial.65:\n",
      "Epoch 24, avg test_loss: 0.012727, test_acc: 0.59\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005911, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004296, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004963, train_acc: 0.86\n",
      "alexnet1d, trial.65:\n",
      "Epoch 25, avg test_loss: 0.016238, test_acc: 0.56\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.006126, train_acc: 0.84\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004529, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.007231, train_acc: 0.86\n",
      "alexnet1d, trial.65:\n",
      "Epoch 26, avg test_loss: 0.014299, test_acc: 0.59\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004141, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.005974, train_acc: 0.88\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.005184, train_acc: 0.88\n",
      "alexnet1d, trial.65:\n",
      "Epoch 27, avg test_loss: 0.013578, test_acc: 0.60\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003789, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004048, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.004922, train_acc: 0.93\n",
      "alexnet1d, trial.65:\n",
      "Epoch 28, avg test_loss: 0.020202, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.002720, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.003654, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.003186, train_acc: 0.91\n",
      "alexnet1d, trial.65:\n",
      "Epoch 29, avg test_loss: 0.018933, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.002122, train_acc: 0.96\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.001043, train_acc: 0.98\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.007067, train_acc: 0.88\n",
      "alexnet1d, trial.65:\n",
      "Epoch 30, avg test_loss: 0.018019, test_acc: 0.56\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.001202, train_acc: 0.98\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.001304, train_acc: 0.98\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.003218, train_acc: 0.91\n",
      "alexnet1d, trial.65:\n",
      "Epoch 31, avg test_loss: 0.022357, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.001583, train_acc: 0.98\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.001958, train_acc: 0.96\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.001661, train_acc: 0.98\n",
      "alexnet1d, trial.65:\n",
      "Epoch 32, avg test_loss: 0.021753, test_acc: 0.50\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号37个\n",
      "错误信号33个\n",
      "信号正确并预测正确的概率为0.314\n",
      "信号错误并预测正确的概率为0.186\n",
      "总正确率为0.50\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012450, train_acc: 0.48\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012373, train_acc: 0.48\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012251, train_acc: 0.57\n",
      "alexnet1d, trial.66:\n",
      "Epoch 0, avg test_loss: 0.009874, test_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012332, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012295, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012199, train_acc: 0.59\n",
      "alexnet1d, trial.66:\n",
      "Epoch 1, avg test_loss: 0.009553, test_acc: 0.66\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012387, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012251, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012253, train_acc: 0.55\n",
      "alexnet1d, trial.66:\n",
      "Epoch 2, avg test_loss: 0.009357, test_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012753, train_acc: 0.50\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012151, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012378, train_acc: 0.54\n",
      "alexnet1d, trial.66:\n",
      "Epoch 3, avg test_loss: 0.009531, test_acc: 0.66\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012052, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012937, train_acc: 0.39\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012280, train_acc: 0.55\n",
      "alexnet1d, trial.66:\n",
      "Epoch 4, avg test_loss: 0.009528, test_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012566, train_acc: 0.46\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012072, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012266, train_acc: 0.57\n",
      "alexnet1d, trial.66:\n",
      "Epoch 5, avg test_loss: 0.009393, test_acc: 0.66\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012266, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012570, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012806, train_acc: 0.46\n",
      "alexnet1d, trial.66:\n",
      "Epoch 6, avg test_loss: 0.009403, test_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012286, train_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012788, train_acc: 0.45\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011973, train_acc: 0.64\n",
      "alexnet1d, trial.66:\n",
      "Epoch 7, avg test_loss: 0.009329, test_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011696, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011520, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012246, train_acc: 0.59\n",
      "alexnet1d, trial.66:\n",
      "Epoch 8, avg test_loss: 0.008898, test_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011749, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010774, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011754, train_acc: 0.54\n",
      "alexnet1d, trial.66:\n",
      "Epoch 9, avg test_loss: 0.008977, test_acc: 0.63\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011456, train_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010115, train_acc: 0.71\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011139, train_acc: 0.71\n",
      "alexnet1d, trial.66:\n",
      "Epoch 10, avg test_loss: 0.009046, test_acc: 0.63\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010471, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011033, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010311, train_acc: 0.68\n",
      "alexnet1d, trial.66:\n",
      "Epoch 11, avg test_loss: 0.009191, test_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009998, train_acc: 0.75\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010495, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010401, train_acc: 0.66\n",
      "alexnet1d, trial.66:\n",
      "Epoch 12, avg test_loss: 0.009172, test_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010497, train_acc: 0.62\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009763, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010800, train_acc: 0.70\n",
      "alexnet1d, trial.66:\n",
      "Epoch 13, avg test_loss: 0.009548, test_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010720, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009787, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009889, train_acc: 0.68\n",
      "alexnet1d, trial.66:\n",
      "Epoch 14, avg test_loss: 0.009117, test_acc: 0.60\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008963, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008808, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010570, train_acc: 0.66\n",
      "alexnet1d, trial.66:\n",
      "Epoch 15, avg test_loss: 0.009720, test_acc: 0.64\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.012231, train_acc: 0.62\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010731, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.007255, train_acc: 0.82\n",
      "alexnet1d, trial.66:\n",
      "Epoch 16, avg test_loss: 0.010056, test_acc: 0.60\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007966, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008815, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007138, train_acc: 0.82\n",
      "alexnet1d, trial.66:\n",
      "Epoch 17, avg test_loss: 0.009835, test_acc: 0.66\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008804, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007301, train_acc: 0.86\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.010258, train_acc: 0.70\n",
      "alexnet1d, trial.66:\n",
      "Epoch 18, avg test_loss: 0.010643, test_acc: 0.59\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.010025, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007439, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.011787, train_acc: 0.66\n",
      "alexnet1d, trial.66:\n",
      "Epoch 19, avg test_loss: 0.011401, test_acc: 0.57\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.009118, train_acc: 0.73\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008260, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.010868, train_acc: 0.64\n",
      "alexnet1d, trial.66:\n",
      "Epoch 20, avg test_loss: 0.009508, test_acc: 0.60\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007402, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.009260, train_acc: 0.73\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006338, train_acc: 0.88\n",
      "alexnet1d, trial.66:\n",
      "Epoch 21, avg test_loss: 0.011179, test_acc: 0.60\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006603, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007193, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.005640, train_acc: 0.86\n",
      "alexnet1d, trial.66:\n",
      "Epoch 22, avg test_loss: 0.011123, test_acc: 0.63\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006611, train_acc: 0.82\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.004396, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.004269, train_acc: 0.93\n",
      "alexnet1d, trial.66:\n",
      "Epoch 23, avg test_loss: 0.012882, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005019, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005397, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005516, train_acc: 0.88\n",
      "alexnet1d, trial.66:\n",
      "Epoch 24, avg test_loss: 0.016152, test_acc: 0.64\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.006896, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.002514, train_acc: 0.96\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006444, train_acc: 0.82\n",
      "alexnet1d, trial.66:\n",
      "Epoch 25, avg test_loss: 0.015501, test_acc: 0.59\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004522, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004865, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.005606, train_acc: 0.89\n",
      "alexnet1d, trial.66:\n",
      "Epoch 26, avg test_loss: 0.013591, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.002954, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.003594, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.005556, train_acc: 0.86\n",
      "alexnet1d, trial.66:\n",
      "Epoch 27, avg test_loss: 0.017902, test_acc: 0.59\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003336, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.007530, train_acc: 0.88\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.004440, train_acc: 0.89\n",
      "alexnet1d, trial.66:\n",
      "Epoch 28, avg test_loss: 0.013854, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.003498, train_acc: 0.89\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.005632, train_acc: 0.84\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.005270, train_acc: 0.91\n",
      "alexnet1d, trial.66:\n",
      "Epoch 29, avg test_loss: 0.013140, test_acc: 0.67\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号46个\n",
      "错误信号24个\n",
      "信号正确并预测正确的概率为0.457\n",
      "信号错误并预测正确的概率为0.214\n",
      "总正确率为0.67\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012364, train_acc: 0.46\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012417, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012165, train_acc: 0.62\n",
      "alexnet1d, trial.67:\n",
      "Epoch 0, avg test_loss: 0.009569, test_acc: 0.66\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012109, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012896, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012240, train_acc: 0.57\n",
      "alexnet1d, trial.67:\n",
      "Epoch 1, avg test_loss: 0.009592, test_acc: 0.66\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012205, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012449, train_acc: 0.52\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012178, train_acc: 0.57\n",
      "alexnet1d, trial.67:\n",
      "Epoch 2, avg test_loss: 0.009436, test_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011828, train_acc: 0.62\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012505, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.013352, train_acc: 0.45\n",
      "alexnet1d, trial.67:\n",
      "Epoch 3, avg test_loss: 0.009456, test_acc: 0.66\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012142, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012130, train_acc: 0.66\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012296, train_acc: 0.61\n",
      "alexnet1d, trial.67:\n",
      "Epoch 4, avg test_loss: 0.009718, test_acc: 0.70\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012301, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012109, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012081, train_acc: 0.52\n",
      "alexnet1d, trial.67:\n",
      "Epoch 5, avg test_loss: 0.009350, test_acc: 0.66\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011643, train_acc: 0.66\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012628, train_acc: 0.50\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012352, train_acc: 0.52\n",
      "alexnet1d, trial.67:\n",
      "Epoch 6, avg test_loss: 0.008890, test_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012746, train_acc: 0.48\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012226, train_acc: 0.50\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011900, train_acc: 0.54\n",
      "alexnet1d, trial.67:\n",
      "Epoch 7, avg test_loss: 0.009103, test_acc: 0.69\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011121, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.014117, train_acc: 0.52\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011903, train_acc: 0.55\n",
      "alexnet1d, trial.67:\n",
      "Epoch 8, avg test_loss: 0.008696, test_acc: 0.74\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011642, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012114, train_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011107, train_acc: 0.62\n",
      "alexnet1d, trial.67:\n",
      "Epoch 9, avg test_loss: 0.009087, test_acc: 0.69\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011649, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010834, train_acc: 0.71\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010971, train_acc: 0.64\n",
      "alexnet1d, trial.67:\n",
      "Epoch 10, avg test_loss: 0.008426, test_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010390, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.012436, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011025, train_acc: 0.66\n",
      "alexnet1d, trial.67:\n",
      "Epoch 11, avg test_loss: 0.009094, test_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011525, train_acc: 0.55\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011483, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010738, train_acc: 0.66\n",
      "alexnet1d, trial.67:\n",
      "Epoch 12, avg test_loss: 0.008157, test_acc: 0.76\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011032, train_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009915, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010565, train_acc: 0.68\n",
      "alexnet1d, trial.67:\n",
      "Epoch 13, avg test_loss: 0.008377, test_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010142, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010509, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.008363, train_acc: 0.80\n",
      "alexnet1d, trial.67:\n",
      "Epoch 14, avg test_loss: 0.008332, test_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009775, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010343, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009880, train_acc: 0.73\n",
      "alexnet1d, trial.67:\n",
      "Epoch 15, avg test_loss: 0.008728, test_acc: 0.74\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008884, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009685, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.007717, train_acc: 0.82\n",
      "alexnet1d, trial.67:\n",
      "Epoch 16, avg test_loss: 0.009634, test_acc: 0.71\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007934, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007328, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008515, train_acc: 0.80\n",
      "alexnet1d, trial.67:\n",
      "Epoch 17, avg test_loss: 0.010551, test_acc: 0.60\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007597, train_acc: 0.86\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008244, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007236, train_acc: 0.82\n",
      "alexnet1d, trial.67:\n",
      "Epoch 18, avg test_loss: 0.010794, test_acc: 0.67\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006671, train_acc: 0.88\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009628, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009672, train_acc: 0.79\n",
      "alexnet1d, trial.67:\n",
      "Epoch 19, avg test_loss: 0.010953, test_acc: 0.66\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008414, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007235, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.005947, train_acc: 0.88\n",
      "alexnet1d, trial.67:\n",
      "Epoch 20, avg test_loss: 0.010850, test_acc: 0.67\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006563, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007271, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.009173, train_acc: 0.84\n",
      "alexnet1d, trial.67:\n",
      "Epoch 21, avg test_loss: 0.011474, test_acc: 0.69\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005879, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.008400, train_acc: 0.77\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.004703, train_acc: 0.91\n",
      "alexnet1d, trial.67:\n",
      "Epoch 22, avg test_loss: 0.012924, test_acc: 0.66\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.004691, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005164, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.004391, train_acc: 0.89\n",
      "alexnet1d, trial.67:\n",
      "Epoch 23, avg test_loss: 0.012108, test_acc: 0.70\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005312, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004017, train_acc: 0.93\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.004843, train_acc: 0.89\n",
      "alexnet1d, trial.67:\n",
      "Epoch 24, avg test_loss: 0.014664, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.002892, train_acc: 0.95\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004074, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004039, train_acc: 0.93\n",
      "alexnet1d, trial.67:\n",
      "Epoch 25, avg test_loss: 0.016095, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号46个\n",
      "错误信号24个\n",
      "信号正确并预测正确的概率为0.429\n",
      "信号错误并预测正确的概率为0.2\n",
      "总正确率为0.63\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012335, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.015496, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012269, train_acc: 0.61\n",
      "alexnet1d, trial.68:\n",
      "Epoch 0, avg test_loss: 0.009871, test_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012390, train_acc: 0.52\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012303, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012359, train_acc: 0.52\n",
      "alexnet1d, trial.68:\n",
      "Epoch 1, avg test_loss: 0.009813, test_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012300, train_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012451, train_acc: 0.52\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011663, train_acc: 0.66\n",
      "alexnet1d, trial.68:\n",
      "Epoch 2, avg test_loss: 0.010008, test_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012416, train_acc: 0.50\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011840, train_acc: 0.62\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.013319, train_acc: 0.50\n",
      "alexnet1d, trial.68:\n",
      "Epoch 3, avg test_loss: 0.010078, test_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012438, train_acc: 0.52\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011854, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012276, train_acc: 0.55\n",
      "alexnet1d, trial.68:\n",
      "Epoch 4, avg test_loss: 0.009765, test_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012151, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012234, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011996, train_acc: 0.57\n",
      "alexnet1d, trial.68:\n",
      "Epoch 5, avg test_loss: 0.009759, test_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011715, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012279, train_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011514, train_acc: 0.66\n",
      "alexnet1d, trial.68:\n",
      "Epoch 6, avg test_loss: 0.009851, test_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012053, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011519, train_acc: 0.55\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011787, train_acc: 0.59\n",
      "alexnet1d, trial.68:\n",
      "Epoch 7, avg test_loss: 0.010036, test_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011990, train_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011561, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011460, train_acc: 0.62\n",
      "alexnet1d, trial.68:\n",
      "Epoch 8, avg test_loss: 0.010186, test_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011941, train_acc: 0.55\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011339, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011462, train_acc: 0.64\n",
      "alexnet1d, trial.68:\n",
      "Epoch 9, avg test_loss: 0.010740, test_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010828, train_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011041, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012419, train_acc: 0.52\n",
      "alexnet1d, trial.68:\n",
      "Epoch 10, avg test_loss: 0.009242, test_acc: 0.63\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010887, train_acc: 0.77\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010937, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010793, train_acc: 0.64\n",
      "alexnet1d, trial.68:\n",
      "Epoch 11, avg test_loss: 0.011302, test_acc: 0.47\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010673, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010707, train_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011156, train_acc: 0.68\n",
      "alexnet1d, trial.68:\n",
      "Epoch 12, avg test_loss: 0.010692, test_acc: 0.53\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011476, train_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011329, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009993, train_acc: 0.79\n",
      "alexnet1d, trial.68:\n",
      "Epoch 13, avg test_loss: 0.010605, test_acc: 0.59\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010834, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010255, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009684, train_acc: 0.71\n",
      "alexnet1d, trial.68:\n",
      "Epoch 14, avg test_loss: 0.012503, test_acc: 0.53\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010078, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.011578, train_acc: 0.68\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008000, train_acc: 0.82\n",
      "alexnet1d, trial.68:\n",
      "Epoch 15, avg test_loss: 0.012745, test_acc: 0.54\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008235, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.011136, train_acc: 0.80\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009004, train_acc: 0.75\n",
      "alexnet1d, trial.68:\n",
      "Epoch 16, avg test_loss: 0.011116, test_acc: 0.51\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.011098, train_acc: 0.70\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009402, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007994, train_acc: 0.82\n",
      "alexnet1d, trial.68:\n",
      "Epoch 17, avg test_loss: 0.010844, test_acc: 0.57\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008934, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009314, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007884, train_acc: 0.79\n",
      "alexnet1d, trial.68:\n",
      "Epoch 18, avg test_loss: 0.012411, test_acc: 0.56\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007750, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.006724, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.010021, train_acc: 0.75\n",
      "alexnet1d, trial.68:\n",
      "Epoch 19, avg test_loss: 0.014152, test_acc: 0.56\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006291, train_acc: 0.91\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.009602, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008112, train_acc: 0.79\n",
      "alexnet1d, trial.68:\n",
      "Epoch 20, avg test_loss: 0.013918, test_acc: 0.53\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.005889, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006728, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.005907, train_acc: 0.84\n",
      "alexnet1d, trial.68:\n",
      "Epoch 21, avg test_loss: 0.015146, test_acc: 0.50\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.008144, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006458, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.005537, train_acc: 0.86\n",
      "alexnet1d, trial.68:\n",
      "Epoch 22, avg test_loss: 0.016182, test_acc: 0.56\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005594, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.004868, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.009106, train_acc: 0.79\n",
      "alexnet1d, trial.68:\n",
      "Epoch 23, avg test_loss: 0.017838, test_acc: 0.54\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006194, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006299, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005204, train_acc: 0.89\n",
      "alexnet1d, trial.68:\n",
      "Epoch 24, avg test_loss: 0.017035, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005289, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004179, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006972, train_acc: 0.84\n",
      "alexnet1d, trial.68:\n",
      "Epoch 25, avg test_loss: 0.015467, test_acc: 0.56\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005403, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.008781, train_acc: 0.80\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.006340, train_acc: 0.84\n",
      "alexnet1d, trial.68:\n",
      "Epoch 26, avg test_loss: 0.017712, test_acc: 0.57\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.005431, train_acc: 0.88\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004203, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.005822, train_acc: 0.88\n",
      "alexnet1d, trial.68:\n",
      "Epoch 27, avg test_loss: 0.019371, test_acc: 0.54\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.002625, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.006493, train_acc: 0.80\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.004122, train_acc: 0.93\n",
      "alexnet1d, trial.68:\n",
      "Epoch 28, avg test_loss: 0.023248, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.002963, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.003919, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.003819, train_acc: 0.93\n",
      "alexnet1d, trial.68:\n",
      "Epoch 29, avg test_loss: 0.020253, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.004077, train_acc: 0.93\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.004889, train_acc: 0.91\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.002474, train_acc: 0.96\n",
      "alexnet1d, trial.68:\n",
      "Epoch 30, avg test_loss: 0.021526, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号40个\n",
      "错误信号30个\n",
      "信号正确并预测正确的概率为0.357\n",
      "信号错误并预测正确的概率为0.214\n",
      "总正确率为0.57\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012260, train_acc: 0.61\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012694, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012220, train_acc: 0.57\n",
      "alexnet1d, trial.69:\n",
      "Epoch 0, avg test_loss: 0.009916, test_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012655, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012266, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012030, train_acc: 0.61\n",
      "alexnet1d, trial.69:\n",
      "Epoch 1, avg test_loss: 0.009973, test_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012758, train_acc: 0.48\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011986, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.013628, train_acc: 0.48\n",
      "alexnet1d, trial.69:\n",
      "Epoch 2, avg test_loss: 0.010204, test_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.013207, train_acc: 0.50\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011876, train_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012062, train_acc: 0.55\n",
      "alexnet1d, trial.69:\n",
      "Epoch 3, avg test_loss: 0.009836, test_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012663, train_acc: 0.43\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012148, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011917, train_acc: 0.66\n",
      "alexnet1d, trial.69:\n",
      "Epoch 4, avg test_loss: 0.009829, test_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012870, train_acc: 0.43\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011318, train_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011103, train_acc: 0.68\n",
      "alexnet1d, trial.69:\n",
      "Epoch 5, avg test_loss: 0.010049, test_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011779, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012364, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011454, train_acc: 0.62\n",
      "alexnet1d, trial.69:\n",
      "Epoch 6, avg test_loss: 0.009865, test_acc: 0.56\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012049, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011174, train_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011842, train_acc: 0.62\n",
      "alexnet1d, trial.69:\n",
      "Epoch 7, avg test_loss: 0.009811, test_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011664, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011433, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.010971, train_acc: 0.66\n",
      "alexnet1d, trial.69:\n",
      "Epoch 8, avg test_loss: 0.009696, test_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011714, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011627, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010690, train_acc: 0.62\n",
      "alexnet1d, trial.69:\n",
      "Epoch 9, avg test_loss: 0.009414, test_acc: 0.60\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010895, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011182, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012281, train_acc: 0.61\n",
      "alexnet1d, trial.69:\n",
      "Epoch 10, avg test_loss: 0.009132, test_acc: 0.60\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010028, train_acc: 0.75\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011046, train_acc: 0.55\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010977, train_acc: 0.64\n",
      "alexnet1d, trial.69:\n",
      "Epoch 11, avg test_loss: 0.009361, test_acc: 0.54\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010104, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.009581, train_acc: 0.79\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.013684, train_acc: 0.55\n",
      "alexnet1d, trial.69:\n",
      "Epoch 12, avg test_loss: 0.009767, test_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010107, train_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009905, train_acc: 0.75\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011154, train_acc: 0.66\n",
      "alexnet1d, trial.69:\n",
      "Epoch 13, avg test_loss: 0.009454, test_acc: 0.59\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.011777, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010462, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011125, train_acc: 0.57\n",
      "alexnet1d, trial.69:\n",
      "Epoch 14, avg test_loss: 0.009474, test_acc: 0.60\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010026, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009521, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009590, train_acc: 0.73\n",
      "alexnet1d, trial.69:\n",
      "Epoch 15, avg test_loss: 0.009342, test_acc: 0.63\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009852, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.007783, train_acc: 0.84\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009772, train_acc: 0.68\n",
      "alexnet1d, trial.69:\n",
      "Epoch 16, avg test_loss: 0.009295, test_acc: 0.64\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009569, train_acc: 0.71\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007582, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010352, train_acc: 0.73\n",
      "alexnet1d, trial.69:\n",
      "Epoch 17, avg test_loss: 0.010562, test_acc: 0.57\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009968, train_acc: 0.68\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008602, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008200, train_acc: 0.77\n",
      "alexnet1d, trial.69:\n",
      "Epoch 18, avg test_loss: 0.009685, test_acc: 0.67\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009688, train_acc: 0.71\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007546, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008166, train_acc: 0.73\n",
      "alexnet1d, trial.69:\n",
      "Epoch 19, avg test_loss: 0.011141, test_acc: 0.59\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.010922, train_acc: 0.71\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.006880, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008755, train_acc: 0.77\n",
      "alexnet1d, trial.69:\n",
      "Epoch 20, avg test_loss: 0.010442, test_acc: 0.61\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.009386, train_acc: 0.73\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.008177, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.009266, train_acc: 0.71\n",
      "alexnet1d, trial.69:\n",
      "Epoch 21, avg test_loss: 0.009959, test_acc: 0.60\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007000, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006755, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008382, train_acc: 0.77\n",
      "alexnet1d, trial.69:\n",
      "Epoch 22, avg test_loss: 0.010808, test_acc: 0.56\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006710, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006751, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.008082, train_acc: 0.79\n",
      "alexnet1d, trial.69:\n",
      "Epoch 23, avg test_loss: 0.013124, test_acc: 0.61\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005936, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.010048, train_acc: 0.70\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.008898, train_acc: 0.79\n",
      "alexnet1d, trial.69:\n",
      "Epoch 24, avg test_loss: 0.015114, test_acc: 0.44\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.008512, train_acc: 0.79\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005488, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.008510, train_acc: 0.75\n",
      "alexnet1d, trial.69:\n",
      "Epoch 25, avg test_loss: 0.014086, test_acc: 0.60\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005880, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005732, train_acc: 0.84\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.006367, train_acc: 0.82\n",
      "alexnet1d, trial.69:\n",
      "Epoch 26, avg test_loss: 0.013331, test_acc: 0.59\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.006970, train_acc: 0.80\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004967, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.008483, train_acc: 0.77\n",
      "alexnet1d, trial.69:\n",
      "Epoch 27, avg test_loss: 0.012827, test_acc: 0.61\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004297, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004650, train_acc: 0.88\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.006795, train_acc: 0.84\n",
      "alexnet1d, trial.69:\n",
      "Epoch 28, avg test_loss: 0.012685, test_acc: 0.57\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.005439, train_acc: 0.89\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.003602, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.003618, train_acc: 0.93\n",
      "alexnet1d, trial.69:\n",
      "Epoch 29, avg test_loss: 0.013958, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.004749, train_acc: 0.88\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.004230, train_acc: 0.88\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.002220, train_acc: 0.96\n",
      "alexnet1d, trial.69:\n",
      "Epoch 30, avg test_loss: 0.015421, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.003701, train_acc: 0.89\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.003078, train_acc: 0.93\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.002417, train_acc: 0.96\n",
      "alexnet1d, trial.69:\n",
      "Epoch 31, avg test_loss: 0.017033, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.002796, train_acc: 0.96\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.002485, train_acc: 0.95\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.004005, train_acc: 0.88\n",
      "alexnet1d, trial.69:\n",
      "Epoch 32, avg test_loss: 0.018733, test_acc: 0.59\n",
      "Train Epoch 33, lr: 0.000377, 0/280, avg loss: 0.002236, train_acc: 0.93\n",
      "Train Epoch 33, lr: 0.000377, 112/280, avg loss: 0.003594, train_acc: 0.91\n",
      "Train Epoch 33, lr: 0.000377, 224/280, avg loss: 0.000914, train_acc: 1.00\n",
      "alexnet1d, trial.69:\n",
      "Epoch 33, avg test_loss: 0.022338, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号38个\n",
      "错误信号32个\n",
      "信号正确并预测正确的概率为0.314\n",
      "信号错误并预测正确的概率为0.243\n",
      "总正确率为0.56\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012376, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012232, train_acc: 0.61\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012085, train_acc: 0.57\n",
      "alexnet1d, trial.70:\n",
      "Epoch 0, avg test_loss: 0.009708, test_acc: 0.60\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012342, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012298, train_acc: 0.62\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012269, train_acc: 0.61\n",
      "alexnet1d, trial.70:\n",
      "Epoch 1, avg test_loss: 0.009748, test_acc: 0.60\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011640, train_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012108, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012295, train_acc: 0.54\n",
      "alexnet1d, trial.70:\n",
      "Epoch 2, avg test_loss: 0.009758, test_acc: 0.60\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012066, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012340, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012078, train_acc: 0.61\n",
      "alexnet1d, trial.70:\n",
      "Epoch 3, avg test_loss: 0.009669, test_acc: 0.60\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012295, train_acc: 0.52\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012361, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012763, train_acc: 0.55\n",
      "alexnet1d, trial.70:\n",
      "Epoch 4, avg test_loss: 0.009612, test_acc: 0.60\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012782, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011952, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012473, train_acc: 0.48\n",
      "alexnet1d, trial.70:\n",
      "Epoch 5, avg test_loss: 0.009708, test_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011795, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012031, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011956, train_acc: 0.54\n",
      "alexnet1d, trial.70:\n",
      "Epoch 6, avg test_loss: 0.009856, test_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011678, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011597, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012206, train_acc: 0.59\n",
      "alexnet1d, trial.70:\n",
      "Epoch 7, avg test_loss: 0.009974, test_acc: 0.53\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011040, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011575, train_acc: 0.68\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011241, train_acc: 0.70\n",
      "alexnet1d, trial.70:\n",
      "Epoch 8, avg test_loss: 0.009407, test_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011115, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012843, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010669, train_acc: 0.68\n",
      "alexnet1d, trial.70:\n",
      "Epoch 9, avg test_loss: 0.010339, test_acc: 0.51\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010826, train_acc: 0.70\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011027, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012054, train_acc: 0.62\n",
      "alexnet1d, trial.70:\n",
      "Epoch 10, avg test_loss: 0.009714, test_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.012067, train_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010413, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.013027, train_acc: 0.59\n",
      "alexnet1d, trial.70:\n",
      "Epoch 11, avg test_loss: 0.009587, test_acc: 0.60\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.012118, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010911, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.012227, train_acc: 0.57\n",
      "alexnet1d, trial.70:\n",
      "Epoch 12, avg test_loss: 0.009906, test_acc: 0.60\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011370, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011321, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011811, train_acc: 0.59\n",
      "alexnet1d, trial.70:\n",
      "Epoch 13, avg test_loss: 0.009555, test_acc: 0.51\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010916, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010878, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011615, train_acc: 0.62\n",
      "alexnet1d, trial.70:\n",
      "Epoch 14, avg test_loss: 0.009702, test_acc: 0.61\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010834, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010181, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011573, train_acc: 0.66\n",
      "alexnet1d, trial.70:\n",
      "Epoch 15, avg test_loss: 0.010332, test_acc: 0.63\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010421, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010863, train_acc: 0.68\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009821, train_acc: 0.75\n",
      "alexnet1d, trial.70:\n",
      "Epoch 16, avg test_loss: 0.010033, test_acc: 0.56\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009754, train_acc: 0.70\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010023, train_acc: 0.71\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009746, train_acc: 0.75\n",
      "alexnet1d, trial.70:\n",
      "Epoch 17, avg test_loss: 0.010899, test_acc: 0.63\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009657, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008522, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.010306, train_acc: 0.70\n",
      "alexnet1d, trial.70:\n",
      "Epoch 18, avg test_loss: 0.010668, test_acc: 0.57\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008626, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009720, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008670, train_acc: 0.79\n",
      "alexnet1d, trial.70:\n",
      "Epoch 19, avg test_loss: 0.010646, test_acc: 0.57\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008682, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008532, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009379, train_acc: 0.77\n",
      "alexnet1d, trial.70:\n",
      "Epoch 20, avg test_loss: 0.011325, test_acc: 0.59\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008508, train_acc: 0.75\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.009276, train_acc: 0.77\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006883, train_acc: 0.84\n",
      "alexnet1d, trial.70:\n",
      "Epoch 21, avg test_loss: 0.013622, test_acc: 0.59\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007863, train_acc: 0.75\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.008273, train_acc: 0.75\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007589, train_acc: 0.80\n",
      "alexnet1d, trial.70:\n",
      "Epoch 22, avg test_loss: 0.013037, test_acc: 0.53\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007659, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005838, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.008802, train_acc: 0.75\n",
      "alexnet1d, trial.70:\n",
      "Epoch 23, avg test_loss: 0.014102, test_acc: 0.59\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006358, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.008375, train_acc: 0.82\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006938, train_acc: 0.84\n",
      "alexnet1d, trial.70:\n",
      "Epoch 24, avg test_loss: 0.015620, test_acc: 0.53\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004922, train_acc: 0.95\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006347, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.007720, train_acc: 0.79\n",
      "alexnet1d, trial.70:\n",
      "Epoch 25, avg test_loss: 0.019362, test_acc: 0.53\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.003318, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005472, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004507, train_acc: 0.95\n",
      "alexnet1d, trial.70:\n",
      "Epoch 26, avg test_loss: 0.017928, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004975, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.005122, train_acc: 0.88\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004167, train_acc: 0.89\n",
      "alexnet1d, trial.70:\n",
      "Epoch 27, avg test_loss: 0.024323, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.005738, train_acc: 0.86\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.006601, train_acc: 0.86\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.009005, train_acc: 0.75\n",
      "alexnet1d, trial.70:\n",
      "Epoch 28, avg test_loss: 0.019656, test_acc: 0.60\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.006139, train_acc: 0.86\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.006348, train_acc: 0.86\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.004196, train_acc: 0.91\n",
      "alexnet1d, trial.70:\n",
      "Epoch 29, avg test_loss: 0.017533, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.005458, train_acc: 0.93\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.005682, train_acc: 0.86\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.005968, train_acc: 0.91\n",
      "alexnet1d, trial.70:\n",
      "Epoch 30, avg test_loss: 0.018809, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号42个\n",
      "错误信号28个\n",
      "信号正确并预测正确的概率为0.371\n",
      "信号错误并预测正确的概率为0.171\n",
      "总正确率为0.54\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012401, train_acc: 0.43\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012038, train_acc: 0.61\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012756, train_acc: 0.50\n",
      "alexnet1d, trial.71:\n",
      "Epoch 0, avg test_loss: 0.009779, test_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011930, train_acc: 0.70\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.011931, train_acc: 0.66\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012786, train_acc: 0.48\n",
      "alexnet1d, trial.71:\n",
      "Epoch 1, avg test_loss: 0.009707, test_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011734, train_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012407, train_acc: 0.52\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012397, train_acc: 0.54\n",
      "alexnet1d, trial.71:\n",
      "Epoch 2, avg test_loss: 0.009724, test_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011790, train_acc: 0.62\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012290, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012144, train_acc: 0.59\n",
      "alexnet1d, trial.71:\n",
      "Epoch 3, avg test_loss: 0.009713, test_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012404, train_acc: 0.50\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011737, train_acc: 0.68\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.013714, train_acc: 0.48\n",
      "alexnet1d, trial.71:\n",
      "Epoch 4, avg test_loss: 0.009849, test_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012080, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012342, train_acc: 0.50\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011523, train_acc: 0.75\n",
      "alexnet1d, trial.71:\n",
      "Epoch 5, avg test_loss: 0.009628, test_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012060, train_acc: 0.52\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011188, train_acc: 0.66\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012009, train_acc: 0.61\n",
      "alexnet1d, trial.71:\n",
      "Epoch 6, avg test_loss: 0.009916, test_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011626, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011765, train_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012262, train_acc: 0.59\n",
      "alexnet1d, trial.71:\n",
      "Epoch 7, avg test_loss: 0.010408, test_acc: 0.53\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010983, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.013099, train_acc: 0.48\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.010568, train_acc: 0.64\n",
      "alexnet1d, trial.71:\n",
      "Epoch 8, avg test_loss: 0.009830, test_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010842, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.013278, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010432, train_acc: 0.70\n",
      "alexnet1d, trial.71:\n",
      "Epoch 9, avg test_loss: 0.009968, test_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011421, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010372, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012065, train_acc: 0.48\n",
      "alexnet1d, trial.71:\n",
      "Epoch 10, avg test_loss: 0.010003, test_acc: 0.60\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011341, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010679, train_acc: 0.82\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.012178, train_acc: 0.57\n",
      "alexnet1d, trial.71:\n",
      "Epoch 11, avg test_loss: 0.009918, test_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.012020, train_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010513, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011445, train_acc: 0.61\n",
      "alexnet1d, trial.71:\n",
      "Epoch 12, avg test_loss: 0.009686, test_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010199, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011595, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010401, train_acc: 0.75\n",
      "alexnet1d, trial.71:\n",
      "Epoch 13, avg test_loss: 0.010620, test_acc: 0.57\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010388, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010509, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009692, train_acc: 0.71\n",
      "alexnet1d, trial.71:\n",
      "Epoch 14, avg test_loss: 0.010351, test_acc: 0.50\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009786, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010352, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009320, train_acc: 0.73\n",
      "alexnet1d, trial.71:\n",
      "Epoch 15, avg test_loss: 0.010032, test_acc: 0.56\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.012255, train_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.014202, train_acc: 0.62\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009372, train_acc: 0.75\n",
      "alexnet1d, trial.71:\n",
      "Epoch 16, avg test_loss: 0.010435, test_acc: 0.53\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.011809, train_acc: 0.59\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009674, train_acc: 0.71\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009451, train_acc: 0.75\n",
      "alexnet1d, trial.71:\n",
      "Epoch 17, avg test_loss: 0.010845, test_acc: 0.53\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009673, train_acc: 0.68\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009401, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009887, train_acc: 0.73\n",
      "alexnet1d, trial.71:\n",
      "Epoch 18, avg test_loss: 0.011014, test_acc: 0.54\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007963, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.010569, train_acc: 0.66\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009688, train_acc: 0.73\n",
      "alexnet1d, trial.71:\n",
      "Epoch 19, avg test_loss: 0.011348, test_acc: 0.56\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007766, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007696, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008134, train_acc: 0.84\n",
      "alexnet1d, trial.71:\n",
      "Epoch 20, avg test_loss: 0.013596, test_acc: 0.53\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007252, train_acc: 0.77\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.008258, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006545, train_acc: 0.84\n",
      "alexnet1d, trial.71:\n",
      "Epoch 21, avg test_loss: 0.013485, test_acc: 0.60\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006897, train_acc: 0.77\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005075, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006983, train_acc: 0.86\n",
      "alexnet1d, trial.71:\n",
      "Epoch 22, avg test_loss: 0.015333, test_acc: 0.57\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.004901, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007702, train_acc: 0.82\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.005943, train_acc: 0.86\n",
      "alexnet1d, trial.71:\n",
      "Epoch 23, avg test_loss: 0.017671, test_acc: 0.60\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005857, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006343, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005820, train_acc: 0.91\n",
      "alexnet1d, trial.71:\n",
      "Epoch 24, avg test_loss: 0.019611, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.003663, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004651, train_acc: 0.95\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006383, train_acc: 0.88\n",
      "alexnet1d, trial.71:\n",
      "Epoch 25, avg test_loss: 0.019375, test_acc: 0.54\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.003344, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005968, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.003164, train_acc: 0.93\n",
      "alexnet1d, trial.71:\n",
      "Epoch 26, avg test_loss: 0.021195, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.002771, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.001235, train_acc: 1.00\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.005875, train_acc: 0.89\n",
      "alexnet1d, trial.71:\n",
      "Epoch 27, avg test_loss: 0.023863, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004383, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.002206, train_acc: 0.98\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.001638, train_acc: 0.98\n",
      "alexnet1d, trial.71:\n",
      "Epoch 28, avg test_loss: 0.025735, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号41个\n",
      "错误信号29个\n",
      "信号正确并预测正确的概率为0.343\n",
      "信号错误并预测正确的概率为0.214\n",
      "总正确率为0.56\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012440, train_acc: 0.41\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.014047, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012253, train_acc: 0.59\n",
      "alexnet1d, trial.72:\n",
      "Epoch 0, avg test_loss: 0.009837, test_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012325, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012394, train_acc: 0.48\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012267, train_acc: 0.68\n",
      "alexnet1d, trial.72:\n",
      "Epoch 1, avg test_loss: 0.009802, test_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012303, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011970, train_acc: 0.66\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012170, train_acc: 0.55\n",
      "alexnet1d, trial.72:\n",
      "Epoch 2, avg test_loss: 0.009820, test_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012519, train_acc: 0.55\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012292, train_acc: 0.55\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012099, train_acc: 0.55\n",
      "alexnet1d, trial.72:\n",
      "Epoch 3, avg test_loss: 0.009980, test_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011705, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012075, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012046, train_acc: 0.54\n",
      "alexnet1d, trial.72:\n",
      "Epoch 4, avg test_loss: 0.010303, test_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012130, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012288, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011830, train_acc: 0.59\n",
      "alexnet1d, trial.72:\n",
      "Epoch 5, avg test_loss: 0.010770, test_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012082, train_acc: 0.66\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011540, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012117, train_acc: 0.54\n",
      "alexnet1d, trial.72:\n",
      "Epoch 6, avg test_loss: 0.010768, test_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012628, train_acc: 0.52\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011545, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011338, train_acc: 0.59\n",
      "alexnet1d, trial.72:\n",
      "Epoch 7, avg test_loss: 0.011007, test_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012913, train_acc: 0.50\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011363, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011040, train_acc: 0.64\n",
      "alexnet1d, trial.72:\n",
      "Epoch 8, avg test_loss: 0.011648, test_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012908, train_acc: 0.48\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011357, train_acc: 0.75\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011530, train_acc: 0.61\n",
      "alexnet1d, trial.72:\n",
      "Epoch 9, avg test_loss: 0.011071, test_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.012119, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011314, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010458, train_acc: 0.70\n",
      "alexnet1d, trial.72:\n",
      "Epoch 10, avg test_loss: 0.011367, test_acc: 0.56\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010924, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011226, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.013000, train_acc: 0.59\n",
      "alexnet1d, trial.72:\n",
      "Epoch 11, avg test_loss: 0.011093, test_acc: 0.53\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011251, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.012261, train_acc: 0.46\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011658, train_acc: 0.61\n",
      "alexnet1d, trial.72:\n",
      "Epoch 12, avg test_loss: 0.010701, test_acc: 0.63\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.012303, train_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011972, train_acc: 0.55\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010379, train_acc: 0.66\n",
      "alexnet1d, trial.72:\n",
      "Epoch 13, avg test_loss: 0.011120, test_acc: 0.64\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010071, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009914, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011858, train_acc: 0.64\n",
      "alexnet1d, trial.72:\n",
      "Epoch 14, avg test_loss: 0.011224, test_acc: 0.53\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.011479, train_acc: 0.61\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010058, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010829, train_acc: 0.62\n",
      "alexnet1d, trial.72:\n",
      "Epoch 15, avg test_loss: 0.011444, test_acc: 0.60\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010128, train_acc: 0.64\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009898, train_acc: 0.68\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.012583, train_acc: 0.64\n",
      "alexnet1d, trial.72:\n",
      "Epoch 16, avg test_loss: 0.012076, test_acc: 0.61\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.010320, train_acc: 0.68\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009333, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010465, train_acc: 0.68\n",
      "alexnet1d, trial.72:\n",
      "Epoch 17, avg test_loss: 0.011116, test_acc: 0.53\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.010513, train_acc: 0.64\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.010011, train_acc: 0.71\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008329, train_acc: 0.82\n",
      "alexnet1d, trial.72:\n",
      "Epoch 18, avg test_loss: 0.010961, test_acc: 0.61\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.010249, train_acc: 0.66\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.006856, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009778, train_acc: 0.79\n",
      "alexnet1d, trial.72:\n",
      "Epoch 19, avg test_loss: 0.014254, test_acc: 0.60\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007539, train_acc: 0.77\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008494, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009281, train_acc: 0.79\n",
      "alexnet1d, trial.72:\n",
      "Epoch 20, avg test_loss: 0.015003, test_acc: 0.54\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008978, train_acc: 0.73\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007535, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.009600, train_acc: 0.71\n",
      "alexnet1d, trial.72:\n",
      "Epoch 21, avg test_loss: 0.014843, test_acc: 0.63\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.008876, train_acc: 0.75\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007621, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006223, train_acc: 0.84\n",
      "alexnet1d, trial.72:\n",
      "Epoch 22, avg test_loss: 0.015396, test_acc: 0.63\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006091, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006997, train_acc: 0.82\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.010035, train_acc: 0.71\n",
      "alexnet1d, trial.72:\n",
      "Epoch 23, avg test_loss: 0.019901, test_acc: 0.60\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006565, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.007313, train_acc: 0.77\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006886, train_acc: 0.84\n",
      "alexnet1d, trial.72:\n",
      "Epoch 24, avg test_loss: 0.015946, test_acc: 0.60\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.006827, train_acc: 0.80\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006691, train_acc: 0.82\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005947, train_acc: 0.82\n",
      "alexnet1d, trial.72:\n",
      "Epoch 25, avg test_loss: 0.019658, test_acc: 0.54\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.007282, train_acc: 0.84\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005982, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.008385, train_acc: 0.73\n",
      "alexnet1d, trial.72:\n",
      "Epoch 26, avg test_loss: 0.018556, test_acc: 0.56\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.007013, train_acc: 0.84\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.002895, train_acc: 0.98\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.008015, train_acc: 0.73\n",
      "alexnet1d, trial.72:\n",
      "Epoch 27, avg test_loss: 0.018783, test_acc: 0.57\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.002817, train_acc: 0.96\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.009582, train_acc: 0.75\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.005197, train_acc: 0.88\n",
      "alexnet1d, trial.72:\n",
      "Epoch 28, avg test_loss: 0.026835, test_acc: 0.51\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.008024, train_acc: 0.77\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.007451, train_acc: 0.79\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.004763, train_acc: 0.88\n",
      "alexnet1d, trial.72:\n",
      "Epoch 29, avg test_loss: 0.017648, test_acc: 0.57\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.005926, train_acc: 0.88\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.005280, train_acc: 0.88\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.005749, train_acc: 0.89\n",
      "alexnet1d, trial.72:\n",
      "Epoch 30, avg test_loss: 0.017820, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.005407, train_acc: 0.89\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.004062, train_acc: 0.93\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.003530, train_acc: 0.91\n",
      "alexnet1d, trial.72:\n",
      "Epoch 31, avg test_loss: 0.021160, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.004122, train_acc: 0.91\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.003130, train_acc: 0.91\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.003135, train_acc: 0.93\n",
      "alexnet1d, trial.72:\n",
      "Epoch 32, avg test_loss: 0.025596, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 33, lr: 0.000377, 0/280, avg loss: 0.002730, train_acc: 0.96\n",
      "Train Epoch 33, lr: 0.000377, 112/280, avg loss: 0.003307, train_acc: 0.93\n",
      "Train Epoch 33, lr: 0.000377, 224/280, avg loss: 0.002705, train_acc: 0.91\n",
      "alexnet1d, trial.72:\n",
      "Epoch 33, avg test_loss: 0.029430, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号43个\n",
      "错误信号27个\n",
      "信号正确并预测正确的概率为0.4\n",
      "信号错误并预测正确的概率为0.157\n",
      "总正确率为0.56\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012378, train_acc: 0.50\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012082, train_acc: 0.61\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012186, train_acc: 0.61\n",
      "alexnet1d, trial.73:\n",
      "Epoch 0, avg test_loss: 0.009785, test_acc: 0.63\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012310, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.011945, train_acc: 0.75\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012572, train_acc: 0.45\n",
      "alexnet1d, trial.73:\n",
      "Epoch 1, avg test_loss: 0.009626, test_acc: 0.63\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012132, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012613, train_acc: 0.50\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012292, train_acc: 0.55\n",
      "alexnet1d, trial.73:\n",
      "Epoch 2, avg test_loss: 0.009551, test_acc: 0.63\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012184, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011987, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.013087, train_acc: 0.46\n",
      "alexnet1d, trial.73:\n",
      "Epoch 3, avg test_loss: 0.009456, test_acc: 0.63\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012494, train_acc: 0.50\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012367, train_acc: 0.52\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012074, train_acc: 0.71\n",
      "alexnet1d, trial.73:\n",
      "Epoch 4, avg test_loss: 0.009668, test_acc: 0.63\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012512, train_acc: 0.43\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012161, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012119, train_acc: 0.59\n",
      "alexnet1d, trial.73:\n",
      "Epoch 5, avg test_loss: 0.009655, test_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011858, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012140, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012556, train_acc: 0.61\n",
      "alexnet1d, trial.73:\n",
      "Epoch 6, avg test_loss: 0.009756, test_acc: 0.63\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011686, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011649, train_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012122, train_acc: 0.57\n",
      "alexnet1d, trial.73:\n",
      "Epoch 7, avg test_loss: 0.009661, test_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010989, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012226, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011403, train_acc: 0.68\n",
      "alexnet1d, trial.73:\n",
      "Epoch 8, avg test_loss: 0.010264, test_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011426, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011210, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011545, train_acc: 0.59\n",
      "alexnet1d, trial.73:\n",
      "Epoch 9, avg test_loss: 0.009539, test_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011770, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.009545, train_acc: 0.73\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012038, train_acc: 0.52\n",
      "alexnet1d, trial.73:\n",
      "Epoch 10, avg test_loss: 0.011874, test_acc: 0.56\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.009833, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010014, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011148, train_acc: 0.64\n",
      "alexnet1d, trial.73:\n",
      "Epoch 11, avg test_loss: 0.010350, test_acc: 0.56\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011949, train_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011920, train_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011206, train_acc: 0.68\n",
      "alexnet1d, trial.73:\n",
      "Epoch 12, avg test_loss: 0.009806, test_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010289, train_acc: 0.75\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009819, train_acc: 0.77\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010767, train_acc: 0.70\n",
      "alexnet1d, trial.73:\n",
      "Epoch 13, avg test_loss: 0.011135, test_acc: 0.56\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.011687, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009197, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011297, train_acc: 0.68\n",
      "alexnet1d, trial.73:\n",
      "Epoch 14, avg test_loss: 0.011497, test_acc: 0.49\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008959, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010251, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009768, train_acc: 0.71\n",
      "alexnet1d, trial.73:\n",
      "Epoch 15, avg test_loss: 0.010785, test_acc: 0.57\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008767, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008618, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010416, train_acc: 0.70\n",
      "alexnet1d, trial.73:\n",
      "Epoch 16, avg test_loss: 0.011043, test_acc: 0.51\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009872, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009734, train_acc: 0.70\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007937, train_acc: 0.84\n",
      "alexnet1d, trial.73:\n",
      "Epoch 17, avg test_loss: 0.014663, test_acc: 0.47\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008486, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007877, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008724, train_acc: 0.77\n",
      "alexnet1d, trial.73:\n",
      "Epoch 18, avg test_loss: 0.014184, test_acc: 0.50\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008041, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008962, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007085, train_acc: 0.86\n",
      "alexnet1d, trial.73:\n",
      "Epoch 19, avg test_loss: 0.015395, test_acc: 0.53\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006741, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.006835, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007852, train_acc: 0.82\n",
      "alexnet1d, trial.73:\n",
      "Epoch 20, avg test_loss: 0.020285, test_acc: 0.46\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.005811, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.005585, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006653, train_acc: 0.84\n",
      "alexnet1d, trial.73:\n",
      "Epoch 21, avg test_loss: 0.015754, test_acc: 0.53\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006478, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006020, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008641, train_acc: 0.80\n",
      "alexnet1d, trial.73:\n",
      "Epoch 22, avg test_loss: 0.017867, test_acc: 0.47\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006196, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.004837, train_acc: 0.95\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.004587, train_acc: 0.93\n",
      "alexnet1d, trial.73:\n",
      "Epoch 23, avg test_loss: 0.014342, test_acc: 0.49\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005091, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006484, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005229, train_acc: 0.89\n",
      "alexnet1d, trial.73:\n",
      "Epoch 24, avg test_loss: 0.024595, test_acc: 0.46\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004892, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004789, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004859, train_acc: 0.89\n",
      "alexnet1d, trial.73:\n",
      "Epoch 25, avg test_loss: 0.020006, test_acc: 0.47\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.002291, train_acc: 0.96\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.002568, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.005855, train_acc: 0.89\n",
      "alexnet1d, trial.73:\n",
      "Epoch 26, avg test_loss: 0.024437, test_acc: 0.49\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号44个\n",
      "错误信号26个\n",
      "信号正确并预测正确的概率为0.329\n",
      "信号错误并预测正确的概率为0.157\n",
      "总正确率为0.49\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012385, train_acc: 0.52\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012436, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012419, train_acc: 0.59\n",
      "alexnet1d, trial.74:\n",
      "Epoch 0, avg test_loss: 0.009672, test_acc: 0.63\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011836, train_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012579, train_acc: 0.46\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012434, train_acc: 0.52\n",
      "alexnet1d, trial.74:\n",
      "Epoch 1, avg test_loss: 0.009624, test_acc: 0.63\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012420, train_acc: 0.52\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012552, train_acc: 0.48\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012305, train_acc: 0.57\n",
      "alexnet1d, trial.74:\n",
      "Epoch 2, avg test_loss: 0.009803, test_acc: 0.63\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012780, train_acc: 0.50\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012273, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012832, train_acc: 0.52\n",
      "alexnet1d, trial.74:\n",
      "Epoch 3, avg test_loss: 0.009844, test_acc: 0.63\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011612, train_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012613, train_acc: 0.48\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011845, train_acc: 0.57\n",
      "alexnet1d, trial.74:\n",
      "Epoch 4, avg test_loss: 0.009925, test_acc: 0.63\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011637, train_acc: 0.68\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011552, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011544, train_acc: 0.48\n",
      "alexnet1d, trial.74:\n",
      "Epoch 5, avg test_loss: 0.010491, test_acc: 0.63\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011575, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011616, train_acc: 0.50\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011395, train_acc: 0.61\n",
      "alexnet1d, trial.74:\n",
      "Epoch 6, avg test_loss: 0.010937, test_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.010897, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012781, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011759, train_acc: 0.62\n",
      "alexnet1d, trial.74:\n",
      "Epoch 7, avg test_loss: 0.009386, test_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.013070, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011191, train_acc: 0.68\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.010680, train_acc: 0.66\n",
      "alexnet1d, trial.74:\n",
      "Epoch 8, avg test_loss: 0.013473, test_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010824, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.013825, train_acc: 0.52\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012001, train_acc: 0.57\n",
      "alexnet1d, trial.74:\n",
      "Epoch 9, avg test_loss: 0.009687, test_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011866, train_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011828, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011202, train_acc: 0.62\n",
      "alexnet1d, trial.74:\n",
      "Epoch 10, avg test_loss: 0.009463, test_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.012535, train_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010498, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010030, train_acc: 0.75\n",
      "alexnet1d, trial.74:\n",
      "Epoch 11, avg test_loss: 0.009918, test_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010938, train_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011238, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.009283, train_acc: 0.66\n",
      "alexnet1d, trial.74:\n",
      "Epoch 12, avg test_loss: 0.010341, test_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010938, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010608, train_acc: 0.62\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010335, train_acc: 0.70\n",
      "alexnet1d, trial.74:\n",
      "Epoch 13, avg test_loss: 0.010488, test_acc: 0.67\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010169, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010278, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011980, train_acc: 0.66\n",
      "alexnet1d, trial.74:\n",
      "Epoch 14, avg test_loss: 0.010910, test_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010242, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.011138, train_acc: 0.62\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009473, train_acc: 0.71\n",
      "alexnet1d, trial.74:\n",
      "Epoch 15, avg test_loss: 0.012106, test_acc: 0.60\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009818, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008925, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009053, train_acc: 0.75\n",
      "alexnet1d, trial.74:\n",
      "Epoch 16, avg test_loss: 0.011211, test_acc: 0.67\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009536, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010303, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.005892, train_acc: 0.91\n",
      "alexnet1d, trial.74:\n",
      "Epoch 17, avg test_loss: 0.013880, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009726, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007829, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007269, train_acc: 0.79\n",
      "alexnet1d, trial.74:\n",
      "Epoch 18, avg test_loss: 0.013494, test_acc: 0.70\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009926, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009109, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008361, train_acc: 0.71\n",
      "alexnet1d, trial.74:\n",
      "Epoch 19, avg test_loss: 0.015377, test_acc: 0.66\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.011606, train_acc: 0.75\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.009023, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006789, train_acc: 0.86\n",
      "alexnet1d, trial.74:\n",
      "Epoch 20, avg test_loss: 0.012903, test_acc: 0.61\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006810, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007009, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007035, train_acc: 0.82\n",
      "alexnet1d, trial.74:\n",
      "Epoch 21, avg test_loss: 0.014226, test_acc: 0.66\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005923, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005412, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006504, train_acc: 0.86\n",
      "alexnet1d, trial.74:\n",
      "Epoch 22, avg test_loss: 0.017407, test_acc: 0.64\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005290, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.004972, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.004973, train_acc: 0.88\n",
      "alexnet1d, trial.74:\n",
      "Epoch 23, avg test_loss: 0.018887, test_acc: 0.63\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005036, train_acc: 0.93\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.007395, train_acc: 0.82\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005511, train_acc: 0.89\n",
      "alexnet1d, trial.74:\n",
      "Epoch 24, avg test_loss: 0.021822, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005506, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.003663, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004459, train_acc: 0.89\n",
      "alexnet1d, trial.74:\n",
      "Epoch 25, avg test_loss: 0.018400, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.003962, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.003292, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.005442, train_acc: 0.89\n",
      "alexnet1d, trial.74:\n",
      "Epoch 26, avg test_loss: 0.022770, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号44个\n",
      "错误信号26个\n",
      "信号正确并预测正确的概率为0.429\n",
      "信号错误并预测正确的概率为0.186\n",
      "总正确率为0.61\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012465, train_acc: 0.41\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.013093, train_acc: 0.52\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012278, train_acc: 0.55\n",
      "alexnet1d, trial.75:\n",
      "Epoch 0, avg test_loss: 0.009896, test_acc: 0.51\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012081, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012651, train_acc: 0.52\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011829, train_acc: 0.64\n",
      "alexnet1d, trial.75:\n",
      "Epoch 1, avg test_loss: 0.009927, test_acc: 0.51\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012022, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011476, train_acc: 0.66\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012287, train_acc: 0.61\n",
      "alexnet1d, trial.75:\n",
      "Epoch 2, avg test_loss: 0.010035, test_acc: 0.51\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011355, train_acc: 0.70\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012531, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012042, train_acc: 0.62\n",
      "alexnet1d, trial.75:\n",
      "Epoch 3, avg test_loss: 0.010003, test_acc: 0.51\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012334, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011818, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012162, train_acc: 0.61\n",
      "alexnet1d, trial.75:\n",
      "Epoch 4, avg test_loss: 0.010045, test_acc: 0.51\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011964, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011650, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011870, train_acc: 0.62\n",
      "alexnet1d, trial.75:\n",
      "Epoch 5, avg test_loss: 0.010358, test_acc: 0.51\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011370, train_acc: 0.70\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011502, train_acc: 0.66\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011400, train_acc: 0.62\n",
      "alexnet1d, trial.75:\n",
      "Epoch 6, avg test_loss: 0.010328, test_acc: 0.53\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011039, train_acc: 0.73\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011850, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011942, train_acc: 0.54\n",
      "alexnet1d, trial.75:\n",
      "Epoch 7, avg test_loss: 0.009497, test_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010973, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011201, train_acc: 0.68\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.013073, train_acc: 0.57\n",
      "alexnet1d, trial.75:\n",
      "Epoch 8, avg test_loss: 0.011229, test_acc: 0.53\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010339, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012393, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010806, train_acc: 0.66\n",
      "alexnet1d, trial.75:\n",
      "Epoch 9, avg test_loss: 0.009674, test_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010802, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010700, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012606, train_acc: 0.66\n",
      "alexnet1d, trial.75:\n",
      "Epoch 10, avg test_loss: 0.011104, test_acc: 0.54\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.009474, train_acc: 0.75\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011335, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.012090, train_acc: 0.57\n",
      "alexnet1d, trial.75:\n",
      "Epoch 11, avg test_loss: 0.009330, test_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009802, train_acc: 0.75\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.009928, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.009247, train_acc: 0.79\n",
      "alexnet1d, trial.75:\n",
      "Epoch 12, avg test_loss: 0.010479, test_acc: 0.54\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009686, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010363, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.008445, train_acc: 0.82\n",
      "alexnet1d, trial.75:\n",
      "Epoch 13, avg test_loss: 0.010107, test_acc: 0.60\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.008747, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011743, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009054, train_acc: 0.75\n",
      "alexnet1d, trial.75:\n",
      "Epoch 14, avg test_loss: 0.011805, test_acc: 0.54\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.007896, train_acc: 0.80\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010343, train_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010960, train_acc: 0.71\n",
      "alexnet1d, trial.75:\n",
      "Epoch 15, avg test_loss: 0.009882, test_acc: 0.61\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009752, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.007020, train_acc: 0.82\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.007689, train_acc: 0.84\n",
      "alexnet1d, trial.75:\n",
      "Epoch 16, avg test_loss: 0.013314, test_acc: 0.56\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009237, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008191, train_acc: 0.84\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010159, train_acc: 0.73\n",
      "alexnet1d, trial.75:\n",
      "Epoch 17, avg test_loss: 0.010732, test_acc: 0.61\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.006941, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007812, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007546, train_acc: 0.86\n",
      "alexnet1d, trial.75:\n",
      "Epoch 18, avg test_loss: 0.010883, test_acc: 0.60\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008221, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.006209, train_acc: 0.91\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009144, train_acc: 0.71\n",
      "alexnet1d, trial.75:\n",
      "Epoch 19, avg test_loss: 0.014034, test_acc: 0.51\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.009062, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.005124, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006906, train_acc: 0.82\n",
      "alexnet1d, trial.75:\n",
      "Epoch 20, avg test_loss: 0.012916, test_acc: 0.57\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.005806, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.005156, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007226, train_acc: 0.82\n",
      "alexnet1d, trial.75:\n",
      "Epoch 21, avg test_loss: 0.013752, test_acc: 0.67\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005489, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005751, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006561, train_acc: 0.86\n",
      "alexnet1d, trial.75:\n",
      "Epoch 22, avg test_loss: 0.019249, test_acc: 0.51\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005294, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.004954, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006311, train_acc: 0.84\n",
      "alexnet1d, trial.75:\n",
      "Epoch 23, avg test_loss: 0.017830, test_acc: 0.59\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005103, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.003661, train_acc: 0.96\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006166, train_acc: 0.82\n",
      "alexnet1d, trial.75:\n",
      "Epoch 24, avg test_loss: 0.019119, test_acc: 0.59\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.002396, train_acc: 0.98\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.003497, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006652, train_acc: 0.80\n",
      "alexnet1d, trial.75:\n",
      "Epoch 25, avg test_loss: 0.026071, test_acc: 0.51\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.003068, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.003005, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004409, train_acc: 0.89\n",
      "alexnet1d, trial.75:\n",
      "Epoch 26, avg test_loss: 0.019537, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003077, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.002847, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003035, train_acc: 0.91\n",
      "alexnet1d, trial.75:\n",
      "Epoch 27, avg test_loss: 0.024933, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.001483, train_acc: 0.96\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.001285, train_acc: 1.00\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.000857, train_acc: 1.00\n",
      "alexnet1d, trial.75:\n",
      "Epoch 28, avg test_loss: 0.039299, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.002939, train_acc: 0.91\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.000654, train_acc: 0.98\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.001225, train_acc: 0.98\n",
      "alexnet1d, trial.75:\n",
      "Epoch 29, avg test_loss: 0.032606, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号36个\n",
      "错误信号34个\n",
      "信号正确并预测正确的概率为0.343\n",
      "信号错误并预测正确的概率为0.286\n",
      "总正确率为0.63\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012403, train_acc: 0.50\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.011831, train_acc: 0.64\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012289, train_acc: 0.62\n",
      "alexnet1d, trial.76:\n",
      "Epoch 0, avg test_loss: 0.009853, test_acc: 0.60\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012163, train_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012753, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012137, train_acc: 0.62\n",
      "alexnet1d, trial.76:\n",
      "Epoch 1, avg test_loss: 0.009845, test_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012133, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011710, train_acc: 0.66\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011898, train_acc: 0.61\n",
      "alexnet1d, trial.76:\n",
      "Epoch 2, avg test_loss: 0.009851, test_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012093, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012018, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011981, train_acc: 0.59\n",
      "alexnet1d, trial.76:\n",
      "Epoch 3, avg test_loss: 0.009899, test_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012600, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011957, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011761, train_acc: 0.61\n",
      "alexnet1d, trial.76:\n",
      "Epoch 4, avg test_loss: 0.009924, test_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011458, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.014218, train_acc: 0.50\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011838, train_acc: 0.55\n",
      "alexnet1d, trial.76:\n",
      "Epoch 5, avg test_loss: 0.009776, test_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012512, train_acc: 0.48\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012314, train_acc: 0.46\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012329, train_acc: 0.50\n",
      "alexnet1d, trial.76:\n",
      "Epoch 6, avg test_loss: 0.009818, test_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012485, train_acc: 0.45\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012251, train_acc: 0.55\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011890, train_acc: 0.73\n",
      "alexnet1d, trial.76:\n",
      "Epoch 7, avg test_loss: 0.009738, test_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012041, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011633, train_acc: 0.70\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012671, train_acc: 0.50\n",
      "alexnet1d, trial.76:\n",
      "Epoch 8, avg test_loss: 0.009784, test_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012824, train_acc: 0.52\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012099, train_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011386, train_acc: 0.64\n",
      "alexnet1d, trial.76:\n",
      "Epoch 9, avg test_loss: 0.009894, test_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.012351, train_acc: 0.50\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012392, train_acc: 0.55\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011815, train_acc: 0.64\n",
      "alexnet1d, trial.76:\n",
      "Epoch 10, avg test_loss: 0.009730, test_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011330, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011623, train_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011880, train_acc: 0.62\n",
      "alexnet1d, trial.76:\n",
      "Epoch 11, avg test_loss: 0.009764, test_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011929, train_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010661, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.012601, train_acc: 0.66\n",
      "alexnet1d, trial.76:\n",
      "Epoch 12, avg test_loss: 0.009637, test_acc: 0.63\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.012921, train_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011144, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011822, train_acc: 0.55\n",
      "alexnet1d, trial.76:\n",
      "Epoch 13, avg test_loss: 0.009416, test_acc: 0.56\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.012409, train_acc: 0.48\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010726, train_acc: 0.62\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011260, train_acc: 0.64\n",
      "alexnet1d, trial.76:\n",
      "Epoch 14, avg test_loss: 0.009386, test_acc: 0.56\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.011103, train_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.011347, train_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.012410, train_acc: 0.55\n",
      "alexnet1d, trial.76:\n",
      "Epoch 15, avg test_loss: 0.009287, test_acc: 0.64\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.011656, train_acc: 0.55\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.011787, train_acc: 0.62\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010157, train_acc: 0.73\n",
      "alexnet1d, trial.76:\n",
      "Epoch 16, avg test_loss: 0.009544, test_acc: 0.57\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.010494, train_acc: 0.70\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009658, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.012271, train_acc: 0.54\n",
      "alexnet1d, trial.76:\n",
      "Epoch 17, avg test_loss: 0.009348, test_acc: 0.64\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008791, train_acc: 0.88\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.010254, train_acc: 0.68\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.010075, train_acc: 0.70\n",
      "alexnet1d, trial.76:\n",
      "Epoch 18, avg test_loss: 0.009591, test_acc: 0.60\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.010418, train_acc: 0.71\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009632, train_acc: 0.68\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.010502, train_acc: 0.62\n",
      "alexnet1d, trial.76:\n",
      "Epoch 19, avg test_loss: 0.011464, test_acc: 0.59\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.009781, train_acc: 0.68\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.010591, train_acc: 0.70\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.011593, train_acc: 0.64\n",
      "alexnet1d, trial.76:\n",
      "Epoch 20, avg test_loss: 0.010074, test_acc: 0.63\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.010452, train_acc: 0.73\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.012073, train_acc: 0.59\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.009716, train_acc: 0.75\n",
      "alexnet1d, trial.76:\n",
      "Epoch 21, avg test_loss: 0.009694, test_acc: 0.64\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.010279, train_acc: 0.70\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.008624, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.009663, train_acc: 0.71\n",
      "alexnet1d, trial.76:\n",
      "Epoch 22, avg test_loss: 0.009693, test_acc: 0.67\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.010106, train_acc: 0.66\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007748, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.009712, train_acc: 0.71\n",
      "alexnet1d, trial.76:\n",
      "Epoch 23, avg test_loss: 0.010314, test_acc: 0.67\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.009298, train_acc: 0.70\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.008002, train_acc: 0.79\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.008955, train_acc: 0.79\n",
      "alexnet1d, trial.76:\n",
      "Epoch 24, avg test_loss: 0.012109, test_acc: 0.69\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.007343, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.007468, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.007063, train_acc: 0.84\n",
      "alexnet1d, trial.76:\n",
      "Epoch 25, avg test_loss: 0.011031, test_acc: 0.70\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.006369, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005754, train_acc: 0.80\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.007608, train_acc: 0.84\n",
      "alexnet1d, trial.76:\n",
      "Epoch 26, avg test_loss: 0.011897, test_acc: 0.69\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.006481, train_acc: 0.86\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.005910, train_acc: 0.86\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.008041, train_acc: 0.84\n",
      "alexnet1d, trial.76:\n",
      "Epoch 27, avg test_loss: 0.014532, test_acc: 0.61\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.009366, train_acc: 0.82\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.007540, train_acc: 0.79\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.004636, train_acc: 0.89\n",
      "alexnet1d, trial.76:\n",
      "Epoch 28, avg test_loss: 0.014239, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.007510, train_acc: 0.84\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.006776, train_acc: 0.86\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.006374, train_acc: 0.88\n",
      "alexnet1d, trial.76:\n",
      "Epoch 29, avg test_loss: 0.013048, test_acc: 0.69\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.004372, train_acc: 0.89\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.004469, train_acc: 0.93\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.004136, train_acc: 0.95\n",
      "alexnet1d, trial.76:\n",
      "Epoch 30, avg test_loss: 0.015771, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.005685, train_acc: 0.88\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.006297, train_acc: 0.80\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.002836, train_acc: 0.95\n",
      "alexnet1d, trial.76:\n",
      "Epoch 31, avg test_loss: 0.018043, test_acc: 0.67\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.005148, train_acc: 0.84\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.003613, train_acc: 0.95\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.003441, train_acc: 0.91\n",
      "alexnet1d, trial.76:\n",
      "Epoch 32, avg test_loss: 0.018427, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号40个\n",
      "错误信号30个\n",
      "信号正确并预测正确的概率为0.386\n",
      "信号错误并预测正确的概率为0.243\n",
      "总正确率为0.63\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012529, train_acc: 0.41\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.014020, train_acc: 0.68\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012283, train_acc: 0.68\n",
      "alexnet1d, trial.77:\n",
      "Epoch 0, avg test_loss: 0.009902, test_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012417, train_acc: 0.43\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012293, train_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012202, train_acc: 0.59\n",
      "alexnet1d, trial.77:\n",
      "Epoch 1, avg test_loss: 0.009989, test_acc: 0.50\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012102, train_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011654, train_acc: 0.66\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011590, train_acc: 0.62\n",
      "alexnet1d, trial.77:\n",
      "Epoch 2, avg test_loss: 0.010423, test_acc: 0.50\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011368, train_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011566, train_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011904, train_acc: 0.61\n",
      "alexnet1d, trial.77:\n",
      "Epoch 3, avg test_loss: 0.010171, test_acc: 0.50\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012648, train_acc: 0.52\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012241, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011915, train_acc: 0.68\n",
      "alexnet1d, trial.77:\n",
      "Epoch 4, avg test_loss: 0.010001, test_acc: 0.50\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012238, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011762, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011910, train_acc: 0.57\n",
      "alexnet1d, trial.77:\n",
      "Epoch 5, avg test_loss: 0.010357, test_acc: 0.50\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012372, train_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011604, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011418, train_acc: 0.62\n",
      "alexnet1d, trial.77:\n",
      "Epoch 6, avg test_loss: 0.010372, test_acc: 0.50\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011540, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011102, train_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011772, train_acc: 0.54\n",
      "alexnet1d, trial.77:\n",
      "Epoch 7, avg test_loss: 0.010615, test_acc: 0.50\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.009634, train_acc: 0.73\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011919, train_acc: 0.52\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.013618, train_acc: 0.64\n",
      "alexnet1d, trial.77:\n",
      "Epoch 8, avg test_loss: 0.009870, test_acc: 0.53\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012211, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011806, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011735, train_acc: 0.57\n",
      "alexnet1d, trial.77:\n",
      "Epoch 9, avg test_loss: 0.010211, test_acc: 0.49\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011027, train_acc: 0.73\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012891, train_acc: 0.45\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010435, train_acc: 0.66\n",
      "alexnet1d, trial.77:\n",
      "Epoch 10, avg test_loss: 0.010184, test_acc: 0.49\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.012447, train_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010878, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010666, train_acc: 0.75\n",
      "alexnet1d, trial.77:\n",
      "Epoch 11, avg test_loss: 0.010487, test_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010598, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.013561, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010174, train_acc: 0.73\n",
      "alexnet1d, trial.77:\n",
      "Epoch 12, avg test_loss: 0.010533, test_acc: 0.53\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010118, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010392, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010982, train_acc: 0.68\n",
      "alexnet1d, trial.77:\n",
      "Epoch 13, avg test_loss: 0.010831, test_acc: 0.54\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010134, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010615, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009874, train_acc: 0.77\n",
      "alexnet1d, trial.77:\n",
      "Epoch 14, avg test_loss: 0.010852, test_acc: 0.57\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009482, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010571, train_acc: 0.68\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009619, train_acc: 0.75\n",
      "alexnet1d, trial.77:\n",
      "Epoch 15, avg test_loss: 0.011852, test_acc: 0.53\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009566, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008886, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009695, train_acc: 0.73\n",
      "alexnet1d, trial.77:\n",
      "Epoch 16, avg test_loss: 0.010841, test_acc: 0.54\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009616, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008295, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009330, train_acc: 0.79\n",
      "alexnet1d, trial.77:\n",
      "Epoch 17, avg test_loss: 0.010607, test_acc: 0.57\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008666, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.010210, train_acc: 0.71\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007551, train_acc: 0.82\n",
      "alexnet1d, trial.77:\n",
      "Epoch 18, avg test_loss: 0.012133, test_acc: 0.57\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009471, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007151, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007781, train_acc: 0.77\n",
      "alexnet1d, trial.77:\n",
      "Epoch 19, avg test_loss: 0.012826, test_acc: 0.57\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007638, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008698, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009324, train_acc: 0.77\n",
      "alexnet1d, trial.77:\n",
      "Epoch 20, avg test_loss: 0.013118, test_acc: 0.60\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006427, train_acc: 0.91\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007084, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008375, train_acc: 0.80\n",
      "alexnet1d, trial.77:\n",
      "Epoch 21, avg test_loss: 0.015885, test_acc: 0.54\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006074, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.004748, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006287, train_acc: 0.86\n",
      "alexnet1d, trial.77:\n",
      "Epoch 22, avg test_loss: 0.015698, test_acc: 0.53\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005937, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006830, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006081, train_acc: 0.86\n",
      "alexnet1d, trial.77:\n",
      "Epoch 23, avg test_loss: 0.017565, test_acc: 0.57\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.007035, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004473, train_acc: 0.93\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.004841, train_acc: 0.88\n",
      "alexnet1d, trial.77:\n",
      "Epoch 24, avg test_loss: 0.017249, test_acc: 0.59\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.003192, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005563, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006645, train_acc: 0.84\n",
      "alexnet1d, trial.77:\n",
      "Epoch 25, avg test_loss: 0.017343, test_acc: 0.54\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.003808, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004744, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.002950, train_acc: 0.95\n",
      "alexnet1d, trial.77:\n",
      "Epoch 26, avg test_loss: 0.019560, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004254, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.002852, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004662, train_acc: 0.89\n",
      "alexnet1d, trial.77:\n",
      "Epoch 27, avg test_loss: 0.021537, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.001725, train_acc: 1.00\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.001598, train_acc: 0.96\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.003751, train_acc: 0.95\n",
      "alexnet1d, trial.77:\n",
      "Epoch 28, avg test_loss: 0.023761, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.002882, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.003606, train_acc: 0.89\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.002724, train_acc: 0.96\n",
      "alexnet1d, trial.77:\n",
      "Epoch 29, avg test_loss: 0.026743, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号35个\n",
      "错误信号35个\n",
      "信号正确并预测正确的概率为0.357\n",
      "信号错误并预测正确的概率为0.171\n",
      "总正确率为0.53\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012374, train_acc: 0.48\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012199, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.013401, train_acc: 0.54\n",
      "alexnet1d, trial.78:\n",
      "Epoch 0, avg test_loss: 0.010202, test_acc: 0.56\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012464, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012093, train_acc: 0.70\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012447, train_acc: 0.46\n",
      "alexnet1d, trial.78:\n",
      "Epoch 1, avg test_loss: 0.009838, test_acc: 0.56\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012594, train_acc: 0.50\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012002, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011736, train_acc: 0.62\n",
      "alexnet1d, trial.78:\n",
      "Epoch 2, avg test_loss: 0.010012, test_acc: 0.56\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012045, train_acc: 0.55\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.010757, train_acc: 0.70\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.013354, train_acc: 0.55\n",
      "alexnet1d, trial.78:\n",
      "Epoch 3, avg test_loss: 0.009945, test_acc: 0.56\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011732, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012022, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012640, train_acc: 0.45\n",
      "alexnet1d, trial.78:\n",
      "Epoch 4, avg test_loss: 0.009862, test_acc: 0.56\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012708, train_acc: 0.41\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011983, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011786, train_acc: 0.64\n",
      "alexnet1d, trial.78:\n",
      "Epoch 5, avg test_loss: 0.009929, test_acc: 0.56\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011930, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011826, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011826, train_acc: 0.61\n",
      "alexnet1d, trial.78:\n",
      "Epoch 6, avg test_loss: 0.010170, test_acc: 0.56\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012143, train_acc: 0.52\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011273, train_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012556, train_acc: 0.57\n",
      "alexnet1d, trial.78:\n",
      "Epoch 7, avg test_loss: 0.010288, test_acc: 0.56\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011185, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012235, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011996, train_acc: 0.61\n",
      "alexnet1d, trial.78:\n",
      "Epoch 8, avg test_loss: 0.010536, test_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011137, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010644, train_acc: 0.79\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011616, train_acc: 0.61\n",
      "alexnet1d, trial.78:\n",
      "Epoch 9, avg test_loss: 0.011115, test_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011381, train_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010287, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010403, train_acc: 0.70\n",
      "alexnet1d, trial.78:\n",
      "Epoch 10, avg test_loss: 0.011455, test_acc: 0.50\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011555, train_acc: 0.55\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010626, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010677, train_acc: 0.70\n",
      "alexnet1d, trial.78:\n",
      "Epoch 11, avg test_loss: 0.013557, test_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009413, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.008996, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010554, train_acc: 0.68\n",
      "alexnet1d, trial.78:\n",
      "Epoch 12, avg test_loss: 0.012069, test_acc: 0.51\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010270, train_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011268, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009924, train_acc: 0.71\n",
      "alexnet1d, trial.78:\n",
      "Epoch 13, avg test_loss: 0.013474, test_acc: 0.56\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.008969, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.012272, train_acc: 0.59\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009765, train_acc: 0.73\n",
      "alexnet1d, trial.78:\n",
      "Epoch 14, avg test_loss: 0.012050, test_acc: 0.47\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008153, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010740, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009531, train_acc: 0.73\n",
      "alexnet1d, trial.78:\n",
      "Epoch 15, avg test_loss: 0.012410, test_acc: 0.49\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010405, train_acc: 0.64\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009370, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009890, train_acc: 0.73\n",
      "alexnet1d, trial.78:\n",
      "Epoch 16, avg test_loss: 0.013373, test_acc: 0.54\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.010832, train_acc: 0.62\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010953, train_acc: 0.68\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008375, train_acc: 0.73\n",
      "alexnet1d, trial.78:\n",
      "Epoch 17, avg test_loss: 0.013142, test_acc: 0.57\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008102, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008297, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.010167, train_acc: 0.64\n",
      "alexnet1d, trial.78:\n",
      "Epoch 18, avg test_loss: 0.014612, test_acc: 0.54\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007037, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007560, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.010013, train_acc: 0.75\n",
      "alexnet1d, trial.78:\n",
      "Epoch 19, avg test_loss: 0.014522, test_acc: 0.51\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008851, train_acc: 0.70\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.005908, train_acc: 0.88\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009487, train_acc: 0.73\n",
      "alexnet1d, trial.78:\n",
      "Epoch 20, avg test_loss: 0.015905, test_acc: 0.57\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008466, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006381, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007715, train_acc: 0.77\n",
      "alexnet1d, trial.78:\n",
      "Epoch 21, avg test_loss: 0.016605, test_acc: 0.47\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006953, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005342, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006823, train_acc: 0.80\n",
      "alexnet1d, trial.78:\n",
      "Epoch 22, avg test_loss: 0.019292, test_acc: 0.56\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007756, train_acc: 0.79\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005792, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.007465, train_acc: 0.82\n",
      "alexnet1d, trial.78:\n",
      "Epoch 23, avg test_loss: 0.020330, test_acc: 0.51\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004714, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004519, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005616, train_acc: 0.91\n",
      "alexnet1d, trial.78:\n",
      "Epoch 24, avg test_loss: 0.022636, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005062, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006771, train_acc: 0.82\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.003646, train_acc: 0.89\n",
      "alexnet1d, trial.78:\n",
      "Epoch 25, avg test_loss: 0.022888, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004575, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004790, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.003190, train_acc: 0.96\n",
      "alexnet1d, trial.78:\n",
      "Epoch 26, avg test_loss: 0.026552, test_acc: 0.50\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.008842, train_acc: 0.82\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.003588, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004856, train_acc: 0.93\n",
      "alexnet1d, trial.78:\n",
      "Epoch 27, avg test_loss: 0.027095, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号39个\n",
      "错误信号31个\n",
      "信号正确并预测正确的概率为0.414\n",
      "信号错误并预测正确的概率为0.143\n",
      "总正确率为0.56\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012405, train_acc: 0.46\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012154, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012638, train_acc: 0.50\n",
      "alexnet1d, trial.79:\n",
      "Epoch 0, avg test_loss: 0.009638, test_acc: 0.67\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012762, train_acc: 0.41\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012450, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012360, train_acc: 0.54\n",
      "alexnet1d, trial.79:\n",
      "Epoch 1, avg test_loss: 0.009780, test_acc: 0.67\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012216, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012200, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012153, train_acc: 0.57\n",
      "alexnet1d, trial.79:\n",
      "Epoch 2, avg test_loss: 0.009462, test_acc: 0.67\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011868, train_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011810, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.013810, train_acc: 0.41\n",
      "alexnet1d, trial.79:\n",
      "Epoch 3, avg test_loss: 0.009346, test_acc: 0.67\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011461, train_acc: 0.70\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012485, train_acc: 0.52\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012284, train_acc: 0.50\n",
      "alexnet1d, trial.79:\n",
      "Epoch 4, avg test_loss: 0.009540, test_acc: 0.67\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012259, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012252, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012182, train_acc: 0.54\n",
      "alexnet1d, trial.79:\n",
      "Epoch 5, avg test_loss: 0.009580, test_acc: 0.67\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011942, train_acc: 0.66\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012324, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.013132, train_acc: 0.39\n",
      "alexnet1d, trial.79:\n",
      "Epoch 6, avg test_loss: 0.009461, test_acc: 0.67\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012461, train_acc: 0.50\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012187, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011948, train_acc: 0.64\n",
      "alexnet1d, trial.79:\n",
      "Epoch 7, avg test_loss: 0.009623, test_acc: 0.67\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012418, train_acc: 0.52\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011811, train_acc: 0.55\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012271, train_acc: 0.52\n",
      "alexnet1d, trial.79:\n",
      "Epoch 8, avg test_loss: 0.009449, test_acc: 0.67\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011371, train_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012129, train_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011565, train_acc: 0.55\n",
      "alexnet1d, trial.79:\n",
      "Epoch 9, avg test_loss: 0.009811, test_acc: 0.56\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011770, train_acc: 0.52\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011189, train_acc: 0.73\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012841, train_acc: 0.57\n",
      "alexnet1d, trial.79:\n",
      "Epoch 10, avg test_loss: 0.009538, test_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.012491, train_acc: 0.52\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011938, train_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011840, train_acc: 0.66\n",
      "alexnet1d, trial.79:\n",
      "Epoch 11, avg test_loss: 0.009955, test_acc: 0.53\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011443, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011497, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011806, train_acc: 0.61\n",
      "alexnet1d, trial.79:\n",
      "Epoch 12, avg test_loss: 0.010945, test_acc: 0.49\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010578, train_acc: 0.62\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011352, train_acc: 0.75\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010458, train_acc: 0.62\n",
      "alexnet1d, trial.79:\n",
      "Epoch 13, avg test_loss: 0.012071, test_acc: 0.49\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.011076, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010790, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011792, train_acc: 0.62\n",
      "alexnet1d, trial.79:\n",
      "Epoch 14, avg test_loss: 0.009324, test_acc: 0.60\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010763, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010473, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010292, train_acc: 0.75\n",
      "alexnet1d, trial.79:\n",
      "Epoch 15, avg test_loss: 0.010299, test_acc: 0.49\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.011071, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.011664, train_acc: 0.61\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009922, train_acc: 0.77\n",
      "alexnet1d, trial.79:\n",
      "Epoch 16, avg test_loss: 0.010542, test_acc: 0.51\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007520, train_acc: 0.86\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010080, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010557, train_acc: 0.64\n",
      "alexnet1d, trial.79:\n",
      "Epoch 17, avg test_loss: 0.010541, test_acc: 0.60\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007258, train_acc: 0.86\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009298, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008935, train_acc: 0.80\n",
      "alexnet1d, trial.79:\n",
      "Epoch 18, avg test_loss: 0.010549, test_acc: 0.61\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006796, train_acc: 0.88\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.011596, train_acc: 0.66\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008425, train_acc: 0.79\n",
      "alexnet1d, trial.79:\n",
      "Epoch 19, avg test_loss: 0.013606, test_acc: 0.49\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007696, train_acc: 0.77\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008143, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009280, train_acc: 0.77\n",
      "alexnet1d, trial.79:\n",
      "Epoch 20, avg test_loss: 0.009417, test_acc: 0.67\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006617, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.009169, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.009567, train_acc: 0.77\n",
      "alexnet1d, trial.79:\n",
      "Epoch 21, avg test_loss: 0.013292, test_acc: 0.53\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.008465, train_acc: 0.75\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006395, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006996, train_acc: 0.82\n",
      "alexnet1d, trial.79:\n",
      "Epoch 22, avg test_loss: 0.011494, test_acc: 0.61\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006695, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007689, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.005113, train_acc: 0.89\n",
      "alexnet1d, trial.79:\n",
      "Epoch 23, avg test_loss: 0.013986, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.008371, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006548, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.004325, train_acc: 0.91\n",
      "alexnet1d, trial.79:\n",
      "Epoch 24, avg test_loss: 0.012878, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004273, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.003565, train_acc: 0.96\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004727, train_acc: 0.91\n",
      "alexnet1d, trial.79:\n",
      "Epoch 25, avg test_loss: 0.018854, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.003251, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.008504, train_acc: 0.82\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.003828, train_acc: 0.93\n",
      "alexnet1d, trial.79:\n",
      "Epoch 26, avg test_loss: 0.017754, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号47个\n",
      "错误信号23个\n",
      "信号正确并预测正确的概率为0.429\n",
      "信号错误并预测正确的概率为0.186\n",
      "总正确率为0.61\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012376, train_acc: 0.50\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.011873, train_acc: 0.62\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.013282, train_acc: 0.50\n",
      "alexnet1d, trial.80:\n",
      "Epoch 0, avg test_loss: 0.009779, test_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011925, train_acc: 0.70\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012374, train_acc: 0.48\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012259, train_acc: 0.50\n",
      "alexnet1d, trial.80:\n",
      "Epoch 1, avg test_loss: 0.009754, test_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012290, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012317, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012093, train_acc: 0.59\n",
      "alexnet1d, trial.80:\n",
      "Epoch 2, avg test_loss: 0.009700, test_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012142, train_acc: 0.55\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012347, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012617, train_acc: 0.52\n",
      "alexnet1d, trial.80:\n",
      "Epoch 3, avg test_loss: 0.009690, test_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012538, train_acc: 0.50\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011982, train_acc: 0.68\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012179, train_acc: 0.59\n",
      "alexnet1d, trial.80:\n",
      "Epoch 4, avg test_loss: 0.009780, test_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012206, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012470, train_acc: 0.48\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011985, train_acc: 0.62\n",
      "alexnet1d, trial.80:\n",
      "Epoch 5, avg test_loss: 0.009685, test_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011954, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011522, train_acc: 0.66\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012185, train_acc: 0.55\n",
      "alexnet1d, trial.80:\n",
      "Epoch 6, avg test_loss: 0.009661, test_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012166, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.013207, train_acc: 0.41\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012294, train_acc: 0.55\n",
      "alexnet1d, trial.80:\n",
      "Epoch 7, avg test_loss: 0.009637, test_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011740, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012042, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012727, train_acc: 0.48\n",
      "alexnet1d, trial.80:\n",
      "Epoch 8, avg test_loss: 0.009574, test_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011869, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011964, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011686, train_acc: 0.68\n",
      "alexnet1d, trial.80:\n",
      "Epoch 9, avg test_loss: 0.009501, test_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011792, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011947, train_acc: 0.52\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011648, train_acc: 0.59\n",
      "alexnet1d, trial.80:\n",
      "Epoch 10, avg test_loss: 0.009305, test_acc: 0.60\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011463, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.013162, train_acc: 0.55\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011740, train_acc: 0.68\n",
      "alexnet1d, trial.80:\n",
      "Epoch 11, avg test_loss: 0.009304, test_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.012204, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011715, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011954, train_acc: 0.68\n",
      "alexnet1d, trial.80:\n",
      "Epoch 12, avg test_loss: 0.009342, test_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011613, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011087, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011370, train_acc: 0.61\n",
      "alexnet1d, trial.80:\n",
      "Epoch 13, avg test_loss: 0.009235, test_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010296, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010436, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.012259, train_acc: 0.55\n",
      "alexnet1d, trial.80:\n",
      "Epoch 14, avg test_loss: 0.010467, test_acc: 0.61\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.012548, train_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010435, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010699, train_acc: 0.66\n",
      "alexnet1d, trial.80:\n",
      "Epoch 15, avg test_loss: 0.009041, test_acc: 0.67\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010588, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010895, train_acc: 0.64\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.011326, train_acc: 0.62\n",
      "alexnet1d, trial.80:\n",
      "Epoch 16, avg test_loss: 0.009762, test_acc: 0.60\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.010023, train_acc: 0.71\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008475, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.011606, train_acc: 0.66\n",
      "alexnet1d, trial.80:\n",
      "Epoch 17, avg test_loss: 0.009830, test_acc: 0.63\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008670, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009778, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009824, train_acc: 0.79\n",
      "alexnet1d, trial.80:\n",
      "Epoch 18, avg test_loss: 0.009957, test_acc: 0.60\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007722, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009561, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008309, train_acc: 0.75\n",
      "alexnet1d, trial.80:\n",
      "Epoch 19, avg test_loss: 0.010324, test_acc: 0.60\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008126, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008593, train_acc: 0.75\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009263, train_acc: 0.73\n",
      "alexnet1d, trial.80:\n",
      "Epoch 20, avg test_loss: 0.012045, test_acc: 0.60\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.009877, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.008209, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.010543, train_acc: 0.71\n",
      "alexnet1d, trial.80:\n",
      "Epoch 21, avg test_loss: 0.010582, test_acc: 0.60\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007920, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006518, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.011432, train_acc: 0.66\n",
      "alexnet1d, trial.80:\n",
      "Epoch 22, avg test_loss: 0.011551, test_acc: 0.63\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007065, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.008782, train_acc: 0.79\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.008784, train_acc: 0.75\n",
      "alexnet1d, trial.80:\n",
      "Epoch 23, avg test_loss: 0.009960, test_acc: 0.64\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.007121, train_acc: 0.82\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.007048, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.007855, train_acc: 0.82\n",
      "alexnet1d, trial.80:\n",
      "Epoch 24, avg test_loss: 0.011704, test_acc: 0.61\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.006688, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.008056, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005994, train_acc: 0.89\n",
      "alexnet1d, trial.80:\n",
      "Epoch 25, avg test_loss: 0.011877, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.007543, train_acc: 0.82\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004794, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.008348, train_acc: 0.79\n",
      "alexnet1d, trial.80:\n",
      "Epoch 26, avg test_loss: 0.011859, test_acc: 0.61\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.006752, train_acc: 0.82\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.003731, train_acc: 0.96\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.007399, train_acc: 0.82\n",
      "alexnet1d, trial.80:\n",
      "Epoch 27, avg test_loss: 0.013052, test_acc: 0.69\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004506, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004543, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.004397, train_acc: 0.89\n",
      "alexnet1d, trial.80:\n",
      "Epoch 28, avg test_loss: 0.013747, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.002404, train_acc: 0.98\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.004496, train_acc: 0.91\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.003624, train_acc: 0.91\n",
      "alexnet1d, trial.80:\n",
      "Epoch 29, avg test_loss: 0.015087, test_acc: 0.69\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.003674, train_acc: 0.93\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.003501, train_acc: 0.93\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.004144, train_acc: 0.93\n",
      "alexnet1d, trial.80:\n",
      "Epoch 30, avg test_loss: 0.015330, test_acc: 0.66\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号41个\n",
      "错误信号29个\n",
      "信号正确并预测正确的概率为0.443\n",
      "信号错误并预测正确的概率为0.214\n",
      "总正确率为0.66\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012404, train_acc: 0.48\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012491, train_acc: 0.52\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.014562, train_acc: 0.52\n",
      "alexnet1d, trial.81:\n",
      "Epoch 0, avg test_loss: 0.009478, test_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.013030, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012386, train_acc: 0.52\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012366, train_acc: 0.50\n",
      "alexnet1d, trial.81:\n",
      "Epoch 1, avg test_loss: 0.009810, test_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012194, train_acc: 0.66\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012465, train_acc: 0.48\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012225, train_acc: 0.59\n",
      "alexnet1d, trial.81:\n",
      "Epoch 2, avg test_loss: 0.009652, test_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012191, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012106, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012183, train_acc: 0.55\n",
      "alexnet1d, trial.81:\n",
      "Epoch 3, avg test_loss: 0.009566, test_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012298, train_acc: 0.52\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012018, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011676, train_acc: 0.66\n",
      "alexnet1d, trial.81:\n",
      "Epoch 4, avg test_loss: 0.009617, test_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012352, train_acc: 0.52\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011767, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011784, train_acc: 0.66\n",
      "alexnet1d, trial.81:\n",
      "Epoch 5, avg test_loss: 0.009614, test_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011231, train_acc: 0.68\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012771, train_acc: 0.52\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012223, train_acc: 0.54\n",
      "alexnet1d, trial.81:\n",
      "Epoch 6, avg test_loss: 0.009718, test_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012312, train_acc: 0.52\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011556, train_acc: 0.68\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.013375, train_acc: 0.45\n",
      "alexnet1d, trial.81:\n",
      "Epoch 7, avg test_loss: 0.009564, test_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011515, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012153, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011434, train_acc: 0.71\n",
      "alexnet1d, trial.81:\n",
      "Epoch 8, avg test_loss: 0.009766, test_acc: 0.60\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011749, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011598, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011441, train_acc: 0.66\n",
      "alexnet1d, trial.81:\n",
      "Epoch 9, avg test_loss: 0.009519, test_acc: 0.60\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011047, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011509, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012725, train_acc: 0.55\n",
      "alexnet1d, trial.81:\n",
      "Epoch 10, avg test_loss: 0.009538, test_acc: 0.63\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011085, train_acc: 0.75\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011243, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011541, train_acc: 0.61\n",
      "alexnet1d, trial.81:\n",
      "Epoch 11, avg test_loss: 0.009347, test_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010900, train_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011064, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010724, train_acc: 0.59\n",
      "alexnet1d, trial.81:\n",
      "Epoch 12, avg test_loss: 0.009740, test_acc: 0.57\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010020, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009955, train_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010194, train_acc: 0.66\n",
      "alexnet1d, trial.81:\n",
      "Epoch 13, avg test_loss: 0.009849, test_acc: 0.63\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010376, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010352, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011019, train_acc: 0.68\n",
      "alexnet1d, trial.81:\n",
      "Epoch 14, avg test_loss: 0.010008, test_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010801, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.011700, train_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009335, train_acc: 0.80\n",
      "alexnet1d, trial.81:\n",
      "Epoch 15, avg test_loss: 0.010806, test_acc: 0.64\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010229, train_acc: 0.68\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010886, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009187, train_acc: 0.79\n",
      "alexnet1d, trial.81:\n",
      "Epoch 16, avg test_loss: 0.009943, test_acc: 0.64\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009492, train_acc: 0.70\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008548, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009298, train_acc: 0.73\n",
      "alexnet1d, trial.81:\n",
      "Epoch 17, avg test_loss: 0.010114, test_acc: 0.66\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009113, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009587, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008827, train_acc: 0.71\n",
      "alexnet1d, trial.81:\n",
      "Epoch 18, avg test_loss: 0.012463, test_acc: 0.64\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008523, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.011486, train_acc: 0.75\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008468, train_acc: 0.82\n",
      "alexnet1d, trial.81:\n",
      "Epoch 19, avg test_loss: 0.011110, test_acc: 0.64\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007128, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008296, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.010676, train_acc: 0.77\n",
      "alexnet1d, trial.81:\n",
      "Epoch 20, avg test_loss: 0.013351, test_acc: 0.61\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008234, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007818, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.009034, train_acc: 0.77\n",
      "alexnet1d, trial.81:\n",
      "Epoch 21, avg test_loss: 0.011462, test_acc: 0.60\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.009074, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006750, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008453, train_acc: 0.75\n",
      "alexnet1d, trial.81:\n",
      "Epoch 22, avg test_loss: 0.012506, test_acc: 0.61\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005592, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006684, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.005531, train_acc: 0.82\n",
      "alexnet1d, trial.81:\n",
      "Epoch 23, avg test_loss: 0.016021, test_acc: 0.56\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005429, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.007400, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.009176, train_acc: 0.75\n",
      "alexnet1d, trial.81:\n",
      "Epoch 24, avg test_loss: 0.014607, test_acc: 0.61\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004932, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004143, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006876, train_acc: 0.86\n",
      "alexnet1d, trial.81:\n",
      "Epoch 25, avg test_loss: 0.015266, test_acc: 0.57\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005439, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004265, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.005407, train_acc: 0.89\n",
      "alexnet1d, trial.81:\n",
      "Epoch 26, avg test_loss: 0.018642, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004193, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.003192, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.006663, train_acc: 0.89\n",
      "alexnet1d, trial.81:\n",
      "Epoch 27, avg test_loss: 0.017995, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004672, train_acc: 0.89\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004154, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.004538, train_acc: 0.89\n",
      "alexnet1d, trial.81:\n",
      "Epoch 28, avg test_loss: 0.018810, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.002290, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.003420, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.002983, train_acc: 0.95\n",
      "alexnet1d, trial.81:\n",
      "Epoch 29, avg test_loss: 0.018790, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号43个\n",
      "错误信号27个\n",
      "信号正确并预测正确的概率为0.443\n",
      "信号错误并预测正确的概率为0.129\n",
      "总正确率为0.57\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012308, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012619, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012010, train_acc: 0.68\n",
      "alexnet1d, trial.82:\n",
      "Epoch 0, avg test_loss: 0.009711, test_acc: 0.60\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011980, train_acc: 0.62\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012802, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012142, train_acc: 0.59\n",
      "alexnet1d, trial.82:\n",
      "Epoch 1, avg test_loss: 0.009673, test_acc: 0.60\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011834, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012517, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012067, train_acc: 0.57\n",
      "alexnet1d, trial.82:\n",
      "Epoch 2, avg test_loss: 0.009578, test_acc: 0.60\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012738, train_acc: 0.50\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011706, train_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012325, train_acc: 0.54\n",
      "alexnet1d, trial.82:\n",
      "Epoch 3, avg test_loss: 0.009639, test_acc: 0.60\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011697, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012210, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012214, train_acc: 0.57\n",
      "alexnet1d, trial.82:\n",
      "Epoch 4, avg test_loss: 0.009660, test_acc: 0.60\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012143, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012019, train_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012772, train_acc: 0.39\n",
      "alexnet1d, trial.82:\n",
      "Epoch 5, avg test_loss: 0.009618, test_acc: 0.60\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011480, train_acc: 0.68\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012229, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.013192, train_acc: 0.45\n",
      "alexnet1d, trial.82:\n",
      "Epoch 6, avg test_loss: 0.009486, test_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011702, train_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011680, train_acc: 0.70\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011376, train_acc: 0.70\n",
      "alexnet1d, trial.82:\n",
      "Epoch 7, avg test_loss: 0.009875, test_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011262, train_acc: 0.68\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012250, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012597, train_acc: 0.64\n",
      "alexnet1d, trial.82:\n",
      "Epoch 8, avg test_loss: 0.010140, test_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011398, train_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012970, train_acc: 0.52\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011127, train_acc: 0.64\n",
      "alexnet1d, trial.82:\n",
      "Epoch 9, avg test_loss: 0.009676, test_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011697, train_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011544, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.013110, train_acc: 0.48\n",
      "alexnet1d, trial.82:\n",
      "Epoch 10, avg test_loss: 0.009955, test_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011654, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.009686, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.012591, train_acc: 0.50\n",
      "alexnet1d, trial.82:\n",
      "Epoch 11, avg test_loss: 0.009737, test_acc: 0.67\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010597, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011984, train_acc: 0.48\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011088, train_acc: 0.68\n",
      "alexnet1d, trial.82:\n",
      "Epoch 12, avg test_loss: 0.009854, test_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009885, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011922, train_acc: 0.62\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011124, train_acc: 0.66\n",
      "alexnet1d, trial.82:\n",
      "Epoch 13, avg test_loss: 0.009587, test_acc: 0.59\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010135, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.008948, train_acc: 0.79\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.012412, train_acc: 0.62\n",
      "alexnet1d, trial.82:\n",
      "Epoch 14, avg test_loss: 0.009563, test_acc: 0.63\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010492, train_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.011060, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010145, train_acc: 0.71\n",
      "alexnet1d, trial.82:\n",
      "Epoch 15, avg test_loss: 0.009616, test_acc: 0.61\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010572, train_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009699, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010358, train_acc: 0.71\n",
      "alexnet1d, trial.82:\n",
      "Epoch 16, avg test_loss: 0.010011, test_acc: 0.57\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007987, train_acc: 0.84\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009757, train_acc: 0.68\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009968, train_acc: 0.68\n",
      "alexnet1d, trial.82:\n",
      "Epoch 17, avg test_loss: 0.010108, test_acc: 0.57\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009961, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.010186, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009703, train_acc: 0.79\n",
      "alexnet1d, trial.82:\n",
      "Epoch 18, avg test_loss: 0.012008, test_acc: 0.57\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008584, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008578, train_acc: 0.75\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008234, train_acc: 0.77\n",
      "alexnet1d, trial.82:\n",
      "Epoch 19, avg test_loss: 0.011853, test_acc: 0.60\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007156, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.009832, train_acc: 0.71\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009175, train_acc: 0.77\n",
      "alexnet1d, trial.82:\n",
      "Epoch 20, avg test_loss: 0.010679, test_acc: 0.60\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007980, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007434, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007581, train_acc: 0.82\n",
      "alexnet1d, trial.82:\n",
      "Epoch 21, avg test_loss: 0.014168, test_acc: 0.53\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006433, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007496, train_acc: 0.77\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008961, train_acc: 0.86\n",
      "alexnet1d, trial.82:\n",
      "Epoch 22, avg test_loss: 0.015033, test_acc: 0.57\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.003990, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005592, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.007703, train_acc: 0.79\n",
      "alexnet1d, trial.82:\n",
      "Epoch 23, avg test_loss: 0.011325, test_acc: 0.63\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005232, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004961, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.008211, train_acc: 0.82\n",
      "alexnet1d, trial.82:\n",
      "Epoch 24, avg test_loss: 0.015404, test_acc: 0.57\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005226, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004765, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006360, train_acc: 0.77\n",
      "alexnet1d, trial.82:\n",
      "Epoch 25, avg test_loss: 0.017307, test_acc: 0.57\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004252, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005907, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.005018, train_acc: 0.91\n",
      "alexnet1d, trial.82:\n",
      "Epoch 26, avg test_loss: 0.017009, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.002267, train_acc: 0.98\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.005443, train_acc: 0.88\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.005999, train_acc: 0.88\n",
      "alexnet1d, trial.82:\n",
      "Epoch 27, avg test_loss: 0.020804, test_acc: 0.50\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.002692, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.003228, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.005487, train_acc: 0.88\n",
      "alexnet1d, trial.82:\n",
      "Epoch 28, avg test_loss: 0.018368, test_acc: 0.57\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.003099, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.005662, train_acc: 0.88\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.002896, train_acc: 0.96\n",
      "alexnet1d, trial.82:\n",
      "Epoch 29, avg test_loss: 0.021102, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.003749, train_acc: 0.95\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.002238, train_acc: 0.96\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.002941, train_acc: 0.95\n",
      "alexnet1d, trial.82:\n",
      "Epoch 30, avg test_loss: 0.018435, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.002823, train_acc: 0.95\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.002075, train_acc: 0.95\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.002716, train_acc: 0.95\n",
      "alexnet1d, trial.82:\n",
      "Epoch 31, avg test_loss: 0.023920, test_acc: 0.49\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号42个\n",
      "错误信号28个\n",
      "信号正确并预测正确的概率为0.357\n",
      "信号错误并预测正确的概率为0.129\n",
      "总正确率为0.49\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012368, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012314, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012388, train_acc: 0.57\n",
      "alexnet1d, trial.83:\n",
      "Epoch 0, avg test_loss: 0.009388, test_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012043, train_acc: 0.66\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012212, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012312, train_acc: 0.55\n",
      "alexnet1d, trial.83:\n",
      "Epoch 1, avg test_loss: 0.009827, test_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012312, train_acc: 0.52\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012294, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011963, train_acc: 0.68\n",
      "alexnet1d, trial.83:\n",
      "Epoch 2, avg test_loss: 0.009644, test_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012168, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012028, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011627, train_acc: 0.68\n",
      "alexnet1d, trial.83:\n",
      "Epoch 3, avg test_loss: 0.009507, test_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012444, train_acc: 0.52\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011532, train_acc: 0.66\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012037, train_acc: 0.59\n",
      "alexnet1d, trial.83:\n",
      "Epoch 4, avg test_loss: 0.009423, test_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012187, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011548, train_acc: 0.70\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012414, train_acc: 0.50\n",
      "alexnet1d, trial.83:\n",
      "Epoch 5, avg test_loss: 0.009418, test_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011534, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011726, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011667, train_acc: 0.64\n",
      "alexnet1d, trial.83:\n",
      "Epoch 6, avg test_loss: 0.009419, test_acc: 0.63\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012036, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012679, train_acc: 0.50\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011830, train_acc: 0.57\n",
      "alexnet1d, trial.83:\n",
      "Epoch 7, avg test_loss: 0.009359, test_acc: 0.69\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011973, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012601, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011027, train_acc: 0.62\n",
      "alexnet1d, trial.83:\n",
      "Epoch 8, avg test_loss: 0.009460, test_acc: 0.63\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010989, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011617, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010363, train_acc: 0.66\n",
      "alexnet1d, trial.83:\n",
      "Epoch 9, avg test_loss: 0.009675, test_acc: 0.60\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010377, train_acc: 0.71\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010743, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011572, train_acc: 0.59\n",
      "alexnet1d, trial.83:\n",
      "Epoch 10, avg test_loss: 0.009550, test_acc: 0.63\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011306, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010402, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011755, train_acc: 0.59\n",
      "alexnet1d, trial.83:\n",
      "Epoch 11, avg test_loss: 0.009722, test_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010784, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.009320, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010616, train_acc: 0.66\n",
      "alexnet1d, trial.83:\n",
      "Epoch 12, avg test_loss: 0.010406, test_acc: 0.57\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.008094, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.012711, train_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.012344, train_acc: 0.62\n",
      "alexnet1d, trial.83:\n",
      "Epoch 13, avg test_loss: 0.010464, test_acc: 0.50\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009999, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011040, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011141, train_acc: 0.64\n",
      "alexnet1d, trial.83:\n",
      "Epoch 14, avg test_loss: 0.009876, test_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010461, train_acc: 0.68\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009283, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009905, train_acc: 0.70\n",
      "alexnet1d, trial.83:\n",
      "Epoch 15, avg test_loss: 0.010137, test_acc: 0.63\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009324, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010626, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.011557, train_acc: 0.64\n",
      "alexnet1d, trial.83:\n",
      "Epoch 16, avg test_loss: 0.010709, test_acc: 0.57\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008802, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009189, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008251, train_acc: 0.79\n",
      "alexnet1d, trial.83:\n",
      "Epoch 17, avg test_loss: 0.011247, test_acc: 0.56\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009708, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007922, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009160, train_acc: 0.73\n",
      "alexnet1d, trial.83:\n",
      "Epoch 18, avg test_loss: 0.011091, test_acc: 0.56\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007175, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007213, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009709, train_acc: 0.75\n",
      "alexnet1d, trial.83:\n",
      "Epoch 19, avg test_loss: 0.012266, test_acc: 0.66\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.009049, train_acc: 0.75\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.010540, train_acc: 0.75\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007817, train_acc: 0.77\n",
      "alexnet1d, trial.83:\n",
      "Epoch 20, avg test_loss: 0.011345, test_acc: 0.59\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006456, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007917, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.009525, train_acc: 0.75\n",
      "alexnet1d, trial.83:\n",
      "Epoch 21, avg test_loss: 0.012055, test_acc: 0.56\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007520, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006380, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007850, train_acc: 0.84\n",
      "alexnet1d, trial.83:\n",
      "Epoch 22, avg test_loss: 0.012101, test_acc: 0.59\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006054, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006187, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.009146, train_acc: 0.88\n",
      "alexnet1d, trial.83:\n",
      "Epoch 23, avg test_loss: 0.014776, test_acc: 0.59\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006266, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005534, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005274, train_acc: 0.91\n",
      "alexnet1d, trial.83:\n",
      "Epoch 24, avg test_loss: 0.016241, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.006034, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006608, train_acc: 0.80\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006287, train_acc: 0.84\n",
      "alexnet1d, trial.83:\n",
      "Epoch 25, avg test_loss: 0.015759, test_acc: 0.54\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004657, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005157, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.006987, train_acc: 0.80\n",
      "alexnet1d, trial.83:\n",
      "Epoch 26, avg test_loss: 0.017887, test_acc: 0.51\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.006168, train_acc: 0.82\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004090, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.006735, train_acc: 0.86\n",
      "alexnet1d, trial.83:\n",
      "Epoch 27, avg test_loss: 0.015005, test_acc: 0.56\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004871, train_acc: 0.88\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.007237, train_acc: 0.80\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.005659, train_acc: 0.86\n",
      "alexnet1d, trial.83:\n",
      "Epoch 28, avg test_loss: 0.016035, test_acc: 0.56\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.003908, train_acc: 0.96\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.004859, train_acc: 0.89\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.002186, train_acc: 0.98\n",
      "alexnet1d, trial.83:\n",
      "Epoch 29, avg test_loss: 0.021765, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.004794, train_acc: 0.88\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.003556, train_acc: 0.95\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.003535, train_acc: 0.93\n",
      "alexnet1d, trial.83:\n",
      "Epoch 30, avg test_loss: 0.019369, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.003839, train_acc: 0.93\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.001721, train_acc: 0.98\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.003023, train_acc: 0.93\n",
      "alexnet1d, trial.83:\n",
      "Epoch 31, avg test_loss: 0.024825, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号43个\n",
      "错误信号27个\n",
      "信号正确并预测正确的概率为0.371\n",
      "信号错误并预测正确的概率为0.2\n",
      "总正确率为0.57\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012239, train_acc: 0.71\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012385, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.013231, train_acc: 0.48\n",
      "alexnet1d, trial.84:\n",
      "Epoch 0, avg test_loss: 0.009906, test_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012391, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012154, train_acc: 0.66\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012161, train_acc: 0.57\n",
      "alexnet1d, trial.84:\n",
      "Epoch 1, avg test_loss: 0.009949, test_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012187, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012017, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012037, train_acc: 0.55\n",
      "alexnet1d, trial.84:\n",
      "Epoch 2, avg test_loss: 0.010054, test_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012159, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011891, train_acc: 0.55\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011698, train_acc: 0.64\n",
      "alexnet1d, trial.84:\n",
      "Epoch 3, avg test_loss: 0.010009, test_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011893, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012578, train_acc: 0.48\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011088, train_acc: 0.68\n",
      "alexnet1d, trial.84:\n",
      "Epoch 4, avg test_loss: 0.010201, test_acc: 0.53\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011538, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012700, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.013080, train_acc: 0.54\n",
      "alexnet1d, trial.84:\n",
      "Epoch 5, avg test_loss: 0.010226, test_acc: 0.56\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011501, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.010821, train_acc: 0.71\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012102, train_acc: 0.61\n",
      "alexnet1d, trial.84:\n",
      "Epoch 6, avg test_loss: 0.010197, test_acc: 0.56\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011894, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.010422, train_acc: 0.75\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.013005, train_acc: 0.57\n",
      "alexnet1d, trial.84:\n",
      "Epoch 7, avg test_loss: 0.011365, test_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011926, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.009567, train_acc: 0.70\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.014654, train_acc: 0.68\n",
      "alexnet1d, trial.84:\n",
      "Epoch 8, avg test_loss: 0.011274, test_acc: 0.51\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010743, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010463, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012834, train_acc: 0.54\n",
      "alexnet1d, trial.84:\n",
      "Epoch 9, avg test_loss: 0.010166, test_acc: 0.63\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011278, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011153, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010722, train_acc: 0.62\n",
      "alexnet1d, trial.84:\n",
      "Epoch 10, avg test_loss: 0.010570, test_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010450, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010682, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.008932, train_acc: 0.75\n",
      "alexnet1d, trial.84:\n",
      "Epoch 11, avg test_loss: 0.012784, test_acc: 0.56\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011903, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.008084, train_acc: 0.80\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.013892, train_acc: 0.57\n",
      "alexnet1d, trial.84:\n",
      "Epoch 12, avg test_loss: 0.012737, test_acc: 0.56\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.008796, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010657, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.008822, train_acc: 0.77\n",
      "alexnet1d, trial.84:\n",
      "Epoch 13, avg test_loss: 0.013306, test_acc: 0.57\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.007765, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009474, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010747, train_acc: 0.70\n",
      "alexnet1d, trial.84:\n",
      "Epoch 14, avg test_loss: 0.016303, test_acc: 0.56\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008126, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008831, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.007966, train_acc: 0.80\n",
      "alexnet1d, trial.84:\n",
      "Epoch 15, avg test_loss: 0.016620, test_acc: 0.51\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.006825, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.007808, train_acc: 0.80\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.007214, train_acc: 0.80\n",
      "alexnet1d, trial.84:\n",
      "Epoch 16, avg test_loss: 0.019333, test_acc: 0.56\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009494, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.005812, train_acc: 0.91\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.006853, train_acc: 0.82\n",
      "alexnet1d, trial.84:\n",
      "Epoch 17, avg test_loss: 0.017334, test_acc: 0.49\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.004623, train_acc: 0.96\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.006786, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.006995, train_acc: 0.82\n",
      "alexnet1d, trial.84:\n",
      "Epoch 18, avg test_loss: 0.023596, test_acc: 0.57\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007796, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008119, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.004919, train_acc: 0.89\n",
      "alexnet1d, trial.84:\n",
      "Epoch 19, avg test_loss: 0.020672, test_acc: 0.47\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.005099, train_acc: 0.89\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.004370, train_acc: 0.91\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006949, train_acc: 0.79\n",
      "alexnet1d, trial.84:\n",
      "Epoch 20, avg test_loss: 0.020519, test_acc: 0.57\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.005874, train_acc: 0.91\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.004514, train_acc: 0.93\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.005991, train_acc: 0.80\n",
      "alexnet1d, trial.84:\n",
      "Epoch 21, avg test_loss: 0.025818, test_acc: 0.60\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.004669, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005157, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.004479, train_acc: 0.93\n",
      "alexnet1d, trial.84:\n",
      "Epoch 22, avg test_loss: 0.027385, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.003388, train_acc: 0.95\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007801, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.004835, train_acc: 0.86\n",
      "alexnet1d, trial.84:\n",
      "Epoch 23, avg test_loss: 0.026824, test_acc: 0.56\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006707, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004884, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.004948, train_acc: 0.89\n",
      "alexnet1d, trial.84:\n",
      "Epoch 24, avg test_loss: 0.025590, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004577, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.002300, train_acc: 0.96\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004323, train_acc: 0.89\n",
      "alexnet1d, trial.84:\n",
      "Epoch 25, avg test_loss: 0.023740, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号38个\n",
      "错误信号32个\n",
      "信号正确并预测正确的概率为0.314\n",
      "信号错误并预测正确的概率为0.243\n",
      "总正确率为0.56\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012460, train_acc: 0.38\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012775, train_acc: 0.52\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012349, train_acc: 0.52\n",
      "alexnet1d, trial.85:\n",
      "Epoch 0, avg test_loss: 0.009557, test_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012609, train_acc: 0.48\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012353, train_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012130, train_acc: 0.61\n",
      "alexnet1d, trial.85:\n",
      "Epoch 1, avg test_loss: 0.009602, test_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012111, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012497, train_acc: 0.50\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011848, train_acc: 0.62\n",
      "alexnet1d, trial.85:\n",
      "Epoch 2, avg test_loss: 0.009516, test_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012287, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011535, train_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012406, train_acc: 0.54\n",
      "alexnet1d, trial.85:\n",
      "Epoch 3, avg test_loss: 0.009513, test_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012276, train_acc: 0.48\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011727, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011886, train_acc: 0.62\n",
      "alexnet1d, trial.85:\n",
      "Epoch 4, avg test_loss: 0.009516, test_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011917, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012560, train_acc: 0.48\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012392, train_acc: 0.54\n",
      "alexnet1d, trial.85:\n",
      "Epoch 5, avg test_loss: 0.009541, test_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011171, train_acc: 0.75\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011261, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011701, train_acc: 0.59\n",
      "alexnet1d, trial.85:\n",
      "Epoch 6, avg test_loss: 0.009587, test_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011053, train_acc: 0.68\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012234, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011108, train_acc: 0.64\n",
      "alexnet1d, trial.85:\n",
      "Epoch 7, avg test_loss: 0.009606, test_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011055, train_acc: 0.73\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011184, train_acc: 0.52\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.010878, train_acc: 0.64\n",
      "alexnet1d, trial.85:\n",
      "Epoch 8, avg test_loss: 0.010703, test_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011386, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010860, train_acc: 0.71\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011070, train_acc: 0.68\n",
      "alexnet1d, trial.85:\n",
      "Epoch 9, avg test_loss: 0.010463, test_acc: 0.50\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010906, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011782, train_acc: 0.71\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.009889, train_acc: 0.68\n",
      "alexnet1d, trial.85:\n",
      "Epoch 10, avg test_loss: 0.011656, test_acc: 0.53\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011086, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011583, train_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010492, train_acc: 0.75\n",
      "alexnet1d, trial.85:\n",
      "Epoch 11, avg test_loss: 0.010737, test_acc: 0.51\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009792, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011391, train_acc: 0.77\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010295, train_acc: 0.62\n",
      "alexnet1d, trial.85:\n",
      "Epoch 12, avg test_loss: 0.012088, test_acc: 0.50\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010472, train_acc: 0.62\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.008333, train_acc: 0.82\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.014684, train_acc: 0.64\n",
      "alexnet1d, trial.85:\n",
      "Epoch 13, avg test_loss: 0.013120, test_acc: 0.51\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010637, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010399, train_acc: 0.62\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009119, train_acc: 0.80\n",
      "alexnet1d, trial.85:\n",
      "Epoch 14, avg test_loss: 0.011534, test_acc: 0.53\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008399, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.011107, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011301, train_acc: 0.71\n",
      "alexnet1d, trial.85:\n",
      "Epoch 15, avg test_loss: 0.012845, test_acc: 0.53\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.007936, train_acc: 0.84\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009222, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.011084, train_acc: 0.68\n",
      "alexnet1d, trial.85:\n",
      "Epoch 16, avg test_loss: 0.015038, test_acc: 0.49\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009072, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007273, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008073, train_acc: 0.79\n",
      "alexnet1d, trial.85:\n",
      "Epoch 17, avg test_loss: 0.017981, test_acc: 0.50\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.006386, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007566, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008451, train_acc: 0.73\n",
      "alexnet1d, trial.85:\n",
      "Epoch 18, avg test_loss: 0.018085, test_acc: 0.49\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.004972, train_acc: 0.91\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.005731, train_acc: 0.88\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007517, train_acc: 0.84\n",
      "alexnet1d, trial.85:\n",
      "Epoch 19, avg test_loss: 0.015522, test_acc: 0.51\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006138, train_acc: 0.89\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007967, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.010013, train_acc: 0.77\n",
      "alexnet1d, trial.85:\n",
      "Epoch 20, avg test_loss: 0.020606, test_acc: 0.54\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007468, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007094, train_acc: 0.77\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008352, train_acc: 0.82\n",
      "alexnet1d, trial.85:\n",
      "Epoch 21, avg test_loss: 0.016774, test_acc: 0.47\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005900, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005455, train_acc: 0.93\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006518, train_acc: 0.82\n",
      "alexnet1d, trial.85:\n",
      "Epoch 22, avg test_loss: 0.018932, test_acc: 0.49\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006410, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005162, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.005895, train_acc: 0.86\n",
      "alexnet1d, trial.85:\n",
      "Epoch 23, avg test_loss: 0.025308, test_acc: 0.44\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006670, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004020, train_acc: 0.93\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.003606, train_acc: 0.91\n",
      "alexnet1d, trial.85:\n",
      "Epoch 24, avg test_loss: 0.027553, test_acc: 0.41\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004514, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.003742, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005395, train_acc: 0.86\n",
      "alexnet1d, trial.85:\n",
      "Epoch 25, avg test_loss: 0.029079, test_acc: 0.43\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.002877, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004572, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.003545, train_acc: 0.91\n",
      "alexnet1d, trial.85:\n",
      "Epoch 26, avg test_loss: 0.035261, test_acc: 0.49\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004274, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.000858, train_acc: 1.00\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003618, train_acc: 0.93\n",
      "alexnet1d, trial.85:\n",
      "Epoch 27, avg test_loss: 0.034284, test_acc: 0.47\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003832, train_acc: 0.96\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.001987, train_acc: 0.96\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.001655, train_acc: 0.96\n",
      "alexnet1d, trial.85:\n",
      "Epoch 28, avg test_loss: 0.040391, test_acc: 0.47\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号45个\n",
      "错误信号25个\n",
      "信号正确并预测正确的概率为0.4\n",
      "信号错误并预测正确的概率为0.071\n",
      "总正确率为0.47\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012433, train_acc: 0.48\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012847, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012048, train_acc: 0.59\n",
      "alexnet1d, trial.86:\n",
      "Epoch 0, avg test_loss: 0.009486, test_acc: 0.63\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012704, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012393, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011975, train_acc: 0.66\n",
      "alexnet1d, trial.86:\n",
      "Epoch 1, avg test_loss: 0.009670, test_acc: 0.63\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012383, train_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012146, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012287, train_acc: 0.52\n",
      "alexnet1d, trial.86:\n",
      "Epoch 2, avg test_loss: 0.009443, test_acc: 0.63\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012328, train_acc: 0.55\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012756, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011953, train_acc: 0.62\n",
      "alexnet1d, trial.86:\n",
      "Epoch 3, avg test_loss: 0.009516, test_acc: 0.63\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012173, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012035, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012422, train_acc: 0.57\n",
      "alexnet1d, trial.86:\n",
      "Epoch 4, avg test_loss: 0.009536, test_acc: 0.63\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012078, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012216, train_acc: 0.50\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011793, train_acc: 0.64\n",
      "alexnet1d, trial.86:\n",
      "Epoch 5, avg test_loss: 0.009360, test_acc: 0.63\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012208, train_acc: 0.52\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011977, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011513, train_acc: 0.66\n",
      "alexnet1d, trial.86:\n",
      "Epoch 6, avg test_loss: 0.009199, test_acc: 0.67\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011877, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011507, train_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012055, train_acc: 0.62\n",
      "alexnet1d, trial.86:\n",
      "Epoch 7, avg test_loss: 0.009018, test_acc: 0.70\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011422, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011095, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011942, train_acc: 0.55\n",
      "alexnet1d, trial.86:\n",
      "Epoch 8, avg test_loss: 0.009191, test_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011760, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011898, train_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011999, train_acc: 0.70\n",
      "alexnet1d, trial.86:\n",
      "Epoch 9, avg test_loss: 0.009179, test_acc: 0.73\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011836, train_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012862, train_acc: 0.45\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011317, train_acc: 0.59\n",
      "alexnet1d, trial.86:\n",
      "Epoch 10, avg test_loss: 0.008791, test_acc: 0.74\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011314, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.012248, train_acc: 0.52\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011093, train_acc: 0.61\n",
      "alexnet1d, trial.86:\n",
      "Epoch 11, avg test_loss: 0.008852, test_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011745, train_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010189, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.012546, train_acc: 0.52\n",
      "alexnet1d, trial.86:\n",
      "Epoch 12, avg test_loss: 0.008948, test_acc: 0.74\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011074, train_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009999, train_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011439, train_acc: 0.57\n",
      "alexnet1d, trial.86:\n",
      "Epoch 13, avg test_loss: 0.009495, test_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010582, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010251, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009901, train_acc: 0.64\n",
      "alexnet1d, trial.86:\n",
      "Epoch 14, avg test_loss: 0.010057, test_acc: 0.63\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010452, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010946, train_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009702, train_acc: 0.75\n",
      "alexnet1d, trial.86:\n",
      "Epoch 15, avg test_loss: 0.010770, test_acc: 0.69\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009486, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.007243, train_acc: 0.86\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010028, train_acc: 0.70\n",
      "alexnet1d, trial.86:\n",
      "Epoch 16, avg test_loss: 0.011397, test_acc: 0.61\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008505, train_acc: 0.71\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008350, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010470, train_acc: 0.64\n",
      "alexnet1d, trial.86:\n",
      "Epoch 17, avg test_loss: 0.012600, test_acc: 0.51\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.010623, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008050, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.010012, train_acc: 0.73\n",
      "alexnet1d, trial.86:\n",
      "Epoch 18, avg test_loss: 0.012006, test_acc: 0.61\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.010125, train_acc: 0.70\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007508, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008257, train_acc: 0.77\n",
      "alexnet1d, trial.86:\n",
      "Epoch 19, avg test_loss: 0.012257, test_acc: 0.67\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007005, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.006313, train_acc: 0.89\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009061, train_acc: 0.73\n",
      "alexnet1d, trial.86:\n",
      "Epoch 20, avg test_loss: 0.012457, test_acc: 0.57\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008409, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007293, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008834, train_acc: 0.73\n",
      "alexnet1d, trial.86:\n",
      "Epoch 21, avg test_loss: 0.012618, test_acc: 0.66\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007872, train_acc: 0.79\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006720, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.005420, train_acc: 0.89\n",
      "alexnet1d, trial.86:\n",
      "Epoch 22, avg test_loss: 0.015614, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.004865, train_acc: 0.95\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007532, train_acc: 0.79\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.005659, train_acc: 0.84\n",
      "alexnet1d, trial.86:\n",
      "Epoch 23, avg test_loss: 0.015433, test_acc: 0.66\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006233, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004393, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006456, train_acc: 0.88\n",
      "alexnet1d, trial.86:\n",
      "Epoch 24, avg test_loss: 0.017722, test_acc: 0.57\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005646, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005515, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005499, train_acc: 0.95\n",
      "alexnet1d, trial.86:\n",
      "Epoch 25, avg test_loss: 0.018685, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004208, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005407, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.002266, train_acc: 0.98\n",
      "alexnet1d, trial.86:\n",
      "Epoch 26, avg test_loss: 0.019297, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.001849, train_acc: 0.96\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004198, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.005222, train_acc: 0.88\n",
      "alexnet1d, trial.86:\n",
      "Epoch 27, avg test_loss: 0.021668, test_acc: 0.60\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.002263, train_acc: 0.96\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004970, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.003739, train_acc: 0.91\n",
      "alexnet1d, trial.86:\n",
      "Epoch 28, avg test_loss: 0.020669, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号44个\n",
      "错误信号26个\n",
      "信号正确并预测正确的概率为0.4\n",
      "信号错误并预测正确的概率为0.243\n",
      "总正确率为0.64\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012422, train_acc: 0.50\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012071, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012591, train_acc: 0.62\n",
      "alexnet1d, trial.87:\n",
      "Epoch 0, avg test_loss: 0.009883, test_acc: 0.49\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012500, train_acc: 0.48\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012429, train_acc: 0.46\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012109, train_acc: 0.68\n",
      "alexnet1d, trial.87:\n",
      "Epoch 1, avg test_loss: 0.009819, test_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012402, train_acc: 0.52\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012792, train_acc: 0.52\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011754, train_acc: 0.64\n",
      "alexnet1d, trial.87:\n",
      "Epoch 2, avg test_loss: 0.010022, test_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.010916, train_acc: 0.68\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.013448, train_acc: 0.48\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012242, train_acc: 0.55\n",
      "alexnet1d, trial.87:\n",
      "Epoch 3, avg test_loss: 0.009818, test_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012130, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011867, train_acc: 0.68\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011894, train_acc: 0.68\n",
      "alexnet1d, trial.87:\n",
      "Epoch 4, avg test_loss: 0.009713, test_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012107, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011925, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011791, train_acc: 0.61\n",
      "alexnet1d, trial.87:\n",
      "Epoch 5, avg test_loss: 0.009686, test_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011173, train_acc: 0.68\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012036, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012046, train_acc: 0.54\n",
      "alexnet1d, trial.87:\n",
      "Epoch 6, avg test_loss: 0.010049, test_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011898, train_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011410, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011540, train_acc: 0.52\n",
      "alexnet1d, trial.87:\n",
      "Epoch 7, avg test_loss: 0.010190, test_acc: 0.53\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011175, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.010884, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011887, train_acc: 0.62\n",
      "alexnet1d, trial.87:\n",
      "Epoch 8, avg test_loss: 0.011407, test_acc: 0.43\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011759, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010807, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011893, train_acc: 0.62\n",
      "alexnet1d, trial.87:\n",
      "Epoch 9, avg test_loss: 0.011673, test_acc: 0.50\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010135, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010858, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012366, train_acc: 0.59\n",
      "alexnet1d, trial.87:\n",
      "Epoch 10, avg test_loss: 0.011571, test_acc: 0.43\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010037, train_acc: 0.75\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010266, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011870, train_acc: 0.66\n",
      "alexnet1d, trial.87:\n",
      "Epoch 11, avg test_loss: 0.011899, test_acc: 0.49\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010511, train_acc: 0.75\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010716, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010363, train_acc: 0.71\n",
      "alexnet1d, trial.87:\n",
      "Epoch 12, avg test_loss: 0.011125, test_acc: 0.49\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.008957, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011024, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009254, train_acc: 0.71\n",
      "alexnet1d, trial.87:\n",
      "Epoch 13, avg test_loss: 0.012035, test_acc: 0.50\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009214, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010372, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009995, train_acc: 0.64\n",
      "alexnet1d, trial.87:\n",
      "Epoch 14, avg test_loss: 0.013706, test_acc: 0.40\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008787, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009634, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010782, train_acc: 0.70\n",
      "alexnet1d, trial.87:\n",
      "Epoch 15, avg test_loss: 0.013825, test_acc: 0.50\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010687, train_acc: 0.68\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009149, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009436, train_acc: 0.75\n",
      "alexnet1d, trial.87:\n",
      "Epoch 16, avg test_loss: 0.012981, test_acc: 0.51\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008377, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009997, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007963, train_acc: 0.75\n",
      "alexnet1d, trial.87:\n",
      "Epoch 17, avg test_loss: 0.014294, test_acc: 0.50\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009159, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009107, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007231, train_acc: 0.79\n",
      "alexnet1d, trial.87:\n",
      "Epoch 18, avg test_loss: 0.016474, test_acc: 0.47\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006527, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.006898, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007340, train_acc: 0.82\n",
      "alexnet1d, trial.87:\n",
      "Epoch 19, avg test_loss: 0.017566, test_acc: 0.41\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007643, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008085, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007271, train_acc: 0.79\n",
      "alexnet1d, trial.87:\n",
      "Epoch 20, avg test_loss: 0.020231, test_acc: 0.44\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007347, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.005494, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007491, train_acc: 0.80\n",
      "alexnet1d, trial.87:\n",
      "Epoch 21, avg test_loss: 0.020443, test_acc: 0.50\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007829, train_acc: 0.77\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006322, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007648, train_acc: 0.80\n",
      "alexnet1d, trial.87:\n",
      "Epoch 22, avg test_loss: 0.021511, test_acc: 0.43\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007591, train_acc: 0.73\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006782, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.003846, train_acc: 0.91\n",
      "alexnet1d, trial.87:\n",
      "Epoch 23, avg test_loss: 0.019118, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004083, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.007782, train_acc: 0.77\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005637, train_acc: 0.86\n",
      "alexnet1d, trial.87:\n",
      "Epoch 24, avg test_loss: 0.020413, test_acc: 0.43\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005403, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005884, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005515, train_acc: 0.82\n",
      "alexnet1d, trial.87:\n",
      "Epoch 25, avg test_loss: 0.019892, test_acc: 0.44\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004010, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005111, train_acc: 0.86\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004047, train_acc: 0.91\n",
      "alexnet1d, trial.87:\n",
      "Epoch 26, avg test_loss: 0.022958, test_acc: 0.47\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.005146, train_acc: 0.88\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004864, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.007294, train_acc: 0.80\n",
      "alexnet1d, trial.87:\n",
      "Epoch 27, avg test_loss: 0.023755, test_acc: 0.49\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003600, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.002392, train_acc: 0.98\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.004894, train_acc: 0.88\n",
      "alexnet1d, trial.87:\n",
      "Epoch 28, avg test_loss: 0.023975, test_acc: 0.56\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.003661, train_acc: 0.88\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.002957, train_acc: 0.96\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.003730, train_acc: 0.88\n",
      "alexnet1d, trial.87:\n",
      "Epoch 29, avg test_loss: 0.031595, test_acc: 0.47\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.002621, train_acc: 0.95\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.004751, train_acc: 0.88\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.001928, train_acc: 0.98\n",
      "alexnet1d, trial.87:\n",
      "Epoch 30, avg test_loss: 0.032262, test_acc: 0.46\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.001321, train_acc: 1.00\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.003186, train_acc: 0.93\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.002821, train_acc: 0.91\n",
      "alexnet1d, trial.87:\n",
      "Epoch 31, avg test_loss: 0.031076, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号41个\n",
      "错误信号29个\n",
      "信号正确并预测正确的概率为0.329\n",
      "信号错误并预测正确的概率为0.229\n",
      "总正确率为0.56\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012347, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012546, train_acc: 0.61\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012221, train_acc: 0.57\n",
      "alexnet1d, trial.88:\n",
      "Epoch 0, avg test_loss: 0.009845, test_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012227, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012240, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011937, train_acc: 0.62\n",
      "alexnet1d, trial.88:\n",
      "Epoch 1, avg test_loss: 0.009826, test_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011865, train_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011696, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012604, train_acc: 0.57\n",
      "alexnet1d, trial.88:\n",
      "Epoch 2, avg test_loss: 0.009759, test_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011999, train_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012265, train_acc: 0.48\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012427, train_acc: 0.57\n",
      "alexnet1d, trial.88:\n",
      "Epoch 3, avg test_loss: 0.009774, test_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012082, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011763, train_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012319, train_acc: 0.50\n",
      "alexnet1d, trial.88:\n",
      "Epoch 4, avg test_loss: 0.009849, test_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012977, train_acc: 0.46\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012179, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011886, train_acc: 0.64\n",
      "alexnet1d, trial.88:\n",
      "Epoch 5, avg test_loss: 0.009732, test_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011986, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011902, train_acc: 0.66\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011873, train_acc: 0.62\n",
      "alexnet1d, trial.88:\n",
      "Epoch 6, avg test_loss: 0.009715, test_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011924, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012218, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011498, train_acc: 0.62\n",
      "alexnet1d, trial.88:\n",
      "Epoch 7, avg test_loss: 0.009721, test_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011226, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012684, train_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012242, train_acc: 0.50\n",
      "alexnet1d, trial.88:\n",
      "Epoch 8, avg test_loss: 0.009614, test_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012071, train_acc: 0.50\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011739, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012246, train_acc: 0.55\n",
      "alexnet1d, trial.88:\n",
      "Epoch 9, avg test_loss: 0.009579, test_acc: 0.63\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011401, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011448, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012881, train_acc: 0.46\n",
      "alexnet1d, trial.88:\n",
      "Epoch 10, avg test_loss: 0.009678, test_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011552, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011953, train_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011809, train_acc: 0.57\n",
      "alexnet1d, trial.88:\n",
      "Epoch 11, avg test_loss: 0.009261, test_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010999, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011631, train_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011417, train_acc: 0.66\n",
      "alexnet1d, trial.88:\n",
      "Epoch 12, avg test_loss: 0.009191, test_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010470, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010449, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011514, train_acc: 0.62\n",
      "alexnet1d, trial.88:\n",
      "Epoch 13, avg test_loss: 0.009173, test_acc: 0.64\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009460, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.012597, train_acc: 0.57\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.012160, train_acc: 0.59\n",
      "alexnet1d, trial.88:\n",
      "Epoch 14, avg test_loss: 0.009449, test_acc: 0.67\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009901, train_acc: 0.80\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.011437, train_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009387, train_acc: 0.79\n",
      "alexnet1d, trial.88:\n",
      "Epoch 15, avg test_loss: 0.009656, test_acc: 0.63\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010055, train_acc: 0.68\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010121, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009014, train_acc: 0.75\n",
      "alexnet1d, trial.88:\n",
      "Epoch 16, avg test_loss: 0.009678, test_acc: 0.63\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008272, train_acc: 0.86\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.011410, train_acc: 0.59\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.012376, train_acc: 0.62\n",
      "alexnet1d, trial.88:\n",
      "Epoch 17, avg test_loss: 0.010541, test_acc: 0.64\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009670, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008191, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007942, train_acc: 0.84\n",
      "alexnet1d, trial.88:\n",
      "Epoch 18, avg test_loss: 0.010138, test_acc: 0.64\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009570, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.010656, train_acc: 0.70\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.012410, train_acc: 0.57\n",
      "alexnet1d, trial.88:\n",
      "Epoch 19, avg test_loss: 0.009735, test_acc: 0.67\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006756, train_acc: 0.88\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008607, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008676, train_acc: 0.77\n",
      "alexnet1d, trial.88:\n",
      "Epoch 20, avg test_loss: 0.010007, test_acc: 0.64\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008452, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.005575, train_acc: 0.96\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008305, train_acc: 0.82\n",
      "alexnet1d, trial.88:\n",
      "Epoch 21, avg test_loss: 0.010696, test_acc: 0.60\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006254, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.008207, train_acc: 0.77\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.005489, train_acc: 0.82\n",
      "alexnet1d, trial.88:\n",
      "Epoch 22, avg test_loss: 0.010912, test_acc: 0.66\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005416, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006523, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.004903, train_acc: 0.88\n",
      "alexnet1d, trial.88:\n",
      "Epoch 23, avg test_loss: 0.011706, test_acc: 0.66\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005671, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004425, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005962, train_acc: 0.82\n",
      "alexnet1d, trial.88:\n",
      "Epoch 24, avg test_loss: 0.014248, test_acc: 0.67\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004429, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004356, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004568, train_acc: 0.86\n",
      "alexnet1d, trial.88:\n",
      "Epoch 25, avg test_loss: 0.014742, test_acc: 0.70\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004623, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004229, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.006749, train_acc: 0.86\n",
      "alexnet1d, trial.88:\n",
      "Epoch 26, avg test_loss: 0.016792, test_acc: 0.64\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004684, train_acc: 0.88\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.003838, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.005121, train_acc: 0.91\n",
      "alexnet1d, trial.88:\n",
      "Epoch 27, avg test_loss: 0.017067, test_acc: 0.69\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.002835, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.003317, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.003013, train_acc: 0.93\n",
      "alexnet1d, trial.88:\n",
      "Epoch 28, avg test_loss: 0.016116, test_acc: 0.67\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.003638, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.002363, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.003053, train_acc: 0.93\n",
      "alexnet1d, trial.88:\n",
      "Epoch 29, avg test_loss: 0.016479, test_acc: 0.67\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.002184, train_acc: 0.98\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.004399, train_acc: 0.88\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.001738, train_acc: 0.96\n",
      "alexnet1d, trial.88:\n",
      "Epoch 30, avg test_loss: 0.020192, test_acc: 0.67\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号41个\n",
      "错误信号29个\n",
      "信号正确并预测正确的概率为0.5\n",
      "信号错误并预测正确的概率为0.171\n",
      "总正确率为0.67\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012257, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.015387, train_acc: 0.36\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.011174, train_acc: 0.68\n",
      "alexnet1d, trial.89:\n",
      "Epoch 0, avg test_loss: 0.010561, test_acc: 0.51\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.010928, train_acc: 0.70\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012845, train_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011825, train_acc: 0.64\n",
      "alexnet1d, trial.89:\n",
      "Epoch 1, avg test_loss: 0.009968, test_acc: 0.51\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012518, train_acc: 0.52\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011795, train_acc: 0.66\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012298, train_acc: 0.55\n",
      "alexnet1d, trial.89:\n",
      "Epoch 2, avg test_loss: 0.009961, test_acc: 0.51\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012962, train_acc: 0.41\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011981, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012480, train_acc: 0.54\n",
      "alexnet1d, trial.89:\n",
      "Epoch 3, avg test_loss: 0.009999, test_acc: 0.51\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011702, train_acc: 0.68\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.013214, train_acc: 0.43\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011165, train_acc: 0.73\n",
      "alexnet1d, trial.89:\n",
      "Epoch 4, avg test_loss: 0.010056, test_acc: 0.51\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012304, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012301, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012161, train_acc: 0.59\n",
      "alexnet1d, trial.89:\n",
      "Epoch 5, avg test_loss: 0.010183, test_acc: 0.51\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012620, train_acc: 0.52\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011940, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011406, train_acc: 0.68\n",
      "alexnet1d, trial.89:\n",
      "Epoch 6, avg test_loss: 0.010149, test_acc: 0.51\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012086, train_acc: 0.55\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012382, train_acc: 0.52\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011083, train_acc: 0.68\n",
      "alexnet1d, trial.89:\n",
      "Epoch 7, avg test_loss: 0.010242, test_acc: 0.51\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012392, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011241, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.010855, train_acc: 0.73\n",
      "alexnet1d, trial.89:\n",
      "Epoch 8, avg test_loss: 0.010306, test_acc: 0.51\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011700, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010776, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011624, train_acc: 0.59\n",
      "alexnet1d, trial.89:\n",
      "Epoch 9, avg test_loss: 0.010040, test_acc: 0.51\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011184, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012525, train_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011379, train_acc: 0.59\n",
      "alexnet1d, trial.89:\n",
      "Epoch 10, avg test_loss: 0.010224, test_acc: 0.53\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.012093, train_acc: 0.55\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011684, train_acc: 0.50\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010766, train_acc: 0.68\n",
      "alexnet1d, trial.89:\n",
      "Epoch 11, avg test_loss: 0.010437, test_acc: 0.51\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010165, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.012547, train_acc: 0.55\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011462, train_acc: 0.57\n",
      "alexnet1d, trial.89:\n",
      "Epoch 12, avg test_loss: 0.009769, test_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010317, train_acc: 0.75\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010856, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010955, train_acc: 0.68\n",
      "alexnet1d, trial.89:\n",
      "Epoch 13, avg test_loss: 0.010085, test_acc: 0.59\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010962, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009787, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011315, train_acc: 0.57\n",
      "alexnet1d, trial.89:\n",
      "Epoch 14, avg test_loss: 0.013067, test_acc: 0.51\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008204, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010127, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011356, train_acc: 0.68\n",
      "alexnet1d, trial.89:\n",
      "Epoch 15, avg test_loss: 0.011278, test_acc: 0.63\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009069, train_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010468, train_acc: 0.68\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010913, train_acc: 0.64\n",
      "alexnet1d, trial.89:\n",
      "Epoch 16, avg test_loss: 0.012298, test_acc: 0.51\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.010267, train_acc: 0.70\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009573, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009691, train_acc: 0.77\n",
      "alexnet1d, trial.89:\n",
      "Epoch 17, avg test_loss: 0.010949, test_acc: 0.54\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009035, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008516, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008785, train_acc: 0.75\n",
      "alexnet1d, trial.89:\n",
      "Epoch 18, avg test_loss: 0.014038, test_acc: 0.59\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008024, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009632, train_acc: 0.66\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.011688, train_acc: 0.71\n",
      "alexnet1d, trial.89:\n",
      "Epoch 19, avg test_loss: 0.012464, test_acc: 0.63\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008029, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.009622, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009308, train_acc: 0.79\n",
      "alexnet1d, trial.89:\n",
      "Epoch 20, avg test_loss: 0.013006, test_acc: 0.56\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006397, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.008011, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.011432, train_acc: 0.66\n",
      "alexnet1d, trial.89:\n",
      "Epoch 21, avg test_loss: 0.014127, test_acc: 0.57\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007159, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007715, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007813, train_acc: 0.82\n",
      "alexnet1d, trial.89:\n",
      "Epoch 22, avg test_loss: 0.014551, test_acc: 0.60\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006489, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006693, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006128, train_acc: 0.86\n",
      "alexnet1d, trial.89:\n",
      "Epoch 23, avg test_loss: 0.018889, test_acc: 0.60\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005485, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.009049, train_acc: 0.77\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006220, train_acc: 0.88\n",
      "alexnet1d, trial.89:\n",
      "Epoch 24, avg test_loss: 0.018417, test_acc: 0.54\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.008392, train_acc: 0.82\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004296, train_acc: 0.95\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.010665, train_acc: 0.70\n",
      "alexnet1d, trial.89:\n",
      "Epoch 25, avg test_loss: 0.019335, test_acc: 0.63\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.006402, train_acc: 0.82\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005471, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.006059, train_acc: 0.86\n",
      "alexnet1d, trial.89:\n",
      "Epoch 26, avg test_loss: 0.018293, test_acc: 0.59\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004672, train_acc: 0.86\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004003, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.005893, train_acc: 0.84\n",
      "alexnet1d, trial.89:\n",
      "Epoch 27, avg test_loss: 0.022725, test_acc: 0.61\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004942, train_acc: 0.89\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004122, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.006855, train_acc: 0.86\n",
      "alexnet1d, trial.89:\n",
      "Epoch 28, avg test_loss: 0.024974, test_acc: 0.59\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.004441, train_acc: 0.89\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.003674, train_acc: 0.91\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.003833, train_acc: 0.93\n",
      "alexnet1d, trial.89:\n",
      "Epoch 29, avg test_loss: 0.028503, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.004539, train_acc: 0.89\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.003883, train_acc: 0.93\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.002806, train_acc: 0.93\n",
      "alexnet1d, trial.89:\n",
      "Epoch 30, avg test_loss: 0.024358, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.002931, train_acc: 0.95\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.003678, train_acc: 0.93\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.003292, train_acc: 0.91\n",
      "alexnet1d, trial.89:\n",
      "Epoch 31, avg test_loss: 0.026242, test_acc: 0.66\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.002106, train_acc: 0.98\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.002616, train_acc: 0.96\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.001996, train_acc: 0.96\n",
      "alexnet1d, trial.89:\n",
      "Epoch 32, avg test_loss: 0.035398, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号36个\n",
      "错误信号34个\n",
      "信号正确并预测正确的概率为0.443\n",
      "信号错误并预测正确的概率为0.171\n",
      "总正确率为0.61\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012389, train_acc: 0.50\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.013145, train_acc: 0.48\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012343, train_acc: 0.55\n",
      "alexnet1d, trial.90:\n",
      "Epoch 0, avg test_loss: 0.009760, test_acc: 0.63\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012394, train_acc: 0.48\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012005, train_acc: 0.66\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011893, train_acc: 0.61\n",
      "alexnet1d, trial.90:\n",
      "Epoch 1, avg test_loss: 0.010279, test_acc: 0.63\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012728, train_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011931, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012143, train_acc: 0.64\n",
      "alexnet1d, trial.90:\n",
      "Epoch 2, avg test_loss: 0.009814, test_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012185, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012060, train_acc: 0.55\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012006, train_acc: 0.59\n",
      "alexnet1d, trial.90:\n",
      "Epoch 3, avg test_loss: 0.010389, test_acc: 0.60\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011978, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.013619, train_acc: 0.45\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011761, train_acc: 0.66\n",
      "alexnet1d, trial.90:\n",
      "Epoch 4, avg test_loss: 0.009870, test_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011838, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011897, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011683, train_acc: 0.64\n",
      "alexnet1d, trial.90:\n",
      "Epoch 5, avg test_loss: 0.010112, test_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011613, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011471, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.010962, train_acc: 0.66\n",
      "alexnet1d, trial.90:\n",
      "Epoch 6, avg test_loss: 0.010765, test_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011031, train_acc: 0.71\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.010717, train_acc: 0.71\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011189, train_acc: 0.70\n",
      "alexnet1d, trial.90:\n",
      "Epoch 7, avg test_loss: 0.010731, test_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010817, train_acc: 0.68\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011738, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.010647, train_acc: 0.75\n",
      "alexnet1d, trial.90:\n",
      "Epoch 8, avg test_loss: 0.011103, test_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010657, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011115, train_acc: 0.75\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010596, train_acc: 0.70\n",
      "alexnet1d, trial.90:\n",
      "Epoch 9, avg test_loss: 0.011302, test_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011501, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010259, train_acc: 0.71\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.009447, train_acc: 0.79\n",
      "alexnet1d, trial.90:\n",
      "Epoch 10, avg test_loss: 0.011876, test_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010259, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.009062, train_acc: 0.79\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010089, train_acc: 0.71\n",
      "alexnet1d, trial.90:\n",
      "Epoch 11, avg test_loss: 0.013162, test_acc: 0.60\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.007774, train_acc: 0.82\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011842, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011361, train_acc: 0.68\n",
      "alexnet1d, trial.90:\n",
      "Epoch 12, avg test_loss: 0.012720, test_acc: 0.54\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.008204, train_acc: 0.80\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.008747, train_acc: 0.75\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010107, train_acc: 0.68\n",
      "alexnet1d, trial.90:\n",
      "Epoch 13, avg test_loss: 0.013371, test_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009291, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.008235, train_acc: 0.80\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.008842, train_acc: 0.80\n",
      "alexnet1d, trial.90:\n",
      "Epoch 14, avg test_loss: 0.012652, test_acc: 0.54\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008364, train_acc: 0.84\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008224, train_acc: 0.84\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008409, train_acc: 0.71\n",
      "alexnet1d, trial.90:\n",
      "Epoch 15, avg test_loss: 0.013779, test_acc: 0.56\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.007696, train_acc: 0.80\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.005292, train_acc: 0.91\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.007216, train_acc: 0.82\n",
      "alexnet1d, trial.90:\n",
      "Epoch 16, avg test_loss: 0.018747, test_acc: 0.56\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.005327, train_acc: 0.86\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.006632, train_acc: 0.84\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007790, train_acc: 0.77\n",
      "alexnet1d, trial.90:\n",
      "Epoch 17, avg test_loss: 0.017051, test_acc: 0.49\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007374, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.005088, train_acc: 0.91\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.004350, train_acc: 0.93\n",
      "alexnet1d, trial.90:\n",
      "Epoch 18, avg test_loss: 0.020804, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.005509, train_acc: 0.89\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007290, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.005594, train_acc: 0.88\n",
      "alexnet1d, trial.90:\n",
      "Epoch 19, avg test_loss: 0.017924, test_acc: 0.56\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.004082, train_acc: 0.93\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.004563, train_acc: 0.89\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.003610, train_acc: 0.95\n",
      "alexnet1d, trial.90:\n",
      "Epoch 20, avg test_loss: 0.021345, test_acc: 0.50\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.003574, train_acc: 0.91\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006260, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.004259, train_acc: 0.91\n",
      "alexnet1d, trial.90:\n",
      "Epoch 21, avg test_loss: 0.023332, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.004227, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.004157, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.001986, train_acc: 0.96\n",
      "alexnet1d, trial.90:\n",
      "Epoch 22, avg test_loss: 0.022925, test_acc: 0.50\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号44个\n",
      "错误信号26个\n",
      "信号正确并预测正确的概率为0.343\n",
      "信号错误并预测正确的概率为0.157\n",
      "总正确率为0.50\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012294, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012114, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.011827, train_acc: 0.66\n",
      "alexnet1d, trial.91:\n",
      "Epoch 0, avg test_loss: 0.010022, test_acc: 0.49\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011024, train_acc: 0.75\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012850, train_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012179, train_acc: 0.57\n",
      "alexnet1d, trial.91:\n",
      "Epoch 1, avg test_loss: 0.010054, test_acc: 0.49\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012308, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011776, train_acc: 0.68\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012244, train_acc: 0.55\n",
      "alexnet1d, trial.91:\n",
      "Epoch 2, avg test_loss: 0.010176, test_acc: 0.49\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012468, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011482, train_acc: 0.70\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012313, train_acc: 0.55\n",
      "alexnet1d, trial.91:\n",
      "Epoch 3, avg test_loss: 0.010348, test_acc: 0.49\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011539, train_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012236, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011995, train_acc: 0.61\n",
      "alexnet1d, trial.91:\n",
      "Epoch 4, avg test_loss: 0.010206, test_acc: 0.49\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012424, train_acc: 0.52\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012055, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012628, train_acc: 0.54\n",
      "alexnet1d, trial.91:\n",
      "Epoch 5, avg test_loss: 0.010337, test_acc: 0.49\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012082, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011050, train_acc: 0.71\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012241, train_acc: 0.55\n",
      "alexnet1d, trial.91:\n",
      "Epoch 6, avg test_loss: 0.010446, test_acc: 0.49\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011480, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011097, train_acc: 0.79\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012333, train_acc: 0.54\n",
      "alexnet1d, trial.91:\n",
      "Epoch 7, avg test_loss: 0.010281, test_acc: 0.50\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011320, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012205, train_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011176, train_acc: 0.70\n",
      "alexnet1d, trial.91:\n",
      "Epoch 8, avg test_loss: 0.010237, test_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011499, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010847, train_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.013180, train_acc: 0.57\n",
      "alexnet1d, trial.91:\n",
      "Epoch 9, avg test_loss: 0.010809, test_acc: 0.49\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011661, train_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011922, train_acc: 0.52\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011665, train_acc: 0.68\n",
      "alexnet1d, trial.91:\n",
      "Epoch 10, avg test_loss: 0.009962, test_acc: 0.56\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011828, train_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011037, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011482, train_acc: 0.61\n",
      "alexnet1d, trial.91:\n",
      "Epoch 11, avg test_loss: 0.010235, test_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011403, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011631, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010560, train_acc: 0.66\n",
      "alexnet1d, trial.91:\n",
      "Epoch 12, avg test_loss: 0.010142, test_acc: 0.57\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009345, train_acc: 0.75\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011593, train_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011034, train_acc: 0.71\n",
      "alexnet1d, trial.91:\n",
      "Epoch 13, avg test_loss: 0.010251, test_acc: 0.51\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.011160, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010022, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011755, train_acc: 0.64\n",
      "alexnet1d, trial.91:\n",
      "Epoch 14, avg test_loss: 0.010796, test_acc: 0.51\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009804, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009758, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011036, train_acc: 0.70\n",
      "alexnet1d, trial.91:\n",
      "Epoch 15, avg test_loss: 0.010720, test_acc: 0.56\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008389, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009725, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010633, train_acc: 0.68\n",
      "alexnet1d, trial.91:\n",
      "Epoch 16, avg test_loss: 0.010861, test_acc: 0.54\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.010457, train_acc: 0.70\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010330, train_acc: 0.66\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010408, train_acc: 0.75\n",
      "alexnet1d, trial.91:\n",
      "Epoch 17, avg test_loss: 0.009620, test_acc: 0.61\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009193, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008904, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.010241, train_acc: 0.73\n",
      "alexnet1d, trial.91:\n",
      "Epoch 18, avg test_loss: 0.013114, test_acc: 0.53\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009163, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007693, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009130, train_acc: 0.75\n",
      "alexnet1d, trial.91:\n",
      "Epoch 19, avg test_loss: 0.010919, test_acc: 0.53\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007945, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.011230, train_acc: 0.70\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009053, train_acc: 0.77\n",
      "alexnet1d, trial.91:\n",
      "Epoch 20, avg test_loss: 0.012734, test_acc: 0.49\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007422, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006926, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.009737, train_acc: 0.71\n",
      "alexnet1d, trial.91:\n",
      "Epoch 21, avg test_loss: 0.012313, test_acc: 0.57\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006840, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.008722, train_acc: 0.79\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.005600, train_acc: 0.93\n",
      "alexnet1d, trial.91:\n",
      "Epoch 22, avg test_loss: 0.014366, test_acc: 0.50\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.008286, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.008247, train_acc: 0.73\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006392, train_acc: 0.86\n",
      "alexnet1d, trial.91:\n",
      "Epoch 23, avg test_loss: 0.012846, test_acc: 0.51\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005337, train_acc: 0.95\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006137, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.010672, train_acc: 0.75\n",
      "alexnet1d, trial.91:\n",
      "Epoch 24, avg test_loss: 0.015936, test_acc: 0.49\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004574, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005358, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006215, train_acc: 0.86\n",
      "alexnet1d, trial.91:\n",
      "Epoch 25, avg test_loss: 0.016963, test_acc: 0.50\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005207, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005791, train_acc: 0.86\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.006209, train_acc: 0.82\n",
      "alexnet1d, trial.91:\n",
      "Epoch 26, avg test_loss: 0.020655, test_acc: 0.46\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.005081, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004141, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003450, train_acc: 0.95\n",
      "alexnet1d, trial.91:\n",
      "Epoch 27, avg test_loss: 0.022467, test_acc: 0.49\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003621, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.006582, train_acc: 0.84\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.003706, train_acc: 0.95\n",
      "alexnet1d, trial.91:\n",
      "Epoch 28, avg test_loss: 0.019942, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.004477, train_acc: 0.96\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.003304, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.003783, train_acc: 0.89\n",
      "alexnet1d, trial.91:\n",
      "Epoch 29, avg test_loss: 0.021106, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号34个\n",
      "错误信号36个\n",
      "信号正确并预测正确的概率为0.329\n",
      "信号错误并预测正确的概率为0.186\n",
      "总正确率为0.51\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012318, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.013775, train_acc: 0.52\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012334, train_acc: 0.61\n",
      "alexnet1d, trial.92:\n",
      "Epoch 0, avg test_loss: 0.009892, test_acc: 0.53\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012394, train_acc: 0.48\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012223, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012151, train_acc: 0.62\n",
      "alexnet1d, trial.92:\n",
      "Epoch 1, avg test_loss: 0.009878, test_acc: 0.53\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012362, train_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012028, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011642, train_acc: 0.64\n",
      "alexnet1d, trial.92:\n",
      "Epoch 2, avg test_loss: 0.010148, test_acc: 0.53\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012330, train_acc: 0.62\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011631, train_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012144, train_acc: 0.54\n",
      "alexnet1d, trial.92:\n",
      "Epoch 3, avg test_loss: 0.009945, test_acc: 0.53\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012408, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011459, train_acc: 0.68\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011946, train_acc: 0.59\n",
      "alexnet1d, trial.92:\n",
      "Epoch 4, avg test_loss: 0.009966, test_acc: 0.53\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011650, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012084, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011104, train_acc: 0.66\n",
      "alexnet1d, trial.92:\n",
      "Epoch 5, avg test_loss: 0.009948, test_acc: 0.51\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012093, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011929, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011719, train_acc: 0.61\n",
      "alexnet1d, trial.92:\n",
      "Epoch 6, avg test_loss: 0.010057, test_acc: 0.51\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011981, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011313, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011113, train_acc: 0.62\n",
      "alexnet1d, trial.92:\n",
      "Epoch 7, avg test_loss: 0.010726, test_acc: 0.56\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011710, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.010830, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012622, train_acc: 0.59\n",
      "alexnet1d, trial.92:\n",
      "Epoch 8, avg test_loss: 0.010764, test_acc: 0.50\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011621, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011006, train_acc: 0.71\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011924, train_acc: 0.54\n",
      "alexnet1d, trial.92:\n",
      "Epoch 9, avg test_loss: 0.010401, test_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011772, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.009690, train_acc: 0.79\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012485, train_acc: 0.54\n",
      "alexnet1d, trial.92:\n",
      "Epoch 10, avg test_loss: 0.010956, test_acc: 0.56\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.009694, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010806, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010661, train_acc: 0.77\n",
      "alexnet1d, trial.92:\n",
      "Epoch 11, avg test_loss: 0.011117, test_acc: 0.46\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010903, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.008997, train_acc: 0.79\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010718, train_acc: 0.61\n",
      "alexnet1d, trial.92:\n",
      "Epoch 12, avg test_loss: 0.011550, test_acc: 0.57\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010345, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.012818, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010528, train_acc: 0.70\n",
      "alexnet1d, trial.92:\n",
      "Epoch 13, avg test_loss: 0.011579, test_acc: 0.54\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010134, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009400, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010207, train_acc: 0.75\n",
      "alexnet1d, trial.92:\n",
      "Epoch 14, avg test_loss: 0.011481, test_acc: 0.46\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009975, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010460, train_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008705, train_acc: 0.79\n",
      "alexnet1d, trial.92:\n",
      "Epoch 15, avg test_loss: 0.011530, test_acc: 0.53\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008324, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009699, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010047, train_acc: 0.68\n",
      "alexnet1d, trial.92:\n",
      "Epoch 16, avg test_loss: 0.012088, test_acc: 0.49\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.010958, train_acc: 0.71\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009140, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007003, train_acc: 0.93\n",
      "alexnet1d, trial.92:\n",
      "Epoch 17, avg test_loss: 0.011635, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008799, train_acc: 0.71\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007027, train_acc: 0.86\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008188, train_acc: 0.73\n",
      "alexnet1d, trial.92:\n",
      "Epoch 18, avg test_loss: 0.015342, test_acc: 0.47\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006579, train_acc: 0.91\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008008, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009368, train_acc: 0.73\n",
      "alexnet1d, trial.92:\n",
      "Epoch 19, avg test_loss: 0.015347, test_acc: 0.51\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006594, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007114, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009256, train_acc: 0.73\n",
      "alexnet1d, trial.92:\n",
      "Epoch 20, avg test_loss: 0.014640, test_acc: 0.49\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007194, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006102, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007150, train_acc: 0.77\n",
      "alexnet1d, trial.92:\n",
      "Epoch 21, avg test_loss: 0.017357, test_acc: 0.49\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005644, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.008058, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006639, train_acc: 0.84\n",
      "alexnet1d, trial.92:\n",
      "Epoch 22, avg test_loss: 0.016066, test_acc: 0.51\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007961, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005033, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006370, train_acc: 0.86\n",
      "alexnet1d, trial.92:\n",
      "Epoch 23, avg test_loss: 0.022875, test_acc: 0.49\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005164, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006071, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005236, train_acc: 0.89\n",
      "alexnet1d, trial.92:\n",
      "Epoch 24, avg test_loss: 0.016089, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004157, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005721, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.007015, train_acc: 0.86\n",
      "alexnet1d, trial.92:\n",
      "Epoch 25, avg test_loss: 0.020328, test_acc: 0.46\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004089, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.007436, train_acc: 0.79\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004295, train_acc: 0.91\n",
      "alexnet1d, trial.92:\n",
      "Epoch 26, avg test_loss: 0.016299, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003713, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.003362, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.006336, train_acc: 0.82\n",
      "alexnet1d, trial.92:\n",
      "Epoch 27, avg test_loss: 0.021291, test_acc: 0.50\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003307, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004872, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.003851, train_acc: 0.91\n",
      "alexnet1d, trial.92:\n",
      "Epoch 28, avg test_loss: 0.023558, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号37个\n",
      "错误信号33个\n",
      "信号正确并预测正确的概率为0.371\n",
      "信号错误并预测正确的概率为0.2\n",
      "总正确率为0.57\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012432, train_acc: 0.38\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012442, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012850, train_acc: 0.50\n",
      "alexnet1d, trial.93:\n",
      "Epoch 0, avg test_loss: 0.009627, test_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012139, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012375, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012236, train_acc: 0.62\n",
      "alexnet1d, trial.93:\n",
      "Epoch 1, avg test_loss: 0.009655, test_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011899, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011644, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012286, train_acc: 0.54\n",
      "alexnet1d, trial.93:\n",
      "Epoch 2, avg test_loss: 0.009625, test_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012008, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012275, train_acc: 0.50\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011919, train_acc: 0.66\n",
      "alexnet1d, trial.93:\n",
      "Epoch 3, avg test_loss: 0.009632, test_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011532, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012367, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012357, train_acc: 0.50\n",
      "alexnet1d, trial.93:\n",
      "Epoch 4, avg test_loss: 0.009697, test_acc: 0.63\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011953, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012066, train_acc: 0.52\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011932, train_acc: 0.54\n",
      "alexnet1d, trial.93:\n",
      "Epoch 5, avg test_loss: 0.009747, test_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011516, train_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012264, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012158, train_acc: 0.57\n",
      "alexnet1d, trial.93:\n",
      "Epoch 6, avg test_loss: 0.009541, test_acc: 0.63\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011442, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011963, train_acc: 0.68\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011811, train_acc: 0.59\n",
      "alexnet1d, trial.93:\n",
      "Epoch 7, avg test_loss: 0.009331, test_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010788, train_acc: 0.71\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012423, train_acc: 0.50\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012227, train_acc: 0.70\n",
      "alexnet1d, trial.93:\n",
      "Epoch 8, avg test_loss: 0.009444, test_acc: 0.67\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011386, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010164, train_acc: 0.71\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011811, train_acc: 0.64\n",
      "alexnet1d, trial.93:\n",
      "Epoch 9, avg test_loss: 0.009598, test_acc: 0.63\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011821, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011108, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010474, train_acc: 0.71\n",
      "alexnet1d, trial.93:\n",
      "Epoch 10, avg test_loss: 0.009385, test_acc: 0.63\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010507, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011796, train_acc: 0.55\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010137, train_acc: 0.66\n",
      "alexnet1d, trial.93:\n",
      "Epoch 11, avg test_loss: 0.009056, test_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010146, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010882, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010225, train_acc: 0.70\n",
      "alexnet1d, trial.93:\n",
      "Epoch 12, avg test_loss: 0.009234, test_acc: 0.63\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009198, train_acc: 0.75\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009754, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009913, train_acc: 0.68\n",
      "alexnet1d, trial.93:\n",
      "Epoch 13, avg test_loss: 0.010003, test_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010522, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011058, train_acc: 0.64\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.008374, train_acc: 0.77\n",
      "alexnet1d, trial.93:\n",
      "Epoch 14, avg test_loss: 0.009846, test_acc: 0.63\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008820, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009733, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009217, train_acc: 0.71\n",
      "alexnet1d, trial.93:\n",
      "Epoch 15, avg test_loss: 0.010752, test_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.007973, train_acc: 0.80\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008286, train_acc: 0.80\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.007628, train_acc: 0.82\n",
      "alexnet1d, trial.93:\n",
      "Epoch 16, avg test_loss: 0.012436, test_acc: 0.59\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007303, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009691, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007963, train_acc: 0.73\n",
      "alexnet1d, trial.93:\n",
      "Epoch 17, avg test_loss: 0.011469, test_acc: 0.54\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008829, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008740, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007853, train_acc: 0.73\n",
      "alexnet1d, trial.93:\n",
      "Epoch 18, avg test_loss: 0.013944, test_acc: 0.49\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.010923, train_acc: 0.71\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.006652, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.006113, train_acc: 0.88\n",
      "alexnet1d, trial.93:\n",
      "Epoch 19, avg test_loss: 0.013578, test_acc: 0.56\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006435, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.006866, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006120, train_acc: 0.86\n",
      "alexnet1d, trial.93:\n",
      "Epoch 20, avg test_loss: 0.014053, test_acc: 0.59\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.005126, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.003385, train_acc: 0.93\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006404, train_acc: 0.84\n",
      "alexnet1d, trial.93:\n",
      "Epoch 21, avg test_loss: 0.017180, test_acc: 0.56\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005879, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.004031, train_acc: 0.93\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.005378, train_acc: 0.84\n",
      "alexnet1d, trial.93:\n",
      "Epoch 22, avg test_loss: 0.021474, test_acc: 0.44\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005811, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.003217, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.003716, train_acc: 0.93\n",
      "alexnet1d, trial.93:\n",
      "Epoch 23, avg test_loss: 0.022585, test_acc: 0.49\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.003367, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.003936, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005804, train_acc: 0.86\n",
      "alexnet1d, trial.93:\n",
      "Epoch 24, avg test_loss: 0.026208, test_acc: 0.49\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.003407, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006935, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.002549, train_acc: 0.93\n",
      "alexnet1d, trial.93:\n",
      "Epoch 25, avg test_loss: 0.028823, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.002775, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.002441, train_acc: 0.96\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.002279, train_acc: 0.96\n",
      "alexnet1d, trial.93:\n",
      "Epoch 26, avg test_loss: 0.028546, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.001332, train_acc: 0.98\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004198, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003315, train_acc: 0.93\n",
      "alexnet1d, trial.93:\n",
      "Epoch 27, avg test_loss: 0.021911, test_acc: 0.46\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号43个\n",
      "错误信号27个\n",
      "信号正确并预测正确的概率为0.257\n",
      "信号错误并预测正确的概率为0.2\n",
      "总正确率为0.46\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012378, train_acc: 0.50\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012158, train_acc: 0.66\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.011937, train_acc: 0.64\n",
      "alexnet1d, trial.94:\n",
      "Epoch 0, avg test_loss: 0.010037, test_acc: 0.51\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012235, train_acc: 0.66\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012000, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011696, train_acc: 0.61\n",
      "alexnet1d, trial.94:\n",
      "Epoch 1, avg test_loss: 0.010526, test_acc: 0.51\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012118, train_acc: 0.68\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011638, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012076, train_acc: 0.64\n",
      "alexnet1d, trial.94:\n",
      "Epoch 2, avg test_loss: 0.009927, test_acc: 0.51\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012248, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012158, train_acc: 0.55\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011858, train_acc: 0.62\n",
      "alexnet1d, trial.94:\n",
      "Epoch 3, avg test_loss: 0.010090, test_acc: 0.51\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012493, train_acc: 0.46\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011255, train_acc: 0.70\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011325, train_acc: 0.70\n",
      "alexnet1d, trial.94:\n",
      "Epoch 4, avg test_loss: 0.010543, test_acc: 0.51\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011579, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011920, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011460, train_acc: 0.57\n",
      "alexnet1d, trial.94:\n",
      "Epoch 5, avg test_loss: 0.010180, test_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011502, train_acc: 0.68\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011700, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011006, train_acc: 0.59\n",
      "alexnet1d, trial.94:\n",
      "Epoch 6, avg test_loss: 0.010586, test_acc: 0.53\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011244, train_acc: 0.71\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.010509, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.010888, train_acc: 0.68\n",
      "alexnet1d, trial.94:\n",
      "Epoch 7, avg test_loss: 0.011145, test_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010284, train_acc: 0.70\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011175, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.010932, train_acc: 0.73\n",
      "alexnet1d, trial.94:\n",
      "Epoch 8, avg test_loss: 0.010377, test_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011381, train_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.009923, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010035, train_acc: 0.70\n",
      "alexnet1d, trial.94:\n",
      "Epoch 9, avg test_loss: 0.011381, test_acc: 0.53\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.008971, train_acc: 0.75\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011678, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011365, train_acc: 0.62\n",
      "alexnet1d, trial.94:\n",
      "Epoch 10, avg test_loss: 0.011202, test_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.009186, train_acc: 0.75\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011155, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010205, train_acc: 0.71\n",
      "alexnet1d, trial.94:\n",
      "Epoch 11, avg test_loss: 0.010259, test_acc: 0.56\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010185, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010479, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.009434, train_acc: 0.70\n",
      "alexnet1d, trial.94:\n",
      "Epoch 12, avg test_loss: 0.011345, test_acc: 0.53\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.007687, train_acc: 0.84\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010452, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.014449, train_acc: 0.57\n",
      "alexnet1d, trial.94:\n",
      "Epoch 13, avg test_loss: 0.012316, test_acc: 0.53\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009027, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009793, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009709, train_acc: 0.71\n",
      "alexnet1d, trial.94:\n",
      "Epoch 14, avg test_loss: 0.010485, test_acc: 0.57\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009018, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.007505, train_acc: 0.84\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009880, train_acc: 0.73\n",
      "alexnet1d, trial.94:\n",
      "Epoch 15, avg test_loss: 0.011978, test_acc: 0.56\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008769, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008429, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010754, train_acc: 0.71\n",
      "alexnet1d, trial.94:\n",
      "Epoch 16, avg test_loss: 0.013465, test_acc: 0.56\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007731, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007834, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008004, train_acc: 0.79\n",
      "alexnet1d, trial.94:\n",
      "Epoch 17, avg test_loss: 0.013862, test_acc: 0.59\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.006759, train_acc: 0.86\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.014348, train_acc: 0.64\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007047, train_acc: 0.79\n",
      "alexnet1d, trial.94:\n",
      "Epoch 18, avg test_loss: 0.013996, test_acc: 0.53\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007219, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009224, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009335, train_acc: 0.75\n",
      "alexnet1d, trial.94:\n",
      "Epoch 19, avg test_loss: 0.012050, test_acc: 0.53\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008775, train_acc: 0.77\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007818, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006534, train_acc: 0.88\n",
      "alexnet1d, trial.94:\n",
      "Epoch 20, avg test_loss: 0.012177, test_acc: 0.60\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006513, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006805, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007920, train_acc: 0.82\n",
      "alexnet1d, trial.94:\n",
      "Epoch 21, avg test_loss: 0.013787, test_acc: 0.59\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006301, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006575, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007462, train_acc: 0.82\n",
      "alexnet1d, trial.94:\n",
      "Epoch 22, avg test_loss: 0.016803, test_acc: 0.59\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.003329, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.004948, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.008947, train_acc: 0.80\n",
      "alexnet1d, trial.94:\n",
      "Epoch 23, avg test_loss: 0.015721, test_acc: 0.56\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004823, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005706, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.007121, train_acc: 0.82\n",
      "alexnet1d, trial.94:\n",
      "Epoch 24, avg test_loss: 0.019594, test_acc: 0.56\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.007608, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005135, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004819, train_acc: 0.91\n",
      "alexnet1d, trial.94:\n",
      "Epoch 25, avg test_loss: 0.014412, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004608, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004721, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.005025, train_acc: 0.86\n",
      "alexnet1d, trial.94:\n",
      "Epoch 26, avg test_loss: 0.015262, test_acc: 0.60\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004521, train_acc: 0.88\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004394, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004992, train_acc: 0.89\n",
      "alexnet1d, trial.94:\n",
      "Epoch 27, avg test_loss: 0.018486, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004421, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.005257, train_acc: 0.86\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.002237, train_acc: 0.95\n",
      "alexnet1d, trial.94:\n",
      "Epoch 28, avg test_loss: 0.021805, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.002600, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.001258, train_acc: 0.98\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.002396, train_acc: 0.96\n",
      "alexnet1d, trial.94:\n",
      "Epoch 29, avg test_loss: 0.024288, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号36个\n",
      "错误信号34个\n",
      "信号正确并预测正确的概率为0.343\n",
      "信号错误并预测正确的概率为0.257\n",
      "总正确率为0.60\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012482, train_acc: 0.41\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.011715, train_acc: 0.66\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012083, train_acc: 0.62\n",
      "alexnet1d, trial.95:\n",
      "Epoch 0, avg test_loss: 0.009883, test_acc: 0.51\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012213, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012273, train_acc: 0.66\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012409, train_acc: 0.52\n",
      "alexnet1d, trial.95:\n",
      "Epoch 1, avg test_loss: 0.009893, test_acc: 0.51\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012393, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012374, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011890, train_acc: 0.66\n",
      "alexnet1d, trial.95:\n",
      "Epoch 2, avg test_loss: 0.009923, test_acc: 0.51\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012269, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012121, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011756, train_acc: 0.62\n",
      "alexnet1d, trial.95:\n",
      "Epoch 3, avg test_loss: 0.010140, test_acc: 0.51\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011724, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012224, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011986, train_acc: 0.61\n",
      "alexnet1d, trial.95:\n",
      "Epoch 4, avg test_loss: 0.009979, test_acc: 0.51\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011456, train_acc: 0.70\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012224, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012047, train_acc: 0.62\n",
      "alexnet1d, trial.95:\n",
      "Epoch 5, avg test_loss: 0.009966, test_acc: 0.51\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012533, train_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012226, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012034, train_acc: 0.61\n",
      "alexnet1d, trial.95:\n",
      "Epoch 6, avg test_loss: 0.009959, test_acc: 0.51\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011899, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011752, train_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011841, train_acc: 0.64\n",
      "alexnet1d, trial.95:\n",
      "Epoch 7, avg test_loss: 0.009992, test_acc: 0.51\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011909, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011959, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012033, train_acc: 0.59\n",
      "alexnet1d, trial.95:\n",
      "Epoch 8, avg test_loss: 0.009963, test_acc: 0.51\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011608, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012105, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012704, train_acc: 0.50\n",
      "alexnet1d, trial.95:\n",
      "Epoch 9, avg test_loss: 0.009852, test_acc: 0.51\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010402, train_acc: 0.73\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011468, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011651, train_acc: 0.61\n",
      "alexnet1d, trial.95:\n",
      "Epoch 10, avg test_loss: 0.009747, test_acc: 0.51\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011855, train_acc: 0.52\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011459, train_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011605, train_acc: 0.57\n",
      "alexnet1d, trial.95:\n",
      "Epoch 11, avg test_loss: 0.009597, test_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.012394, train_acc: 0.48\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011195, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.012517, train_acc: 0.48\n",
      "alexnet1d, trial.95:\n",
      "Epoch 12, avg test_loss: 0.009445, test_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011896, train_acc: 0.54\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010538, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011275, train_acc: 0.61\n",
      "alexnet1d, trial.95:\n",
      "Epoch 13, avg test_loss: 0.009182, test_acc: 0.60\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010773, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011237, train_acc: 0.62\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010278, train_acc: 0.68\n",
      "alexnet1d, trial.95:\n",
      "Epoch 14, avg test_loss: 0.009433, test_acc: 0.61\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010064, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.011102, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008986, train_acc: 0.75\n",
      "alexnet1d, trial.95:\n",
      "Epoch 15, avg test_loss: 0.010585, test_acc: 0.60\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009030, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010414, train_acc: 0.62\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.011094, train_acc: 0.73\n",
      "alexnet1d, trial.95:\n",
      "Epoch 16, avg test_loss: 0.010407, test_acc: 0.59\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008685, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007950, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009093, train_acc: 0.71\n",
      "alexnet1d, trial.95:\n",
      "Epoch 17, avg test_loss: 0.010360, test_acc: 0.60\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008057, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009211, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008211, train_acc: 0.75\n",
      "alexnet1d, trial.95:\n",
      "Epoch 18, avg test_loss: 0.010522, test_acc: 0.66\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008040, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.010484, train_acc: 0.62\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008232, train_acc: 0.77\n",
      "alexnet1d, trial.95:\n",
      "Epoch 19, avg test_loss: 0.012539, test_acc: 0.51\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.009275, train_acc: 0.73\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.010581, train_acc: 0.62\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007599, train_acc: 0.77\n",
      "alexnet1d, trial.95:\n",
      "Epoch 20, avg test_loss: 0.011855, test_acc: 0.53\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007985, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.008373, train_acc: 0.77\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007899, train_acc: 0.79\n",
      "alexnet1d, trial.95:\n",
      "Epoch 21, avg test_loss: 0.010342, test_acc: 0.64\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007324, train_acc: 0.79\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007902, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007618, train_acc: 0.80\n",
      "alexnet1d, trial.95:\n",
      "Epoch 22, avg test_loss: 0.011719, test_acc: 0.60\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006134, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.008129, train_acc: 0.70\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006793, train_acc: 0.80\n",
      "alexnet1d, trial.95:\n",
      "Epoch 23, avg test_loss: 0.015267, test_acc: 0.56\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006181, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006960, train_acc: 0.79\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.008305, train_acc: 0.79\n",
      "alexnet1d, trial.95:\n",
      "Epoch 24, avg test_loss: 0.012267, test_acc: 0.59\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.008017, train_acc: 0.80\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.009784, train_acc: 0.71\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006324, train_acc: 0.84\n",
      "alexnet1d, trial.95:\n",
      "Epoch 25, avg test_loss: 0.013241, test_acc: 0.51\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005563, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.007759, train_acc: 0.73\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.007980, train_acc: 0.77\n",
      "alexnet1d, trial.95:\n",
      "Epoch 26, avg test_loss: 0.014336, test_acc: 0.51\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.006377, train_acc: 0.86\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004219, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.005154, train_acc: 0.89\n",
      "alexnet1d, trial.95:\n",
      "Epoch 27, avg test_loss: 0.015902, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004113, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004869, train_acc: 0.88\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.004957, train_acc: 0.88\n",
      "alexnet1d, trial.95:\n",
      "Epoch 28, avg test_loss: 0.018721, test_acc: 0.56\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.003315, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.003677, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.007930, train_acc: 0.79\n",
      "alexnet1d, trial.95:\n",
      "Epoch 29, avg test_loss: 0.019108, test_acc: 0.57\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.004938, train_acc: 0.91\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.004035, train_acc: 0.89\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.002884, train_acc: 0.95\n",
      "alexnet1d, trial.95:\n",
      "Epoch 30, avg test_loss: 0.021720, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.003788, train_acc: 0.89\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.003816, train_acc: 0.91\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.003169, train_acc: 0.95\n",
      "alexnet1d, trial.95:\n",
      "Epoch 31, avg test_loss: 0.016137, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.003710, train_acc: 0.91\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.005198, train_acc: 0.89\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.003973, train_acc: 0.88\n",
      "alexnet1d, trial.95:\n",
      "Epoch 32, avg test_loss: 0.021378, test_acc: 0.47\n",
      "Train Epoch 33, lr: 0.000377, 0/280, avg loss: 0.002262, train_acc: 0.98\n",
      "Train Epoch 33, lr: 0.000377, 112/280, avg loss: 0.002681, train_acc: 0.95\n",
      "Train Epoch 33, lr: 0.000377, 224/280, avg loss: 0.005138, train_acc: 0.93\n",
      "alexnet1d, trial.95:\n",
      "Epoch 33, avg test_loss: 0.026207, test_acc: 0.50\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号36个\n",
      "错误信号34个\n",
      "信号正确并预测正确的概率为0.3\n",
      "信号错误并预测正确的概率为0.2\n",
      "总正确率为0.50\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012431, train_acc: 0.34\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.011887, train_acc: 0.61\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012235, train_acc: 0.57\n",
      "alexnet1d, trial.96:\n",
      "Epoch 0, avg test_loss: 0.009877, test_acc: 0.53\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012268, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012010, train_acc: 0.66\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011991, train_acc: 0.61\n",
      "alexnet1d, trial.96:\n",
      "Epoch 1, avg test_loss: 0.009953, test_acc: 0.53\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011057, train_acc: 0.73\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.013076, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.013364, train_acc: 0.45\n",
      "alexnet1d, trial.96:\n",
      "Epoch 2, avg test_loss: 0.009907, test_acc: 0.53\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011726, train_acc: 0.68\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012239, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012194, train_acc: 0.57\n",
      "alexnet1d, trial.96:\n",
      "Epoch 3, avg test_loss: 0.009880, test_acc: 0.53\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012237, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012226, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011915, train_acc: 0.66\n",
      "alexnet1d, trial.96:\n",
      "Epoch 4, avg test_loss: 0.009911, test_acc: 0.53\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011900, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011617, train_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012495, train_acc: 0.54\n",
      "alexnet1d, trial.96:\n",
      "Epoch 5, avg test_loss: 0.010182, test_acc: 0.53\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011132, train_acc: 0.68\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012876, train_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012559, train_acc: 0.54\n",
      "alexnet1d, trial.96:\n",
      "Epoch 6, avg test_loss: 0.010086, test_acc: 0.53\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012189, train_acc: 0.55\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011986, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011723, train_acc: 0.62\n",
      "alexnet1d, trial.96:\n",
      "Epoch 7, avg test_loss: 0.010026, test_acc: 0.53\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012669, train_acc: 0.46\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011729, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011679, train_acc: 0.75\n",
      "alexnet1d, trial.96:\n",
      "Epoch 8, avg test_loss: 0.010616, test_acc: 0.51\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011246, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011885, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012555, train_acc: 0.55\n",
      "alexnet1d, trial.96:\n",
      "Epoch 9, avg test_loss: 0.010436, test_acc: 0.46\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011139, train_acc: 0.70\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011581, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012191, train_acc: 0.59\n",
      "alexnet1d, trial.96:\n",
      "Epoch 10, avg test_loss: 0.010675, test_acc: 0.51\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010344, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011733, train_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010064, train_acc: 0.71\n",
      "alexnet1d, trial.96:\n",
      "Epoch 11, avg test_loss: 0.011922, test_acc: 0.53\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011075, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010941, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.009123, train_acc: 0.77\n",
      "alexnet1d, trial.96:\n",
      "Epoch 12, avg test_loss: 0.012496, test_acc: 0.46\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.008912, train_acc: 0.79\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.012342, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010309, train_acc: 0.70\n",
      "alexnet1d, trial.96:\n",
      "Epoch 13, avg test_loss: 0.011720, test_acc: 0.50\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009728, train_acc: 0.82\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010726, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011147, train_acc: 0.73\n",
      "alexnet1d, trial.96:\n",
      "Epoch 14, avg test_loss: 0.011834, test_acc: 0.50\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010482, train_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010389, train_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009584, train_acc: 0.75\n",
      "alexnet1d, trial.96:\n",
      "Epoch 15, avg test_loss: 0.011140, test_acc: 0.50\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010127, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010124, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010190, train_acc: 0.70\n",
      "alexnet1d, trial.96:\n",
      "Epoch 16, avg test_loss: 0.012137, test_acc: 0.49\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.010077, train_acc: 0.70\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009729, train_acc: 0.70\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010502, train_acc: 0.73\n",
      "alexnet1d, trial.96:\n",
      "Epoch 17, avg test_loss: 0.013220, test_acc: 0.51\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009173, train_acc: 0.68\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009697, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009676, train_acc: 0.77\n",
      "alexnet1d, trial.96:\n",
      "Epoch 18, avg test_loss: 0.012340, test_acc: 0.53\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009830, train_acc: 0.70\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009090, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.010799, train_acc: 0.68\n",
      "alexnet1d, trial.96:\n",
      "Epoch 19, avg test_loss: 0.012656, test_acc: 0.53\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008590, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.009078, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008625, train_acc: 0.82\n",
      "alexnet1d, trial.96:\n",
      "Epoch 20, avg test_loss: 0.012515, test_acc: 0.47\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.009550, train_acc: 0.71\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.009565, train_acc: 0.73\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008678, train_acc: 0.77\n",
      "alexnet1d, trial.96:\n",
      "Epoch 21, avg test_loss: 0.013286, test_acc: 0.49\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.008910, train_acc: 0.73\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.008116, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007457, train_acc: 0.82\n",
      "alexnet1d, trial.96:\n",
      "Epoch 22, avg test_loss: 0.015162, test_acc: 0.51\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005065, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.011357, train_acc: 0.64\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.011342, train_acc: 0.71\n",
      "alexnet1d, trial.96:\n",
      "Epoch 23, avg test_loss: 0.014483, test_acc: 0.54\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.007882, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.009291, train_acc: 0.79\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.007944, train_acc: 0.80\n",
      "alexnet1d, trial.96:\n",
      "Epoch 24, avg test_loss: 0.015025, test_acc: 0.47\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.007703, train_acc: 0.82\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.007951, train_acc: 0.73\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.007519, train_acc: 0.86\n",
      "alexnet1d, trial.96:\n",
      "Epoch 25, avg test_loss: 0.014636, test_acc: 0.50\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.006166, train_acc: 0.86\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.007527, train_acc: 0.86\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.009916, train_acc: 0.70\n",
      "alexnet1d, trial.96:\n",
      "Epoch 26, avg test_loss: 0.013916, test_acc: 0.50\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.006466, train_acc: 0.86\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.006825, train_acc: 0.84\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.007713, train_acc: 0.80\n",
      "alexnet1d, trial.96:\n",
      "Epoch 27, avg test_loss: 0.017212, test_acc: 0.53\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.005493, train_acc: 0.89\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.007465, train_acc: 0.84\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.005219, train_acc: 0.89\n",
      "alexnet1d, trial.96:\n",
      "Epoch 28, avg test_loss: 0.022893, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.004754, train_acc: 0.89\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.003394, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.007004, train_acc: 0.82\n",
      "alexnet1d, trial.96:\n",
      "Epoch 29, avg test_loss: 0.027967, test_acc: 0.53\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.005545, train_acc: 0.86\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.003995, train_acc: 0.91\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.005404, train_acc: 0.91\n",
      "alexnet1d, trial.96:\n",
      "Epoch 30, avg test_loss: 0.030750, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.003750, train_acc: 0.89\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.003565, train_acc: 0.91\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.004446, train_acc: 0.86\n",
      "alexnet1d, trial.96:\n",
      "Epoch 31, avg test_loss: 0.026797, test_acc: 0.51\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.005295, train_acc: 0.84\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.005932, train_acc: 0.84\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.004004, train_acc: 0.93\n",
      "alexnet1d, trial.96:\n",
      "Epoch 32, avg test_loss: 0.037616, test_acc: 0.49\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 33, lr: 0.000377, 0/280, avg loss: 0.005705, train_acc: 0.86\n",
      "Train Epoch 33, lr: 0.000377, 112/280, avg loss: 0.003788, train_acc: 0.93\n",
      "Train Epoch 33, lr: 0.000377, 224/280, avg loss: 0.003773, train_acc: 0.93\n",
      "alexnet1d, trial.96:\n",
      "Epoch 33, avg test_loss: 0.030746, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号37个\n",
      "错误信号33个\n",
      "信号正确并预测正确的概率为0.271\n",
      "信号错误并预测正确的概率为0.243\n",
      "总正确率为0.51\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012357, train_acc: 0.50\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012288, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012760, train_acc: 0.59\n",
      "alexnet1d, trial.97:\n",
      "Epoch 0, avg test_loss: 0.009634, test_acc: 0.60\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011666, train_acc: 0.66\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012743, train_acc: 0.43\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012288, train_acc: 0.59\n",
      "alexnet1d, trial.97:\n",
      "Epoch 1, avg test_loss: 0.009770, test_acc: 0.60\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012093, train_acc: 0.70\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011998, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011983, train_acc: 0.61\n",
      "alexnet1d, trial.97:\n",
      "Epoch 2, avg test_loss: 0.009596, test_acc: 0.60\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012356, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012270, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011888, train_acc: 0.62\n",
      "alexnet1d, trial.97:\n",
      "Epoch 3, avg test_loss: 0.009585, test_acc: 0.60\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012291, train_acc: 0.52\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011461, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012553, train_acc: 0.57\n",
      "alexnet1d, trial.97:\n",
      "Epoch 4, avg test_loss: 0.009566, test_acc: 0.60\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011854, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012852, train_acc: 0.50\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011921, train_acc: 0.57\n",
      "alexnet1d, trial.97:\n",
      "Epoch 5, avg test_loss: 0.009587, test_acc: 0.60\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012150, train_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012206, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011381, train_acc: 0.62\n",
      "alexnet1d, trial.97:\n",
      "Epoch 6, avg test_loss: 0.009370, test_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011866, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.010351, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012928, train_acc: 0.48\n",
      "alexnet1d, trial.97:\n",
      "Epoch 7, avg test_loss: 0.009362, test_acc: 0.60\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011530, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011352, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012255, train_acc: 0.50\n",
      "alexnet1d, trial.97:\n",
      "Epoch 8, avg test_loss: 0.009274, test_acc: 0.67\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010314, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011682, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.015076, train_acc: 0.57\n",
      "alexnet1d, trial.97:\n",
      "Epoch 9, avg test_loss: 0.009324, test_acc: 0.69\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.012526, train_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010692, train_acc: 0.70\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011162, train_acc: 0.71\n",
      "alexnet1d, trial.97:\n",
      "Epoch 10, avg test_loss: 0.009271, test_acc: 0.63\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011626, train_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011046, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011901, train_acc: 0.59\n",
      "alexnet1d, trial.97:\n",
      "Epoch 11, avg test_loss: 0.008936, test_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011609, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011580, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010531, train_acc: 0.64\n",
      "alexnet1d, trial.97:\n",
      "Epoch 12, avg test_loss: 0.009114, test_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009861, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011809, train_acc: 0.62\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010823, train_acc: 0.70\n",
      "alexnet1d, trial.97:\n",
      "Epoch 13, avg test_loss: 0.008635, test_acc: 0.67\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010989, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009768, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009048, train_acc: 0.75\n",
      "alexnet1d, trial.97:\n",
      "Epoch 14, avg test_loss: 0.008539, test_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009535, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009270, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008844, train_acc: 0.80\n",
      "alexnet1d, trial.97:\n",
      "Epoch 15, avg test_loss: 0.007979, test_acc: 0.69\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009843, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010058, train_acc: 0.68\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010572, train_acc: 0.62\n",
      "alexnet1d, trial.97:\n",
      "Epoch 16, avg test_loss: 0.008908, test_acc: 0.64\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009696, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008983, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.006882, train_acc: 0.82\n",
      "alexnet1d, trial.97:\n",
      "Epoch 17, avg test_loss: 0.008344, test_acc: 0.70\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008803, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007972, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009151, train_acc: 0.77\n",
      "alexnet1d, trial.97:\n",
      "Epoch 18, avg test_loss: 0.009385, test_acc: 0.56\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.005731, train_acc: 0.89\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.014359, train_acc: 0.59\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007656, train_acc: 0.84\n",
      "alexnet1d, trial.97:\n",
      "Epoch 19, avg test_loss: 0.008521, test_acc: 0.69\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008674, train_acc: 0.75\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008508, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008476, train_acc: 0.82\n",
      "alexnet1d, trial.97:\n",
      "Epoch 20, avg test_loss: 0.008771, test_acc: 0.67\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008954, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007174, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006140, train_acc: 0.86\n",
      "alexnet1d, trial.97:\n",
      "Epoch 21, avg test_loss: 0.008029, test_acc: 0.71\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006028, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005989, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006996, train_acc: 0.79\n",
      "alexnet1d, trial.97:\n",
      "Epoch 22, avg test_loss: 0.011264, test_acc: 0.66\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006865, train_acc: 0.80\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006315, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.007129, train_acc: 0.86\n",
      "alexnet1d, trial.97:\n",
      "Epoch 23, avg test_loss: 0.009464, test_acc: 0.71\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.007020, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004587, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006811, train_acc: 0.86\n",
      "alexnet1d, trial.97:\n",
      "Epoch 24, avg test_loss: 0.010448, test_acc: 0.64\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.006767, train_acc: 0.82\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004652, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005349, train_acc: 0.91\n",
      "alexnet1d, trial.97:\n",
      "Epoch 25, avg test_loss: 0.011373, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.003217, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004090, train_acc: 0.96\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004559, train_acc: 0.93\n",
      "alexnet1d, trial.97:\n",
      "Epoch 26, avg test_loss: 0.011122, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004495, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.005131, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004444, train_acc: 0.89\n",
      "alexnet1d, trial.97:\n",
      "Epoch 27, avg test_loss: 0.016064, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003533, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.003272, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.002270, train_acc: 0.95\n",
      "alexnet1d, trial.97:\n",
      "Epoch 28, avg test_loss: 0.016871, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号42个\n",
      "错误信号28个\n",
      "信号正确并预测正确的概率为0.343\n",
      "信号错误并预测正确的概率为0.257\n",
      "总正确率为0.60\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012341, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012338, train_acc: 0.52\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012047, train_acc: 0.61\n",
      "alexnet1d, trial.98:\n",
      "Epoch 0, avg test_loss: 0.009659, test_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011885, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012052, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012418, train_acc: 0.48\n",
      "alexnet1d, trial.98:\n",
      "Epoch 1, avg test_loss: 0.009834, test_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012305, train_acc: 0.52\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012204, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012148, train_acc: 0.59\n",
      "alexnet1d, trial.98:\n",
      "Epoch 2, avg test_loss: 0.009717, test_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012123, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011406, train_acc: 0.70\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012682, train_acc: 0.52\n",
      "alexnet1d, trial.98:\n",
      "Epoch 3, avg test_loss: 0.009716, test_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012299, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011928, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012639, train_acc: 0.48\n",
      "alexnet1d, trial.98:\n",
      "Epoch 4, avg test_loss: 0.009701, test_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012303, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012093, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012168, train_acc: 0.61\n",
      "alexnet1d, trial.98:\n",
      "Epoch 5, avg test_loss: 0.009657, test_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011310, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012123, train_acc: 0.52\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012749, train_acc: 0.54\n",
      "alexnet1d, trial.98:\n",
      "Epoch 6, avg test_loss: 0.009673, test_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012348, train_acc: 0.52\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011350, train_acc: 0.68\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011621, train_acc: 0.64\n",
      "alexnet1d, trial.98:\n",
      "Epoch 7, avg test_loss: 0.009779, test_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011126, train_acc: 0.71\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.010849, train_acc: 0.71\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012213, train_acc: 0.61\n",
      "alexnet1d, trial.98:\n",
      "Epoch 8, avg test_loss: 0.010369, test_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012149, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011768, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010601, train_acc: 0.70\n",
      "alexnet1d, trial.98:\n",
      "Epoch 9, avg test_loss: 0.010612, test_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010620, train_acc: 0.71\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012336, train_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010518, train_acc: 0.59\n",
      "alexnet1d, trial.98:\n",
      "Epoch 10, avg test_loss: 0.010643, test_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011026, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.009899, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.009732, train_acc: 0.70\n",
      "alexnet1d, trial.98:\n",
      "Epoch 11, avg test_loss: 0.011900, test_acc: 0.56\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009806, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.009844, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.012114, train_acc: 0.55\n",
      "alexnet1d, trial.98:\n",
      "Epoch 12, avg test_loss: 0.011381, test_acc: 0.56\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010417, train_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.008604, train_acc: 0.79\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.008723, train_acc: 0.77\n",
      "alexnet1d, trial.98:\n",
      "Epoch 13, avg test_loss: 0.013065, test_acc: 0.59\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010245, train_acc: 0.62\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.008804, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009923, train_acc: 0.68\n",
      "alexnet1d, trial.98:\n",
      "Epoch 14, avg test_loss: 0.012647, test_acc: 0.51\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008638, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008796, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009564, train_acc: 0.75\n",
      "alexnet1d, trial.98:\n",
      "Epoch 15, avg test_loss: 0.013746, test_acc: 0.54\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009740, train_acc: 0.68\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010171, train_acc: 0.68\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009017, train_acc: 0.71\n",
      "alexnet1d, trial.98:\n",
      "Epoch 16, avg test_loss: 0.014313, test_acc: 0.54\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009100, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.006943, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.006578, train_acc: 0.84\n",
      "alexnet1d, trial.98:\n",
      "Epoch 17, avg test_loss: 0.014680, test_acc: 0.60\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.006387, train_acc: 0.88\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008563, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009006, train_acc: 0.75\n",
      "alexnet1d, trial.98:\n",
      "Epoch 18, avg test_loss: 0.013620, test_acc: 0.51\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007131, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008140, train_acc: 0.75\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008748, train_acc: 0.73\n",
      "alexnet1d, trial.98:\n",
      "Epoch 19, avg test_loss: 0.017339, test_acc: 0.54\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008031, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.006451, train_acc: 0.88\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009271, train_acc: 0.75\n",
      "alexnet1d, trial.98:\n",
      "Epoch 20, avg test_loss: 0.015849, test_acc: 0.54\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006325, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007224, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008398, train_acc: 0.77\n",
      "alexnet1d, trial.98:\n",
      "Epoch 21, avg test_loss: 0.019120, test_acc: 0.56\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005434, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006313, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006574, train_acc: 0.82\n",
      "alexnet1d, trial.98:\n",
      "Epoch 22, avg test_loss: 0.018692, test_acc: 0.50\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005793, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.004577, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006214, train_acc: 0.84\n",
      "alexnet1d, trial.98:\n",
      "Epoch 23, avg test_loss: 0.018316, test_acc: 0.49\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005255, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006087, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.008670, train_acc: 0.82\n",
      "alexnet1d, trial.98:\n",
      "Epoch 24, avg test_loss: 0.017454, test_acc: 0.51\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.003762, train_acc: 0.95\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005922, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005325, train_acc: 0.89\n",
      "alexnet1d, trial.98:\n",
      "Epoch 25, avg test_loss: 0.021917, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.006422, train_acc: 0.86\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004671, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004516, train_acc: 0.91\n",
      "alexnet1d, trial.98:\n",
      "Epoch 26, avg test_loss: 0.018717, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004217, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004656, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.001997, train_acc: 0.98\n",
      "alexnet1d, trial.98:\n",
      "Epoch 27, avg test_loss: 0.019585, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004728, train_acc: 0.86\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.003873, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.001899, train_acc: 0.98\n",
      "alexnet1d, trial.98:\n",
      "Epoch 28, avg test_loss: 0.024905, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号40个\n",
      "错误信号30个\n",
      "信号正确并预测正确的概率为0.329\n",
      "信号错误并预测正确的概率为0.286\n",
      "总正确率为0.61\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012389, train_acc: 0.48\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012291, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012001, train_acc: 0.59\n",
      "alexnet1d, trial.99:\n",
      "Epoch 0, avg test_loss: 0.009494, test_acc: 0.60\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012488, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012361, train_acc: 0.46\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012271, train_acc: 0.57\n",
      "alexnet1d, trial.99:\n",
      "Epoch 1, avg test_loss: 0.009721, test_acc: 0.60\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012130, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012702, train_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012305, train_acc: 0.55\n",
      "alexnet1d, trial.99:\n",
      "Epoch 2, avg test_loss: 0.009597, test_acc: 0.60\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012691, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012233, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011819, train_acc: 0.66\n",
      "alexnet1d, trial.99:\n",
      "Epoch 3, avg test_loss: 0.009641, test_acc: 0.60\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012431, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012464, train_acc: 0.52\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011519, train_acc: 0.68\n",
      "alexnet1d, trial.99:\n",
      "Epoch 4, avg test_loss: 0.009623, test_acc: 0.60\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012034, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011710, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012095, train_acc: 0.57\n",
      "alexnet1d, trial.99:\n",
      "Epoch 5, avg test_loss: 0.009583, test_acc: 0.60\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012359, train_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012682, train_acc: 0.48\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012046, train_acc: 0.59\n",
      "alexnet1d, trial.99:\n",
      "Epoch 6, avg test_loss: 0.009588, test_acc: 0.60\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012547, train_acc: 0.50\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011922, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011294, train_acc: 0.68\n",
      "alexnet1d, trial.99:\n",
      "Epoch 7, avg test_loss: 0.009424, test_acc: 0.60\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012005, train_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011165, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011477, train_acc: 0.68\n",
      "alexnet1d, trial.99:\n",
      "Epoch 8, avg test_loss: 0.009469, test_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011860, train_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012438, train_acc: 0.55\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011117, train_acc: 0.75\n",
      "alexnet1d, trial.99:\n",
      "Epoch 9, avg test_loss: 0.009712, test_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011504, train_acc: 0.70\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011668, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011157, train_acc: 0.62\n",
      "alexnet1d, trial.99:\n",
      "Epoch 10, avg test_loss: 0.009400, test_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010681, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.012997, train_acc: 0.50\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011180, train_acc: 0.73\n",
      "alexnet1d, trial.99:\n",
      "Epoch 11, avg test_loss: 0.009639, test_acc: 0.60\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011495, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011635, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.013211, train_acc: 0.55\n",
      "alexnet1d, trial.99:\n",
      "Epoch 12, avg test_loss: 0.009528, test_acc: 0.63\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009145, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010895, train_acc: 0.77\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009909, train_acc: 0.70\n",
      "alexnet1d, trial.99:\n",
      "Epoch 13, avg test_loss: 0.010223, test_acc: 0.53\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010799, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009781, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009875, train_acc: 0.73\n",
      "alexnet1d, trial.99:\n",
      "Epoch 14, avg test_loss: 0.010533, test_acc: 0.59\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008274, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.011335, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008606, train_acc: 0.75\n",
      "alexnet1d, trial.99:\n",
      "Epoch 15, avg test_loss: 0.010808, test_acc: 0.60\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009361, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010800, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009425, train_acc: 0.77\n",
      "alexnet1d, trial.99:\n",
      "Epoch 16, avg test_loss: 0.012703, test_acc: 0.51\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008358, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.006753, train_acc: 0.89\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008197, train_acc: 0.77\n",
      "alexnet1d, trial.99:\n",
      "Epoch 17, avg test_loss: 0.010977, test_acc: 0.61\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007829, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009126, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008580, train_acc: 0.82\n",
      "alexnet1d, trial.99:\n",
      "Epoch 18, avg test_loss: 0.012213, test_acc: 0.53\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006825, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.006681, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009408, train_acc: 0.77\n",
      "alexnet1d, trial.99:\n",
      "Epoch 19, avg test_loss: 0.015184, test_acc: 0.57\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008708, train_acc: 0.77\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.006430, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006233, train_acc: 0.79\n",
      "alexnet1d, trial.99:\n",
      "Epoch 20, avg test_loss: 0.014259, test_acc: 0.57\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.004503, train_acc: 0.95\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007046, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.004756, train_acc: 0.95\n",
      "alexnet1d, trial.99:\n",
      "Epoch 21, avg test_loss: 0.016581, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.003487, train_acc: 0.95\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005381, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.004817, train_acc: 0.89\n",
      "alexnet1d, trial.99:\n",
      "Epoch 22, avg test_loss: 0.015723, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.003735, train_acc: 0.95\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.004534, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.002834, train_acc: 0.95\n",
      "alexnet1d, trial.99:\n",
      "Epoch 23, avg test_loss: 0.020430, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.003108, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.003316, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.002926, train_acc: 0.95\n",
      "alexnet1d, trial.99:\n",
      "Epoch 24, avg test_loss: 0.023844, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号42个\n",
      "错误信号28个\n",
      "信号正确并预测正确的概率为0.414\n",
      "信号错误并预测正确的概率为0.129\n",
      "总正确率为0.54\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012361, train_acc: 0.45\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.011909, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.011637, train_acc: 0.64\n",
      "alexnet1d, trial.100:\n",
      "Epoch 0, avg test_loss: 0.010262, test_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012218, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012593, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012887, train_acc: 0.46\n",
      "alexnet1d, trial.100:\n",
      "Epoch 1, avg test_loss: 0.010012, test_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011962, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011711, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011915, train_acc: 0.57\n",
      "alexnet1d, trial.100:\n",
      "Epoch 2, avg test_loss: 0.010379, test_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012336, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012283, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012446, train_acc: 0.57\n",
      "alexnet1d, trial.100:\n",
      "Epoch 3, avg test_loss: 0.010283, test_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011703, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012017, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012883, train_acc: 0.52\n",
      "alexnet1d, trial.100:\n",
      "Epoch 4, avg test_loss: 0.010739, test_acc: 0.56\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011744, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012085, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011536, train_acc: 0.59\n",
      "alexnet1d, trial.100:\n",
      "Epoch 5, avg test_loss: 0.010173, test_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011307, train_acc: 0.71\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011991, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011582, train_acc: 0.61\n",
      "alexnet1d, trial.100:\n",
      "Epoch 6, avg test_loss: 0.010252, test_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011105, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.010794, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012244, train_acc: 0.55\n",
      "alexnet1d, trial.100:\n",
      "Epoch 7, avg test_loss: 0.010541, test_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010945, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.010891, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011087, train_acc: 0.59\n",
      "alexnet1d, trial.100:\n",
      "Epoch 8, avg test_loss: 0.010796, test_acc: 0.56\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011176, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010590, train_acc: 0.73\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010429, train_acc: 0.62\n",
      "alexnet1d, trial.100:\n",
      "Epoch 9, avg test_loss: 0.013652, test_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010016, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010512, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011667, train_acc: 0.57\n",
      "alexnet1d, trial.100:\n",
      "Epoch 10, avg test_loss: 0.010288, test_acc: 0.50\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011129, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010803, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010526, train_acc: 0.70\n",
      "alexnet1d, trial.100:\n",
      "Epoch 11, avg test_loss: 0.012944, test_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010658, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.009346, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.009634, train_acc: 0.71\n",
      "alexnet1d, trial.100:\n",
      "Epoch 12, avg test_loss: 0.011086, test_acc: 0.46\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009199, train_acc: 0.80\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009037, train_acc: 0.79\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009153, train_acc: 0.68\n",
      "alexnet1d, trial.100:\n",
      "Epoch 13, avg test_loss: 0.013828, test_acc: 0.50\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.008504, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009896, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010095, train_acc: 0.73\n",
      "alexnet1d, trial.100:\n",
      "Epoch 14, avg test_loss: 0.013194, test_acc: 0.57\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009551, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008082, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009357, train_acc: 0.75\n",
      "alexnet1d, trial.100:\n",
      "Epoch 15, avg test_loss: 0.011547, test_acc: 0.53\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008086, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009650, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009083, train_acc: 0.71\n",
      "alexnet1d, trial.100:\n",
      "Epoch 16, avg test_loss: 0.012923, test_acc: 0.57\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008299, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007681, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009218, train_acc: 0.79\n",
      "alexnet1d, trial.100:\n",
      "Epoch 17, avg test_loss: 0.014549, test_acc: 0.44\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007888, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008721, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007775, train_acc: 0.84\n",
      "alexnet1d, trial.100:\n",
      "Epoch 18, avg test_loss: 0.016836, test_acc: 0.57\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007395, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.006363, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008120, train_acc: 0.82\n",
      "alexnet1d, trial.100:\n",
      "Epoch 19, avg test_loss: 0.014838, test_acc: 0.61\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006501, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007094, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007386, train_acc: 0.82\n",
      "alexnet1d, trial.100:\n",
      "Epoch 20, avg test_loss: 0.015033, test_acc: 0.56\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006040, train_acc: 0.89\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.004476, train_acc: 0.89\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006064, train_acc: 0.86\n",
      "alexnet1d, trial.100:\n",
      "Epoch 21, avg test_loss: 0.016928, test_acc: 0.60\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005866, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005091, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007501, train_acc: 0.80\n",
      "alexnet1d, trial.100:\n",
      "Epoch 22, avg test_loss: 0.018410, test_acc: 0.50\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.004964, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006430, train_acc: 0.79\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.005731, train_acc: 0.86\n",
      "alexnet1d, trial.100:\n",
      "Epoch 23, avg test_loss: 0.023249, test_acc: 0.56\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.007152, train_acc: 0.82\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.003335, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006708, train_acc: 0.88\n",
      "alexnet1d, trial.100:\n",
      "Epoch 24, avg test_loss: 0.020553, test_acc: 0.49\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005397, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006500, train_acc: 0.80\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.002839, train_acc: 0.96\n",
      "alexnet1d, trial.100:\n",
      "Epoch 25, avg test_loss: 0.024014, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004216, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005636, train_acc: 0.84\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004137, train_acc: 0.93\n",
      "alexnet1d, trial.100:\n",
      "Epoch 26, avg test_loss: 0.023311, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003749, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004108, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.002025, train_acc: 0.96\n",
      "alexnet1d, trial.100:\n",
      "Epoch 27, avg test_loss: 0.024627, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.002472, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.003137, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.005003, train_acc: 0.89\n",
      "alexnet1d, trial.100:\n",
      "Epoch 28, avg test_loss: 0.026746, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号38个\n",
      "错误信号32个\n",
      "信号正确并预测正确的概率为0.314\n",
      "信号错误并预测正确的概率为0.214\n",
      "总正确率为0.53\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012373, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.010457, train_acc: 0.71\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012349, train_acc: 0.54\n",
      "alexnet1d, trial.101:\n",
      "Epoch 0, avg test_loss: 0.009887, test_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012324, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012386, train_acc: 0.46\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012334, train_acc: 0.52\n",
      "alexnet1d, trial.101:\n",
      "Epoch 1, avg test_loss: 0.009885, test_acc: 0.53\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012138, train_acc: 0.71\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012629, train_acc: 0.43\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012295, train_acc: 0.54\n",
      "alexnet1d, trial.101:\n",
      "Epoch 2, avg test_loss: 0.009896, test_acc: 0.53\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011942, train_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012158, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012327, train_acc: 0.55\n",
      "alexnet1d, trial.101:\n",
      "Epoch 3, avg test_loss: 0.010096, test_acc: 0.53\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012681, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011807, train_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011993, train_acc: 0.59\n",
      "alexnet1d, trial.101:\n",
      "Epoch 4, avg test_loss: 0.009980, test_acc: 0.53\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011723, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011838, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011888, train_acc: 0.59\n",
      "alexnet1d, trial.101:\n",
      "Epoch 5, avg test_loss: 0.010127, test_acc: 0.53\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011452, train_acc: 0.68\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011606, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012189, train_acc: 0.52\n",
      "alexnet1d, trial.101:\n",
      "Epoch 6, avg test_loss: 0.010174, test_acc: 0.51\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012607, train_acc: 0.52\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011838, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011463, train_acc: 0.62\n",
      "alexnet1d, trial.101:\n",
      "Epoch 7, avg test_loss: 0.010289, test_acc: 0.51\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012392, train_acc: 0.52\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012472, train_acc: 0.55\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.010955, train_acc: 0.66\n",
      "alexnet1d, trial.101:\n",
      "Epoch 8, avg test_loss: 0.009860, test_acc: 0.56\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011043, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012586, train_acc: 0.50\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011386, train_acc: 0.66\n",
      "alexnet1d, trial.101:\n",
      "Epoch 9, avg test_loss: 0.010331, test_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011809, train_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010398, train_acc: 0.71\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011166, train_acc: 0.64\n",
      "alexnet1d, trial.101:\n",
      "Epoch 10, avg test_loss: 0.010487, test_acc: 0.49\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010081, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011833, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011594, train_acc: 0.64\n",
      "alexnet1d, trial.101:\n",
      "Epoch 11, avg test_loss: 0.010264, test_acc: 0.56\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011776, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010763, train_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010130, train_acc: 0.66\n",
      "alexnet1d, trial.101:\n",
      "Epoch 12, avg test_loss: 0.011037, test_acc: 0.50\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010562, train_acc: 0.75\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010040, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.012097, train_acc: 0.61\n",
      "alexnet1d, trial.101:\n",
      "Epoch 13, avg test_loss: 0.010775, test_acc: 0.54\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.011297, train_acc: 0.62\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010959, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009490, train_acc: 0.79\n",
      "alexnet1d, trial.101:\n",
      "Epoch 14, avg test_loss: 0.011722, test_acc: 0.54\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010304, train_acc: 0.68\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.012257, train_acc: 0.61\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010944, train_acc: 0.61\n",
      "alexnet1d, trial.101:\n",
      "Epoch 15, avg test_loss: 0.011116, test_acc: 0.51\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010410, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010255, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009954, train_acc: 0.66\n",
      "alexnet1d, trial.101:\n",
      "Epoch 16, avg test_loss: 0.011722, test_acc: 0.54\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.011068, train_acc: 0.64\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009501, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010077, train_acc: 0.73\n",
      "alexnet1d, trial.101:\n",
      "Epoch 17, avg test_loss: 0.011238, test_acc: 0.56\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009180, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009209, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.012741, train_acc: 0.64\n",
      "alexnet1d, trial.101:\n",
      "Epoch 18, avg test_loss: 0.012543, test_acc: 0.56\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008844, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009703, train_acc: 0.68\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.010072, train_acc: 0.75\n",
      "alexnet1d, trial.101:\n",
      "Epoch 19, avg test_loss: 0.014118, test_acc: 0.51\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.009214, train_acc: 0.77\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008693, train_acc: 0.75\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009891, train_acc: 0.75\n",
      "alexnet1d, trial.101:\n",
      "Epoch 20, avg test_loss: 0.013121, test_acc: 0.60\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007420, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.009676, train_acc: 0.75\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007350, train_acc: 0.82\n",
      "alexnet1d, trial.101:\n",
      "Epoch 21, avg test_loss: 0.012843, test_acc: 0.54\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.009232, train_acc: 0.75\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007498, train_acc: 0.77\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007043, train_acc: 0.79\n",
      "alexnet1d, trial.101:\n",
      "Epoch 22, avg test_loss: 0.015241, test_acc: 0.60\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006406, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.011487, train_acc: 0.62\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.007512, train_acc: 0.84\n",
      "alexnet1d, trial.101:\n",
      "Epoch 23, avg test_loss: 0.016781, test_acc: 0.56\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.008452, train_acc: 0.79\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.008848, train_acc: 0.77\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.007277, train_acc: 0.82\n",
      "alexnet1d, trial.101:\n",
      "Epoch 24, avg test_loss: 0.012524, test_acc: 0.60\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.007189, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006361, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006727, train_acc: 0.86\n",
      "alexnet1d, trial.101:\n",
      "Epoch 25, avg test_loss: 0.015040, test_acc: 0.64\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.008653, train_acc: 0.77\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005387, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004952, train_acc: 0.86\n",
      "alexnet1d, trial.101:\n",
      "Epoch 26, avg test_loss: 0.017936, test_acc: 0.56\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004857, train_acc: 0.88\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004342, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.006316, train_acc: 0.86\n",
      "alexnet1d, trial.101:\n",
      "Epoch 27, avg test_loss: 0.021987, test_acc: 0.50\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004577, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.005444, train_acc: 0.84\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.005503, train_acc: 0.86\n",
      "alexnet1d, trial.101:\n",
      "Epoch 28, avg test_loss: 0.028421, test_acc: 0.57\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.001966, train_acc: 0.98\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.006496, train_acc: 0.86\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.010929, train_acc: 0.73\n",
      "alexnet1d, trial.101:\n",
      "Epoch 29, avg test_loss: 0.024745, test_acc: 0.54\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.003659, train_acc: 0.91\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.002974, train_acc: 0.96\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.005641, train_acc: 0.86\n",
      "alexnet1d, trial.101:\n",
      "Epoch 30, avg test_loss: 0.027861, test_acc: 0.57\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.002659, train_acc: 0.95\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.006535, train_acc: 0.80\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.007739, train_acc: 0.79\n",
      "alexnet1d, trial.101:\n",
      "Epoch 31, avg test_loss: 0.024865, test_acc: 0.47\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.005110, train_acc: 0.88\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.004379, train_acc: 0.89\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.005641, train_acc: 0.86\n",
      "alexnet1d, trial.101:\n",
      "Epoch 32, avg test_loss: 0.017117, test_acc: 0.61\n",
      "Train Epoch 33, lr: 0.000377, 0/280, avg loss: 0.006262, train_acc: 0.86\n",
      "Train Epoch 33, lr: 0.000377, 112/280, avg loss: 0.004870, train_acc: 0.91\n",
      "Train Epoch 33, lr: 0.000377, 224/280, avg loss: 0.003600, train_acc: 0.95\n",
      "alexnet1d, trial.101:\n",
      "Epoch 33, avg test_loss: 0.021062, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 34, lr: 0.000377, 0/280, avg loss: 0.003941, train_acc: 0.89\n",
      "Train Epoch 34, lr: 0.000377, 112/280, avg loss: 0.002715, train_acc: 0.93\n",
      "Train Epoch 34, lr: 0.000377, 224/280, avg loss: 0.004675, train_acc: 0.91\n",
      "alexnet1d, trial.101:\n",
      "Epoch 34, avg test_loss: 0.024722, test_acc: 0.49\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 35, lr: 0.000321, 0/280, avg loss: 0.002973, train_acc: 0.95\n",
      "Train Epoch 35, lr: 0.000321, 112/280, avg loss: 0.002256, train_acc: 0.96\n",
      "Train Epoch 35, lr: 0.000321, 224/280, avg loss: 0.002672, train_acc: 0.96\n",
      "alexnet1d, trial.101:\n",
      "Epoch 35, avg test_loss: 0.029156, test_acc: 0.49\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 36, lr: 0.000321, 0/280, avg loss: 0.001838, train_acc: 0.98\n",
      "Train Epoch 36, lr: 0.000321, 112/280, avg loss: 0.002158, train_acc: 0.96\n",
      "Train Epoch 36, lr: 0.000321, 224/280, avg loss: 0.003532, train_acc: 0.91\n",
      "alexnet1d, trial.101:\n",
      "Epoch 36, avg test_loss: 0.032332, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号37个\n",
      "错误信号33个\n",
      "信号正确并预测正确的概率为0.314\n",
      "信号错误并预测正确的概率为0.214\n",
      "总正确率为0.53\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012389, train_acc: 0.52\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012245, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012371, train_acc: 0.52\n",
      "alexnet1d, trial.102:\n",
      "Epoch 0, avg test_loss: 0.009434, test_acc: 0.73\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012270, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012355, train_acc: 0.52\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012258, train_acc: 0.55\n",
      "alexnet1d, trial.102:\n",
      "Epoch 1, avg test_loss: 0.009415, test_acc: 0.73\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011864, train_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012888, train_acc: 0.46\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012212, train_acc: 0.54\n",
      "alexnet1d, trial.102:\n",
      "Epoch 2, avg test_loss: 0.009608, test_acc: 0.73\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012359, train_acc: 0.50\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012303, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012296, train_acc: 0.61\n",
      "alexnet1d, trial.102:\n",
      "Epoch 3, avg test_loss: 0.009629, test_acc: 0.73\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012431, train_acc: 0.50\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012175, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011984, train_acc: 0.57\n",
      "alexnet1d, trial.102:\n",
      "Epoch 4, avg test_loss: 0.009363, test_acc: 0.70\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012182, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012501, train_acc: 0.50\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011998, train_acc: 0.62\n",
      "alexnet1d, trial.102:\n",
      "Epoch 5, avg test_loss: 0.009424, test_acc: 0.56\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011149, train_acc: 0.71\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011930, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011530, train_acc: 0.62\n",
      "alexnet1d, trial.102:\n",
      "Epoch 6, avg test_loss: 0.009599, test_acc: 0.60\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011254, train_acc: 0.68\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012044, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011616, train_acc: 0.62\n",
      "alexnet1d, trial.102:\n",
      "Epoch 7, avg test_loss: 0.009469, test_acc: 0.69\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011413, train_acc: 0.68\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012301, train_acc: 0.46\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012159, train_acc: 0.57\n",
      "alexnet1d, trial.102:\n",
      "Epoch 8, avg test_loss: 0.008843, test_acc: 0.74\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010934, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011278, train_acc: 0.71\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011915, train_acc: 0.68\n",
      "alexnet1d, trial.102:\n",
      "Epoch 9, avg test_loss: 0.008582, test_acc: 0.73\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011596, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011189, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011032, train_acc: 0.71\n",
      "alexnet1d, trial.102:\n",
      "Epoch 10, avg test_loss: 0.008504, test_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010134, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011578, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010605, train_acc: 0.68\n",
      "alexnet1d, trial.102:\n",
      "Epoch 11, avg test_loss: 0.008327, test_acc: 0.69\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011171, train_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011117, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010156, train_acc: 0.73\n",
      "alexnet1d, trial.102:\n",
      "Epoch 12, avg test_loss: 0.008113, test_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010719, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009741, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.008565, train_acc: 0.79\n",
      "alexnet1d, trial.102:\n",
      "Epoch 13, avg test_loss: 0.009334, test_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.007392, train_acc: 0.86\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.008364, train_acc: 0.80\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.008362, train_acc: 0.77\n",
      "alexnet1d, trial.102:\n",
      "Epoch 14, avg test_loss: 0.010551, test_acc: 0.59\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.011238, train_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009105, train_acc: 0.80\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.007926, train_acc: 0.84\n",
      "alexnet1d, trial.102:\n",
      "Epoch 15, avg test_loss: 0.010158, test_acc: 0.67\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.007987, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.007278, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010856, train_acc: 0.68\n",
      "alexnet1d, trial.102:\n",
      "Epoch 16, avg test_loss: 0.008518, test_acc: 0.70\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007433, train_acc: 0.86\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008962, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007093, train_acc: 0.82\n",
      "alexnet1d, trial.102:\n",
      "Epoch 17, avg test_loss: 0.010202, test_acc: 0.64\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.006700, train_acc: 0.88\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009278, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007759, train_acc: 0.84\n",
      "alexnet1d, trial.102:\n",
      "Epoch 18, avg test_loss: 0.012335, test_acc: 0.63\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008230, train_acc: 0.75\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007466, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.006228, train_acc: 0.84\n",
      "alexnet1d, trial.102:\n",
      "Epoch 19, avg test_loss: 0.012303, test_acc: 0.63\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.004719, train_acc: 0.91\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.004491, train_acc: 0.91\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.004748, train_acc: 0.91\n",
      "alexnet1d, trial.102:\n",
      "Epoch 20, avg test_loss: 0.009623, test_acc: 0.77\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.004531, train_acc: 0.89\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006826, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.013097, train_acc: 0.77\n",
      "alexnet1d, trial.102:\n",
      "Epoch 21, avg test_loss: 0.014926, test_acc: 0.67\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.003054, train_acc: 0.96\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007716, train_acc: 0.79\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.011198, train_acc: 0.77\n",
      "alexnet1d, trial.102:\n",
      "Epoch 22, avg test_loss: 0.011236, test_acc: 0.70\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.003329, train_acc: 0.96\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005411, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.003120, train_acc: 0.96\n",
      "alexnet1d, trial.102:\n",
      "Epoch 23, avg test_loss: 0.013001, test_acc: 0.66\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.002738, train_acc: 0.96\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006104, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.002254, train_acc: 0.96\n",
      "alexnet1d, trial.102:\n",
      "Epoch 24, avg test_loss: 0.010472, test_acc: 0.76\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.002569, train_acc: 0.98\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.002158, train_acc: 0.95\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.002367, train_acc: 0.95\n",
      "alexnet1d, trial.102:\n",
      "Epoch 25, avg test_loss: 0.013251, test_acc: 0.74\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号51个\n",
      "错误信号19个\n",
      "信号正确并预测正确的概率为0.571\n",
      "信号错误并预测正确的概率为0.171\n",
      "总正确率为0.74\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012231, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.011347, train_acc: 0.68\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012394, train_acc: 0.59\n",
      "alexnet1d, trial.103:\n",
      "Epoch 0, avg test_loss: 0.009897, test_acc: 0.47\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012326, train_acc: 0.46\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012221, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012175, train_acc: 0.62\n",
      "alexnet1d, trial.103:\n",
      "Epoch 1, avg test_loss: 0.009878, test_acc: 0.53\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012310, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012291, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011869, train_acc: 0.66\n",
      "alexnet1d, trial.103:\n",
      "Epoch 2, avg test_loss: 0.009915, test_acc: 0.53\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012124, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011847, train_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012917, train_acc: 0.52\n",
      "alexnet1d, trial.103:\n",
      "Epoch 3, avg test_loss: 0.010038, test_acc: 0.53\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012297, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012137, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012197, train_acc: 0.57\n",
      "alexnet1d, trial.103:\n",
      "Epoch 4, avg test_loss: 0.009927, test_acc: 0.53\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012232, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012349, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011475, train_acc: 0.70\n",
      "alexnet1d, trial.103:\n",
      "Epoch 5, avg test_loss: 0.009938, test_acc: 0.53\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011709, train_acc: 0.66\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012169, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011910, train_acc: 0.59\n",
      "alexnet1d, trial.103:\n",
      "Epoch 6, avg test_loss: 0.010015, test_acc: 0.53\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011488, train_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011421, train_acc: 0.68\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.013415, train_acc: 0.45\n",
      "alexnet1d, trial.103:\n",
      "Epoch 7, avg test_loss: 0.010148, test_acc: 0.53\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012059, train_acc: 0.52\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011118, train_acc: 0.70\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.013074, train_acc: 0.48\n",
      "alexnet1d, trial.103:\n",
      "Epoch 8, avg test_loss: 0.010022, test_acc: 0.53\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011612, train_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011903, train_acc: 0.50\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011988, train_acc: 0.62\n",
      "alexnet1d, trial.103:\n",
      "Epoch 9, avg test_loss: 0.010049, test_acc: 0.53\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011676, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011704, train_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011510, train_acc: 0.66\n",
      "alexnet1d, trial.103:\n",
      "Epoch 10, avg test_loss: 0.010189, test_acc: 0.53\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010995, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011512, train_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.013056, train_acc: 0.61\n",
      "alexnet1d, trial.103:\n",
      "Epoch 11, avg test_loss: 0.010129, test_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011136, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011627, train_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011015, train_acc: 0.68\n",
      "alexnet1d, trial.103:\n",
      "Epoch 12, avg test_loss: 0.009815, test_acc: 0.56\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011096, train_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.012366, train_acc: 0.54\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010528, train_acc: 0.70\n",
      "alexnet1d, trial.103:\n",
      "Epoch 13, avg test_loss: 0.009831, test_acc: 0.56\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010845, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011959, train_acc: 0.55\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010076, train_acc: 0.79\n",
      "alexnet1d, trial.103:\n",
      "Epoch 14, avg test_loss: 0.010172, test_acc: 0.57\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009825, train_acc: 0.62\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.012263, train_acc: 0.57\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.012378, train_acc: 0.59\n",
      "alexnet1d, trial.103:\n",
      "Epoch 15, avg test_loss: 0.009922, test_acc: 0.60\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.011095, train_acc: 0.68\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008657, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009551, train_acc: 0.73\n",
      "alexnet1d, trial.103:\n",
      "Epoch 16, avg test_loss: 0.010784, test_acc: 0.57\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009847, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010513, train_acc: 0.61\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009085, train_acc: 0.75\n",
      "alexnet1d, trial.103:\n",
      "Epoch 17, avg test_loss: 0.010068, test_acc: 0.57\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009920, train_acc: 0.71\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.010096, train_acc: 0.68\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.012628, train_acc: 0.62\n",
      "alexnet1d, trial.103:\n",
      "Epoch 18, avg test_loss: 0.011366, test_acc: 0.63\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007925, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009185, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009400, train_acc: 0.75\n",
      "alexnet1d, trial.103:\n",
      "Epoch 19, avg test_loss: 0.009988, test_acc: 0.60\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008179, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.009518, train_acc: 0.73\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.010742, train_acc: 0.73\n",
      "alexnet1d, trial.103:\n",
      "Epoch 20, avg test_loss: 0.012751, test_acc: 0.61\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008946, train_acc: 0.77\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007199, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.009279, train_acc: 0.79\n",
      "alexnet1d, trial.103:\n",
      "Epoch 21, avg test_loss: 0.014460, test_acc: 0.59\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.011173, train_acc: 0.62\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007809, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008173, train_acc: 0.75\n",
      "alexnet1d, trial.103:\n",
      "Epoch 22, avg test_loss: 0.010899, test_acc: 0.60\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007232, train_acc: 0.82\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007438, train_acc: 0.73\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.008453, train_acc: 0.80\n",
      "alexnet1d, trial.103:\n",
      "Epoch 23, avg test_loss: 0.012868, test_acc: 0.64\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006772, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.008756, train_acc: 0.73\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.007221, train_acc: 0.82\n",
      "alexnet1d, trial.103:\n",
      "Epoch 24, avg test_loss: 0.012382, test_acc: 0.61\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.007089, train_acc: 0.82\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.008923, train_acc: 0.73\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006181, train_acc: 0.88\n",
      "alexnet1d, trial.103:\n",
      "Epoch 25, avg test_loss: 0.013222, test_acc: 0.64\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.006012, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.009093, train_acc: 0.75\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.005075, train_acc: 0.89\n",
      "alexnet1d, trial.103:\n",
      "Epoch 26, avg test_loss: 0.013452, test_acc: 0.69\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.006212, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004279, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.006861, train_acc: 0.86\n",
      "alexnet1d, trial.103:\n",
      "Epoch 27, avg test_loss: 0.016782, test_acc: 0.66\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.006196, train_acc: 0.84\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.005122, train_acc: 0.86\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.002459, train_acc: 0.96\n",
      "alexnet1d, trial.103:\n",
      "Epoch 28, avg test_loss: 0.016337, test_acc: 0.67\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.002952, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.005370, train_acc: 0.89\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.006138, train_acc: 0.86\n",
      "alexnet1d, trial.103:\n",
      "Epoch 29, avg test_loss: 0.016545, test_acc: 0.67\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.002967, train_acc: 0.91\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.004630, train_acc: 0.91\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.002876, train_acc: 0.96\n",
      "alexnet1d, trial.103:\n",
      "Epoch 30, avg test_loss: 0.023131, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.005341, train_acc: 0.89\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.002021, train_acc: 0.96\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.004278, train_acc: 0.91\n",
      "alexnet1d, trial.103:\n",
      "Epoch 31, avg test_loss: 0.019540, test_acc: 0.67\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号37个\n",
      "错误信号33个\n",
      "信号正确并预测正确的概率为0.457\n",
      "信号错误并预测正确的概率为0.214\n",
      "总正确率为0.67\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012406, train_acc: 0.39\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.013119, train_acc: 0.61\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012048, train_acc: 0.61\n",
      "alexnet1d, trial.104:\n",
      "Epoch 0, avg test_loss: 0.009920, test_acc: 0.51\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012256, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012272, train_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011870, train_acc: 0.66\n",
      "alexnet1d, trial.104:\n",
      "Epoch 1, avg test_loss: 0.010624, test_acc: 0.51\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012606, train_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012141, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011909, train_acc: 0.62\n",
      "alexnet1d, trial.104:\n",
      "Epoch 2, avg test_loss: 0.010051, test_acc: 0.51\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011748, train_acc: 0.62\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012551, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012347, train_acc: 0.57\n",
      "alexnet1d, trial.104:\n",
      "Epoch 3, avg test_loss: 0.010386, test_acc: 0.51\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.010964, train_acc: 0.70\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012614, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012444, train_acc: 0.54\n",
      "alexnet1d, trial.104:\n",
      "Epoch 4, avg test_loss: 0.010144, test_acc: 0.51\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012969, train_acc: 0.43\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011639, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011884, train_acc: 0.66\n",
      "alexnet1d, trial.104:\n",
      "Epoch 5, avg test_loss: 0.010100, test_acc: 0.51\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011648, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012163, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.013882, train_acc: 0.46\n",
      "alexnet1d, trial.104:\n",
      "Epoch 6, avg test_loss: 0.010802, test_acc: 0.51\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012791, train_acc: 0.46\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012024, train_acc: 0.55\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011536, train_acc: 0.71\n",
      "alexnet1d, trial.104:\n",
      "Epoch 7, avg test_loss: 0.010012, test_acc: 0.53\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011572, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.010951, train_acc: 0.71\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.014041, train_acc: 0.50\n",
      "alexnet1d, trial.104:\n",
      "Epoch 8, avg test_loss: 0.010709, test_acc: 0.53\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.013157, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011161, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012067, train_acc: 0.61\n",
      "alexnet1d, trial.104:\n",
      "Epoch 9, avg test_loss: 0.010122, test_acc: 0.53\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011148, train_acc: 0.71\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011969, train_acc: 0.52\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012626, train_acc: 0.50\n",
      "alexnet1d, trial.104:\n",
      "Epoch 10, avg test_loss: 0.010985, test_acc: 0.51\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010739, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.012324, train_acc: 0.50\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011693, train_acc: 0.66\n",
      "alexnet1d, trial.104:\n",
      "Epoch 11, avg test_loss: 0.010924, test_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011215, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011189, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011395, train_acc: 0.70\n",
      "alexnet1d, trial.104:\n",
      "Epoch 12, avg test_loss: 0.010062, test_acc: 0.67\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010422, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011287, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011335, train_acc: 0.59\n",
      "alexnet1d, trial.104:\n",
      "Epoch 13, avg test_loss: 0.011293, test_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.011424, train_acc: 0.59\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010679, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009805, train_acc: 0.77\n",
      "alexnet1d, trial.104:\n",
      "Epoch 14, avg test_loss: 0.010938, test_acc: 0.67\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.011321, train_acc: 0.68\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008773, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010568, train_acc: 0.68\n",
      "alexnet1d, trial.104:\n",
      "Epoch 15, avg test_loss: 0.012045, test_acc: 0.61\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.011305, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009287, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.011714, train_acc: 0.66\n",
      "alexnet1d, trial.104:\n",
      "Epoch 16, avg test_loss: 0.012439, test_acc: 0.64\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008780, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009963, train_acc: 0.68\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009347, train_acc: 0.75\n",
      "alexnet1d, trial.104:\n",
      "Epoch 17, avg test_loss: 0.011865, test_acc: 0.69\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.010772, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009908, train_acc: 0.71\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008757, train_acc: 0.77\n",
      "alexnet1d, trial.104:\n",
      "Epoch 18, avg test_loss: 0.013208, test_acc: 0.63\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009434, train_acc: 0.70\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009012, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.006966, train_acc: 0.88\n",
      "alexnet1d, trial.104:\n",
      "Epoch 19, avg test_loss: 0.012413, test_acc: 0.61\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007505, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.006968, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007169, train_acc: 0.82\n",
      "alexnet1d, trial.104:\n",
      "Epoch 20, avg test_loss: 0.016450, test_acc: 0.61\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.005999, train_acc: 0.91\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.009069, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006600, train_acc: 0.84\n",
      "alexnet1d, trial.104:\n",
      "Epoch 21, avg test_loss: 0.016856, test_acc: 0.66\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006793, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007863, train_acc: 0.79\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.010206, train_acc: 0.79\n",
      "alexnet1d, trial.104:\n",
      "Epoch 22, avg test_loss: 0.013654, test_acc: 0.63\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007629, train_acc: 0.80\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.010115, train_acc: 0.71\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006977, train_acc: 0.82\n",
      "alexnet1d, trial.104:\n",
      "Epoch 23, avg test_loss: 0.013276, test_acc: 0.63\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005559, train_acc: 0.93\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005696, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005580, train_acc: 0.91\n",
      "alexnet1d, trial.104:\n",
      "Epoch 24, avg test_loss: 0.014778, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004643, train_acc: 0.96\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005135, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.007476, train_acc: 0.80\n",
      "alexnet1d, trial.104:\n",
      "Epoch 25, avg test_loss: 0.017634, test_acc: 0.61\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.003780, train_acc: 0.96\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004434, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.007471, train_acc: 0.84\n",
      "alexnet1d, trial.104:\n",
      "Epoch 26, avg test_loss: 0.020879, test_acc: 0.60\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.002265, train_acc: 0.98\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004085, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.002757, train_acc: 0.96\n",
      "alexnet1d, trial.104:\n",
      "Epoch 27, avg test_loss: 0.021391, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.002491, train_acc: 0.96\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004672, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.004692, train_acc: 0.93\n",
      "alexnet1d, trial.104:\n",
      "Epoch 28, avg test_loss: 0.024903, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.002760, train_acc: 0.96\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.004389, train_acc: 0.89\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.002118, train_acc: 0.95\n",
      "alexnet1d, trial.104:\n",
      "Epoch 29, avg test_loss: 0.028320, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号36个\n",
      "错误信号34个\n",
      "信号正确并预测正确的概率为0.314\n",
      "信号错误并预测正确的概率为0.257\n",
      "总正确率为0.57\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012351, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.018380, train_acc: 0.43\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012197, train_acc: 0.68\n",
      "alexnet1d, trial.105:\n",
      "Epoch 0, avg test_loss: 0.009892, test_acc: 0.51\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012281, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012268, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012052, train_acc: 0.68\n",
      "alexnet1d, trial.105:\n",
      "Epoch 1, avg test_loss: 0.009909, test_acc: 0.51\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012142, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011857, train_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.013478, train_acc: 0.52\n",
      "alexnet1d, trial.105:\n",
      "Epoch 2, avg test_loss: 0.010333, test_acc: 0.51\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011837, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011540, train_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012304, train_acc: 0.54\n",
      "alexnet1d, trial.105:\n",
      "Epoch 3, avg test_loss: 0.010030, test_acc: 0.51\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012022, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011989, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011350, train_acc: 0.66\n",
      "alexnet1d, trial.105:\n",
      "Epoch 4, avg test_loss: 0.010217, test_acc: 0.51\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011633, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011908, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011526, train_acc: 0.62\n",
      "alexnet1d, trial.105:\n",
      "Epoch 5, avg test_loss: 0.010189, test_acc: 0.51\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011856, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011253, train_acc: 0.71\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.013239, train_acc: 0.50\n",
      "alexnet1d, trial.105:\n",
      "Epoch 6, avg test_loss: 0.010140, test_acc: 0.51\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012276, train_acc: 0.55\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011674, train_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012324, train_acc: 0.52\n",
      "alexnet1d, trial.105:\n",
      "Epoch 7, avg test_loss: 0.009944, test_acc: 0.51\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011628, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012004, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011867, train_acc: 0.61\n",
      "alexnet1d, trial.105:\n",
      "Epoch 8, avg test_loss: 0.009966, test_acc: 0.51\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011421, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011881, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011516, train_acc: 0.61\n",
      "alexnet1d, trial.105:\n",
      "Epoch 9, avg test_loss: 0.010102, test_acc: 0.51\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.012314, train_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012902, train_acc: 0.52\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012568, train_acc: 0.54\n",
      "alexnet1d, trial.105:\n",
      "Epoch 10, avg test_loss: 0.010009, test_acc: 0.51\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.012035, train_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011433, train_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011504, train_acc: 0.64\n",
      "alexnet1d, trial.105:\n",
      "Epoch 11, avg test_loss: 0.009780, test_acc: 0.51\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011521, train_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011608, train_acc: 0.55\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011528, train_acc: 0.55\n",
      "alexnet1d, trial.105:\n",
      "Epoch 12, avg test_loss: 0.010061, test_acc: 0.51\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.012021, train_acc: 0.55\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010960, train_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010443, train_acc: 0.66\n",
      "alexnet1d, trial.105:\n",
      "Epoch 13, avg test_loss: 0.009675, test_acc: 0.54\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.011086, train_acc: 0.57\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011287, train_acc: 0.62\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010995, train_acc: 0.66\n",
      "alexnet1d, trial.105:\n",
      "Epoch 14, avg test_loss: 0.009480, test_acc: 0.56\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010897, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010705, train_acc: 0.62\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010309, train_acc: 0.79\n",
      "alexnet1d, trial.105:\n",
      "Epoch 15, avg test_loss: 0.009679, test_acc: 0.56\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.007975, train_acc: 0.84\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010406, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010750, train_acc: 0.68\n",
      "alexnet1d, trial.105:\n",
      "Epoch 16, avg test_loss: 0.009763, test_acc: 0.63\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.011232, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009464, train_acc: 0.71\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010098, train_acc: 0.73\n",
      "alexnet1d, trial.105:\n",
      "Epoch 17, avg test_loss: 0.010770, test_acc: 0.51\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009547, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009147, train_acc: 0.70\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009876, train_acc: 0.79\n",
      "alexnet1d, trial.105:\n",
      "Epoch 18, avg test_loss: 0.009737, test_acc: 0.61\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008740, train_acc: 0.75\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.012512, train_acc: 0.70\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009356, train_acc: 0.75\n",
      "alexnet1d, trial.105:\n",
      "Epoch 19, avg test_loss: 0.009446, test_acc: 0.60\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.009663, train_acc: 0.68\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008792, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009623, train_acc: 0.73\n",
      "alexnet1d, trial.105:\n",
      "Epoch 20, avg test_loss: 0.010175, test_acc: 0.57\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007778, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006831, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.011219, train_acc: 0.71\n",
      "alexnet1d, trial.105:\n",
      "Epoch 21, avg test_loss: 0.010598, test_acc: 0.60\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007337, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006789, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008224, train_acc: 0.79\n",
      "alexnet1d, trial.105:\n",
      "Epoch 22, avg test_loss: 0.011345, test_acc: 0.57\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006554, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.008328, train_acc: 0.71\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.007501, train_acc: 0.80\n",
      "alexnet1d, trial.105:\n",
      "Epoch 23, avg test_loss: 0.011755, test_acc: 0.61\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.008262, train_acc: 0.77\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.007845, train_acc: 0.80\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005956, train_acc: 0.88\n",
      "alexnet1d, trial.105:\n",
      "Epoch 24, avg test_loss: 0.013543, test_acc: 0.54\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004610, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.010119, train_acc: 0.73\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.007355, train_acc: 0.82\n",
      "alexnet1d, trial.105:\n",
      "Epoch 25, avg test_loss: 0.011814, test_acc: 0.60\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005093, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.006836, train_acc: 0.82\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004687, train_acc: 0.89\n",
      "alexnet1d, trial.105:\n",
      "Epoch 26, avg test_loss: 0.012665, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004949, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004827, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003733, train_acc: 0.91\n",
      "alexnet1d, trial.105:\n",
      "Epoch 27, avg test_loss: 0.015565, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004261, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.002943, train_acc: 0.98\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.003417, train_acc: 0.95\n",
      "alexnet1d, trial.105:\n",
      "Epoch 28, avg test_loss: 0.018508, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.003416, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.005303, train_acc: 0.84\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.001557, train_acc: 1.00\n",
      "alexnet1d, trial.105:\n",
      "Epoch 29, avg test_loss: 0.021957, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号36个\n",
      "错误信号34个\n",
      "信号正确并预测正确的概率为0.314\n",
      "信号错误并预测正确的概率为0.271\n",
      "总正确率为0.59\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012385, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.014759, train_acc: 0.43\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012025, train_acc: 0.73\n",
      "alexnet1d, trial.106:\n",
      "Epoch 0, avg test_loss: 0.009475, test_acc: 0.66\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012217, train_acc: 0.62\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012764, train_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012916, train_acc: 0.46\n",
      "alexnet1d, trial.106:\n",
      "Epoch 1, avg test_loss: 0.009358, test_acc: 0.66\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012320, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011844, train_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012774, train_acc: 0.46\n",
      "alexnet1d, trial.106:\n",
      "Epoch 2, avg test_loss: 0.009426, test_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012118, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012382, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012629, train_acc: 0.48\n",
      "alexnet1d, trial.106:\n",
      "Epoch 3, avg test_loss: 0.009434, test_acc: 0.66\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012172, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012192, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012398, train_acc: 0.52\n",
      "alexnet1d, trial.106:\n",
      "Epoch 4, avg test_loss: 0.009432, test_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011940, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011974, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012621, train_acc: 0.50\n",
      "alexnet1d, trial.106:\n",
      "Epoch 5, avg test_loss: 0.009373, test_acc: 0.66\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012638, train_acc: 0.46\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012122, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011930, train_acc: 0.64\n",
      "alexnet1d, trial.106:\n",
      "Epoch 6, avg test_loss: 0.009419, test_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012453, train_acc: 0.48\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011958, train_acc: 0.55\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012686, train_acc: 0.57\n",
      "alexnet1d, trial.106:\n",
      "Epoch 7, avg test_loss: 0.009088, test_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011862, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011942, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011974, train_acc: 0.54\n",
      "alexnet1d, trial.106:\n",
      "Epoch 8, avg test_loss: 0.009042, test_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011551, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012636, train_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011981, train_acc: 0.57\n",
      "alexnet1d, trial.106:\n",
      "Epoch 9, avg test_loss: 0.009091, test_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011509, train_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011928, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011320, train_acc: 0.64\n",
      "alexnet1d, trial.106:\n",
      "Epoch 10, avg test_loss: 0.009028, test_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011494, train_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011110, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010610, train_acc: 0.66\n",
      "alexnet1d, trial.106:\n",
      "Epoch 11, avg test_loss: 0.009036, test_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010594, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011737, train_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010994, train_acc: 0.57\n",
      "alexnet1d, trial.106:\n",
      "Epoch 12, avg test_loss: 0.009326, test_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011856, train_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009142, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.012635, train_acc: 0.59\n",
      "alexnet1d, trial.106:\n",
      "Epoch 13, avg test_loss: 0.009354, test_acc: 0.57\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010020, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011142, train_acc: 0.62\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011342, train_acc: 0.70\n",
      "alexnet1d, trial.106:\n",
      "Epoch 14, avg test_loss: 0.009815, test_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010295, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010251, train_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011268, train_acc: 0.66\n",
      "alexnet1d, trial.106:\n",
      "Epoch 15, avg test_loss: 0.009675, test_acc: 0.59\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010327, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010223, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009098, train_acc: 0.79\n",
      "alexnet1d, trial.106:\n",
      "Epoch 16, avg test_loss: 0.010334, test_acc: 0.66\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008179, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009367, train_acc: 0.71\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.012287, train_acc: 0.64\n",
      "alexnet1d, trial.106:\n",
      "Epoch 17, avg test_loss: 0.010457, test_acc: 0.57\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007807, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007503, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008204, train_acc: 0.80\n",
      "alexnet1d, trial.106:\n",
      "Epoch 18, avg test_loss: 0.011160, test_acc: 0.57\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006808, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007695, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007481, train_acc: 0.84\n",
      "alexnet1d, trial.106:\n",
      "Epoch 19, avg test_loss: 0.011918, test_acc: 0.56\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006592, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.006516, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007212, train_acc: 0.82\n",
      "alexnet1d, trial.106:\n",
      "Epoch 20, avg test_loss: 0.013100, test_acc: 0.59\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006163, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006262, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006466, train_acc: 0.86\n",
      "alexnet1d, trial.106:\n",
      "Epoch 21, avg test_loss: 0.012336, test_acc: 0.60\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.004962, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.008329, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007376, train_acc: 0.86\n",
      "alexnet1d, trial.106:\n",
      "Epoch 22, avg test_loss: 0.015534, test_acc: 0.67\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.004941, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.009696, train_acc: 0.77\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.008760, train_acc: 0.77\n",
      "alexnet1d, trial.106:\n",
      "Epoch 23, avg test_loss: 0.012890, test_acc: 0.57\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005853, train_acc: 0.82\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005691, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.007410, train_acc: 0.82\n",
      "alexnet1d, trial.106:\n",
      "Epoch 24, avg test_loss: 0.012416, test_acc: 0.57\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004264, train_acc: 0.96\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005419, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005300, train_acc: 0.86\n",
      "alexnet1d, trial.106:\n",
      "Epoch 25, avg test_loss: 0.014593, test_acc: 0.59\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005688, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004941, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.005241, train_acc: 0.89\n",
      "alexnet1d, trial.106:\n",
      "Epoch 26, avg test_loss: 0.015650, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003560, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.008129, train_acc: 0.86\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004929, train_acc: 0.88\n",
      "alexnet1d, trial.106:\n",
      "Epoch 27, avg test_loss: 0.017519, test_acc: 0.53\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004439, train_acc: 0.89\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004975, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.002928, train_acc: 0.96\n",
      "alexnet1d, trial.106:\n",
      "Epoch 28, avg test_loss: 0.015042, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.003558, train_acc: 0.89\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.003637, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.004848, train_acc: 0.88\n",
      "alexnet1d, trial.106:\n",
      "Epoch 29, avg test_loss: 0.015741, test_acc: 0.63\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.002893, train_acc: 0.96\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.002706, train_acc: 0.96\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.002835, train_acc: 0.95\n",
      "alexnet1d, trial.106:\n",
      "Epoch 30, avg test_loss: 0.018987, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.002037, train_acc: 0.95\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.003873, train_acc: 0.91\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.003205, train_acc: 0.91\n",
      "alexnet1d, trial.106:\n",
      "Epoch 31, avg test_loss: 0.019600, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号46个\n",
      "错误信号24个\n",
      "信号正确并预测正确的概率为0.443\n",
      "信号错误并预测正确的概率为0.171\n",
      "总正确率为0.61\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012373, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012987, train_acc: 0.62\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012300, train_acc: 0.55\n",
      "alexnet1d, trial.107:\n",
      "Epoch 0, avg test_loss: 0.009908, test_acc: 0.51\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012313, train_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012025, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.014511, train_acc: 0.52\n",
      "alexnet1d, trial.107:\n",
      "Epoch 1, avg test_loss: 0.010629, test_acc: 0.51\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012299, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012456, train_acc: 0.52\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012208, train_acc: 0.59\n",
      "alexnet1d, trial.107:\n",
      "Epoch 2, avg test_loss: 0.009890, test_acc: 0.51\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012174, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012360, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012468, train_acc: 0.54\n",
      "alexnet1d, trial.107:\n",
      "Epoch 3, avg test_loss: 0.009976, test_acc: 0.51\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012105, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012519, train_acc: 0.50\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011461, train_acc: 0.68\n",
      "alexnet1d, trial.107:\n",
      "Epoch 4, avg test_loss: 0.010106, test_acc: 0.51\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012141, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011820, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.010954, train_acc: 0.73\n",
      "alexnet1d, trial.107:\n",
      "Epoch 5, avg test_loss: 0.010126, test_acc: 0.51\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011619, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012063, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012086, train_acc: 0.55\n",
      "alexnet1d, trial.107:\n",
      "Epoch 6, avg test_loss: 0.010209, test_acc: 0.51\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011486, train_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011762, train_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012177, train_acc: 0.59\n",
      "alexnet1d, trial.107:\n",
      "Epoch 7, avg test_loss: 0.010164, test_acc: 0.51\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011239, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012168, train_acc: 0.55\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011880, train_acc: 0.62\n",
      "alexnet1d, trial.107:\n",
      "Epoch 8, avg test_loss: 0.009980, test_acc: 0.51\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012051, train_acc: 0.52\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011510, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012480, train_acc: 0.52\n",
      "alexnet1d, trial.107:\n",
      "Epoch 9, avg test_loss: 0.010741, test_acc: 0.51\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.012109, train_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011572, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011686, train_acc: 0.61\n",
      "alexnet1d, trial.107:\n",
      "Epoch 10, avg test_loss: 0.010060, test_acc: 0.51\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011458, train_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011712, train_acc: 0.55\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010843, train_acc: 0.61\n",
      "alexnet1d, trial.107:\n",
      "Epoch 11, avg test_loss: 0.010446, test_acc: 0.56\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.012440, train_acc: 0.52\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011512, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011205, train_acc: 0.62\n",
      "alexnet1d, trial.107:\n",
      "Epoch 12, avg test_loss: 0.011346, test_acc: 0.56\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011382, train_acc: 0.62\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011431, train_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010744, train_acc: 0.71\n",
      "alexnet1d, trial.107:\n",
      "Epoch 13, avg test_loss: 0.010134, test_acc: 0.51\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010368, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010196, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009842, train_acc: 0.71\n",
      "alexnet1d, trial.107:\n",
      "Epoch 14, avg test_loss: 0.012541, test_acc: 0.56\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009242, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009468, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009045, train_acc: 0.73\n",
      "alexnet1d, trial.107:\n",
      "Epoch 15, avg test_loss: 0.012088, test_acc: 0.53\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009085, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010272, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008704, train_acc: 0.79\n",
      "alexnet1d, trial.107:\n",
      "Epoch 16, avg test_loss: 0.013511, test_acc: 0.57\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008906, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009251, train_acc: 0.70\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007948, train_acc: 0.82\n",
      "alexnet1d, trial.107:\n",
      "Epoch 17, avg test_loss: 0.015206, test_acc: 0.56\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.010135, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007763, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009184, train_acc: 0.77\n",
      "alexnet1d, trial.107:\n",
      "Epoch 18, avg test_loss: 0.013362, test_acc: 0.60\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008305, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008567, train_acc: 0.75\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009611, train_acc: 0.68\n",
      "alexnet1d, trial.107:\n",
      "Epoch 19, avg test_loss: 0.015126, test_acc: 0.51\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.009315, train_acc: 0.66\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008578, train_acc: 0.73\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007822, train_acc: 0.86\n",
      "alexnet1d, trial.107:\n",
      "Epoch 20, avg test_loss: 0.013462, test_acc: 0.57\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.005906, train_acc: 0.93\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.008942, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007495, train_acc: 0.82\n",
      "alexnet1d, trial.107:\n",
      "Epoch 21, avg test_loss: 0.014057, test_acc: 0.59\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007580, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006130, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007487, train_acc: 0.79\n",
      "alexnet1d, trial.107:\n",
      "Epoch 22, avg test_loss: 0.018738, test_acc: 0.54\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006688, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006626, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.005612, train_acc: 0.84\n",
      "alexnet1d, trial.107:\n",
      "Epoch 23, avg test_loss: 0.018875, test_acc: 0.53\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004412, train_acc: 0.93\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004738, train_acc: 0.95\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006059, train_acc: 0.82\n",
      "alexnet1d, trial.107:\n",
      "Epoch 24, avg test_loss: 0.020020, test_acc: 0.60\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004660, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005214, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.002434, train_acc: 0.98\n",
      "alexnet1d, trial.107:\n",
      "Epoch 25, avg test_loss: 0.022407, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004094, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005872, train_acc: 0.84\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.002639, train_acc: 0.96\n",
      "alexnet1d, trial.107:\n",
      "Epoch 26, avg test_loss: 0.025513, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003784, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.002357, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003614, train_acc: 0.91\n",
      "alexnet1d, trial.107:\n",
      "Epoch 27, avg test_loss: 0.028700, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.002499, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.003193, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.001932, train_acc: 0.98\n",
      "alexnet1d, trial.107:\n",
      "Epoch 28, avg test_loss: 0.025186, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号36个\n",
      "错误信号34个\n",
      "信号正确并预测正确的概率为0.343\n",
      "信号错误并预测正确的概率为0.243\n",
      "总正确率为0.59\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012395, train_acc: 0.50\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.016201, train_acc: 0.45\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012617, train_acc: 0.43\n",
      "alexnet1d, trial.108:\n",
      "Epoch 0, avg test_loss: 0.010011, test_acc: 0.37\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012582, train_acc: 0.41\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012356, train_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012365, train_acc: 0.52\n",
      "alexnet1d, trial.108:\n",
      "Epoch 1, avg test_loss: 0.009744, test_acc: 0.63\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012001, train_acc: 0.68\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012165, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012472, train_acc: 0.54\n",
      "alexnet1d, trial.108:\n",
      "Epoch 2, avg test_loss: 0.009638, test_acc: 0.63\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012175, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012048, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012994, train_acc: 0.50\n",
      "alexnet1d, trial.108:\n",
      "Epoch 3, avg test_loss: 0.009669, test_acc: 0.63\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011877, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011943, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012174, train_acc: 0.59\n",
      "alexnet1d, trial.108:\n",
      "Epoch 4, avg test_loss: 0.009656, test_acc: 0.63\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011737, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012465, train_acc: 0.52\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011784, train_acc: 0.64\n",
      "alexnet1d, trial.108:\n",
      "Epoch 5, avg test_loss: 0.009670, test_acc: 0.63\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012021, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011636, train_acc: 0.68\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011908, train_acc: 0.57\n",
      "alexnet1d, trial.108:\n",
      "Epoch 6, avg test_loss: 0.009651, test_acc: 0.63\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012537, train_acc: 0.46\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011834, train_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011797, train_acc: 0.64\n",
      "alexnet1d, trial.108:\n",
      "Epoch 7, avg test_loss: 0.009667, test_acc: 0.63\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011316, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012932, train_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012047, train_acc: 0.57\n",
      "alexnet1d, trial.108:\n",
      "Epoch 8, avg test_loss: 0.009481, test_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011745, train_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011598, train_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011844, train_acc: 0.59\n",
      "alexnet1d, trial.108:\n",
      "Epoch 9, avg test_loss: 0.009397, test_acc: 0.63\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011405, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011701, train_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011370, train_acc: 0.70\n",
      "alexnet1d, trial.108:\n",
      "Epoch 10, avg test_loss: 0.009444, test_acc: 0.56\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011057, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010988, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011186, train_acc: 0.66\n",
      "alexnet1d, trial.108:\n",
      "Epoch 11, avg test_loss: 0.009347, test_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011326, train_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010490, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011757, train_acc: 0.59\n",
      "alexnet1d, trial.108:\n",
      "Epoch 12, avg test_loss: 0.009663, test_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009848, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.012289, train_acc: 0.57\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011185, train_acc: 0.70\n",
      "alexnet1d, trial.108:\n",
      "Epoch 13, avg test_loss: 0.009549, test_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010013, train_acc: 0.79\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010781, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011144, train_acc: 0.62\n",
      "alexnet1d, trial.108:\n",
      "Epoch 14, avg test_loss: 0.009533, test_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010450, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008370, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010278, train_acc: 0.70\n",
      "alexnet1d, trial.108:\n",
      "Epoch 15, avg test_loss: 0.009631, test_acc: 0.63\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.007918, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009425, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010761, train_acc: 0.64\n",
      "alexnet1d, trial.108:\n",
      "Epoch 16, avg test_loss: 0.010343, test_acc: 0.60\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009036, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.011765, train_acc: 0.68\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009688, train_acc: 0.71\n",
      "alexnet1d, trial.108:\n",
      "Epoch 17, avg test_loss: 0.010086, test_acc: 0.63\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008395, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009530, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008709, train_acc: 0.73\n",
      "alexnet1d, trial.108:\n",
      "Epoch 18, avg test_loss: 0.010957, test_acc: 0.64\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008442, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009928, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007344, train_acc: 0.80\n",
      "alexnet1d, trial.108:\n",
      "Epoch 19, avg test_loss: 0.010881, test_acc: 0.64\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.009126, train_acc: 0.75\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008849, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008728, train_acc: 0.77\n",
      "alexnet1d, trial.108:\n",
      "Epoch 20, avg test_loss: 0.010708, test_acc: 0.63\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007203, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.008028, train_acc: 0.77\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008167, train_acc: 0.82\n",
      "alexnet1d, trial.108:\n",
      "Epoch 21, avg test_loss: 0.010810, test_acc: 0.69\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007503, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.008875, train_acc: 0.77\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008483, train_acc: 0.75\n",
      "alexnet1d, trial.108:\n",
      "Epoch 22, avg test_loss: 0.011322, test_acc: 0.66\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.009249, train_acc: 0.71\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005288, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.007783, train_acc: 0.79\n",
      "alexnet1d, trial.108:\n",
      "Epoch 23, avg test_loss: 0.011354, test_acc: 0.69\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005949, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005959, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006270, train_acc: 0.86\n",
      "alexnet1d, trial.108:\n",
      "Epoch 24, avg test_loss: 0.013134, test_acc: 0.71\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005629, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005448, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006231, train_acc: 0.88\n",
      "alexnet1d, trial.108:\n",
      "Epoch 25, avg test_loss: 0.011745, test_acc: 0.69\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005852, train_acc: 0.82\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005358, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004972, train_acc: 0.91\n",
      "alexnet1d, trial.108:\n",
      "Epoch 26, avg test_loss: 0.014476, test_acc: 0.69\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004038, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.007513, train_acc: 0.86\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.005183, train_acc: 0.88\n",
      "alexnet1d, trial.108:\n",
      "Epoch 27, avg test_loss: 0.013292, test_acc: 0.67\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003540, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004751, train_acc: 0.89\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.004900, train_acc: 0.84\n",
      "alexnet1d, trial.108:\n",
      "Epoch 28, avg test_loss: 0.016459, test_acc: 0.66\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.006939, train_acc: 0.80\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.003365, train_acc: 0.96\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.005406, train_acc: 0.84\n",
      "alexnet1d, trial.108:\n",
      "Epoch 29, avg test_loss: 0.014659, test_acc: 0.66\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.003168, train_acc: 0.93\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.002540, train_acc: 0.96\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.003742, train_acc: 0.89\n",
      "alexnet1d, trial.108:\n",
      "Epoch 30, avg test_loss: 0.014917, test_acc: 0.69\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.002581, train_acc: 0.98\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.002336, train_acc: 0.95\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.003750, train_acc: 0.93\n",
      "alexnet1d, trial.108:\n",
      "Epoch 31, avg test_loss: 0.021014, test_acc: 0.67\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.001943, train_acc: 0.96\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.002649, train_acc: 0.98\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.008906, train_acc: 0.82\n",
      "alexnet1d, trial.108:\n",
      "Epoch 32, avg test_loss: 0.017141, test_acc: 0.74\n",
      "Train Epoch 33, lr: 0.000377, 0/280, avg loss: 0.003326, train_acc: 0.91\n",
      "Train Epoch 33, lr: 0.000377, 112/280, avg loss: 0.004371, train_acc: 0.93\n",
      "Train Epoch 33, lr: 0.000377, 224/280, avg loss: 0.003454, train_acc: 0.93\n",
      "alexnet1d, trial.108:\n",
      "Epoch 33, avg test_loss: 0.017007, test_acc: 0.66\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号44个\n",
      "错误信号26个\n",
      "信号正确并预测正确的概率为0.443\n",
      "信号错误并预测正确的概率为0.214\n",
      "总正确率为0.66\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012422, train_acc: 0.45\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012419, train_acc: 0.48\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012254, train_acc: 0.73\n",
      "alexnet1d, trial.109:\n",
      "Epoch 0, avg test_loss: 0.009958, test_acc: 0.51\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012050, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.014417, train_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011924, train_acc: 0.62\n",
      "alexnet1d, trial.109:\n",
      "Epoch 1, avg test_loss: 0.009948, test_acc: 0.51\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011838, train_acc: 0.66\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012383, train_acc: 0.48\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012045, train_acc: 0.71\n",
      "alexnet1d, trial.109:\n",
      "Epoch 2, avg test_loss: 0.009902, test_acc: 0.51\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012351, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012240, train_acc: 0.55\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012310, train_acc: 0.55\n",
      "alexnet1d, trial.109:\n",
      "Epoch 3, avg test_loss: 0.009973, test_acc: 0.51\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012043, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012120, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011927, train_acc: 0.59\n",
      "alexnet1d, trial.109:\n",
      "Epoch 4, avg test_loss: 0.010412, test_acc: 0.51\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011288, train_acc: 0.68\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.010852, train_acc: 0.71\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012211, train_acc: 0.59\n",
      "alexnet1d, trial.109:\n",
      "Epoch 5, avg test_loss: 0.010222, test_acc: 0.51\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011388, train_acc: 0.68\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011247, train_acc: 0.70\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.013386, train_acc: 0.41\n",
      "alexnet1d, trial.109:\n",
      "Epoch 6, avg test_loss: 0.010069, test_acc: 0.51\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012162, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012341, train_acc: 0.52\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011953, train_acc: 0.62\n",
      "alexnet1d, trial.109:\n",
      "Epoch 7, avg test_loss: 0.009965, test_acc: 0.51\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012156, train_acc: 0.55\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011590, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012024, train_acc: 0.55\n",
      "alexnet1d, trial.109:\n",
      "Epoch 8, avg test_loss: 0.010393, test_acc: 0.51\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012220, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011349, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012509, train_acc: 0.54\n",
      "alexnet1d, trial.109:\n",
      "Epoch 9, avg test_loss: 0.010239, test_acc: 0.51\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011954, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012064, train_acc: 0.52\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011367, train_acc: 0.62\n",
      "alexnet1d, trial.109:\n",
      "Epoch 10, avg test_loss: 0.010099, test_acc: 0.51\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011310, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.012006, train_acc: 0.52\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011350, train_acc: 0.66\n",
      "alexnet1d, trial.109:\n",
      "Epoch 11, avg test_loss: 0.010011, test_acc: 0.51\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011382, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011396, train_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011869, train_acc: 0.57\n",
      "alexnet1d, trial.109:\n",
      "Epoch 12, avg test_loss: 0.009984, test_acc: 0.51\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011171, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.012342, train_acc: 0.54\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011094, train_acc: 0.62\n",
      "alexnet1d, trial.109:\n",
      "Epoch 13, avg test_loss: 0.010016, test_acc: 0.53\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.012022, train_acc: 0.55\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011155, train_acc: 0.64\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011344, train_acc: 0.57\n",
      "alexnet1d, trial.109:\n",
      "Epoch 14, avg test_loss: 0.009928, test_acc: 0.53\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.011209, train_acc: 0.59\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010879, train_acc: 0.61\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.013083, train_acc: 0.54\n",
      "alexnet1d, trial.109:\n",
      "Epoch 15, avg test_loss: 0.010451, test_acc: 0.57\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009811, train_acc: 0.62\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010129, train_acc: 0.68\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.011482, train_acc: 0.70\n",
      "alexnet1d, trial.109:\n",
      "Epoch 16, avg test_loss: 0.010107, test_acc: 0.61\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.010796, train_acc: 0.71\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010620, train_acc: 0.66\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.011589, train_acc: 0.57\n",
      "alexnet1d, trial.109:\n",
      "Epoch 17, avg test_loss: 0.012379, test_acc: 0.50\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.010598, train_acc: 0.66\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.010088, train_acc: 0.70\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.011716, train_acc: 0.66\n",
      "alexnet1d, trial.109:\n",
      "Epoch 18, avg test_loss: 0.010281, test_acc: 0.61\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009219, train_acc: 0.75\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009424, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008423, train_acc: 0.77\n",
      "alexnet1d, trial.109:\n",
      "Epoch 19, avg test_loss: 0.012037, test_acc: 0.60\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007630, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007606, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.013919, train_acc: 0.59\n",
      "alexnet1d, trial.109:\n",
      "Epoch 20, avg test_loss: 0.010524, test_acc: 0.66\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.009102, train_acc: 0.73\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.009973, train_acc: 0.70\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008605, train_acc: 0.73\n",
      "alexnet1d, trial.109:\n",
      "Epoch 21, avg test_loss: 0.012171, test_acc: 0.59\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.009103, train_acc: 0.71\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.011212, train_acc: 0.68\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.009047, train_acc: 0.82\n",
      "alexnet1d, trial.109:\n",
      "Epoch 22, avg test_loss: 0.010542, test_acc: 0.66\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.008733, train_acc: 0.82\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006774, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.007576, train_acc: 0.79\n",
      "alexnet1d, trial.109:\n",
      "Epoch 23, avg test_loss: 0.012350, test_acc: 0.66\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005310, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004623, train_acc: 0.93\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.009541, train_acc: 0.73\n",
      "alexnet1d, trial.109:\n",
      "Epoch 24, avg test_loss: 0.015139, test_acc: 0.60\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.006180, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006394, train_acc: 0.82\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004917, train_acc: 0.86\n",
      "alexnet1d, trial.109:\n",
      "Epoch 25, avg test_loss: 0.012578, test_acc: 0.73\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005683, train_acc: 0.86\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005679, train_acc: 0.80\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.005625, train_acc: 0.88\n",
      "alexnet1d, trial.109:\n",
      "Epoch 26, avg test_loss: 0.015412, test_acc: 0.66\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004663, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.005177, train_acc: 0.84\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.007694, train_acc: 0.82\n",
      "alexnet1d, trial.109:\n",
      "Epoch 27, avg test_loss: 0.017692, test_acc: 0.70\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.002195, train_acc: 0.96\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.005447, train_acc: 0.89\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.004581, train_acc: 0.88\n",
      "alexnet1d, trial.109:\n",
      "Epoch 28, avg test_loss: 0.018411, test_acc: 0.66\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.005045, train_acc: 0.88\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.004108, train_acc: 0.89\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.002899, train_acc: 0.93\n",
      "alexnet1d, trial.109:\n",
      "Epoch 29, avg test_loss: 0.015956, test_acc: 0.69\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.003384, train_acc: 0.91\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.003202, train_acc: 0.95\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.002144, train_acc: 0.96\n",
      "alexnet1d, trial.109:\n",
      "Epoch 30, avg test_loss: 0.019236, test_acc: 0.70\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.002122, train_acc: 0.98\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.001931, train_acc: 0.98\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.002887, train_acc: 0.93\n",
      "alexnet1d, trial.109:\n",
      "Epoch 31, avg test_loss: 0.021430, test_acc: 0.73\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.001464, train_acc: 0.96\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.002399, train_acc: 0.95\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.000833, train_acc: 1.00\n",
      "alexnet1d, trial.109:\n",
      "Epoch 32, avg test_loss: 0.025331, test_acc: 0.66\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号36个\n",
      "错误信号34个\n",
      "信号正确并预测正确的概率为0.371\n",
      "信号错误并预测正确的概率为0.286\n",
      "总正确率为0.66\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012496, train_acc: 0.38\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.014535, train_acc: 0.50\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012286, train_acc: 0.54\n",
      "alexnet1d, trial.110:\n",
      "Epoch 0, avg test_loss: 0.009842, test_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012088, train_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012038, train_acc: 0.62\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.013191, train_acc: 0.45\n",
      "alexnet1d, trial.110:\n",
      "Epoch 1, avg test_loss: 0.009886, test_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011919, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012286, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012285, train_acc: 0.54\n",
      "alexnet1d, trial.110:\n",
      "Epoch 2, avg test_loss: 0.009844, test_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012816, train_acc: 0.43\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012115, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011504, train_acc: 0.68\n",
      "alexnet1d, trial.110:\n",
      "Epoch 3, avg test_loss: 0.010087, test_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011778, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011223, train_acc: 0.66\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.013486, train_acc: 0.46\n",
      "alexnet1d, trial.110:\n",
      "Epoch 4, avg test_loss: 0.010457, test_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012329, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011919, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012099, train_acc: 0.59\n",
      "alexnet1d, trial.110:\n",
      "Epoch 5, avg test_loss: 0.009881, test_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012099, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012279, train_acc: 0.52\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011855, train_acc: 0.61\n",
      "alexnet1d, trial.110:\n",
      "Epoch 6, avg test_loss: 0.009870, test_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012051, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012054, train_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012032, train_acc: 0.57\n",
      "alexnet1d, trial.110:\n",
      "Epoch 7, avg test_loss: 0.010233, test_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012025, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011697, train_acc: 0.68\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011751, train_acc: 0.68\n",
      "alexnet1d, trial.110:\n",
      "Epoch 8, avg test_loss: 0.009852, test_acc: 0.51\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011681, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012085, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011769, train_acc: 0.62\n",
      "alexnet1d, trial.110:\n",
      "Epoch 9, avg test_loss: 0.010164, test_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010440, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011997, train_acc: 0.55\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011008, train_acc: 0.68\n",
      "alexnet1d, trial.110:\n",
      "Epoch 10, avg test_loss: 0.009516, test_acc: 0.56\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010796, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010538, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010844, train_acc: 0.66\n",
      "alexnet1d, trial.110:\n",
      "Epoch 11, avg test_loss: 0.010355, test_acc: 0.60\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009186, train_acc: 0.77\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010728, train_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.012727, train_acc: 0.64\n",
      "alexnet1d, trial.110:\n",
      "Epoch 12, avg test_loss: 0.010530, test_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009513, train_acc: 0.77\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011460, train_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011442, train_acc: 0.62\n",
      "alexnet1d, trial.110:\n",
      "Epoch 13, avg test_loss: 0.009662, test_acc: 0.59\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010874, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011765, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.008630, train_acc: 0.77\n",
      "alexnet1d, trial.110:\n",
      "Epoch 14, avg test_loss: 0.009722, test_acc: 0.60\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010154, train_acc: 0.68\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009332, train_acc: 0.84\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008657, train_acc: 0.77\n",
      "alexnet1d, trial.110:\n",
      "Epoch 15, avg test_loss: 0.010512, test_acc: 0.56\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.007661, train_acc: 0.80\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009165, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009354, train_acc: 0.70\n",
      "alexnet1d, trial.110:\n",
      "Epoch 16, avg test_loss: 0.010997, test_acc: 0.59\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009031, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007890, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007115, train_acc: 0.88\n",
      "alexnet1d, trial.110:\n",
      "Epoch 17, avg test_loss: 0.010941, test_acc: 0.60\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009864, train_acc: 0.62\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.005996, train_acc: 0.93\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009914, train_acc: 0.77\n",
      "alexnet1d, trial.110:\n",
      "Epoch 18, avg test_loss: 0.012846, test_acc: 0.53\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.010164, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007138, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009268, train_acc: 0.77\n",
      "alexnet1d, trial.110:\n",
      "Epoch 19, avg test_loss: 0.011747, test_acc: 0.63\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.009191, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.006317, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009446, train_acc: 0.77\n",
      "alexnet1d, trial.110:\n",
      "Epoch 20, avg test_loss: 0.014451, test_acc: 0.51\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008076, train_acc: 0.77\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007123, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.009073, train_acc: 0.77\n",
      "alexnet1d, trial.110:\n",
      "Epoch 21, avg test_loss: 0.012080, test_acc: 0.60\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007328, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.008314, train_acc: 0.77\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.005547, train_acc: 0.88\n",
      "alexnet1d, trial.110:\n",
      "Epoch 22, avg test_loss: 0.011228, test_acc: 0.59\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007302, train_acc: 0.82\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007429, train_acc: 0.80\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.007064, train_acc: 0.80\n",
      "alexnet1d, trial.110:\n",
      "Epoch 23, avg test_loss: 0.012926, test_acc: 0.61\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006019, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.007642, train_acc: 0.80\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.007149, train_acc: 0.82\n",
      "alexnet1d, trial.110:\n",
      "Epoch 24, avg test_loss: 0.013805, test_acc: 0.63\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.006069, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006269, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004615, train_acc: 0.95\n",
      "alexnet1d, trial.110:\n",
      "Epoch 25, avg test_loss: 0.014870, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005873, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005636, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.005234, train_acc: 0.91\n",
      "alexnet1d, trial.110:\n",
      "Epoch 26, avg test_loss: 0.014205, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.005397, train_acc: 0.88\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004653, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.005374, train_acc: 0.86\n",
      "alexnet1d, trial.110:\n",
      "Epoch 27, avg test_loss: 0.014911, test_acc: 0.60\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003270, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.005040, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.005030, train_acc: 0.93\n",
      "alexnet1d, trial.110:\n",
      "Epoch 28, avg test_loss: 0.021245, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.008604, train_acc: 0.80\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.010302, train_acc: 0.80\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.009955, train_acc: 0.79\n",
      "alexnet1d, trial.110:\n",
      "Epoch 29, avg test_loss: 0.013595, test_acc: 0.64\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.005137, train_acc: 0.89\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.007102, train_acc: 0.73\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.007113, train_acc: 0.80\n",
      "alexnet1d, trial.110:\n",
      "Epoch 30, avg test_loss: 0.012573, test_acc: 0.66\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.006426, train_acc: 0.84\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.005565, train_acc: 0.88\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.006031, train_acc: 0.86\n",
      "alexnet1d, trial.110:\n",
      "Epoch 31, avg test_loss: 0.014946, test_acc: 0.59\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.006513, train_acc: 0.82\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.005269, train_acc: 0.93\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.004616, train_acc: 0.93\n",
      "alexnet1d, trial.110:\n",
      "Epoch 32, avg test_loss: 0.013678, test_acc: 0.69\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号40个\n",
      "错误信号30个\n",
      "信号正确并预测正确的概率为0.414\n",
      "信号错误并预测正确的概率为0.271\n",
      "总正确率为0.69\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012484, train_acc: 0.41\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.013288, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012325, train_acc: 0.61\n",
      "alexnet1d, trial.111:\n",
      "Epoch 0, avg test_loss: 0.009914, test_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012303, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012253, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012114, train_acc: 0.57\n",
      "alexnet1d, trial.111:\n",
      "Epoch 1, avg test_loss: 0.010500, test_acc: 0.50\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.013249, train_acc: 0.46\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011994, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011928, train_acc: 0.64\n",
      "alexnet1d, trial.111:\n",
      "Epoch 2, avg test_loss: 0.010709, test_acc: 0.50\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011430, train_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012430, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012904, train_acc: 0.54\n",
      "alexnet1d, trial.111:\n",
      "Epoch 3, avg test_loss: 0.010294, test_acc: 0.50\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012384, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011762, train_acc: 0.66\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012485, train_acc: 0.52\n",
      "alexnet1d, trial.111:\n",
      "Epoch 4, avg test_loss: 0.010016, test_acc: 0.50\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012309, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012037, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012153, train_acc: 0.59\n",
      "alexnet1d, trial.111:\n",
      "Epoch 5, avg test_loss: 0.010066, test_acc: 0.50\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012794, train_acc: 0.45\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012149, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011421, train_acc: 0.68\n",
      "alexnet1d, trial.111:\n",
      "Epoch 6, avg test_loss: 0.010223, test_acc: 0.50\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011011, train_acc: 0.71\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.013119, train_acc: 0.48\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011289, train_acc: 0.68\n",
      "alexnet1d, trial.111:\n",
      "Epoch 7, avg test_loss: 0.010284, test_acc: 0.50\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012030, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011431, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012289, train_acc: 0.52\n",
      "alexnet1d, trial.111:\n",
      "Epoch 8, avg test_loss: 0.010196, test_acc: 0.50\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012152, train_acc: 0.55\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011142, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011540, train_acc: 0.62\n",
      "alexnet1d, trial.111:\n",
      "Epoch 9, avg test_loss: 0.010278, test_acc: 0.50\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011426, train_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012148, train_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012257, train_acc: 0.52\n",
      "alexnet1d, trial.111:\n",
      "Epoch 10, avg test_loss: 0.010119, test_acc: 0.50\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011357, train_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011568, train_acc: 0.55\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011040, train_acc: 0.64\n",
      "alexnet1d, trial.111:\n",
      "Epoch 11, avg test_loss: 0.010675, test_acc: 0.49\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010740, train_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011541, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010906, train_acc: 0.70\n",
      "alexnet1d, trial.111:\n",
      "Epoch 12, avg test_loss: 0.010142, test_acc: 0.47\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011229, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010393, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011654, train_acc: 0.71\n",
      "alexnet1d, trial.111:\n",
      "Epoch 13, avg test_loss: 0.010404, test_acc: 0.46\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010315, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011404, train_acc: 0.62\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011094, train_acc: 0.71\n",
      "alexnet1d, trial.111:\n",
      "Epoch 14, avg test_loss: 0.010247, test_acc: 0.50\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.011174, train_acc: 0.59\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010176, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009507, train_acc: 0.79\n",
      "alexnet1d, trial.111:\n",
      "Epoch 15, avg test_loss: 0.010676, test_acc: 0.59\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010612, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009693, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008515, train_acc: 0.80\n",
      "alexnet1d, trial.111:\n",
      "Epoch 16, avg test_loss: 0.011481, test_acc: 0.51\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009239, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.011325, train_acc: 0.62\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009838, train_acc: 0.84\n",
      "alexnet1d, trial.111:\n",
      "Epoch 17, avg test_loss: 0.010226, test_acc: 0.63\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.010606, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009788, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009025, train_acc: 0.73\n",
      "alexnet1d, trial.111:\n",
      "Epoch 18, avg test_loss: 0.014639, test_acc: 0.51\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.013406, train_acc: 0.61\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.010762, train_acc: 0.68\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009839, train_acc: 0.79\n",
      "alexnet1d, trial.111:\n",
      "Epoch 19, avg test_loss: 0.011751, test_acc: 0.47\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.009424, train_acc: 0.77\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008739, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.011776, train_acc: 0.61\n",
      "alexnet1d, trial.111:\n",
      "Epoch 20, avg test_loss: 0.010717, test_acc: 0.50\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008701, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007523, train_acc: 0.89\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.009233, train_acc: 0.71\n",
      "alexnet1d, trial.111:\n",
      "Epoch 21, avg test_loss: 0.010893, test_acc: 0.61\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.009556, train_acc: 0.71\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.008551, train_acc: 0.77\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007553, train_acc: 0.84\n",
      "alexnet1d, trial.111:\n",
      "Epoch 22, avg test_loss: 0.012022, test_acc: 0.54\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.008241, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007427, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.005793, train_acc: 0.86\n",
      "alexnet1d, trial.111:\n",
      "Epoch 23, avg test_loss: 0.012502, test_acc: 0.57\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.007676, train_acc: 0.79\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004788, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.009289, train_acc: 0.77\n",
      "alexnet1d, trial.111:\n",
      "Epoch 24, avg test_loss: 0.012785, test_acc: 0.56\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005079, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004062, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005565, train_acc: 0.86\n",
      "alexnet1d, trial.111:\n",
      "Epoch 25, avg test_loss: 0.015230, test_acc: 0.59\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.006117, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.003906, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.007995, train_acc: 0.80\n",
      "alexnet1d, trial.111:\n",
      "Epoch 26, avg test_loss: 0.014273, test_acc: 0.61\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.006198, train_acc: 0.88\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004113, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.005372, train_acc: 0.88\n",
      "alexnet1d, trial.111:\n",
      "Epoch 27, avg test_loss: 0.016878, test_acc: 0.54\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004161, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.003875, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.003240, train_acc: 0.93\n",
      "alexnet1d, trial.111:\n",
      "Epoch 28, avg test_loss: 0.017678, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.002086, train_acc: 0.96\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.002940, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.003722, train_acc: 0.95\n",
      "alexnet1d, trial.111:\n",
      "Epoch 29, avg test_loss: 0.019330, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.003005, train_acc: 0.95\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.002346, train_acc: 0.95\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.001863, train_acc: 1.00\n",
      "alexnet1d, trial.111:\n",
      "Epoch 30, avg test_loss: 0.020706, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.002803, train_acc: 0.95\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.002265, train_acc: 0.95\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.002830, train_acc: 0.95\n",
      "alexnet1d, trial.111:\n",
      "Epoch 31, avg test_loss: 0.024629, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号35个\n",
      "错误信号35个\n",
      "信号正确并预测正确的概率为0.314\n",
      "信号错误并预测正确的概率为0.243\n",
      "总正确率为0.56\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012396, train_acc: 0.52\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012769, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012327, train_acc: 0.59\n",
      "alexnet1d, trial.112:\n",
      "Epoch 0, avg test_loss: 0.009602, test_acc: 0.63\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012212, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012461, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012312, train_acc: 0.55\n",
      "alexnet1d, trial.112:\n",
      "Epoch 1, avg test_loss: 0.009725, test_acc: 0.63\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012061, train_acc: 0.66\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012157, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011773, train_acc: 0.62\n",
      "alexnet1d, trial.112:\n",
      "Epoch 2, avg test_loss: 0.009551, test_acc: 0.63\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011880, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012661, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012689, train_acc: 0.50\n",
      "alexnet1d, trial.112:\n",
      "Epoch 3, avg test_loss: 0.009519, test_acc: 0.63\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012593, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012074, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012151, train_acc: 0.57\n",
      "alexnet1d, trial.112:\n",
      "Epoch 4, avg test_loss: 0.009696, test_acc: 0.63\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012338, train_acc: 0.48\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012428, train_acc: 0.52\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012530, train_acc: 0.55\n",
      "alexnet1d, trial.112:\n",
      "Epoch 5, avg test_loss: 0.009633, test_acc: 0.63\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012072, train_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012254, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011861, train_acc: 0.62\n",
      "alexnet1d, trial.112:\n",
      "Epoch 6, avg test_loss: 0.009587, test_acc: 0.63\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012178, train_acc: 0.52\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011763, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011971, train_acc: 0.50\n",
      "alexnet1d, trial.112:\n",
      "Epoch 7, avg test_loss: 0.009651, test_acc: 0.63\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011358, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012825, train_acc: 0.52\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012394, train_acc: 0.54\n",
      "alexnet1d, trial.112:\n",
      "Epoch 8, avg test_loss: 0.009971, test_acc: 0.56\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011661, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012111, train_acc: 0.55\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012284, train_acc: 0.57\n",
      "alexnet1d, trial.112:\n",
      "Epoch 9, avg test_loss: 0.009867, test_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011974, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012043, train_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012162, train_acc: 0.55\n",
      "alexnet1d, trial.112:\n",
      "Epoch 10, avg test_loss: 0.009428, test_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010880, train_acc: 0.77\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.012186, train_acc: 0.54\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.012990, train_acc: 0.45\n",
      "alexnet1d, trial.112:\n",
      "Epoch 11, avg test_loss: 0.009565, test_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010964, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.012858, train_acc: 0.55\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011867, train_acc: 0.57\n",
      "alexnet1d, trial.112:\n",
      "Epoch 12, avg test_loss: 0.009481, test_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011526, train_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011454, train_acc: 0.62\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.012283, train_acc: 0.52\n",
      "alexnet1d, trial.112:\n",
      "Epoch 13, avg test_loss: 0.009369, test_acc: 0.63\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.012023, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.012196, train_acc: 0.46\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011745, train_acc: 0.59\n",
      "alexnet1d, trial.112:\n",
      "Epoch 14, avg test_loss: 0.009565, test_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010768, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.011348, train_acc: 0.59\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011313, train_acc: 0.64\n",
      "alexnet1d, trial.112:\n",
      "Epoch 15, avg test_loss: 0.009184, test_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010923, train_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.011311, train_acc: 0.59\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.011792, train_acc: 0.61\n",
      "alexnet1d, trial.112:\n",
      "Epoch 16, avg test_loss: 0.009616, test_acc: 0.67\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.010519, train_acc: 0.66\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.011553, train_acc: 0.59\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.011242, train_acc: 0.64\n",
      "alexnet1d, trial.112:\n",
      "Epoch 17, avg test_loss: 0.009160, test_acc: 0.69\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.011040, train_acc: 0.62\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.012567, train_acc: 0.61\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009121, train_acc: 0.73\n",
      "alexnet1d, trial.112:\n",
      "Epoch 18, avg test_loss: 0.009373, test_acc: 0.67\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009163, train_acc: 0.75\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.010215, train_acc: 0.70\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.010963, train_acc: 0.71\n",
      "alexnet1d, trial.112:\n",
      "Epoch 19, avg test_loss: 0.009853, test_acc: 0.64\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.009969, train_acc: 0.75\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.011324, train_acc: 0.64\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009259, train_acc: 0.75\n",
      "alexnet1d, trial.112:\n",
      "Epoch 20, avg test_loss: 0.009886, test_acc: 0.59\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.009339, train_acc: 0.70\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.008560, train_acc: 0.73\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008011, train_acc: 0.73\n",
      "alexnet1d, trial.112:\n",
      "Epoch 21, avg test_loss: 0.010362, test_acc: 0.61\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006996, train_acc: 0.79\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.008812, train_acc: 0.77\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008701, train_acc: 0.82\n",
      "alexnet1d, trial.112:\n",
      "Epoch 22, avg test_loss: 0.010813, test_acc: 0.57\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.008246, train_acc: 0.79\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006961, train_acc: 0.82\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006081, train_acc: 0.88\n",
      "alexnet1d, trial.112:\n",
      "Epoch 23, avg test_loss: 0.012568, test_acc: 0.54\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006866, train_acc: 0.79\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.007238, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005630, train_acc: 0.84\n",
      "alexnet1d, trial.112:\n",
      "Epoch 24, avg test_loss: 0.011325, test_acc: 0.67\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.006417, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004089, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.007731, train_acc: 0.77\n",
      "alexnet1d, trial.112:\n",
      "Epoch 25, avg test_loss: 0.015933, test_acc: 0.53\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005532, train_acc: 0.86\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.003287, train_acc: 0.96\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.006850, train_acc: 0.80\n",
      "alexnet1d, trial.112:\n",
      "Epoch 26, avg test_loss: 0.014281, test_acc: 0.60\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004230, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.006174, train_acc: 0.88\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.006463, train_acc: 0.79\n",
      "alexnet1d, trial.112:\n",
      "Epoch 27, avg test_loss: 0.013734, test_acc: 0.66\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004678, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.006045, train_acc: 0.86\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.003049, train_acc: 0.95\n",
      "alexnet1d, trial.112:\n",
      "Epoch 28, avg test_loss: 0.017890, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.004452, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.002799, train_acc: 0.96\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.003366, train_acc: 0.93\n",
      "alexnet1d, trial.112:\n",
      "Epoch 29, avg test_loss: 0.018638, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.002324, train_acc: 0.96\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.003494, train_acc: 0.95\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.002304, train_acc: 0.95\n",
      "alexnet1d, trial.112:\n",
      "Epoch 30, avg test_loss: 0.021610, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.002298, train_acc: 0.96\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.002398, train_acc: 0.95\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.002907, train_acc: 0.95\n",
      "alexnet1d, trial.112:\n",
      "Epoch 31, avg test_loss: 0.023715, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号44个\n",
      "错误信号26个\n",
      "信号正确并预测正确的概率为0.371\n",
      "信号错误并预测正确的概率为0.257\n",
      "总正确率为0.63\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012396, train_acc: 0.48\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.013092, train_acc: 0.61\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012279, train_acc: 0.55\n",
      "alexnet1d, trial.113:\n",
      "Epoch 0, avg test_loss: 0.009979, test_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012235, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.011841, train_acc: 0.70\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012184, train_acc: 0.57\n",
      "alexnet1d, trial.113:\n",
      "Epoch 1, avg test_loss: 0.010790, test_acc: 0.50\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011131, train_acc: 0.66\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011859, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012416, train_acc: 0.54\n",
      "alexnet1d, trial.113:\n",
      "Epoch 2, avg test_loss: 0.010711, test_acc: 0.50\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012044, train_acc: 0.55\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011512, train_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011299, train_acc: 0.66\n",
      "alexnet1d, trial.113:\n",
      "Epoch 3, avg test_loss: 0.011585, test_acc: 0.50\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012191, train_acc: 0.52\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012674, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011756, train_acc: 0.52\n",
      "alexnet1d, trial.113:\n",
      "Epoch 4, avg test_loss: 0.013481, test_acc: 0.51\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.010808, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011470, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.010881, train_acc: 0.73\n",
      "alexnet1d, trial.113:\n",
      "Epoch 5, avg test_loss: 0.013811, test_acc: 0.50\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011614, train_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.010667, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.009733, train_acc: 0.71\n",
      "alexnet1d, trial.113:\n",
      "Epoch 6, avg test_loss: 0.015558, test_acc: 0.47\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.010691, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011121, train_acc: 0.70\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.010488, train_acc: 0.73\n",
      "alexnet1d, trial.113:\n",
      "Epoch 7, avg test_loss: 0.013908, test_acc: 0.51\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010749, train_acc: 0.70\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.010311, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.010316, train_acc: 0.61\n",
      "alexnet1d, trial.113:\n",
      "Epoch 8, avg test_loss: 0.019960, test_acc: 0.47\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.009708, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011566, train_acc: 0.73\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010207, train_acc: 0.71\n",
      "alexnet1d, trial.113:\n",
      "Epoch 9, avg test_loss: 0.012276, test_acc: 0.43\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.009897, train_acc: 0.70\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.009674, train_acc: 0.70\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010498, train_acc: 0.61\n",
      "alexnet1d, trial.113:\n",
      "Epoch 10, avg test_loss: 0.015703, test_acc: 0.46\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010255, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010217, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.009056, train_acc: 0.73\n",
      "alexnet1d, trial.113:\n",
      "Epoch 11, avg test_loss: 0.015641, test_acc: 0.46\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.008732, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.008823, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.009641, train_acc: 0.77\n",
      "alexnet1d, trial.113:\n",
      "Epoch 12, avg test_loss: 0.019645, test_acc: 0.47\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.008427, train_acc: 0.80\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009231, train_acc: 0.79\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010229, train_acc: 0.71\n",
      "alexnet1d, trial.113:\n",
      "Epoch 13, avg test_loss: 0.019907, test_acc: 0.41\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009741, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009726, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.007675, train_acc: 0.82\n",
      "alexnet1d, trial.113:\n",
      "Epoch 14, avg test_loss: 0.022076, test_acc: 0.53\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008493, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009437, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008839, train_acc: 0.75\n",
      "alexnet1d, trial.113:\n",
      "Epoch 15, avg test_loss: 0.018771, test_acc: 0.59\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.006989, train_acc: 0.82\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.006673, train_acc: 0.86\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008035, train_acc: 0.84\n",
      "alexnet1d, trial.113:\n",
      "Epoch 16, avg test_loss: 0.018686, test_acc: 0.44\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007838, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007297, train_acc: 0.86\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008596, train_acc: 0.80\n",
      "alexnet1d, trial.113:\n",
      "Epoch 17, avg test_loss: 0.031666, test_acc: 0.59\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.006373, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007374, train_acc: 0.86\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007812, train_acc: 0.82\n",
      "alexnet1d, trial.113:\n",
      "Epoch 18, avg test_loss: 0.029218, test_acc: 0.49\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.004931, train_acc: 0.91\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.006112, train_acc: 0.88\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009288, train_acc: 0.80\n",
      "alexnet1d, trial.113:\n",
      "Epoch 19, avg test_loss: 0.028739, test_acc: 0.54\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.005449, train_acc: 0.89\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008653, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006067, train_acc: 0.86\n",
      "alexnet1d, trial.113:\n",
      "Epoch 20, avg test_loss: 0.025401, test_acc: 0.59\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.004097, train_acc: 0.91\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.005254, train_acc: 0.89\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006216, train_acc: 0.86\n",
      "alexnet1d, trial.113:\n",
      "Epoch 21, avg test_loss: 0.034656, test_acc: 0.59\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.004109, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.003160, train_acc: 0.96\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007469, train_acc: 0.77\n",
      "alexnet1d, trial.113:\n",
      "Epoch 22, avg test_loss: 0.031897, test_acc: 0.57\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.003971, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.004715, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.002047, train_acc: 0.98\n",
      "alexnet1d, trial.113:\n",
      "Epoch 23, avg test_loss: 0.045143, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005072, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.002093, train_acc: 0.96\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.003806, train_acc: 0.89\n",
      "alexnet1d, trial.113:\n",
      "Epoch 24, avg test_loss: 0.034961, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.002862, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004147, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.002730, train_acc: 0.95\n",
      "alexnet1d, trial.113:\n",
      "Epoch 25, avg test_loss: 0.044233, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.000921, train_acc: 1.00\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.001890, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004278, train_acc: 0.91\n",
      "alexnet1d, trial.113:\n",
      "Epoch 26, avg test_loss: 0.051958, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号35个\n",
      "错误信号35个\n",
      "信号正确并预测正确的概率为0.329\n",
      "信号错误并预测正确的概率为0.2\n",
      "总正确率为0.53\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012364, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.014610, train_acc: 0.48\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012463, train_acc: 0.52\n",
      "alexnet1d, trial.114:\n",
      "Epoch 0, avg test_loss: 0.009807, test_acc: 0.71\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012239, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012212, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012574, train_acc: 0.45\n",
      "alexnet1d, trial.114:\n",
      "Epoch 1, avg test_loss: 0.009528, test_acc: 0.71\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012364, train_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012541, train_acc: 0.45\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012358, train_acc: 0.54\n",
      "alexnet1d, trial.114:\n",
      "Epoch 2, avg test_loss: 0.009581, test_acc: 0.71\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012248, train_acc: 0.55\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012295, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012110, train_acc: 0.59\n",
      "alexnet1d, trial.114:\n",
      "Epoch 3, avg test_loss: 0.009313, test_acc: 0.71\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012255, train_acc: 0.48\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012661, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012195, train_acc: 0.54\n",
      "alexnet1d, trial.114:\n",
      "Epoch 4, avg test_loss: 0.009395, test_acc: 0.69\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012489, train_acc: 0.50\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011968, train_acc: 0.71\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012282, train_acc: 0.52\n",
      "alexnet1d, trial.114:\n",
      "Epoch 5, avg test_loss: 0.009465, test_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012197, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012041, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011730, train_acc: 0.61\n",
      "alexnet1d, trial.114:\n",
      "Epoch 6, avg test_loss: 0.010016, test_acc: 0.60\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011481, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012254, train_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.013894, train_acc: 0.50\n",
      "alexnet1d, trial.114:\n",
      "Epoch 7, avg test_loss: 0.008873, test_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011865, train_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011126, train_acc: 0.71\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012379, train_acc: 0.52\n",
      "alexnet1d, trial.114:\n",
      "Epoch 8, avg test_loss: 0.009848, test_acc: 0.60\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012293, train_acc: 0.50\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011522, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011822, train_acc: 0.61\n",
      "alexnet1d, trial.114:\n",
      "Epoch 9, avg test_loss: 0.009309, test_acc: 0.69\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011477, train_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012657, train_acc: 0.55\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011491, train_acc: 0.66\n",
      "alexnet1d, trial.114:\n",
      "Epoch 10, avg test_loss: 0.009820, test_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.009811, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010917, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011018, train_acc: 0.62\n",
      "alexnet1d, trial.114:\n",
      "Epoch 11, avg test_loss: 0.009838, test_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010629, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010284, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011714, train_acc: 0.57\n",
      "alexnet1d, trial.114:\n",
      "Epoch 12, avg test_loss: 0.009231, test_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011030, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009876, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011718, train_acc: 0.71\n",
      "alexnet1d, trial.114:\n",
      "Epoch 13, avg test_loss: 0.009100, test_acc: 0.64\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010154, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010005, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010794, train_acc: 0.62\n",
      "alexnet1d, trial.114:\n",
      "Epoch 14, avg test_loss: 0.010185, test_acc: 0.53\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010740, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.011346, train_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009253, train_acc: 0.73\n",
      "alexnet1d, trial.114:\n",
      "Epoch 15, avg test_loss: 0.009611, test_acc: 0.64\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008753, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010558, train_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.007125, train_acc: 0.80\n",
      "alexnet1d, trial.114:\n",
      "Epoch 16, avg test_loss: 0.011629, test_acc: 0.53\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008645, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008814, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010414, train_acc: 0.71\n",
      "alexnet1d, trial.114:\n",
      "Epoch 17, avg test_loss: 0.009999, test_acc: 0.60\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008431, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008870, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008555, train_acc: 0.71\n",
      "alexnet1d, trial.114:\n",
      "Epoch 18, avg test_loss: 0.012337, test_acc: 0.53\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007850, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008747, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009344, train_acc: 0.77\n",
      "alexnet1d, trial.114:\n",
      "Epoch 19, avg test_loss: 0.011222, test_acc: 0.60\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007528, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007902, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006381, train_acc: 0.82\n",
      "alexnet1d, trial.114:\n",
      "Epoch 20, avg test_loss: 0.013003, test_acc: 0.54\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006991, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006918, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006848, train_acc: 0.80\n",
      "alexnet1d, trial.114:\n",
      "Epoch 21, avg test_loss: 0.011715, test_acc: 0.69\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006319, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.004247, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.005348, train_acc: 0.88\n",
      "alexnet1d, trial.114:\n",
      "Epoch 22, avg test_loss: 0.017518, test_acc: 0.47\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007498, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.003388, train_acc: 0.96\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.005165, train_acc: 0.88\n",
      "alexnet1d, trial.114:\n",
      "Epoch 23, avg test_loss: 0.015559, test_acc: 0.61\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.003994, train_acc: 0.95\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006112, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006675, train_acc: 0.84\n",
      "alexnet1d, trial.114:\n",
      "Epoch 24, avg test_loss: 0.013395, test_acc: 0.66\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.003684, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.007410, train_acc: 0.80\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005125, train_acc: 0.88\n",
      "alexnet1d, trial.114:\n",
      "Epoch 25, avg test_loss: 0.019544, test_acc: 0.56\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004739, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004255, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.007193, train_acc: 0.82\n",
      "alexnet1d, trial.114:\n",
      "Epoch 26, avg test_loss: 0.014198, test_acc: 0.61\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.005124, train_acc: 0.88\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.005535, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003205, train_acc: 0.95\n",
      "alexnet1d, trial.114:\n",
      "Epoch 27, avg test_loss: 0.019093, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004884, train_acc: 0.89\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004435, train_acc: 0.89\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.005131, train_acc: 0.88\n",
      "alexnet1d, trial.114:\n",
      "Epoch 28, avg test_loss: 0.017582, test_acc: 0.61\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.004141, train_acc: 0.89\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.003585, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.004617, train_acc: 0.89\n",
      "alexnet1d, trial.114:\n",
      "Epoch 29, avg test_loss: 0.026390, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.004306, train_acc: 0.88\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.001357, train_acc: 1.00\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.002797, train_acc: 0.93\n",
      "alexnet1d, trial.114:\n",
      "Epoch 30, avg test_loss: 0.020400, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.002694, train_acc: 0.96\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.001926, train_acc: 0.98\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.001591, train_acc: 0.96\n",
      "alexnet1d, trial.114:\n",
      "Epoch 31, avg test_loss: 0.029533, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号50个\n",
      "错误信号20个\n",
      "信号正确并预测正确的概率为0.371\n",
      "信号错误并预测正确的概率为0.186\n",
      "总正确率为0.56\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012318, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.011928, train_acc: 0.64\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012063, train_acc: 0.61\n",
      "alexnet1d, trial.115:\n",
      "Epoch 0, avg test_loss: 0.009685, test_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012396, train_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012134, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012326, train_acc: 0.57\n",
      "alexnet1d, trial.115:\n",
      "Epoch 1, avg test_loss: 0.009649, test_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012169, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011862, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.014444, train_acc: 0.43\n",
      "alexnet1d, trial.115:\n",
      "Epoch 2, avg test_loss: 0.009612, test_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011495, train_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012072, train_acc: 0.55\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012088, train_acc: 0.61\n",
      "alexnet1d, trial.115:\n",
      "Epoch 3, avg test_loss: 0.009796, test_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012084, train_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012365, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011801, train_acc: 0.64\n",
      "alexnet1d, trial.115:\n",
      "Epoch 4, avg test_loss: 0.009686, test_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011754, train_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011646, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011816, train_acc: 0.59\n",
      "alexnet1d, trial.115:\n",
      "Epoch 5, avg test_loss: 0.009648, test_acc: 0.60\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011007, train_acc: 0.70\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012818, train_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.010926, train_acc: 0.68\n",
      "alexnet1d, trial.115:\n",
      "Epoch 6, avg test_loss: 0.009839, test_acc: 0.53\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012160, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011476, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011731, train_acc: 0.52\n",
      "alexnet1d, trial.115:\n",
      "Epoch 7, avg test_loss: 0.009588, test_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011430, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.010784, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011735, train_acc: 0.66\n",
      "alexnet1d, trial.115:\n",
      "Epoch 8, avg test_loss: 0.009785, test_acc: 0.56\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011430, train_acc: 0.75\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010696, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011090, train_acc: 0.59\n",
      "alexnet1d, trial.115:\n",
      "Epoch 9, avg test_loss: 0.009278, test_acc: 0.63\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.012135, train_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012020, train_acc: 0.55\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011034, train_acc: 0.64\n",
      "alexnet1d, trial.115:\n",
      "Epoch 10, avg test_loss: 0.009684, test_acc: 0.60\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010795, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011410, train_acc: 0.54\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010242, train_acc: 0.71\n",
      "alexnet1d, trial.115:\n",
      "Epoch 11, avg test_loss: 0.009455, test_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009721, train_acc: 0.77\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.013317, train_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011619, train_acc: 0.62\n",
      "alexnet1d, trial.115:\n",
      "Epoch 12, avg test_loss: 0.009291, test_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009024, train_acc: 0.84\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.012165, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009703, train_acc: 0.66\n",
      "alexnet1d, trial.115:\n",
      "Epoch 13, avg test_loss: 0.009339, test_acc: 0.60\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009601, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.008990, train_acc: 0.80\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011564, train_acc: 0.66\n",
      "alexnet1d, trial.115:\n",
      "Epoch 14, avg test_loss: 0.009640, test_acc: 0.63\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010450, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010007, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010313, train_acc: 0.71\n",
      "alexnet1d, trial.115:\n",
      "Epoch 15, avg test_loss: 0.009952, test_acc: 0.63\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008781, train_acc: 0.82\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009872, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009616, train_acc: 0.77\n",
      "alexnet1d, trial.115:\n",
      "Epoch 16, avg test_loss: 0.009776, test_acc: 0.64\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007833, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008710, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008791, train_acc: 0.73\n",
      "alexnet1d, trial.115:\n",
      "Epoch 17, avg test_loss: 0.010401, test_acc: 0.59\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007975, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008633, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007573, train_acc: 0.84\n",
      "alexnet1d, trial.115:\n",
      "Epoch 18, avg test_loss: 0.012500, test_acc: 0.57\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007523, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.006066, train_acc: 0.89\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.010442, train_acc: 0.70\n",
      "alexnet1d, trial.115:\n",
      "Epoch 19, avg test_loss: 0.011621, test_acc: 0.70\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006255, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008539, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008556, train_acc: 0.79\n",
      "alexnet1d, trial.115:\n",
      "Epoch 20, avg test_loss: 0.010855, test_acc: 0.66\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.003883, train_acc: 0.95\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.005584, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.009145, train_acc: 0.75\n",
      "alexnet1d, trial.115:\n",
      "Epoch 21, avg test_loss: 0.012458, test_acc: 0.56\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006151, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.004471, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.005368, train_acc: 0.89\n",
      "alexnet1d, trial.115:\n",
      "Epoch 22, avg test_loss: 0.013690, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.003825, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005179, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.004853, train_acc: 0.91\n",
      "alexnet1d, trial.115:\n",
      "Epoch 23, avg test_loss: 0.016263, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.003372, train_acc: 0.95\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.002868, train_acc: 0.93\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.003860, train_acc: 0.95\n",
      "alexnet1d, trial.115:\n",
      "Epoch 24, avg test_loss: 0.015486, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.002729, train_acc: 0.96\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004577, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.003140, train_acc: 0.93\n",
      "alexnet1d, trial.115:\n",
      "Epoch 25, avg test_loss: 0.014730, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号41个\n",
      "错误信号29个\n",
      "信号正确并预测正确的概率为0.414\n",
      "信号错误并预测正确的概率为0.229\n",
      "总正确率为0.64\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012330, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012578, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012470, train_acc: 0.54\n",
      "alexnet1d, trial.116:\n",
      "Epoch 0, avg test_loss: 0.009453, test_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012669, train_acc: 0.52\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012075, train_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012357, train_acc: 0.57\n",
      "alexnet1d, trial.116:\n",
      "Epoch 1, avg test_loss: 0.009627, test_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011912, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012520, train_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012197, train_acc: 0.55\n",
      "alexnet1d, trial.116:\n",
      "Epoch 2, avg test_loss: 0.009642, test_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012541, train_acc: 0.46\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012149, train_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012027, train_acc: 0.70\n",
      "alexnet1d, trial.116:\n",
      "Epoch 3, avg test_loss: 0.009801, test_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012102, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012388, train_acc: 0.52\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012500, train_acc: 0.55\n",
      "alexnet1d, trial.116:\n",
      "Epoch 4, avg test_loss: 0.009818, test_acc: 0.60\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011806, train_acc: 0.68\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012806, train_acc: 0.52\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011448, train_acc: 0.61\n",
      "alexnet1d, trial.116:\n",
      "Epoch 5, avg test_loss: 0.010053, test_acc: 0.53\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011103, train_acc: 0.66\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011698, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.010977, train_acc: 0.62\n",
      "alexnet1d, trial.116:\n",
      "Epoch 6, avg test_loss: 0.010602, test_acc: 0.53\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011161, train_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012519, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011627, train_acc: 0.57\n",
      "alexnet1d, trial.116:\n",
      "Epoch 7, avg test_loss: 0.010079, test_acc: 0.56\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012030, train_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011253, train_acc: 0.68\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011962, train_acc: 0.59\n",
      "alexnet1d, trial.116:\n",
      "Epoch 8, avg test_loss: 0.010133, test_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011737, train_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011357, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.013363, train_acc: 0.50\n",
      "alexnet1d, trial.116:\n",
      "Epoch 9, avg test_loss: 0.010228, test_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011420, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010263, train_acc: 0.71\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011258, train_acc: 0.70\n",
      "alexnet1d, trial.116:\n",
      "Epoch 10, avg test_loss: 0.010222, test_acc: 0.56\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010901, train_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011118, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010912, train_acc: 0.71\n",
      "alexnet1d, trial.116:\n",
      "Epoch 11, avg test_loss: 0.010017, test_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010384, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010413, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.009747, train_acc: 0.77\n",
      "alexnet1d, trial.116:\n",
      "Epoch 12, avg test_loss: 0.012608, test_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009692, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010077, train_acc: 0.75\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010045, train_acc: 0.80\n",
      "alexnet1d, trial.116:\n",
      "Epoch 13, avg test_loss: 0.010964, test_acc: 0.60\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.008974, train_acc: 0.79\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010393, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.008510, train_acc: 0.79\n",
      "alexnet1d, trial.116:\n",
      "Epoch 14, avg test_loss: 0.015561, test_acc: 0.53\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009488, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008145, train_acc: 0.82\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008545, train_acc: 0.75\n",
      "alexnet1d, trial.116:\n",
      "Epoch 15, avg test_loss: 0.012763, test_acc: 0.60\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.007939, train_acc: 0.86\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008946, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008776, train_acc: 0.75\n",
      "alexnet1d, trial.116:\n",
      "Epoch 16, avg test_loss: 0.013378, test_acc: 0.59\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007062, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008224, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.005890, train_acc: 0.89\n",
      "alexnet1d, trial.116:\n",
      "Epoch 17, avg test_loss: 0.015119, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.006766, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008292, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008312, train_acc: 0.80\n",
      "alexnet1d, trial.116:\n",
      "Epoch 18, avg test_loss: 0.013654, test_acc: 0.54\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006798, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008728, train_acc: 0.75\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009501, train_acc: 0.75\n",
      "alexnet1d, trial.116:\n",
      "Epoch 19, avg test_loss: 0.013650, test_acc: 0.59\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006305, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.005192, train_acc: 0.89\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007585, train_acc: 0.91\n",
      "alexnet1d, trial.116:\n",
      "Epoch 20, avg test_loss: 0.018072, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006946, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006145, train_acc: 0.93\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.010233, train_acc: 0.79\n",
      "alexnet1d, trial.116:\n",
      "Epoch 21, avg test_loss: 0.015710, test_acc: 0.60\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005198, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005510, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.004829, train_acc: 0.89\n",
      "alexnet1d, trial.116:\n",
      "Epoch 22, avg test_loss: 0.018208, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005986, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.008239, train_acc: 0.80\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.004355, train_acc: 0.91\n",
      "alexnet1d, trial.116:\n",
      "Epoch 23, avg test_loss: 0.017270, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号43个\n",
      "错误信号27个\n",
      "信号正确并预测正确的概率为0.443\n",
      "信号错误并预测正确的概率为0.114\n",
      "总正确率为0.56\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012332, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012229, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012880, train_acc: 0.52\n",
      "alexnet1d, trial.117:\n",
      "Epoch 0, avg test_loss: 0.009764, test_acc: 0.56\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012197, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012175, train_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012224, train_acc: 0.59\n",
      "alexnet1d, trial.117:\n",
      "Epoch 1, avg test_loss: 0.009775, test_acc: 0.56\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011733, train_acc: 0.66\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.013164, train_acc: 0.48\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012861, train_acc: 0.52\n",
      "alexnet1d, trial.117:\n",
      "Epoch 2, avg test_loss: 0.009772, test_acc: 0.56\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011657, train_acc: 0.68\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012679, train_acc: 0.46\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012306, train_acc: 0.54\n",
      "alexnet1d, trial.117:\n",
      "Epoch 3, avg test_loss: 0.009789, test_acc: 0.56\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012038, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012651, train_acc: 0.50\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012052, train_acc: 0.61\n",
      "alexnet1d, trial.117:\n",
      "Epoch 4, avg test_loss: 0.009720, test_acc: 0.56\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012317, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012019, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012255, train_acc: 0.57\n",
      "alexnet1d, trial.117:\n",
      "Epoch 5, avg test_loss: 0.009703, test_acc: 0.56\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011787, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012373, train_acc: 0.52\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012103, train_acc: 0.59\n",
      "alexnet1d, trial.117:\n",
      "Epoch 6, avg test_loss: 0.009562, test_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012634, train_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011933, train_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011595, train_acc: 0.68\n",
      "alexnet1d, trial.117:\n",
      "Epoch 7, avg test_loss: 0.009633, test_acc: 0.56\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011668, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012217, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012556, train_acc: 0.64\n",
      "alexnet1d, trial.117:\n",
      "Epoch 8, avg test_loss: 0.009596, test_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011278, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011001, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012150, train_acc: 0.52\n",
      "alexnet1d, trial.117:\n",
      "Epoch 9, avg test_loss: 0.009519, test_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011365, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011454, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011258, train_acc: 0.66\n",
      "alexnet1d, trial.117:\n",
      "Epoch 10, avg test_loss: 0.009538, test_acc: 0.67\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010190, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010621, train_acc: 0.73\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.013070, train_acc: 0.54\n",
      "alexnet1d, trial.117:\n",
      "Epoch 11, avg test_loss: 0.009484, test_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010677, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.009152, train_acc: 0.79\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010507, train_acc: 0.68\n",
      "alexnet1d, trial.117:\n",
      "Epoch 12, avg test_loss: 0.009205, test_acc: 0.67\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.008854, train_acc: 0.80\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009274, train_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010314, train_acc: 0.70\n",
      "alexnet1d, trial.117:\n",
      "Epoch 13, avg test_loss: 0.009465, test_acc: 0.60\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.008836, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010157, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011356, train_acc: 0.62\n",
      "alexnet1d, trial.117:\n",
      "Epoch 14, avg test_loss: 0.009303, test_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010038, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009123, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009707, train_acc: 0.70\n",
      "alexnet1d, trial.117:\n",
      "Epoch 15, avg test_loss: 0.010067, test_acc: 0.63\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008348, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009101, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.011900, train_acc: 0.68\n",
      "alexnet1d, trial.117:\n",
      "Epoch 16, avg test_loss: 0.009608, test_acc: 0.66\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007818, train_acc: 0.84\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010423, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008588, train_acc: 0.80\n",
      "alexnet1d, trial.117:\n",
      "Epoch 17, avg test_loss: 0.010285, test_acc: 0.61\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008491, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009944, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008199, train_acc: 0.84\n",
      "alexnet1d, trial.117:\n",
      "Epoch 18, avg test_loss: 0.009769, test_acc: 0.64\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009092, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.010646, train_acc: 0.70\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.006762, train_acc: 0.84\n",
      "alexnet1d, trial.117:\n",
      "Epoch 19, avg test_loss: 0.010725, test_acc: 0.61\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008325, train_acc: 0.73\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008024, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007708, train_acc: 0.82\n",
      "alexnet1d, trial.117:\n",
      "Epoch 20, avg test_loss: 0.013464, test_acc: 0.60\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.005033, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.005757, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.010148, train_acc: 0.71\n",
      "alexnet1d, trial.117:\n",
      "Epoch 21, avg test_loss: 0.010433, test_acc: 0.66\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005954, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007137, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007143, train_acc: 0.88\n",
      "alexnet1d, trial.117:\n",
      "Epoch 22, avg test_loss: 0.011094, test_acc: 0.63\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005711, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007136, train_acc: 0.79\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.004776, train_acc: 0.91\n",
      "alexnet1d, trial.117:\n",
      "Epoch 23, avg test_loss: 0.015254, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005297, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004727, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005054, train_acc: 0.89\n",
      "alexnet1d, trial.117:\n",
      "Epoch 24, avg test_loss: 0.012830, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004151, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005570, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.002475, train_acc: 0.93\n",
      "alexnet1d, trial.117:\n",
      "Epoch 25, avg test_loss: 0.018090, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004933, train_acc: 0.86\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004392, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.003919, train_acc: 0.93\n",
      "alexnet1d, trial.117:\n",
      "Epoch 26, avg test_loss: 0.015894, test_acc: 0.67\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号39个\n",
      "错误信号31个\n",
      "信号正确并预测正确的概率为0.343\n",
      "信号错误并预测正确的概率为0.329\n",
      "总正确率为0.67\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012378, train_acc: 0.45\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012498, train_acc: 0.50\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012082, train_acc: 0.57\n",
      "alexnet1d, trial.118:\n",
      "Epoch 0, avg test_loss: 0.009997, test_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012943, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.013131, train_acc: 0.45\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012177, train_acc: 0.55\n",
      "alexnet1d, trial.118:\n",
      "Epoch 1, avg test_loss: 0.009754, test_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011906, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012747, train_acc: 0.45\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011747, train_acc: 0.70\n",
      "alexnet1d, trial.118:\n",
      "Epoch 2, avg test_loss: 0.009765, test_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011946, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011715, train_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012750, train_acc: 0.52\n",
      "alexnet1d, trial.118:\n",
      "Epoch 3, avg test_loss: 0.009759, test_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011928, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011968, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012909, train_acc: 0.50\n",
      "alexnet1d, trial.118:\n",
      "Epoch 4, avg test_loss: 0.009692, test_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011582, train_acc: 0.68\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012613, train_acc: 0.48\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012055, train_acc: 0.57\n",
      "alexnet1d, trial.118:\n",
      "Epoch 5, avg test_loss: 0.009695, test_acc: 0.51\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011328, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012388, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012163, train_acc: 0.52\n",
      "alexnet1d, trial.118:\n",
      "Epoch 6, avg test_loss: 0.009435, test_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012143, train_acc: 0.50\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011767, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012521, train_acc: 0.57\n",
      "alexnet1d, trial.118:\n",
      "Epoch 7, avg test_loss: 0.009374, test_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011739, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011845, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012558, train_acc: 0.54\n",
      "alexnet1d, trial.118:\n",
      "Epoch 8, avg test_loss: 0.009729, test_acc: 0.56\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011819, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011622, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011539, train_acc: 0.66\n",
      "alexnet1d, trial.118:\n",
      "Epoch 9, avg test_loss: 0.009383, test_acc: 0.56\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011256, train_acc: 0.70\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011302, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011785, train_acc: 0.62\n",
      "alexnet1d, trial.118:\n",
      "Epoch 10, avg test_loss: 0.009423, test_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010210, train_acc: 0.73\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011365, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.012036, train_acc: 0.61\n",
      "alexnet1d, trial.118:\n",
      "Epoch 11, avg test_loss: 0.009175, test_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011204, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011226, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010553, train_acc: 0.66\n",
      "alexnet1d, trial.118:\n",
      "Epoch 12, avg test_loss: 0.010102, test_acc: 0.57\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010732, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009861, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011132, train_acc: 0.62\n",
      "alexnet1d, trial.118:\n",
      "Epoch 13, avg test_loss: 0.009461, test_acc: 0.57\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.011454, train_acc: 0.62\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010023, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011023, train_acc: 0.68\n",
      "alexnet1d, trial.118:\n",
      "Epoch 14, avg test_loss: 0.008909, test_acc: 0.67\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010320, train_acc: 0.68\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009228, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010264, train_acc: 0.68\n",
      "alexnet1d, trial.118:\n",
      "Epoch 15, avg test_loss: 0.009172, test_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009418, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008555, train_acc: 0.82\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010827, train_acc: 0.57\n",
      "alexnet1d, trial.118:\n",
      "Epoch 16, avg test_loss: 0.010387, test_acc: 0.57\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007950, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007144, train_acc: 0.88\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.011403, train_acc: 0.70\n",
      "alexnet1d, trial.118:\n",
      "Epoch 17, avg test_loss: 0.010211, test_acc: 0.66\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007971, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009862, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007589, train_acc: 0.79\n",
      "alexnet1d, trial.118:\n",
      "Epoch 18, avg test_loss: 0.011697, test_acc: 0.56\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007594, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007908, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.010387, train_acc: 0.70\n",
      "alexnet1d, trial.118:\n",
      "Epoch 19, avg test_loss: 0.009254, test_acc: 0.66\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007385, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008720, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007316, train_acc: 0.82\n",
      "alexnet1d, trial.118:\n",
      "Epoch 20, avg test_loss: 0.009442, test_acc: 0.70\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006519, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006107, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.005956, train_acc: 0.88\n",
      "alexnet1d, trial.118:\n",
      "Epoch 21, avg test_loss: 0.011457, test_acc: 0.69\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006301, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005512, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006172, train_acc: 0.86\n",
      "alexnet1d, trial.118:\n",
      "Epoch 22, avg test_loss: 0.012547, test_acc: 0.59\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006085, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.004576, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006320, train_acc: 0.84\n",
      "alexnet1d, trial.118:\n",
      "Epoch 23, avg test_loss: 0.013298, test_acc: 0.64\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004658, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004082, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.008294, train_acc: 0.84\n",
      "alexnet1d, trial.118:\n",
      "Epoch 24, avg test_loss: 0.015174, test_acc: 0.69\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004573, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.003844, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.003466, train_acc: 0.89\n",
      "alexnet1d, trial.118:\n",
      "Epoch 25, avg test_loss: 0.014745, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.002444, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004945, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.006562, train_acc: 0.88\n",
      "alexnet1d, trial.118:\n",
      "Epoch 26, avg test_loss: 0.019311, test_acc: 0.67\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.002882, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.002201, train_acc: 0.96\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.007676, train_acc: 0.95\n",
      "alexnet1d, trial.118:\n",
      "Epoch 27, avg test_loss: 0.017833, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.002791, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.003591, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.002756, train_acc: 0.95\n",
      "alexnet1d, trial.118:\n",
      "Epoch 28, avg test_loss: 0.017123, test_acc: 0.69\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.003326, train_acc: 0.91\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.002165, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.002080, train_acc: 0.96\n",
      "alexnet1d, trial.118:\n",
      "Epoch 29, avg test_loss: 0.016531, test_acc: 0.69\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号40个\n",
      "错误信号30个\n",
      "信号正确并预测正确的概率为0.443\n",
      "信号错误并预测正确的概率为0.243\n",
      "总正确率为0.69\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012391, train_acc: 0.46\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012278, train_acc: 0.61\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.013209, train_acc: 0.55\n",
      "alexnet1d, trial.119:\n",
      "Epoch 0, avg test_loss: 0.009730, test_acc: 0.60\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012056, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012578, train_acc: 0.32\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012506, train_acc: 0.48\n",
      "alexnet1d, trial.119:\n",
      "Epoch 1, avg test_loss: 0.009737, test_acc: 0.60\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012115, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012404, train_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011840, train_acc: 0.66\n",
      "alexnet1d, trial.119:\n",
      "Epoch 2, avg test_loss: 0.009617, test_acc: 0.60\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012533, train_acc: 0.50\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012041, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011970, train_acc: 0.61\n",
      "alexnet1d, trial.119:\n",
      "Epoch 3, avg test_loss: 0.009589, test_acc: 0.60\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012252, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012363, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012326, train_acc: 0.57\n",
      "alexnet1d, trial.119:\n",
      "Epoch 4, avg test_loss: 0.009580, test_acc: 0.60\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011555, train_acc: 0.68\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012585, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012129, train_acc: 0.59\n",
      "alexnet1d, trial.119:\n",
      "Epoch 5, avg test_loss: 0.009592, test_acc: 0.60\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012586, train_acc: 0.50\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012422, train_acc: 0.52\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011780, train_acc: 0.66\n",
      "alexnet1d, trial.119:\n",
      "Epoch 6, avg test_loss: 0.009559, test_acc: 0.60\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011700, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012947, train_acc: 0.45\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011874, train_acc: 0.64\n",
      "alexnet1d, trial.119:\n",
      "Epoch 7, avg test_loss: 0.009396, test_acc: 0.60\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012056, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011539, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011908, train_acc: 0.59\n",
      "alexnet1d, trial.119:\n",
      "Epoch 8, avg test_loss: 0.009290, test_acc: 0.60\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012212, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012006, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012340, train_acc: 0.52\n",
      "alexnet1d, trial.119:\n",
      "Epoch 9, avg test_loss: 0.009170, test_acc: 0.60\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011615, train_acc: 0.50\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012317, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012091, train_acc: 0.55\n",
      "alexnet1d, trial.119:\n",
      "Epoch 10, avg test_loss: 0.009086, test_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.012187, train_acc: 0.50\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011443, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010937, train_acc: 0.62\n",
      "alexnet1d, trial.119:\n",
      "Epoch 11, avg test_loss: 0.009018, test_acc: 0.69\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010745, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010776, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011235, train_acc: 0.70\n",
      "alexnet1d, trial.119:\n",
      "Epoch 12, avg test_loss: 0.008902, test_acc: 0.63\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010676, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011853, train_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010771, train_acc: 0.68\n",
      "alexnet1d, trial.119:\n",
      "Epoch 13, avg test_loss: 0.009041, test_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.011323, train_acc: 0.64\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011637, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010652, train_acc: 0.75\n",
      "alexnet1d, trial.119:\n",
      "Epoch 14, avg test_loss: 0.009237, test_acc: 0.60\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010075, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010104, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010127, train_acc: 0.70\n",
      "alexnet1d, trial.119:\n",
      "Epoch 15, avg test_loss: 0.008687, test_acc: 0.64\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.012047, train_acc: 0.68\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010321, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010190, train_acc: 0.75\n",
      "alexnet1d, trial.119:\n",
      "Epoch 16, avg test_loss: 0.009805, test_acc: 0.61\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.010836, train_acc: 0.70\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008652, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010311, train_acc: 0.66\n",
      "alexnet1d, trial.119:\n",
      "Epoch 17, avg test_loss: 0.008908, test_acc: 0.60\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009943, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.011094, train_acc: 0.68\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.010359, train_acc: 0.64\n",
      "alexnet1d, trial.119:\n",
      "Epoch 18, avg test_loss: 0.009150, test_acc: 0.61\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.010089, train_acc: 0.70\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.010009, train_acc: 0.66\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009267, train_acc: 0.79\n",
      "alexnet1d, trial.119:\n",
      "Epoch 19, avg test_loss: 0.009524, test_acc: 0.61\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008107, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008786, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009547, train_acc: 0.75\n",
      "alexnet1d, trial.119:\n",
      "Epoch 20, avg test_loss: 0.009916, test_acc: 0.56\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.009232, train_acc: 0.77\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.008299, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007698, train_acc: 0.82\n",
      "alexnet1d, trial.119:\n",
      "Epoch 21, avg test_loss: 0.010633, test_acc: 0.57\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.008485, train_acc: 0.73\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.009170, train_acc: 0.77\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006873, train_acc: 0.84\n",
      "alexnet1d, trial.119:\n",
      "Epoch 22, avg test_loss: 0.013243, test_acc: 0.60\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.010771, train_acc: 0.75\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006047, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.007199, train_acc: 0.86\n",
      "alexnet1d, trial.119:\n",
      "Epoch 23, avg test_loss: 0.010904, test_acc: 0.60\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.008300, train_acc: 0.80\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.007786, train_acc: 0.80\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.008342, train_acc: 0.84\n",
      "alexnet1d, trial.119:\n",
      "Epoch 24, avg test_loss: 0.011822, test_acc: 0.61\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005989, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006628, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006421, train_acc: 0.82\n",
      "alexnet1d, trial.119:\n",
      "Epoch 25, avg test_loss: 0.012175, test_acc: 0.56\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005761, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004023, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.006747, train_acc: 0.84\n",
      "alexnet1d, trial.119:\n",
      "Epoch 26, avg test_loss: 0.016388, test_acc: 0.61\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004682, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.003824, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003870, train_acc: 0.93\n",
      "alexnet1d, trial.119:\n",
      "Epoch 27, avg test_loss: 0.015548, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004665, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.005798, train_acc: 0.89\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.005434, train_acc: 0.88\n",
      "alexnet1d, trial.119:\n",
      "Epoch 28, avg test_loss: 0.019152, test_acc: 0.57\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.005031, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.002256, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.004661, train_acc: 0.88\n",
      "alexnet1d, trial.119:\n",
      "Epoch 29, avg test_loss: 0.019912, test_acc: 0.54\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.006388, train_acc: 0.88\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.001986, train_acc: 0.98\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.004693, train_acc: 0.93\n",
      "alexnet1d, trial.119:\n",
      "Epoch 30, avg test_loss: 0.018853, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.004694, train_acc: 0.88\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.005400, train_acc: 0.88\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.005398, train_acc: 0.91\n",
      "alexnet1d, trial.119:\n",
      "Epoch 31, avg test_loss: 0.019976, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.003298, train_acc: 0.91\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.003388, train_acc: 0.95\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.002813, train_acc: 0.95\n",
      "alexnet1d, trial.119:\n",
      "Epoch 32, avg test_loss: 0.020073, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号42个\n",
      "错误信号28个\n",
      "信号正确并预测正确的概率为0.343\n",
      "信号错误并预测正确的概率为0.2\n",
      "总正确率为0.54\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012390, train_acc: 0.48\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012318, train_acc: 0.61\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.011723, train_acc: 0.61\n",
      "alexnet1d, trial.120:\n",
      "Epoch 0, avg test_loss: 0.010333, test_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011107, train_acc: 0.70\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012744, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012310, train_acc: 0.50\n",
      "alexnet1d, trial.120:\n",
      "Epoch 1, avg test_loss: 0.009873, test_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012375, train_acc: 0.50\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012532, train_acc: 0.46\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012404, train_acc: 0.55\n",
      "alexnet1d, trial.120:\n",
      "Epoch 2, avg test_loss: 0.009852, test_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012455, train_acc: 0.46\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012204, train_acc: 0.62\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012041, train_acc: 0.64\n",
      "alexnet1d, trial.120:\n",
      "Epoch 3, avg test_loss: 0.009759, test_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012068, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011713, train_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012711, train_acc: 0.55\n",
      "alexnet1d, trial.120:\n",
      "Epoch 4, avg test_loss: 0.009786, test_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012536, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012139, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012075, train_acc: 0.61\n",
      "alexnet1d, trial.120:\n",
      "Epoch 5, avg test_loss: 0.009738, test_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012296, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011635, train_acc: 0.68\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012105, train_acc: 0.54\n",
      "alexnet1d, trial.120:\n",
      "Epoch 6, avg test_loss: 0.009745, test_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012271, train_acc: 0.55\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011871, train_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012134, train_acc: 0.57\n",
      "alexnet1d, trial.120:\n",
      "Epoch 7, avg test_loss: 0.009749, test_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011596, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012588, train_acc: 0.55\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012387, train_acc: 0.52\n",
      "alexnet1d, trial.120:\n",
      "Epoch 8, avg test_loss: 0.009687, test_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011691, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012019, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012263, train_acc: 0.54\n",
      "alexnet1d, trial.120:\n",
      "Epoch 9, avg test_loss: 0.009666, test_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011744, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011582, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012509, train_acc: 0.54\n",
      "alexnet1d, trial.120:\n",
      "Epoch 10, avg test_loss: 0.009651, test_acc: 0.56\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011954, train_acc: 0.54\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011586, train_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.012208, train_acc: 0.62\n",
      "alexnet1d, trial.120:\n",
      "Epoch 11, avg test_loss: 0.009430, test_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011684, train_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011348, train_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.012411, train_acc: 0.54\n",
      "alexnet1d, trial.120:\n",
      "Epoch 12, avg test_loss: 0.009329, test_acc: 0.63\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011125, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010837, train_acc: 0.75\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010758, train_acc: 0.70\n",
      "alexnet1d, trial.120:\n",
      "Epoch 13, avg test_loss: 0.009657, test_acc: 0.60\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010127, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.012521, train_acc: 0.64\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011926, train_acc: 0.64\n",
      "alexnet1d, trial.120:\n",
      "Epoch 14, avg test_loss: 0.009378, test_acc: 0.61\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.012048, train_acc: 0.55\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.012023, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011564, train_acc: 0.64\n",
      "alexnet1d, trial.120:\n",
      "Epoch 15, avg test_loss: 0.009226, test_acc: 0.63\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.011525, train_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010059, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.011555, train_acc: 0.64\n",
      "alexnet1d, trial.120:\n",
      "Epoch 16, avg test_loss: 0.009729, test_acc: 0.56\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009264, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.011267, train_acc: 0.66\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010868, train_acc: 0.70\n",
      "alexnet1d, trial.120:\n",
      "Epoch 17, avg test_loss: 0.009561, test_acc: 0.63\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.010449, train_acc: 0.66\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009577, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008882, train_acc: 0.73\n",
      "alexnet1d, trial.120:\n",
      "Epoch 18, avg test_loss: 0.010958, test_acc: 0.50\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009664, train_acc: 0.75\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009966, train_acc: 0.62\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007614, train_acc: 0.88\n",
      "alexnet1d, trial.120:\n",
      "Epoch 19, avg test_loss: 0.009381, test_acc: 0.66\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008468, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008972, train_acc: 0.73\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.010086, train_acc: 0.73\n",
      "alexnet1d, trial.120:\n",
      "Epoch 20, avg test_loss: 0.010774, test_acc: 0.54\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007437, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.009437, train_acc: 0.77\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.010100, train_acc: 0.68\n",
      "alexnet1d, trial.120:\n",
      "Epoch 21, avg test_loss: 0.010083, test_acc: 0.63\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.009821, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006212, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.009253, train_acc: 0.75\n",
      "alexnet1d, trial.120:\n",
      "Epoch 22, avg test_loss: 0.013285, test_acc: 0.54\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.008553, train_acc: 0.80\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007590, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.007922, train_acc: 0.77\n",
      "alexnet1d, trial.120:\n",
      "Epoch 23, avg test_loss: 0.010513, test_acc: 0.66\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.007510, train_acc: 0.80\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.008585, train_acc: 0.82\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.008342, train_acc: 0.79\n",
      "alexnet1d, trial.120:\n",
      "Epoch 24, avg test_loss: 0.011885, test_acc: 0.61\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.006266, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.009038, train_acc: 0.80\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006350, train_acc: 0.86\n",
      "alexnet1d, trial.120:\n",
      "Epoch 25, avg test_loss: 0.010896, test_acc: 0.60\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.006779, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.007058, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.006599, train_acc: 0.82\n",
      "alexnet1d, trial.120:\n",
      "Epoch 26, avg test_loss: 0.012781, test_acc: 0.60\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004810, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.005198, train_acc: 0.86\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004956, train_acc: 0.88\n",
      "alexnet1d, trial.120:\n",
      "Epoch 27, avg test_loss: 0.013003, test_acc: 0.60\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003786, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.007884, train_acc: 0.84\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.005243, train_acc: 0.88\n",
      "alexnet1d, trial.120:\n",
      "Epoch 28, avg test_loss: 0.014851, test_acc: 0.60\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.003921, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.004531, train_acc: 0.86\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.002556, train_acc: 0.98\n",
      "alexnet1d, trial.120:\n",
      "Epoch 29, avg test_loss: 0.015542, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.004486, train_acc: 0.93\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.003400, train_acc: 0.95\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.003175, train_acc: 0.95\n",
      "alexnet1d, trial.120:\n",
      "Epoch 30, avg test_loss: 0.018968, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.001176, train_acc: 1.00\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.005419, train_acc: 0.88\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.003556, train_acc: 0.93\n",
      "alexnet1d, trial.120:\n",
      "Epoch 31, avg test_loss: 0.016935, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.003176, train_acc: 0.96\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.003595, train_acc: 0.93\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.003223, train_acc: 0.95\n",
      "alexnet1d, trial.120:\n",
      "Epoch 32, avg test_loss: 0.019739, test_acc: 0.67\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号40个\n",
      "错误信号30个\n",
      "信号正确并预测正确的概率为0.371\n",
      "信号错误并预测正确的概率为0.3\n",
      "总正确率为0.67\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012389, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012434, train_acc: 0.43\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012171, train_acc: 0.59\n",
      "alexnet1d, trial.121:\n",
      "Epoch 0, avg test_loss: 0.009194, test_acc: 0.66\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011374, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.013268, train_acc: 0.41\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012277, train_acc: 0.59\n",
      "alexnet1d, trial.121:\n",
      "Epoch 1, avg test_loss: 0.009760, test_acc: 0.66\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012578, train_acc: 0.41\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012210, train_acc: 0.70\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012233, train_acc: 0.61\n",
      "alexnet1d, trial.121:\n",
      "Epoch 2, avg test_loss: 0.009713, test_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012168, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012474, train_acc: 0.48\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012201, train_acc: 0.57\n",
      "alexnet1d, trial.121:\n",
      "Epoch 3, avg test_loss: 0.009448, test_acc: 0.66\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012917, train_acc: 0.39\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012061, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012409, train_acc: 0.54\n",
      "alexnet1d, trial.121:\n",
      "Epoch 4, avg test_loss: 0.009383, test_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012550, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012159, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012645, train_acc: 0.50\n",
      "alexnet1d, trial.121:\n",
      "Epoch 5, avg test_loss: 0.009389, test_acc: 0.66\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011830, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012226, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012296, train_acc: 0.54\n",
      "alexnet1d, trial.121:\n",
      "Epoch 6, avg test_loss: 0.009523, test_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012381, train_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011840, train_acc: 0.68\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012431, train_acc: 0.46\n",
      "alexnet1d, trial.121:\n",
      "Epoch 7, avg test_loss: 0.009478, test_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011821, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012208, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.013156, train_acc: 0.41\n",
      "alexnet1d, trial.121:\n",
      "Epoch 8, avg test_loss: 0.009199, test_acc: 0.67\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012982, train_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012445, train_acc: 0.52\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012709, train_acc: 0.45\n",
      "alexnet1d, trial.121:\n",
      "Epoch 9, avg test_loss: 0.009633, test_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.012218, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012082, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012011, train_acc: 0.57\n",
      "alexnet1d, trial.121:\n",
      "Epoch 10, avg test_loss: 0.009226, test_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.012578, train_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010832, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011289, train_acc: 0.68\n",
      "alexnet1d, trial.121:\n",
      "Epoch 11, avg test_loss: 0.009323, test_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.012006, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011734, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011477, train_acc: 0.59\n",
      "alexnet1d, trial.121:\n",
      "Epoch 12, avg test_loss: 0.009863, test_acc: 0.57\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.012020, train_acc: 0.62\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009533, train_acc: 0.80\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011657, train_acc: 0.61\n",
      "alexnet1d, trial.121:\n",
      "Epoch 13, avg test_loss: 0.009513, test_acc: 0.60\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010834, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010881, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010879, train_acc: 0.70\n",
      "alexnet1d, trial.121:\n",
      "Epoch 14, avg test_loss: 0.010248, test_acc: 0.57\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010368, train_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010072, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010368, train_acc: 0.71\n",
      "alexnet1d, trial.121:\n",
      "Epoch 15, avg test_loss: 0.009144, test_acc: 0.64\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010784, train_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008933, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.013101, train_acc: 0.59\n",
      "alexnet1d, trial.121:\n",
      "Epoch 16, avg test_loss: 0.010113, test_acc: 0.56\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008529, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009946, train_acc: 0.66\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009461, train_acc: 0.75\n",
      "alexnet1d, trial.121:\n",
      "Epoch 17, avg test_loss: 0.009463, test_acc: 0.61\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008155, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009591, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.010999, train_acc: 0.71\n",
      "alexnet1d, trial.121:\n",
      "Epoch 18, avg test_loss: 0.011569, test_acc: 0.57\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009412, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007941, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.010378, train_acc: 0.71\n",
      "alexnet1d, trial.121:\n",
      "Epoch 19, avg test_loss: 0.010171, test_acc: 0.66\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008167, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007781, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009496, train_acc: 0.70\n",
      "alexnet1d, trial.121:\n",
      "Epoch 20, avg test_loss: 0.011778, test_acc: 0.60\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006641, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.008003, train_acc: 0.75\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008664, train_acc: 0.77\n",
      "alexnet1d, trial.121:\n",
      "Epoch 21, avg test_loss: 0.009673, test_acc: 0.64\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007711, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007821, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008020, train_acc: 0.79\n",
      "alexnet1d, trial.121:\n",
      "Epoch 22, avg test_loss: 0.013416, test_acc: 0.59\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006345, train_acc: 0.82\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006916, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006149, train_acc: 0.86\n",
      "alexnet1d, trial.121:\n",
      "Epoch 23, avg test_loss: 0.012011, test_acc: 0.63\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006030, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004637, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006667, train_acc: 0.84\n",
      "alexnet1d, trial.121:\n",
      "Epoch 24, avg test_loss: 0.015892, test_acc: 0.67\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005995, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004696, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.008629, train_acc: 0.80\n",
      "alexnet1d, trial.121:\n",
      "Epoch 25, avg test_loss: 0.014720, test_acc: 0.63\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.006292, train_acc: 0.82\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.008785, train_acc: 0.73\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.005063, train_acc: 0.86\n",
      "alexnet1d, trial.121:\n",
      "Epoch 26, avg test_loss: 0.015734, test_acc: 0.61\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.005061, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.006032, train_acc: 0.86\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003862, train_acc: 0.95\n",
      "alexnet1d, trial.121:\n",
      "Epoch 27, avg test_loss: 0.015889, test_acc: 0.66\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.005689, train_acc: 0.86\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004218, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.003864, train_acc: 0.93\n",
      "alexnet1d, trial.121:\n",
      "Epoch 28, avg test_loss: 0.017426, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.003311, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.003435, train_acc: 0.91\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.005864, train_acc: 0.86\n",
      "alexnet1d, trial.121:\n",
      "Epoch 29, avg test_loss: 0.022406, test_acc: 0.60\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.003081, train_acc: 0.93\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.002595, train_acc: 0.95\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.002644, train_acc: 0.96\n",
      "alexnet1d, trial.121:\n",
      "Epoch 30, avg test_loss: 0.021936, test_acc: 0.67\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.002029, train_acc: 0.98\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.001825, train_acc: 0.95\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.003093, train_acc: 0.93\n",
      "alexnet1d, trial.121:\n",
      "Epoch 31, avg test_loss: 0.027206, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号46个\n",
      "错误信号24个\n",
      "信号正确并预测正确的概率为0.414\n",
      "信号错误并预测正确的概率为0.2\n",
      "总正确率为0.61\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012348, train_acc: 0.62\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.015192, train_acc: 0.62\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012209, train_acc: 0.61\n",
      "alexnet1d, trial.122:\n",
      "Epoch 0, avg test_loss: 0.009919, test_acc: 0.53\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012241, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012284, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012208, train_acc: 0.59\n",
      "alexnet1d, trial.122:\n",
      "Epoch 1, avg test_loss: 0.009900, test_acc: 0.51\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012333, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011502, train_acc: 0.70\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012292, train_acc: 0.55\n",
      "alexnet1d, trial.122:\n",
      "Epoch 2, avg test_loss: 0.010045, test_acc: 0.51\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011854, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012103, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011759, train_acc: 0.64\n",
      "alexnet1d, trial.122:\n",
      "Epoch 3, avg test_loss: 0.010001, test_acc: 0.51\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011986, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012039, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012607, train_acc: 0.50\n",
      "alexnet1d, trial.122:\n",
      "Epoch 4, avg test_loss: 0.010076, test_acc: 0.51\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012126, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011971, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012031, train_acc: 0.61\n",
      "alexnet1d, trial.122:\n",
      "Epoch 5, avg test_loss: 0.010044, test_acc: 0.51\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012350, train_acc: 0.52\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012039, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011745, train_acc: 0.61\n",
      "alexnet1d, trial.122:\n",
      "Epoch 6, avg test_loss: 0.010070, test_acc: 0.51\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011653, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012649, train_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012041, train_acc: 0.57\n",
      "alexnet1d, trial.122:\n",
      "Epoch 7, avg test_loss: 0.009994, test_acc: 0.51\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011858, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012363, train_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012636, train_acc: 0.52\n",
      "alexnet1d, trial.122:\n",
      "Epoch 8, avg test_loss: 0.009862, test_acc: 0.53\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012168, train_acc: 0.52\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010688, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011706, train_acc: 0.61\n",
      "alexnet1d, trial.122:\n",
      "Epoch 9, avg test_loss: 0.009753, test_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011666, train_acc: 0.55\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011909, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011072, train_acc: 0.71\n",
      "alexnet1d, trial.122:\n",
      "Epoch 10, avg test_loss: 0.009584, test_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011744, train_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010092, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011820, train_acc: 0.59\n",
      "alexnet1d, trial.122:\n",
      "Epoch 11, avg test_loss: 0.009763, test_acc: 0.56\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011790, train_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011268, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.013339, train_acc: 0.55\n",
      "alexnet1d, trial.122:\n",
      "Epoch 12, avg test_loss: 0.009837, test_acc: 0.57\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.012315, train_acc: 0.52\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010944, train_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011584, train_acc: 0.62\n",
      "alexnet1d, trial.122:\n",
      "Epoch 13, avg test_loss: 0.009474, test_acc: 0.64\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010377, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010904, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010132, train_acc: 0.73\n",
      "alexnet1d, trial.122:\n",
      "Epoch 14, avg test_loss: 0.009387, test_acc: 0.60\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010789, train_acc: 0.62\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010954, train_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009755, train_acc: 0.71\n",
      "alexnet1d, trial.122:\n",
      "Epoch 15, avg test_loss: 0.009852, test_acc: 0.60\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009221, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008408, train_acc: 0.84\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.011551, train_acc: 0.71\n",
      "alexnet1d, trial.122:\n",
      "Epoch 16, avg test_loss: 0.010056, test_acc: 0.64\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008332, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.012751, train_acc: 0.61\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009223, train_acc: 0.73\n",
      "alexnet1d, trial.122:\n",
      "Epoch 17, avg test_loss: 0.010015, test_acc: 0.61\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008245, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.010871, train_acc: 0.70\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007374, train_acc: 0.84\n",
      "alexnet1d, trial.122:\n",
      "Epoch 18, avg test_loss: 0.009530, test_acc: 0.69\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008094, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007627, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009445, train_acc: 0.71\n",
      "alexnet1d, trial.122:\n",
      "Epoch 19, avg test_loss: 0.010323, test_acc: 0.67\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007609, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008013, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.005887, train_acc: 0.86\n",
      "alexnet1d, trial.122:\n",
      "Epoch 20, avg test_loss: 0.011000, test_acc: 0.60\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.005819, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006495, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006706, train_acc: 0.88\n",
      "alexnet1d, trial.122:\n",
      "Epoch 21, avg test_loss: 0.011838, test_acc: 0.64\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005982, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006825, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.009826, train_acc: 0.79\n",
      "alexnet1d, trial.122:\n",
      "Epoch 22, avg test_loss: 0.014372, test_acc: 0.56\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007440, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.004375, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.004424, train_acc: 0.93\n",
      "alexnet1d, trial.122:\n",
      "Epoch 23, avg test_loss: 0.013599, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004950, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.009941, train_acc: 0.73\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.007839, train_acc: 0.80\n",
      "alexnet1d, trial.122:\n",
      "Epoch 24, avg test_loss: 0.012823, test_acc: 0.61\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005065, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004009, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006999, train_acc: 0.89\n",
      "alexnet1d, trial.122:\n",
      "Epoch 25, avg test_loss: 0.014179, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005771, train_acc: 0.84\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004887, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.006181, train_acc: 0.86\n",
      "alexnet1d, trial.122:\n",
      "Epoch 26, avg test_loss: 0.014076, test_acc: 0.63\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.002494, train_acc: 0.96\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.005954, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003247, train_acc: 0.91\n",
      "alexnet1d, trial.122:\n",
      "Epoch 27, avg test_loss: 0.015684, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.002019, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.001175, train_acc: 1.00\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.005509, train_acc: 0.91\n",
      "alexnet1d, trial.122:\n",
      "Epoch 28, avg test_loss: 0.016492, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号36个\n",
      "错误信号34个\n",
      "信号正确并预测正确的概率为0.414\n",
      "信号错误并预测正确的概率为0.229\n",
      "总正确率为0.64\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012372, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.014909, train_acc: 0.48\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012161, train_acc: 0.59\n",
      "alexnet1d, trial.123:\n",
      "Epoch 0, avg test_loss: 0.009909, test_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012140, train_acc: 0.62\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012054, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012067, train_acc: 0.55\n",
      "alexnet1d, trial.123:\n",
      "Epoch 1, avg test_loss: 0.010416, test_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011317, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012852, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012378, train_acc: 0.57\n",
      "alexnet1d, trial.123:\n",
      "Epoch 2, avg test_loss: 0.010029, test_acc: 0.51\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012238, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012068, train_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012340, train_acc: 0.54\n",
      "alexnet1d, trial.123:\n",
      "Epoch 3, avg test_loss: 0.009844, test_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011986, train_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011996, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012641, train_acc: 0.48\n",
      "alexnet1d, trial.123:\n",
      "Epoch 4, avg test_loss: 0.009917, test_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011785, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012110, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012528, train_acc: 0.55\n",
      "alexnet1d, trial.123:\n",
      "Epoch 5, avg test_loss: 0.010190, test_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012386, train_acc: 0.46\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011823, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012396, train_acc: 0.55\n",
      "alexnet1d, trial.123:\n",
      "Epoch 6, avg test_loss: 0.010627, test_acc: 0.50\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011019, train_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011395, train_acc: 0.68\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012293, train_acc: 0.59\n",
      "alexnet1d, trial.123:\n",
      "Epoch 7, avg test_loss: 0.010693, test_acc: 0.53\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011290, train_acc: 0.68\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012101, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012285, train_acc: 0.57\n",
      "alexnet1d, trial.123:\n",
      "Epoch 8, avg test_loss: 0.011201, test_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010722, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012456, train_acc: 0.50\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011840, train_acc: 0.59\n",
      "alexnet1d, trial.123:\n",
      "Epoch 9, avg test_loss: 0.009909, test_acc: 0.60\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011295, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011653, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012809, train_acc: 0.52\n",
      "alexnet1d, trial.123:\n",
      "Epoch 10, avg test_loss: 0.012125, test_acc: 0.53\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010213, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.014492, train_acc: 0.45\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010866, train_acc: 0.73\n",
      "alexnet1d, trial.123:\n",
      "Epoch 11, avg test_loss: 0.010468, test_acc: 0.53\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011287, train_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010919, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011356, train_acc: 0.62\n",
      "alexnet1d, trial.123:\n",
      "Epoch 12, avg test_loss: 0.010036, test_acc: 0.56\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010347, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010419, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011450, train_acc: 0.61\n",
      "alexnet1d, trial.123:\n",
      "Epoch 13, avg test_loss: 0.011292, test_acc: 0.54\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010935, train_acc: 0.62\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011330, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010570, train_acc: 0.68\n",
      "alexnet1d, trial.123:\n",
      "Epoch 14, avg test_loss: 0.011415, test_acc: 0.56\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.012312, train_acc: 0.55\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009415, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009442, train_acc: 0.79\n",
      "alexnet1d, trial.123:\n",
      "Epoch 15, avg test_loss: 0.010853, test_acc: 0.61\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.011268, train_acc: 0.62\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009915, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.011043, train_acc: 0.59\n",
      "alexnet1d, trial.123:\n",
      "Epoch 16, avg test_loss: 0.011795, test_acc: 0.60\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009290, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010067, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.011418, train_acc: 0.70\n",
      "alexnet1d, trial.123:\n",
      "Epoch 17, avg test_loss: 0.012177, test_acc: 0.60\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008841, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008860, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.011022, train_acc: 0.59\n",
      "alexnet1d, trial.123:\n",
      "Epoch 18, avg test_loss: 0.011041, test_acc: 0.61\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009388, train_acc: 0.70\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009887, train_acc: 0.62\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.011660, train_acc: 0.64\n",
      "alexnet1d, trial.123:\n",
      "Epoch 19, avg test_loss: 0.012338, test_acc: 0.64\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007216, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.009482, train_acc: 0.77\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.012084, train_acc: 0.61\n",
      "alexnet1d, trial.123:\n",
      "Epoch 20, avg test_loss: 0.013479, test_acc: 0.56\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008126, train_acc: 0.77\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007420, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.009028, train_acc: 0.79\n",
      "alexnet1d, trial.123:\n",
      "Epoch 21, avg test_loss: 0.012828, test_acc: 0.57\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.009098, train_acc: 0.77\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007968, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006629, train_acc: 0.82\n",
      "alexnet1d, trial.123:\n",
      "Epoch 22, avg test_loss: 0.013973, test_acc: 0.60\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007605, train_acc: 0.79\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.008431, train_acc: 0.80\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.008074, train_acc: 0.79\n",
      "alexnet1d, trial.123:\n",
      "Epoch 23, avg test_loss: 0.014244, test_acc: 0.61\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006497, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.008963, train_acc: 0.77\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.007189, train_acc: 0.84\n",
      "alexnet1d, trial.123:\n",
      "Epoch 24, avg test_loss: 0.015255, test_acc: 0.59\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.009847, train_acc: 0.71\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.007045, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.007613, train_acc: 0.80\n",
      "alexnet1d, trial.123:\n",
      "Epoch 25, avg test_loss: 0.015693, test_acc: 0.57\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.007349, train_acc: 0.80\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.006101, train_acc: 0.86\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004312, train_acc: 0.95\n",
      "alexnet1d, trial.123:\n",
      "Epoch 26, avg test_loss: 0.016790, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.007492, train_acc: 0.82\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.003481, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.007128, train_acc: 0.82\n",
      "alexnet1d, trial.123:\n",
      "Epoch 27, avg test_loss: 0.019995, test_acc: 0.59\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.006449, train_acc: 0.82\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004856, train_acc: 0.89\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.005727, train_acc: 0.84\n",
      "alexnet1d, trial.123:\n",
      "Epoch 28, avg test_loss: 0.019621, test_acc: 0.56\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.004191, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.004755, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.004558, train_acc: 0.91\n",
      "alexnet1d, trial.123:\n",
      "Epoch 29, avg test_loss: 0.019111, test_acc: 0.66\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.003471, train_acc: 0.91\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.005062, train_acc: 0.89\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.003203, train_acc: 0.96\n",
      "alexnet1d, trial.123:\n",
      "Epoch 30, avg test_loss: 0.019676, test_acc: 0.66\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.003868, train_acc: 0.91\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.004602, train_acc: 0.88\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.007933, train_acc: 0.82\n",
      "alexnet1d, trial.123:\n",
      "Epoch 31, avg test_loss: 0.023037, test_acc: 0.63\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.003641, train_acc: 0.89\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.004800, train_acc: 0.86\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.001419, train_acc: 0.98\n",
      "alexnet1d, trial.123:\n",
      "Epoch 32, avg test_loss: 0.021494, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号38个\n",
      "错误信号32个\n",
      "信号正确并预测正确的概率为0.386\n",
      "信号错误并预测正确的概率为0.214\n",
      "总正确率为0.60\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012267, train_acc: 0.68\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012505, train_acc: 0.50\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012761, train_acc: 0.46\n",
      "alexnet1d, trial.124:\n",
      "Epoch 0, avg test_loss: 0.009611, test_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012370, train_acc: 0.52\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012232, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012000, train_acc: 0.64\n",
      "alexnet1d, trial.124:\n",
      "Epoch 1, avg test_loss: 0.009613, test_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012779, train_acc: 0.41\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011639, train_acc: 0.70\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012082, train_acc: 0.59\n",
      "alexnet1d, trial.124:\n",
      "Epoch 2, avg test_loss: 0.009584, test_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011800, train_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012814, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012382, train_acc: 0.50\n",
      "alexnet1d, trial.124:\n",
      "Epoch 3, avg test_loss: 0.009618, test_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012363, train_acc: 0.52\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012740, train_acc: 0.43\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012208, train_acc: 0.57\n",
      "alexnet1d, trial.124:\n",
      "Epoch 4, avg test_loss: 0.009628, test_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012306, train_acc: 0.48\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012098, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011925, train_acc: 0.59\n",
      "alexnet1d, trial.124:\n",
      "Epoch 5, avg test_loss: 0.009445, test_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012114, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012046, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011022, train_acc: 0.68\n",
      "alexnet1d, trial.124:\n",
      "Epoch 6, avg test_loss: 0.009437, test_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011426, train_acc: 0.70\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012198, train_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012206, train_acc: 0.50\n",
      "alexnet1d, trial.124:\n",
      "Epoch 7, avg test_loss: 0.009241, test_acc: 0.67\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011790, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011390, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011781, train_acc: 0.55\n",
      "alexnet1d, trial.124:\n",
      "Epoch 8, avg test_loss: 0.009422, test_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011254, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011776, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010992, train_acc: 0.61\n",
      "alexnet1d, trial.124:\n",
      "Epoch 9, avg test_loss: 0.009315, test_acc: 0.69\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010044, train_acc: 0.70\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010403, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011215, train_acc: 0.59\n",
      "alexnet1d, trial.124:\n",
      "Epoch 10, avg test_loss: 0.009597, test_acc: 0.69\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010867, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011461, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010808, train_acc: 0.61\n",
      "alexnet1d, trial.124:\n",
      "Epoch 11, avg test_loss: 0.009302, test_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011132, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010990, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010664, train_acc: 0.59\n",
      "alexnet1d, trial.124:\n",
      "Epoch 12, avg test_loss: 0.009896, test_acc: 0.67\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.008955, train_acc: 0.80\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.012026, train_acc: 0.55\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009764, train_acc: 0.71\n",
      "alexnet1d, trial.124:\n",
      "Epoch 13, avg test_loss: 0.009350, test_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009855, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009123, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010363, train_acc: 0.61\n",
      "alexnet1d, trial.124:\n",
      "Epoch 14, avg test_loss: 0.011030, test_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009504, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009650, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009326, train_acc: 0.68\n",
      "alexnet1d, trial.124:\n",
      "Epoch 15, avg test_loss: 0.010557, test_acc: 0.64\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009808, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010343, train_acc: 0.64\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008509, train_acc: 0.75\n",
      "alexnet1d, trial.124:\n",
      "Epoch 16, avg test_loss: 0.009911, test_acc: 0.71\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.010041, train_acc: 0.68\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008166, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009642, train_acc: 0.71\n",
      "alexnet1d, trial.124:\n",
      "Epoch 17, avg test_loss: 0.011912, test_acc: 0.66\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008369, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007049, train_acc: 0.86\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.010824, train_acc: 0.75\n",
      "alexnet1d, trial.124:\n",
      "Epoch 18, avg test_loss: 0.011554, test_acc: 0.67\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009906, train_acc: 0.71\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007681, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.010937, train_acc: 0.68\n",
      "alexnet1d, trial.124:\n",
      "Epoch 19, avg test_loss: 0.015449, test_acc: 0.59\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.005837, train_acc: 0.88\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008797, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008277, train_acc: 0.80\n",
      "alexnet1d, trial.124:\n",
      "Epoch 20, avg test_loss: 0.013154, test_acc: 0.59\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008396, train_acc: 0.77\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006059, train_acc: 0.89\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.009132, train_acc: 0.73\n",
      "alexnet1d, trial.124:\n",
      "Epoch 21, avg test_loss: 0.013154, test_acc: 0.61\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.008920, train_acc: 0.79\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007622, train_acc: 0.79\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007638, train_acc: 0.75\n",
      "alexnet1d, trial.124:\n",
      "Epoch 22, avg test_loss: 0.011880, test_acc: 0.67\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005845, train_acc: 0.95\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006905, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006214, train_acc: 0.82\n",
      "alexnet1d, trial.124:\n",
      "Epoch 23, avg test_loss: 0.015642, test_acc: 0.61\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.003757, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004367, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.007984, train_acc: 0.82\n",
      "alexnet1d, trial.124:\n",
      "Epoch 24, avg test_loss: 0.016860, test_acc: 0.61\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.003966, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004296, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004688, train_acc: 0.89\n",
      "alexnet1d, trial.124:\n",
      "Epoch 25, avg test_loss: 0.019738, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004749, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.007482, train_acc: 0.86\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.003328, train_acc: 0.89\n",
      "alexnet1d, trial.124:\n",
      "Epoch 26, avg test_loss: 0.021703, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.002532, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.005190, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004186, train_acc: 0.91\n",
      "alexnet1d, trial.124:\n",
      "Epoch 27, avg test_loss: 0.023127, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.002962, train_acc: 0.96\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.003199, train_acc: 0.89\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.001844, train_acc: 0.96\n",
      "alexnet1d, trial.124:\n",
      "Epoch 28, avg test_loss: 0.026853, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号43个\n",
      "错误信号27个\n",
      "信号正确并预测正确的概率为0.386\n",
      "信号错误并预测正确的概率为0.214\n",
      "总正确率为0.60\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012269, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012192, train_acc: 0.61\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.011676, train_acc: 0.68\n",
      "alexnet1d, trial.125:\n",
      "Epoch 0, avg test_loss: 0.009388, test_acc: 0.60\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012207, train_acc: 0.62\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.013758, train_acc: 0.48\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012242, train_acc: 0.54\n",
      "alexnet1d, trial.125:\n",
      "Epoch 1, avg test_loss: 0.009808, test_acc: 0.60\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012295, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012812, train_acc: 0.36\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011974, train_acc: 0.68\n",
      "alexnet1d, trial.125:\n",
      "Epoch 2, avg test_loss: 0.009809, test_acc: 0.60\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011972, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011442, train_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011738, train_acc: 0.61\n",
      "alexnet1d, trial.125:\n",
      "Epoch 3, avg test_loss: 0.009915, test_acc: 0.60\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011672, train_acc: 0.68\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011949, train_acc: 0.50\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012377, train_acc: 0.57\n",
      "alexnet1d, trial.125:\n",
      "Epoch 4, avg test_loss: 0.009942, test_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011842, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011728, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012070, train_acc: 0.59\n",
      "alexnet1d, trial.125:\n",
      "Epoch 5, avg test_loss: 0.009914, test_acc: 0.60\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011425, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012224, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011923, train_acc: 0.57\n",
      "alexnet1d, trial.125:\n",
      "Epoch 6, avg test_loss: 0.009731, test_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012068, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012306, train_acc: 0.52\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012217, train_acc: 0.64\n",
      "alexnet1d, trial.125:\n",
      "Epoch 7, avg test_loss: 0.010127, test_acc: 0.60\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011762, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011227, train_acc: 0.68\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011628, train_acc: 0.62\n",
      "alexnet1d, trial.125:\n",
      "Epoch 8, avg test_loss: 0.009866, test_acc: 0.60\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010411, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011385, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010931, train_acc: 0.71\n",
      "alexnet1d, trial.125:\n",
      "Epoch 9, avg test_loss: 0.009781, test_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010059, train_acc: 0.70\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010446, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010945, train_acc: 0.62\n",
      "alexnet1d, trial.125:\n",
      "Epoch 10, avg test_loss: 0.009586, test_acc: 0.53\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010953, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.009665, train_acc: 0.75\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011425, train_acc: 0.61\n",
      "alexnet1d, trial.125:\n",
      "Epoch 11, avg test_loss: 0.010794, test_acc: 0.56\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011803, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.009331, train_acc: 0.77\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010061, train_acc: 0.71\n",
      "alexnet1d, trial.125:\n",
      "Epoch 12, avg test_loss: 0.009737, test_acc: 0.57\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.008082, train_acc: 0.84\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.008782, train_acc: 0.86\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009215, train_acc: 0.75\n",
      "alexnet1d, trial.125:\n",
      "Epoch 13, avg test_loss: 0.010758, test_acc: 0.60\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.008658, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009276, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009785, train_acc: 0.73\n",
      "alexnet1d, trial.125:\n",
      "Epoch 14, avg test_loss: 0.010030, test_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010784, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.007993, train_acc: 0.82\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008644, train_acc: 0.79\n",
      "alexnet1d, trial.125:\n",
      "Epoch 15, avg test_loss: 0.009570, test_acc: 0.61\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008227, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009057, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009560, train_acc: 0.68\n",
      "alexnet1d, trial.125:\n",
      "Epoch 16, avg test_loss: 0.012473, test_acc: 0.57\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007956, train_acc: 0.71\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008140, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.006759, train_acc: 0.80\n",
      "alexnet1d, trial.125:\n",
      "Epoch 17, avg test_loss: 0.010693, test_acc: 0.60\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.005909, train_acc: 0.89\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.006502, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008007, train_acc: 0.82\n",
      "alexnet1d, trial.125:\n",
      "Epoch 18, avg test_loss: 0.013081, test_acc: 0.61\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.004724, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.005029, train_acc: 0.91\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007883, train_acc: 0.77\n",
      "alexnet1d, trial.125:\n",
      "Epoch 19, avg test_loss: 0.013890, test_acc: 0.59\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.005881, train_acc: 0.88\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.005816, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.005510, train_acc: 0.88\n",
      "alexnet1d, trial.125:\n",
      "Epoch 20, avg test_loss: 0.015139, test_acc: 0.63\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006743, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.005633, train_acc: 0.89\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006410, train_acc: 0.84\n",
      "alexnet1d, trial.125:\n",
      "Epoch 21, avg test_loss: 0.015021, test_acc: 0.61\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006361, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.003325, train_acc: 0.96\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007592, train_acc: 0.84\n",
      "alexnet1d, trial.125:\n",
      "Epoch 22, avg test_loss: 0.015680, test_acc: 0.69\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.002797, train_acc: 0.95\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.003268, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.004501, train_acc: 0.91\n",
      "alexnet1d, trial.125:\n",
      "Epoch 23, avg test_loss: 0.017722, test_acc: 0.67\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005661, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.008174, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.003709, train_acc: 0.93\n",
      "alexnet1d, trial.125:\n",
      "Epoch 24, avg test_loss: 0.019051, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004734, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.002965, train_acc: 0.96\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.003727, train_acc: 0.93\n",
      "alexnet1d, trial.125:\n",
      "Epoch 25, avg test_loss: 0.018297, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.002809, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.003414, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.003663, train_acc: 0.95\n",
      "alexnet1d, trial.125:\n",
      "Epoch 26, avg test_loss: 0.017720, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号42个\n",
      "错误信号28个\n",
      "信号正确并预测正确的概率为0.4\n",
      "信号错误并预测正确的概率为0.243\n",
      "总正确率为0.64\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012381, train_acc: 0.50\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.014049, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012409, train_acc: 0.57\n",
      "alexnet1d, trial.126:\n",
      "Epoch 0, avg test_loss: 0.009877, test_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012297, train_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012493, train_acc: 0.48\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012045, train_acc: 0.68\n",
      "alexnet1d, trial.126:\n",
      "Epoch 1, avg test_loss: 0.009859, test_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012294, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012371, train_acc: 0.52\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.010909, train_acc: 0.79\n",
      "alexnet1d, trial.126:\n",
      "Epoch 2, avg test_loss: 0.009953, test_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011966, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012243, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011810, train_acc: 0.61\n",
      "alexnet1d, trial.126:\n",
      "Epoch 3, avg test_loss: 0.010057, test_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011933, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012838, train_acc: 0.52\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012116, train_acc: 0.55\n",
      "alexnet1d, trial.126:\n",
      "Epoch 4, avg test_loss: 0.010078, test_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012767, train_acc: 0.50\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012097, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011848, train_acc: 0.59\n",
      "alexnet1d, trial.126:\n",
      "Epoch 5, avg test_loss: 0.010032, test_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011815, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012634, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011952, train_acc: 0.55\n",
      "alexnet1d, trial.126:\n",
      "Epoch 6, avg test_loss: 0.010040, test_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011776, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012784, train_acc: 0.45\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011704, train_acc: 0.62\n",
      "alexnet1d, trial.126:\n",
      "Epoch 7, avg test_loss: 0.009980, test_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012734, train_acc: 0.46\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011185, train_acc: 0.68\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.013220, train_acc: 0.55\n",
      "alexnet1d, trial.126:\n",
      "Epoch 8, avg test_loss: 0.010422, test_acc: 0.53\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011390, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011464, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011743, train_acc: 0.64\n",
      "alexnet1d, trial.126:\n",
      "Epoch 9, avg test_loss: 0.009687, test_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011250, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012716, train_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010982, train_acc: 0.66\n",
      "alexnet1d, trial.126:\n",
      "Epoch 10, avg test_loss: 0.009736, test_acc: 0.54\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011034, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.009660, train_acc: 0.73\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.012876, train_acc: 0.62\n",
      "alexnet1d, trial.126:\n",
      "Epoch 11, avg test_loss: 0.010403, test_acc: 0.51\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009966, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010715, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.016749, train_acc: 0.52\n",
      "alexnet1d, trial.126:\n",
      "Epoch 12, avg test_loss: 0.010589, test_acc: 0.49\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011290, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010625, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011852, train_acc: 0.57\n",
      "alexnet1d, trial.126:\n",
      "Epoch 13, avg test_loss: 0.010446, test_acc: 0.50\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009717, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011115, train_acc: 0.64\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011573, train_acc: 0.57\n",
      "alexnet1d, trial.126:\n",
      "Epoch 14, avg test_loss: 0.011050, test_acc: 0.46\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010307, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.012170, train_acc: 0.54\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010236, train_acc: 0.66\n",
      "alexnet1d, trial.126:\n",
      "Epoch 15, avg test_loss: 0.010423, test_acc: 0.49\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008833, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.011194, train_acc: 0.64\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010147, train_acc: 0.70\n",
      "alexnet1d, trial.126:\n",
      "Epoch 16, avg test_loss: 0.011714, test_acc: 0.47\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009861, train_acc: 0.68\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010322, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.011517, train_acc: 0.73\n",
      "alexnet1d, trial.126:\n",
      "Epoch 17, avg test_loss: 0.011654, test_acc: 0.50\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009530, train_acc: 0.70\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.011069, train_acc: 0.64\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008503, train_acc: 0.86\n",
      "alexnet1d, trial.126:\n",
      "Epoch 18, avg test_loss: 0.012251, test_acc: 0.49\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009164, train_acc: 0.75\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.012131, train_acc: 0.61\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008691, train_acc: 0.77\n",
      "alexnet1d, trial.126:\n",
      "Epoch 19, avg test_loss: 0.013052, test_acc: 0.47\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007462, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.010701, train_acc: 0.70\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.010576, train_acc: 0.68\n",
      "alexnet1d, trial.126:\n",
      "Epoch 20, avg test_loss: 0.011697, test_acc: 0.51\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007627, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.008840, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008187, train_acc: 0.79\n",
      "alexnet1d, trial.126:\n",
      "Epoch 21, avg test_loss: 0.015022, test_acc: 0.47\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006680, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007792, train_acc: 0.79\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006492, train_acc: 0.86\n",
      "alexnet1d, trial.126:\n",
      "Epoch 22, avg test_loss: 0.013172, test_acc: 0.53\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007388, train_acc: 0.77\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007138, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.007069, train_acc: 0.86\n",
      "alexnet1d, trial.126:\n",
      "Epoch 23, avg test_loss: 0.019140, test_acc: 0.46\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005605, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004895, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.007553, train_acc: 0.88\n",
      "alexnet1d, trial.126:\n",
      "Epoch 24, avg test_loss: 0.022255, test_acc: 0.49\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004312, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.003823, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004911, train_acc: 0.89\n",
      "alexnet1d, trial.126:\n",
      "Epoch 25, avg test_loss: 0.018252, test_acc: 0.43\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.007655, train_acc: 0.84\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004435, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004642, train_acc: 0.86\n",
      "alexnet1d, trial.126:\n",
      "Epoch 26, avg test_loss: 0.020633, test_acc: 0.47\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.005012, train_acc: 0.88\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004734, train_acc: 0.86\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003404, train_acc: 0.96\n",
      "alexnet1d, trial.126:\n",
      "Epoch 27, avg test_loss: 0.021642, test_acc: 0.44\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.005750, train_acc: 0.88\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004603, train_acc: 0.89\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.003581, train_acc: 0.93\n",
      "alexnet1d, trial.126:\n",
      "Epoch 28, avg test_loss: 0.023295, test_acc: 0.46\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.003368, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.002532, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.003939, train_acc: 0.91\n",
      "alexnet1d, trial.126:\n",
      "Epoch 29, avg test_loss: 0.026484, test_acc: 0.44\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号38个\n",
      "错误信号32个\n",
      "信号正确并预测正确的概率为0.286\n",
      "信号错误并预测正确的概率为0.157\n",
      "总正确率为0.44\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012464, train_acc: 0.45\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012175, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012861, train_acc: 0.48\n",
      "alexnet1d, trial.127:\n",
      "Epoch 0, avg test_loss: 0.009802, test_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011980, train_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.011962, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012375, train_acc: 0.61\n",
      "alexnet1d, trial.127:\n",
      "Epoch 1, avg test_loss: 0.009817, test_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012135, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012023, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012577, train_acc: 0.52\n",
      "alexnet1d, trial.127:\n",
      "Epoch 2, avg test_loss: 0.009755, test_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012470, train_acc: 0.48\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012129, train_acc: 0.62\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011873, train_acc: 0.57\n",
      "alexnet1d, trial.127:\n",
      "Epoch 3, avg test_loss: 0.009735, test_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011919, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011952, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012187, train_acc: 0.54\n",
      "alexnet1d, trial.127:\n",
      "Epoch 4, avg test_loss: 0.009757, test_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012281, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012193, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012215, train_acc: 0.57\n",
      "alexnet1d, trial.127:\n",
      "Epoch 5, avg test_loss: 0.009629, test_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011776, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011522, train_acc: 0.68\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011968, train_acc: 0.54\n",
      "alexnet1d, trial.127:\n",
      "Epoch 6, avg test_loss: 0.009663, test_acc: 0.60\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011583, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012329, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011863, train_acc: 0.61\n",
      "alexnet1d, trial.127:\n",
      "Epoch 7, avg test_loss: 0.009922, test_acc: 0.53\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012080, train_acc: 0.55\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.010936, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.010982, train_acc: 0.66\n",
      "alexnet1d, trial.127:\n",
      "Epoch 8, avg test_loss: 0.010461, test_acc: 0.53\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.009741, train_acc: 0.71\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010859, train_acc: 0.75\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.013039, train_acc: 0.52\n",
      "alexnet1d, trial.127:\n",
      "Epoch 9, avg test_loss: 0.010205, test_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010559, train_acc: 0.77\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011037, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011200, train_acc: 0.59\n",
      "alexnet1d, trial.127:\n",
      "Epoch 10, avg test_loss: 0.009983, test_acc: 0.50\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011050, train_acc: 0.75\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011731, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.009721, train_acc: 0.71\n",
      "alexnet1d, trial.127:\n",
      "Epoch 11, avg test_loss: 0.009869, test_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010033, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010169, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.007890, train_acc: 0.77\n",
      "alexnet1d, trial.127:\n",
      "Epoch 12, avg test_loss: 0.010667, test_acc: 0.54\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.008389, train_acc: 0.75\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.012298, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010270, train_acc: 0.68\n",
      "alexnet1d, trial.127:\n",
      "Epoch 13, avg test_loss: 0.010830, test_acc: 0.51\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.008849, train_acc: 0.82\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.013339, train_acc: 0.52\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009674, train_acc: 0.70\n",
      "alexnet1d, trial.127:\n",
      "Epoch 14, avg test_loss: 0.010457, test_acc: 0.57\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008012, train_acc: 0.80\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009432, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010040, train_acc: 0.75\n",
      "alexnet1d, trial.127:\n",
      "Epoch 15, avg test_loss: 0.010581, test_acc: 0.54\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008433, train_acc: 0.91\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.007928, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010863, train_acc: 0.70\n",
      "alexnet1d, trial.127:\n",
      "Epoch 16, avg test_loss: 0.011040, test_acc: 0.59\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.006973, train_acc: 0.88\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.011276, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008953, train_acc: 0.80\n",
      "alexnet1d, trial.127:\n",
      "Epoch 17, avg test_loss: 0.011962, test_acc: 0.49\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008985, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008185, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007542, train_acc: 0.84\n",
      "alexnet1d, trial.127:\n",
      "Epoch 18, avg test_loss: 0.011539, test_acc: 0.51\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008550, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009274, train_acc: 0.71\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007080, train_acc: 0.84\n",
      "alexnet1d, trial.127:\n",
      "Epoch 19, avg test_loss: 0.012643, test_acc: 0.53\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007140, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.005867, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007577, train_acc: 0.82\n",
      "alexnet1d, trial.127:\n",
      "Epoch 20, avg test_loss: 0.014782, test_acc: 0.59\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006201, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006240, train_acc: 0.91\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008286, train_acc: 0.75\n",
      "alexnet1d, trial.127:\n",
      "Epoch 21, avg test_loss: 0.015391, test_acc: 0.53\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006394, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006366, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.005327, train_acc: 0.89\n",
      "alexnet1d, trial.127:\n",
      "Epoch 22, avg test_loss: 0.015081, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006228, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006408, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.005316, train_acc: 0.84\n",
      "alexnet1d, trial.127:\n",
      "Epoch 23, avg test_loss: 0.017000, test_acc: 0.51\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004817, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006164, train_acc: 0.93\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.004632, train_acc: 0.91\n",
      "alexnet1d, trial.127:\n",
      "Epoch 24, avg test_loss: 0.014641, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.003334, train_acc: 0.96\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006910, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.003310, train_acc: 0.95\n",
      "alexnet1d, trial.127:\n",
      "Epoch 25, avg test_loss: 0.014895, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.003158, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004414, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.007428, train_acc: 0.80\n",
      "alexnet1d, trial.127:\n",
      "Epoch 26, avg test_loss: 0.019266, test_acc: 0.54\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.002030, train_acc: 0.96\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.001873, train_acc: 1.00\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004253, train_acc: 0.91\n",
      "alexnet1d, trial.127:\n",
      "Epoch 27, avg test_loss: 0.017130, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号40个\n",
      "错误信号30个\n",
      "信号正确并预测正确的概率为0.414\n",
      "信号错误并预测正确的概率为0.214\n",
      "总正确率为0.63\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012344, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012305, train_acc: 0.61\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012113, train_acc: 0.61\n",
      "alexnet1d, trial.128:\n",
      "Epoch 0, avg test_loss: 0.009704, test_acc: 0.63\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012607, train_acc: 0.48\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012076, train_acc: 0.62\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012323, train_acc: 0.55\n",
      "alexnet1d, trial.128:\n",
      "Epoch 1, avg test_loss: 0.009572, test_acc: 0.63\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011748, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012501, train_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012045, train_acc: 0.61\n",
      "alexnet1d, trial.128:\n",
      "Epoch 2, avg test_loss: 0.009663, test_acc: 0.63\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012397, train_acc: 0.55\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012315, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012491, train_acc: 0.50\n",
      "alexnet1d, trial.128:\n",
      "Epoch 3, avg test_loss: 0.009538, test_acc: 0.63\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011779, train_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012006, train_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012521, train_acc: 0.50\n",
      "alexnet1d, trial.128:\n",
      "Epoch 4, avg test_loss: 0.009505, test_acc: 0.63\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012764, train_acc: 0.50\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012089, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011849, train_acc: 0.68\n",
      "alexnet1d, trial.128:\n",
      "Epoch 5, avg test_loss: 0.009703, test_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012848, train_acc: 0.45\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012368, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011628, train_acc: 0.62\n",
      "alexnet1d, trial.128:\n",
      "Epoch 6, avg test_loss: 0.009645, test_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011286, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011929, train_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012088, train_acc: 0.55\n",
      "alexnet1d, trial.128:\n",
      "Epoch 7, avg test_loss: 0.009747, test_acc: 0.63\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012018, train_acc: 0.48\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011267, train_acc: 0.68\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012847, train_acc: 0.59\n",
      "alexnet1d, trial.128:\n",
      "Epoch 8, avg test_loss: 0.009243, test_acc: 0.67\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011119, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010909, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011924, train_acc: 0.64\n",
      "alexnet1d, trial.128:\n",
      "Epoch 9, avg test_loss: 0.009330, test_acc: 0.71\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011116, train_acc: 0.70\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011380, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012964, train_acc: 0.62\n",
      "alexnet1d, trial.128:\n",
      "Epoch 10, avg test_loss: 0.009088, test_acc: 0.67\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010642, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011260, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011403, train_acc: 0.61\n",
      "alexnet1d, trial.128:\n",
      "Epoch 11, avg test_loss: 0.009140, test_acc: 0.69\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010743, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011256, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010476, train_acc: 0.68\n",
      "alexnet1d, trial.128:\n",
      "Epoch 12, avg test_loss: 0.009774, test_acc: 0.60\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011615, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010050, train_acc: 0.75\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011526, train_acc: 0.62\n",
      "alexnet1d, trial.128:\n",
      "Epoch 13, avg test_loss: 0.008952, test_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.011332, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010320, train_acc: 0.62\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011666, train_acc: 0.66\n",
      "alexnet1d, trial.128:\n",
      "Epoch 14, avg test_loss: 0.009895, test_acc: 0.61\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010235, train_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009871, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010403, train_acc: 0.73\n",
      "alexnet1d, trial.128:\n",
      "Epoch 15, avg test_loss: 0.008518, test_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008225, train_acc: 0.82\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009345, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009985, train_acc: 0.75\n",
      "alexnet1d, trial.128:\n",
      "Epoch 16, avg test_loss: 0.012540, test_acc: 0.59\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009003, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008494, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.013826, train_acc: 0.57\n",
      "alexnet1d, trial.128:\n",
      "Epoch 17, avg test_loss: 0.007915, test_acc: 0.71\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009770, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009102, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.010213, train_acc: 0.71\n",
      "alexnet1d, trial.128:\n",
      "Epoch 18, avg test_loss: 0.011221, test_acc: 0.57\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009770, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.006712, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008348, train_acc: 0.82\n",
      "alexnet1d, trial.128:\n",
      "Epoch 19, avg test_loss: 0.009855, test_acc: 0.61\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008562, train_acc: 0.77\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008872, train_acc: 0.71\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006752, train_acc: 0.88\n",
      "alexnet1d, trial.128:\n",
      "Epoch 20, avg test_loss: 0.009534, test_acc: 0.66\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.009222, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006307, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.009781, train_acc: 0.73\n",
      "alexnet1d, trial.128:\n",
      "Epoch 21, avg test_loss: 0.010831, test_acc: 0.63\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007655, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007842, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006985, train_acc: 0.86\n",
      "alexnet1d, trial.128:\n",
      "Epoch 22, avg test_loss: 0.011212, test_acc: 0.63\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006230, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007264, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.008071, train_acc: 0.80\n",
      "alexnet1d, trial.128:\n",
      "Epoch 23, avg test_loss: 0.016612, test_acc: 0.57\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005428, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.007635, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005993, train_acc: 0.86\n",
      "alexnet1d, trial.128:\n",
      "Epoch 24, avg test_loss: 0.010036, test_acc: 0.67\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005802, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006916, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005597, train_acc: 0.84\n",
      "alexnet1d, trial.128:\n",
      "Epoch 25, avg test_loss: 0.011188, test_acc: 0.63\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.003678, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005747, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004862, train_acc: 0.91\n",
      "alexnet1d, trial.128:\n",
      "Epoch 26, avg test_loss: 0.014649, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.005767, train_acc: 0.86\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004275, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.001320, train_acc: 1.00\n",
      "alexnet1d, trial.128:\n",
      "Epoch 27, avg test_loss: 0.016720, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003916, train_acc: 0.89\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.006158, train_acc: 0.86\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.003314, train_acc: 0.95\n",
      "alexnet1d, trial.128:\n",
      "Epoch 28, avg test_loss: 0.020519, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.003190, train_acc: 0.91\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.002559, train_acc: 0.96\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.003015, train_acc: 0.93\n",
      "alexnet1d, trial.128:\n",
      "Epoch 29, avg test_loss: 0.022608, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号44个\n",
      "错误信号26个\n",
      "信号正确并预测正确的概率为0.357\n",
      "信号错误并预测正确的概率为0.186\n",
      "总正确率为0.54\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012153, train_acc: 0.66\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.014088, train_acc: 0.50\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012392, train_acc: 0.54\n",
      "alexnet1d, trial.129:\n",
      "Epoch 0, avg test_loss: 0.009783, test_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012245, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012432, train_acc: 0.48\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012129, train_acc: 0.61\n",
      "alexnet1d, trial.129:\n",
      "Epoch 1, avg test_loss: 0.009741, test_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011938, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.013221, train_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012681, train_acc: 0.48\n",
      "alexnet1d, trial.129:\n",
      "Epoch 2, avg test_loss: 0.009712, test_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012216, train_acc: 0.55\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012136, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012472, train_acc: 0.48\n",
      "alexnet1d, trial.129:\n",
      "Epoch 3, avg test_loss: 0.009773, test_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012226, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012130, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012094, train_acc: 0.61\n",
      "alexnet1d, trial.129:\n",
      "Epoch 4, avg test_loss: 0.009746, test_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012436, train_acc: 0.50\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012199, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012147, train_acc: 0.59\n",
      "alexnet1d, trial.129:\n",
      "Epoch 5, avg test_loss: 0.009725, test_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011964, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012573, train_acc: 0.50\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012023, train_acc: 0.62\n",
      "alexnet1d, trial.129:\n",
      "Epoch 6, avg test_loss: 0.009707, test_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012323, train_acc: 0.55\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011992, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012221, train_acc: 0.55\n",
      "alexnet1d, trial.129:\n",
      "Epoch 7, avg test_loss: 0.009715, test_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011612, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012871, train_acc: 0.52\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012288, train_acc: 0.54\n",
      "alexnet1d, trial.129:\n",
      "Epoch 8, avg test_loss: 0.009716, test_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012135, train_acc: 0.55\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012174, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012413, train_acc: 0.52\n",
      "alexnet1d, trial.129:\n",
      "Epoch 9, avg test_loss: 0.009699, test_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011657, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012059, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011576, train_acc: 0.68\n",
      "alexnet1d, trial.129:\n",
      "Epoch 10, avg test_loss: 0.009675, test_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011832, train_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011481, train_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.012779, train_acc: 0.55\n",
      "alexnet1d, trial.129:\n",
      "Epoch 11, avg test_loss: 0.009680, test_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.013001, train_acc: 0.46\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011357, train_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011838, train_acc: 0.66\n",
      "alexnet1d, trial.129:\n",
      "Epoch 12, avg test_loss: 0.009489, test_acc: 0.60\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011503, train_acc: 0.62\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011821, train_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011449, train_acc: 0.55\n",
      "alexnet1d, trial.129:\n",
      "Epoch 13, avg test_loss: 0.009405, test_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.011290, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011372, train_acc: 0.59\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011227, train_acc: 0.75\n",
      "alexnet1d, trial.129:\n",
      "Epoch 14, avg test_loss: 0.009508, test_acc: 0.57\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010382, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010805, train_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011112, train_acc: 0.59\n",
      "alexnet1d, trial.129:\n",
      "Epoch 15, avg test_loss: 0.009913, test_acc: 0.59\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009901, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010972, train_acc: 0.64\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009222, train_acc: 0.77\n",
      "alexnet1d, trial.129:\n",
      "Epoch 16, avg test_loss: 0.010655, test_acc: 0.53\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009892, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007971, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.011932, train_acc: 0.62\n",
      "alexnet1d, trial.129:\n",
      "Epoch 17, avg test_loss: 0.011958, test_acc: 0.54\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009784, train_acc: 0.68\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009604, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008480, train_acc: 0.80\n",
      "alexnet1d, trial.129:\n",
      "Epoch 18, avg test_loss: 0.012109, test_acc: 0.47\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008980, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009539, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.010086, train_acc: 0.68\n",
      "alexnet1d, trial.129:\n",
      "Epoch 19, avg test_loss: 0.012040, test_acc: 0.59\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.009504, train_acc: 0.71\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.010860, train_acc: 0.68\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008259, train_acc: 0.80\n",
      "alexnet1d, trial.129:\n",
      "Epoch 20, avg test_loss: 0.012339, test_acc: 0.53\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008245, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.009333, train_acc: 0.73\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008220, train_acc: 0.82\n",
      "alexnet1d, trial.129:\n",
      "Epoch 21, avg test_loss: 0.012105, test_acc: 0.53\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007217, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006024, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007710, train_acc: 0.84\n",
      "alexnet1d, trial.129:\n",
      "Epoch 22, avg test_loss: 0.013951, test_acc: 0.47\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.003929, train_acc: 0.96\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.008689, train_acc: 0.77\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.008617, train_acc: 0.80\n",
      "alexnet1d, trial.129:\n",
      "Epoch 23, avg test_loss: 0.014812, test_acc: 0.51\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.007703, train_acc: 0.79\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006082, train_acc: 0.82\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006419, train_acc: 0.82\n",
      "alexnet1d, trial.129:\n",
      "Epoch 24, avg test_loss: 0.014730, test_acc: 0.53\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004144, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.007313, train_acc: 0.82\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005471, train_acc: 0.88\n",
      "alexnet1d, trial.129:\n",
      "Epoch 25, avg test_loss: 0.017272, test_acc: 0.49\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.006407, train_acc: 0.84\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.003827, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.005213, train_acc: 0.86\n",
      "alexnet1d, trial.129:\n",
      "Epoch 26, avg test_loss: 0.015298, test_acc: 0.56\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.002760, train_acc: 0.96\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004055, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.005261, train_acc: 0.89\n",
      "alexnet1d, trial.129:\n",
      "Epoch 27, avg test_loss: 0.017674, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004788, train_acc: 0.89\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.003759, train_acc: 0.89\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.005686, train_acc: 0.86\n",
      "alexnet1d, trial.129:\n",
      "Epoch 28, avg test_loss: 0.018458, test_acc: 0.54\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.006138, train_acc: 0.88\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.004107, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.004480, train_acc: 0.93\n",
      "alexnet1d, trial.129:\n",
      "Epoch 29, avg test_loss: 0.017701, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.004390, train_acc: 0.93\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.004999, train_acc: 0.86\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.004087, train_acc: 0.93\n",
      "alexnet1d, trial.129:\n",
      "Epoch 30, avg test_loss: 0.020624, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.003171, train_acc: 0.91\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.005531, train_acc: 0.84\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.003657, train_acc: 0.89\n",
      "alexnet1d, trial.129:\n",
      "Epoch 31, avg test_loss: 0.020359, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号41个\n",
      "错误信号29个\n",
      "信号正确并预测正确的概率为0.386\n",
      "信号错误并预测正确的概率为0.143\n",
      "总正确率为0.53\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012299, train_acc: 0.62\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.014502, train_acc: 0.52\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012000, train_acc: 0.61\n",
      "alexnet1d, trial.130:\n",
      "Epoch 0, avg test_loss: 0.010080, test_acc: 0.47\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011919, train_acc: 0.66\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.013000, train_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011818, train_acc: 0.62\n",
      "alexnet1d, trial.130:\n",
      "Epoch 1, avg test_loss: 0.010352, test_acc: 0.47\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.010712, train_acc: 0.80\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012312, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012982, train_acc: 0.48\n",
      "alexnet1d, trial.130:\n",
      "Epoch 2, avg test_loss: 0.010449, test_acc: 0.47\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011474, train_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012072, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011827, train_acc: 0.66\n",
      "alexnet1d, trial.130:\n",
      "Epoch 3, avg test_loss: 0.010146, test_acc: 0.47\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011815, train_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011967, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012177, train_acc: 0.55\n",
      "alexnet1d, trial.130:\n",
      "Epoch 4, avg test_loss: 0.010748, test_acc: 0.47\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011652, train_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011017, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012919, train_acc: 0.46\n",
      "alexnet1d, trial.130:\n",
      "Epoch 5, avg test_loss: 0.011151, test_acc: 0.47\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011567, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012023, train_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011539, train_acc: 0.61\n",
      "alexnet1d, trial.130:\n",
      "Epoch 6, avg test_loss: 0.010569, test_acc: 0.47\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.010937, train_acc: 0.70\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012793, train_acc: 0.48\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011925, train_acc: 0.57\n",
      "alexnet1d, trial.130:\n",
      "Epoch 7, avg test_loss: 0.011703, test_acc: 0.47\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010622, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011295, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.013209, train_acc: 0.52\n",
      "alexnet1d, trial.130:\n",
      "Epoch 8, avg test_loss: 0.010971, test_acc: 0.51\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010588, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011356, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010762, train_acc: 0.68\n",
      "alexnet1d, trial.130:\n",
      "Epoch 9, avg test_loss: 0.011322, test_acc: 0.50\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.012395, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010873, train_acc: 0.71\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011260, train_acc: 0.62\n",
      "alexnet1d, trial.130:\n",
      "Epoch 10, avg test_loss: 0.011923, test_acc: 0.49\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011206, train_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010404, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010664, train_acc: 0.59\n",
      "alexnet1d, trial.130:\n",
      "Epoch 11, avg test_loss: 0.012435, test_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010215, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.009168, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.009569, train_acc: 0.79\n",
      "alexnet1d, trial.130:\n",
      "Epoch 12, avg test_loss: 0.013991, test_acc: 0.50\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010652, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011488, train_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011344, train_acc: 0.66\n",
      "alexnet1d, trial.130:\n",
      "Epoch 13, avg test_loss: 0.015563, test_acc: 0.53\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.012015, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010686, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010246, train_acc: 0.70\n",
      "alexnet1d, trial.130:\n",
      "Epoch 14, avg test_loss: 0.011124, test_acc: 0.54\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009677, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009525, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009561, train_acc: 0.70\n",
      "alexnet1d, trial.130:\n",
      "Epoch 15, avg test_loss: 0.013657, test_acc: 0.51\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009077, train_acc: 0.84\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009011, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008994, train_acc: 0.77\n",
      "alexnet1d, trial.130:\n",
      "Epoch 16, avg test_loss: 0.016577, test_acc: 0.53\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009255, train_acc: 0.71\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007952, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009579, train_acc: 0.75\n",
      "alexnet1d, trial.130:\n",
      "Epoch 17, avg test_loss: 0.016698, test_acc: 0.49\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.005422, train_acc: 0.91\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007872, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007465, train_acc: 0.88\n",
      "alexnet1d, trial.130:\n",
      "Epoch 18, avg test_loss: 0.014786, test_acc: 0.53\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008486, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007240, train_acc: 0.88\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007027, train_acc: 0.80\n",
      "alexnet1d, trial.130:\n",
      "Epoch 19, avg test_loss: 0.019834, test_acc: 0.50\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007459, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.006632, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009647, train_acc: 0.77\n",
      "alexnet1d, trial.130:\n",
      "Epoch 20, avg test_loss: 0.019945, test_acc: 0.50\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.005993, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006076, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.009282, train_acc: 0.80\n",
      "alexnet1d, trial.130:\n",
      "Epoch 21, avg test_loss: 0.018803, test_acc: 0.50\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005378, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007738, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008610, train_acc: 0.80\n",
      "alexnet1d, trial.130:\n",
      "Epoch 22, avg test_loss: 0.023787, test_acc: 0.56\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006600, train_acc: 0.80\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005569, train_acc: 0.82\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006895, train_acc: 0.82\n",
      "alexnet1d, trial.130:\n",
      "Epoch 23, avg test_loss: 0.021157, test_acc: 0.50\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004241, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006510, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.004664, train_acc: 0.91\n",
      "alexnet1d, trial.130:\n",
      "Epoch 24, avg test_loss: 0.022199, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.006459, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005627, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.003043, train_acc: 0.95\n",
      "alexnet1d, trial.130:\n",
      "Epoch 25, avg test_loss: 0.026436, test_acc: 0.50\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.003514, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.003598, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.003603, train_acc: 0.93\n",
      "alexnet1d, trial.130:\n",
      "Epoch 26, avg test_loss: 0.027068, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.002334, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.002870, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.002911, train_acc: 0.91\n",
      "alexnet1d, trial.130:\n",
      "Epoch 27, avg test_loss: 0.035469, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号33个\n",
      "错误信号37个\n",
      "信号正确并预测正确的概率为0.257\n",
      "信号错误并预测正确的概率为0.286\n",
      "总正确率为0.54\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012379, train_acc: 0.46\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.010995, train_acc: 0.71\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012126, train_acc: 0.61\n",
      "alexnet1d, trial.131:\n",
      "Epoch 0, avg test_loss: 0.009917, test_acc: 0.46\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012180, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012436, train_acc: 0.39\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012255, train_acc: 0.57\n",
      "alexnet1d, trial.131:\n",
      "Epoch 1, avg test_loss: 0.009904, test_acc: 0.50\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012340, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012533, train_acc: 0.48\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011978, train_acc: 0.61\n",
      "alexnet1d, trial.131:\n",
      "Epoch 2, avg test_loss: 0.010120, test_acc: 0.50\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011516, train_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.013991, train_acc: 0.50\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012232, train_acc: 0.57\n",
      "alexnet1d, trial.131:\n",
      "Epoch 3, avg test_loss: 0.010203, test_acc: 0.50\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012281, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011716, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012357, train_acc: 0.54\n",
      "alexnet1d, trial.131:\n",
      "Epoch 4, avg test_loss: 0.010101, test_acc: 0.50\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011901, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011750, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011913, train_acc: 0.62\n",
      "alexnet1d, trial.131:\n",
      "Epoch 5, avg test_loss: 0.010130, test_acc: 0.50\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012605, train_acc: 0.48\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011678, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011445, train_acc: 0.64\n",
      "alexnet1d, trial.131:\n",
      "Epoch 6, avg test_loss: 0.010307, test_acc: 0.50\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011406, train_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011845, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012693, train_acc: 0.50\n",
      "alexnet1d, trial.131:\n",
      "Epoch 7, avg test_loss: 0.010358, test_acc: 0.50\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010631, train_acc: 0.73\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011946, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011689, train_acc: 0.59\n",
      "alexnet1d, trial.131:\n",
      "Epoch 8, avg test_loss: 0.010218, test_acc: 0.50\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012053, train_acc: 0.48\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010907, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011750, train_acc: 0.61\n",
      "alexnet1d, trial.131:\n",
      "Epoch 9, avg test_loss: 0.010639, test_acc: 0.49\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.012446, train_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011363, train_acc: 0.55\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011166, train_acc: 0.70\n",
      "alexnet1d, trial.131:\n",
      "Epoch 10, avg test_loss: 0.010166, test_acc: 0.53\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010743, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010571, train_acc: 0.73\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.013628, train_acc: 0.48\n",
      "alexnet1d, trial.131:\n",
      "Epoch 11, avg test_loss: 0.010698, test_acc: 0.51\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010763, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010652, train_acc: 0.80\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011823, train_acc: 0.61\n",
      "alexnet1d, trial.131:\n",
      "Epoch 12, avg test_loss: 0.010238, test_acc: 0.53\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009253, train_acc: 0.75\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009919, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010910, train_acc: 0.68\n",
      "alexnet1d, trial.131:\n",
      "Epoch 13, avg test_loss: 0.010067, test_acc: 0.50\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.011249, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010547, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010530, train_acc: 0.71\n",
      "alexnet1d, trial.131:\n",
      "Epoch 14, avg test_loss: 0.011107, test_acc: 0.49\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008985, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.012604, train_acc: 0.61\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010409, train_acc: 0.70\n",
      "alexnet1d, trial.131:\n",
      "Epoch 15, avg test_loss: 0.010359, test_acc: 0.54\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009892, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009931, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010994, train_acc: 0.61\n",
      "alexnet1d, trial.131:\n",
      "Epoch 16, avg test_loss: 0.010377, test_acc: 0.53\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.010685, train_acc: 0.70\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009192, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008786, train_acc: 0.77\n",
      "alexnet1d, trial.131:\n",
      "Epoch 17, avg test_loss: 0.010744, test_acc: 0.57\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008299, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008305, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.010786, train_acc: 0.68\n",
      "alexnet1d, trial.131:\n",
      "Epoch 18, avg test_loss: 0.012258, test_acc: 0.60\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009980, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007119, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008057, train_acc: 0.82\n",
      "alexnet1d, trial.131:\n",
      "Epoch 19, avg test_loss: 0.012820, test_acc: 0.54\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.010030, train_acc: 0.73\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.009868, train_acc: 0.73\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008489, train_acc: 0.79\n",
      "alexnet1d, trial.131:\n",
      "Epoch 20, avg test_loss: 0.011383, test_acc: 0.60\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007971, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007903, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.009623, train_acc: 0.73\n",
      "alexnet1d, trial.131:\n",
      "Epoch 21, avg test_loss: 0.011658, test_acc: 0.57\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.008105, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.009132, train_acc: 0.75\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.009429, train_acc: 0.70\n",
      "alexnet1d, trial.131:\n",
      "Epoch 22, avg test_loss: 0.014296, test_acc: 0.56\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.009005, train_acc: 0.75\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007997, train_acc: 0.79\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.007930, train_acc: 0.79\n",
      "alexnet1d, trial.131:\n",
      "Epoch 23, avg test_loss: 0.011487, test_acc: 0.60\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006620, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.009250, train_acc: 0.73\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.007567, train_acc: 0.75\n",
      "alexnet1d, trial.131:\n",
      "Epoch 24, avg test_loss: 0.013161, test_acc: 0.59\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.006794, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.007252, train_acc: 0.82\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005796, train_acc: 0.89\n",
      "alexnet1d, trial.131:\n",
      "Epoch 25, avg test_loss: 0.013408, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004904, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005271, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004742, train_acc: 0.91\n",
      "alexnet1d, trial.131:\n",
      "Epoch 26, avg test_loss: 0.015724, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003413, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.002356, train_acc: 0.96\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.005442, train_acc: 0.88\n",
      "alexnet1d, trial.131:\n",
      "Epoch 27, avg test_loss: 0.019816, test_acc: 0.57\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003082, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.005189, train_acc: 0.88\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.005670, train_acc: 0.88\n",
      "alexnet1d, trial.131:\n",
      "Epoch 28, avg test_loss: 0.018389, test_acc: 0.50\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.006878, train_acc: 0.84\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.004361, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.003766, train_acc: 0.93\n",
      "alexnet1d, trial.131:\n",
      "Epoch 29, avg test_loss: 0.018261, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.003198, train_acc: 0.95\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.004864, train_acc: 0.89\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.003788, train_acc: 0.95\n",
      "alexnet1d, trial.131:\n",
      "Epoch 30, avg test_loss: 0.025279, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号35个\n",
      "错误信号35个\n",
      "信号正确并预测正确的概率为0.286\n",
      "信号错误并预测正确的概率为0.3\n",
      "总正确率为0.59\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012365, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.014855, train_acc: 0.52\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012207, train_acc: 0.61\n",
      "alexnet1d, trial.132:\n",
      "Epoch 0, avg test_loss: 0.009831, test_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012300, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012413, train_acc: 0.45\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012250, train_acc: 0.61\n",
      "alexnet1d, trial.132:\n",
      "Epoch 1, avg test_loss: 0.009726, test_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012298, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012252, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011991, train_acc: 0.59\n",
      "alexnet1d, trial.132:\n",
      "Epoch 2, avg test_loss: 0.009536, test_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012084, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012267, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011980, train_acc: 0.61\n",
      "alexnet1d, trial.132:\n",
      "Epoch 3, avg test_loss: 0.009565, test_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012132, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012672, train_acc: 0.48\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011702, train_acc: 0.70\n",
      "alexnet1d, trial.132:\n",
      "Epoch 4, avg test_loss: 0.009612, test_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012760, train_acc: 0.45\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012088, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011718, train_acc: 0.68\n",
      "alexnet1d, trial.132:\n",
      "Epoch 5, avg test_loss: 0.009568, test_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012003, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012543, train_acc: 0.50\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011654, train_acc: 0.66\n",
      "alexnet1d, trial.132:\n",
      "Epoch 6, avg test_loss: 0.009451, test_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011932, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.013504, train_acc: 0.41\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011667, train_acc: 0.64\n",
      "alexnet1d, trial.132:\n",
      "Epoch 7, avg test_loss: 0.009505, test_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011671, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012002, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011767, train_acc: 0.61\n",
      "alexnet1d, trial.132:\n",
      "Epoch 8, avg test_loss: 0.009441, test_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011861, train_acc: 0.55\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012102, train_acc: 0.52\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012593, train_acc: 0.59\n",
      "alexnet1d, trial.132:\n",
      "Epoch 9, avg test_loss: 0.009365, test_acc: 0.60\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011590, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011500, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011505, train_acc: 0.71\n",
      "alexnet1d, trial.132:\n",
      "Epoch 10, avg test_loss: 0.009562, test_acc: 0.54\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011738, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010991, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.013368, train_acc: 0.66\n",
      "alexnet1d, trial.132:\n",
      "Epoch 11, avg test_loss: 0.009554, test_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010062, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010548, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.012450, train_acc: 0.62\n",
      "alexnet1d, trial.132:\n",
      "Epoch 12, avg test_loss: 0.009784, test_acc: 0.56\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009846, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011093, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011975, train_acc: 0.52\n",
      "alexnet1d, trial.132:\n",
      "Epoch 13, avg test_loss: 0.009904, test_acc: 0.51\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010946, train_acc: 0.64\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010801, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010583, train_acc: 0.71\n",
      "alexnet1d, trial.132:\n",
      "Epoch 14, avg test_loss: 0.009601, test_acc: 0.61\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009959, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.011201, train_acc: 0.61\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009492, train_acc: 0.79\n",
      "alexnet1d, trial.132:\n",
      "Epoch 15, avg test_loss: 0.010358, test_acc: 0.57\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008713, train_acc: 0.80\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009115, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.011585, train_acc: 0.64\n",
      "alexnet1d, trial.132:\n",
      "Epoch 16, avg test_loss: 0.010023, test_acc: 0.63\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008121, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.011050, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009409, train_acc: 0.64\n",
      "alexnet1d, trial.132:\n",
      "Epoch 17, avg test_loss: 0.010661, test_acc: 0.59\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008881, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009733, train_acc: 0.71\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.011012, train_acc: 0.64\n",
      "alexnet1d, trial.132:\n",
      "Epoch 18, avg test_loss: 0.010113, test_acc: 0.61\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008557, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009293, train_acc: 0.66\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009288, train_acc: 0.79\n",
      "alexnet1d, trial.132:\n",
      "Epoch 19, avg test_loss: 0.010998, test_acc: 0.61\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007390, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007258, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007762, train_acc: 0.82\n",
      "alexnet1d, trial.132:\n",
      "Epoch 20, avg test_loss: 0.012473, test_acc: 0.61\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.005759, train_acc: 0.89\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007962, train_acc: 0.77\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.005640, train_acc: 0.93\n",
      "alexnet1d, trial.132:\n",
      "Epoch 21, avg test_loss: 0.012495, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005486, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007867, train_acc: 0.79\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007037, train_acc: 0.82\n",
      "alexnet1d, trial.132:\n",
      "Epoch 22, avg test_loss: 0.011210, test_acc: 0.63\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005866, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005896, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006948, train_acc: 0.86\n",
      "alexnet1d, trial.132:\n",
      "Epoch 23, avg test_loss: 0.018793, test_acc: 0.53\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006550, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004339, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006869, train_acc: 0.84\n",
      "alexnet1d, trial.132:\n",
      "Epoch 24, avg test_loss: 0.013387, test_acc: 0.54\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.006123, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004736, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006933, train_acc: 0.84\n",
      "alexnet1d, trial.132:\n",
      "Epoch 25, avg test_loss: 0.014617, test_acc: 0.60\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.003761, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.002979, train_acc: 0.96\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.005832, train_acc: 0.86\n",
      "alexnet1d, trial.132:\n",
      "Epoch 26, avg test_loss: 0.016336, test_acc: 0.60\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003875, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004108, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.005800, train_acc: 0.84\n",
      "alexnet1d, trial.132:\n",
      "Epoch 27, avg test_loss: 0.017584, test_acc: 0.57\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003595, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.002703, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.004679, train_acc: 0.86\n",
      "alexnet1d, trial.132:\n",
      "Epoch 28, avg test_loss: 0.017949, test_acc: 0.64\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.003408, train_acc: 0.91\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.004475, train_acc: 0.91\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.002269, train_acc: 0.96\n",
      "alexnet1d, trial.132:\n",
      "Epoch 29, avg test_loss: 0.017739, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.002764, train_acc: 0.93\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.002395, train_acc: 0.95\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.001810, train_acc: 0.96\n",
      "alexnet1d, trial.132:\n",
      "Epoch 30, avg test_loss: 0.018374, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.002122, train_acc: 0.93\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.001389, train_acc: 0.96\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.001063, train_acc: 0.98\n",
      "alexnet1d, trial.132:\n",
      "Epoch 31, avg test_loss: 0.024592, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号43个\n",
      "错误信号27个\n",
      "信号正确并预测正确的概率为0.386\n",
      "信号错误并预测正确的概率为0.214\n",
      "总正确率为0.60\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012373, train_acc: 0.48\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012100, train_acc: 0.64\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012295, train_acc: 0.62\n",
      "alexnet1d, trial.133:\n",
      "Epoch 0, avg test_loss: 0.009720, test_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011976, train_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012320, train_acc: 0.62\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012766, train_acc: 0.50\n",
      "alexnet1d, trial.133:\n",
      "Epoch 1, avg test_loss: 0.009673, test_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012234, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012027, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012793, train_acc: 0.48\n",
      "alexnet1d, trial.133:\n",
      "Epoch 2, avg test_loss: 0.009700, test_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012221, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011841, train_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011924, train_acc: 0.59\n",
      "alexnet1d, trial.133:\n",
      "Epoch 3, avg test_loss: 0.009679, test_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012087, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011873, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011937, train_acc: 0.62\n",
      "alexnet1d, trial.133:\n",
      "Epoch 4, avg test_loss: 0.009756, test_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011548, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011487, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.013191, train_acc: 0.52\n",
      "alexnet1d, trial.133:\n",
      "Epoch 5, avg test_loss: 0.009820, test_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011986, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012028, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012424, train_acc: 0.54\n",
      "alexnet1d, trial.133:\n",
      "Epoch 6, avg test_loss: 0.009761, test_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.010858, train_acc: 0.71\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.014556, train_acc: 0.48\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012144, train_acc: 0.59\n",
      "alexnet1d, trial.133:\n",
      "Epoch 7, avg test_loss: 0.009772, test_acc: 0.53\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011188, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011288, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011638, train_acc: 0.62\n",
      "alexnet1d, trial.133:\n",
      "Epoch 8, avg test_loss: 0.009629, test_acc: 0.60\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011094, train_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011858, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011796, train_acc: 0.61\n",
      "alexnet1d, trial.133:\n",
      "Epoch 9, avg test_loss: 0.009806, test_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.009506, train_acc: 0.70\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012011, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011506, train_acc: 0.61\n",
      "alexnet1d, trial.133:\n",
      "Epoch 10, avg test_loss: 0.009859, test_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010442, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011514, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011741, train_acc: 0.57\n",
      "alexnet1d, trial.133:\n",
      "Epoch 11, avg test_loss: 0.009771, test_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009332, train_acc: 0.77\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011072, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.012651, train_acc: 0.55\n",
      "alexnet1d, trial.133:\n",
      "Epoch 12, avg test_loss: 0.010308, test_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011255, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010278, train_acc: 0.62\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011340, train_acc: 0.61\n",
      "alexnet1d, trial.133:\n",
      "Epoch 13, avg test_loss: 0.011375, test_acc: 0.59\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.012226, train_acc: 0.62\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010265, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.012003, train_acc: 0.61\n",
      "alexnet1d, trial.133:\n",
      "Epoch 14, avg test_loss: 0.010355, test_acc: 0.56\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.011033, train_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010369, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010103, train_acc: 0.73\n",
      "alexnet1d, trial.133:\n",
      "Epoch 15, avg test_loss: 0.009901, test_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008759, train_acc: 0.82\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.011850, train_acc: 0.61\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009058, train_acc: 0.77\n",
      "alexnet1d, trial.133:\n",
      "Epoch 16, avg test_loss: 0.011168, test_acc: 0.67\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009568, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007225, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008254, train_acc: 0.80\n",
      "alexnet1d, trial.133:\n",
      "Epoch 17, avg test_loss: 0.011508, test_acc: 0.64\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009077, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009881, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.006472, train_acc: 0.86\n",
      "alexnet1d, trial.133:\n",
      "Epoch 18, avg test_loss: 0.013005, test_acc: 0.59\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008070, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008135, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.006690, train_acc: 0.80\n",
      "alexnet1d, trial.133:\n",
      "Epoch 19, avg test_loss: 0.013458, test_acc: 0.57\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008433, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.005285, train_acc: 0.89\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007965, train_acc: 0.82\n",
      "alexnet1d, trial.133:\n",
      "Epoch 20, avg test_loss: 0.013427, test_acc: 0.61\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.004145, train_acc: 0.93\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.005221, train_acc: 0.91\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007753, train_acc: 0.82\n",
      "alexnet1d, trial.133:\n",
      "Epoch 21, avg test_loss: 0.014609, test_acc: 0.61\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.004206, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006456, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006665, train_acc: 0.80\n",
      "alexnet1d, trial.133:\n",
      "Epoch 22, avg test_loss: 0.017015, test_acc: 0.49\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.003734, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.008885, train_acc: 0.75\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006864, train_acc: 0.84\n",
      "alexnet1d, trial.133:\n",
      "Epoch 23, avg test_loss: 0.013177, test_acc: 0.61\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004851, train_acc: 0.82\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006887, train_acc: 0.82\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006396, train_acc: 0.84\n",
      "alexnet1d, trial.133:\n",
      "Epoch 24, avg test_loss: 0.017618, test_acc: 0.49\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004291, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005363, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005930, train_acc: 0.82\n",
      "alexnet1d, trial.133:\n",
      "Epoch 25, avg test_loss: 0.015883, test_acc: 0.51\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004533, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005211, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.006072, train_acc: 0.84\n",
      "alexnet1d, trial.133:\n",
      "Epoch 26, avg test_loss: 0.013399, test_acc: 0.63\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004010, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.002706, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004587, train_acc: 0.89\n",
      "alexnet1d, trial.133:\n",
      "Epoch 27, avg test_loss: 0.016419, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003129, train_acc: 0.98\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.001245, train_acc: 1.00\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.001142, train_acc: 1.00\n",
      "alexnet1d, trial.133:\n",
      "Epoch 28, avg test_loss: 0.018615, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.001684, train_acc: 0.96\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.002943, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.002426, train_acc: 0.95\n",
      "alexnet1d, trial.133:\n",
      "Epoch 29, avg test_loss: 0.023916, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.002114, train_acc: 0.95\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.002761, train_acc: 0.91\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.002066, train_acc: 0.95\n",
      "alexnet1d, trial.133:\n",
      "Epoch 30, avg test_loss: 0.027468, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号41个\n",
      "错误信号29个\n",
      "信号正确并预测正确的概率为0.386\n",
      "信号错误并预测正确的概率为0.186\n",
      "总正确率为0.57\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012254, train_acc: 0.64\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.011299, train_acc: 0.68\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012645, train_acc: 0.57\n",
      "alexnet1d, trial.134:\n",
      "Epoch 0, avg test_loss: 0.010036, test_acc: 0.47\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012294, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.011763, train_acc: 0.71\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012506, train_acc: 0.50\n",
      "alexnet1d, trial.134:\n",
      "Epoch 1, avg test_loss: 0.010144, test_acc: 0.47\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012257, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011851, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011798, train_acc: 0.62\n",
      "alexnet1d, trial.134:\n",
      "Epoch 2, avg test_loss: 0.010568, test_acc: 0.47\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012185, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012882, train_acc: 0.48\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012073, train_acc: 0.59\n",
      "alexnet1d, trial.134:\n",
      "Epoch 3, avg test_loss: 0.010276, test_acc: 0.47\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011280, train_acc: 0.70\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011909, train_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.013119, train_acc: 0.52\n",
      "alexnet1d, trial.134:\n",
      "Epoch 4, avg test_loss: 0.010390, test_acc: 0.47\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011566, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012170, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011401, train_acc: 0.70\n",
      "alexnet1d, trial.134:\n",
      "Epoch 5, avg test_loss: 0.010121, test_acc: 0.49\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011765, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011610, train_acc: 0.68\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011989, train_acc: 0.61\n",
      "alexnet1d, trial.134:\n",
      "Epoch 6, avg test_loss: 0.010371, test_acc: 0.49\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011677, train_acc: 0.55\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011873, train_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012112, train_acc: 0.62\n",
      "alexnet1d, trial.134:\n",
      "Epoch 7, avg test_loss: 0.010477, test_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011837, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011030, train_acc: 0.70\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011166, train_acc: 0.61\n",
      "alexnet1d, trial.134:\n",
      "Epoch 8, avg test_loss: 0.010432, test_acc: 0.56\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012074, train_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011213, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011260, train_acc: 0.68\n",
      "alexnet1d, trial.134:\n",
      "Epoch 9, avg test_loss: 0.010519, test_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011434, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011251, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011246, train_acc: 0.64\n",
      "alexnet1d, trial.134:\n",
      "Epoch 10, avg test_loss: 0.010382, test_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011207, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010032, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011430, train_acc: 0.61\n",
      "alexnet1d, trial.134:\n",
      "Epoch 11, avg test_loss: 0.011076, test_acc: 0.54\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009410, train_acc: 0.75\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010980, train_acc: 0.54\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010820, train_acc: 0.66\n",
      "alexnet1d, trial.134:\n",
      "Epoch 12, avg test_loss: 0.011341, test_acc: 0.56\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009252, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010026, train_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011252, train_acc: 0.62\n",
      "alexnet1d, trial.134:\n",
      "Epoch 13, avg test_loss: 0.011411, test_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010541, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.008617, train_acc: 0.80\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010703, train_acc: 0.68\n",
      "alexnet1d, trial.134:\n",
      "Epoch 14, avg test_loss: 0.012847, test_acc: 0.54\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008821, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008105, train_acc: 0.80\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011206, train_acc: 0.66\n",
      "alexnet1d, trial.134:\n",
      "Epoch 15, avg test_loss: 0.011951, test_acc: 0.54\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.007881, train_acc: 0.82\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009556, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010384, train_acc: 0.64\n",
      "alexnet1d, trial.134:\n",
      "Epoch 16, avg test_loss: 0.013365, test_acc: 0.51\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007985, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008315, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007443, train_acc: 0.82\n",
      "alexnet1d, trial.134:\n",
      "Epoch 17, avg test_loss: 0.013076, test_acc: 0.56\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007846, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007088, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007151, train_acc: 0.82\n",
      "alexnet1d, trial.134:\n",
      "Epoch 18, avg test_loss: 0.015651, test_acc: 0.56\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007050, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.005607, train_acc: 0.88\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.006792, train_acc: 0.88\n",
      "alexnet1d, trial.134:\n",
      "Epoch 19, avg test_loss: 0.016398, test_acc: 0.60\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.005865, train_acc: 0.89\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007867, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.005457, train_acc: 0.89\n",
      "alexnet1d, trial.134:\n",
      "Epoch 20, avg test_loss: 0.019886, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006273, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.005690, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007129, train_acc: 0.82\n",
      "alexnet1d, trial.134:\n",
      "Epoch 21, avg test_loss: 0.016968, test_acc: 0.56\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005329, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006107, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.004768, train_acc: 0.91\n",
      "alexnet1d, trial.134:\n",
      "Epoch 22, avg test_loss: 0.019170, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005226, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005243, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.004264, train_acc: 0.88\n",
      "alexnet1d, trial.134:\n",
      "Epoch 23, avg test_loss: 0.026489, test_acc: 0.56\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004417, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004617, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005492, train_acc: 0.86\n",
      "alexnet1d, trial.134:\n",
      "Epoch 24, avg test_loss: 0.020191, test_acc: 0.60\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.003399, train_acc: 0.95\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004224, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.007073, train_acc: 0.79\n",
      "alexnet1d, trial.134:\n",
      "Epoch 25, avg test_loss: 0.024407, test_acc: 0.56\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005318, train_acc: 0.84\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.003334, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004796, train_acc: 0.91\n",
      "alexnet1d, trial.134:\n",
      "Epoch 26, avg test_loss: 0.032348, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.002421, train_acc: 0.96\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004916, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004388, train_acc: 0.89\n",
      "alexnet1d, trial.134:\n",
      "Epoch 27, avg test_loss: 0.025797, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号33个\n",
      "错误信号37个\n",
      "信号正确并预测正确的概率为0.329\n",
      "信号错误并预测正确的概率为0.257\n",
      "总正确率为0.59\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012354, train_acc: 0.64\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012411, train_acc: 0.52\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012083, train_acc: 0.57\n",
      "alexnet1d, trial.135:\n",
      "Epoch 0, avg test_loss: 0.009823, test_acc: 0.53\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012619, train_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.011991, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.013409, train_acc: 0.50\n",
      "alexnet1d, trial.135:\n",
      "Epoch 1, avg test_loss: 0.009876, test_acc: 0.53\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011602, train_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011974, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012362, train_acc: 0.55\n",
      "alexnet1d, trial.135:\n",
      "Epoch 2, avg test_loss: 0.009853, test_acc: 0.53\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012174, train_acc: 0.55\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012036, train_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012124, train_acc: 0.59\n",
      "alexnet1d, trial.135:\n",
      "Epoch 3, avg test_loss: 0.009878, test_acc: 0.53\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011949, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012411, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011741, train_acc: 0.66\n",
      "alexnet1d, trial.135:\n",
      "Epoch 4, avg test_loss: 0.009895, test_acc: 0.53\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012466, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011567, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011625, train_acc: 0.61\n",
      "alexnet1d, trial.135:\n",
      "Epoch 5, avg test_loss: 0.009841, test_acc: 0.53\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011565, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011469, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011971, train_acc: 0.57\n",
      "alexnet1d, trial.135:\n",
      "Epoch 6, avg test_loss: 0.010063, test_acc: 0.51\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011407, train_acc: 0.68\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012049, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011724, train_acc: 0.50\n",
      "alexnet1d, trial.135:\n",
      "Epoch 7, avg test_loss: 0.010038, test_acc: 0.53\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010658, train_acc: 0.77\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011611, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.010862, train_acc: 0.66\n",
      "alexnet1d, trial.135:\n",
      "Epoch 8, avg test_loss: 0.009598, test_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010085, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011770, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.015057, train_acc: 0.55\n",
      "alexnet1d, trial.135:\n",
      "Epoch 9, avg test_loss: 0.010235, test_acc: 0.56\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011309, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011009, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.013751, train_acc: 0.43\n",
      "alexnet1d, trial.135:\n",
      "Epoch 10, avg test_loss: 0.011443, test_acc: 0.50\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010628, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011672, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011215, train_acc: 0.59\n",
      "alexnet1d, trial.135:\n",
      "Epoch 11, avg test_loss: 0.010462, test_acc: 0.47\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011518, train_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010749, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011213, train_acc: 0.73\n",
      "alexnet1d, trial.135:\n",
      "Epoch 12, avg test_loss: 0.010399, test_acc: 0.53\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010723, train_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011725, train_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010604, train_acc: 0.70\n",
      "alexnet1d, trial.135:\n",
      "Epoch 13, avg test_loss: 0.010791, test_acc: 0.53\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009146, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010179, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009296, train_acc: 0.75\n",
      "alexnet1d, trial.135:\n",
      "Epoch 14, avg test_loss: 0.012759, test_acc: 0.50\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009820, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009708, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008988, train_acc: 0.79\n",
      "alexnet1d, trial.135:\n",
      "Epoch 15, avg test_loss: 0.012165, test_acc: 0.49\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008725, train_acc: 0.80\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009122, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008271, train_acc: 0.77\n",
      "alexnet1d, trial.135:\n",
      "Epoch 16, avg test_loss: 0.013169, test_acc: 0.54\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008530, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008321, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009665, train_acc: 0.73\n",
      "alexnet1d, trial.135:\n",
      "Epoch 17, avg test_loss: 0.014315, test_acc: 0.50\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007131, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009000, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008447, train_acc: 0.80\n",
      "alexnet1d, trial.135:\n",
      "Epoch 18, avg test_loss: 0.014869, test_acc: 0.54\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007637, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.006859, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007991, train_acc: 0.79\n",
      "alexnet1d, trial.135:\n",
      "Epoch 19, avg test_loss: 0.014919, test_acc: 0.51\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008389, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007808, train_acc: 0.75\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007715, train_acc: 0.77\n",
      "alexnet1d, trial.135:\n",
      "Epoch 20, avg test_loss: 0.014635, test_acc: 0.47\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008221, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.010109, train_acc: 0.71\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007355, train_acc: 0.84\n",
      "alexnet1d, trial.135:\n",
      "Epoch 21, avg test_loss: 0.018280, test_acc: 0.51\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.004685, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.008648, train_acc: 0.79\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006210, train_acc: 0.86\n",
      "alexnet1d, trial.135:\n",
      "Epoch 22, avg test_loss: 0.020243, test_acc: 0.57\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006812, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005910, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.005912, train_acc: 0.88\n",
      "alexnet1d, trial.135:\n",
      "Epoch 23, avg test_loss: 0.019211, test_acc: 0.46\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004925, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006820, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.008046, train_acc: 0.80\n",
      "alexnet1d, trial.135:\n",
      "Epoch 24, avg test_loss: 0.022597, test_acc: 0.49\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.003795, train_acc: 0.96\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004681, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004819, train_acc: 0.91\n",
      "alexnet1d, trial.135:\n",
      "Epoch 25, avg test_loss: 0.019193, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005040, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.003398, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.006038, train_acc: 0.86\n",
      "alexnet1d, trial.135:\n",
      "Epoch 26, avg test_loss: 0.021460, test_acc: 0.53\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004360, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.005075, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.007799, train_acc: 0.86\n",
      "alexnet1d, trial.135:\n",
      "Epoch 27, avg test_loss: 0.025950, test_acc: 0.50\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003883, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004397, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.004061, train_acc: 0.89\n",
      "alexnet1d, trial.135:\n",
      "Epoch 28, avg test_loss: 0.027160, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.002903, train_acc: 0.96\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.003858, train_acc: 0.88\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.004918, train_acc: 0.88\n",
      "alexnet1d, trial.135:\n",
      "Epoch 29, avg test_loss: 0.024520, test_acc: 0.57\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.003428, train_acc: 0.93\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.003622, train_acc: 0.93\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.001836, train_acc: 0.96\n",
      "alexnet1d, trial.135:\n",
      "Epoch 30, avg test_loss: 0.034462, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.005465, train_acc: 0.86\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.003070, train_acc: 0.95\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.004592, train_acc: 0.91\n",
      "alexnet1d, trial.135:\n",
      "Epoch 31, avg test_loss: 0.028871, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号37个\n",
      "错误信号33个\n",
      "信号正确并预测正确的概率为0.229\n",
      "信号错误并预测正确的概率为0.3\n",
      "总正确率为0.53\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012328, train_acc: 0.64\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.011672, train_acc: 0.62\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012211, train_acc: 0.54\n",
      "alexnet1d, trial.136:\n",
      "Epoch 0, avg test_loss: 0.010108, test_acc: 0.40\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012701, train_acc: 0.43\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012128, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012039, train_acc: 0.64\n",
      "alexnet1d, trial.136:\n",
      "Epoch 1, avg test_loss: 0.009785, test_acc: 0.56\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012440, train_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011895, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.013421, train_acc: 0.50\n",
      "alexnet1d, trial.136:\n",
      "Epoch 2, avg test_loss: 0.009729, test_acc: 0.56\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012467, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012346, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011848, train_acc: 0.64\n",
      "alexnet1d, trial.136:\n",
      "Epoch 3, avg test_loss: 0.009800, test_acc: 0.56\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012453, train_acc: 0.52\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012412, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012168, train_acc: 0.55\n",
      "alexnet1d, trial.136:\n",
      "Epoch 4, avg test_loss: 0.009786, test_acc: 0.56\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012066, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012151, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012273, train_acc: 0.57\n",
      "alexnet1d, trial.136:\n",
      "Epoch 5, avg test_loss: 0.009760, test_acc: 0.56\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011209, train_acc: 0.68\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012382, train_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012136, train_acc: 0.59\n",
      "alexnet1d, trial.136:\n",
      "Epoch 6, avg test_loss: 0.009633, test_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.010732, train_acc: 0.70\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011704, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011596, train_acc: 0.62\n",
      "alexnet1d, trial.136:\n",
      "Epoch 7, avg test_loss: 0.009810, test_acc: 0.56\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011265, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011657, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012169, train_acc: 0.57\n",
      "alexnet1d, trial.136:\n",
      "Epoch 8, avg test_loss: 0.009610, test_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010394, train_acc: 0.77\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010913, train_acc: 0.73\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.013404, train_acc: 0.48\n",
      "alexnet1d, trial.136:\n",
      "Epoch 9, avg test_loss: 0.009666, test_acc: 0.60\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010819, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011505, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011843, train_acc: 0.54\n",
      "alexnet1d, trial.136:\n",
      "Epoch 10, avg test_loss: 0.009523, test_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.012559, train_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011321, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010375, train_acc: 0.62\n",
      "alexnet1d, trial.136:\n",
      "Epoch 11, avg test_loss: 0.009848, test_acc: 0.54\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011216, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.008800, train_acc: 0.75\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.012256, train_acc: 0.62\n",
      "alexnet1d, trial.136:\n",
      "Epoch 12, avg test_loss: 0.009692, test_acc: 0.56\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011214, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010176, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009750, train_acc: 0.71\n",
      "alexnet1d, trial.136:\n",
      "Epoch 13, avg test_loss: 0.010061, test_acc: 0.60\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010008, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011139, train_acc: 0.62\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010648, train_acc: 0.61\n",
      "alexnet1d, trial.136:\n",
      "Epoch 14, avg test_loss: 0.009858, test_acc: 0.61\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.012845, train_acc: 0.57\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.012319, train_acc: 0.55\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009384, train_acc: 0.77\n",
      "alexnet1d, trial.136:\n",
      "Epoch 15, avg test_loss: 0.010056, test_acc: 0.64\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010894, train_acc: 0.68\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010809, train_acc: 0.64\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010034, train_acc: 0.70\n",
      "alexnet1d, trial.136:\n",
      "Epoch 16, avg test_loss: 0.009458, test_acc: 0.63\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009545, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009487, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009270, train_acc: 0.73\n",
      "alexnet1d, trial.136:\n",
      "Epoch 17, avg test_loss: 0.009125, test_acc: 0.69\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009529, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007490, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009227, train_acc: 0.75\n",
      "alexnet1d, trial.136:\n",
      "Epoch 18, avg test_loss: 0.009622, test_acc: 0.67\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008847, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.010219, train_acc: 0.68\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007978, train_acc: 0.84\n",
      "alexnet1d, trial.136:\n",
      "Epoch 19, avg test_loss: 0.012354, test_acc: 0.51\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007696, train_acc: 0.75\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.006285, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008830, train_acc: 0.73\n",
      "alexnet1d, trial.136:\n",
      "Epoch 20, avg test_loss: 0.009782, test_acc: 0.70\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007080, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006684, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006935, train_acc: 0.84\n",
      "alexnet1d, trial.136:\n",
      "Epoch 21, avg test_loss: 0.010747, test_acc: 0.64\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007456, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.008317, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006380, train_acc: 0.88\n",
      "alexnet1d, trial.136:\n",
      "Epoch 22, avg test_loss: 0.010828, test_acc: 0.63\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005789, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006769, train_acc: 0.77\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006497, train_acc: 0.84\n",
      "alexnet1d, trial.136:\n",
      "Epoch 23, avg test_loss: 0.010235, test_acc: 0.64\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005960, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.007286, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005500, train_acc: 0.86\n",
      "alexnet1d, trial.136:\n",
      "Epoch 24, avg test_loss: 0.012438, test_acc: 0.66\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005850, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.003591, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.007334, train_acc: 0.86\n",
      "alexnet1d, trial.136:\n",
      "Epoch 25, avg test_loss: 0.013047, test_acc: 0.61\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004410, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.003753, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.006430, train_acc: 0.82\n",
      "alexnet1d, trial.136:\n",
      "Epoch 26, avg test_loss: 0.011911, test_acc: 0.71\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.002588, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.003964, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003233, train_acc: 0.95\n",
      "alexnet1d, trial.136:\n",
      "Epoch 27, avg test_loss: 0.012856, test_acc: 0.70\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003404, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.007080, train_acc: 0.80\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.005603, train_acc: 0.89\n",
      "alexnet1d, trial.136:\n",
      "Epoch 28, avg test_loss: 0.016871, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.004124, train_acc: 0.91\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.005298, train_acc: 0.89\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.003913, train_acc: 0.93\n",
      "alexnet1d, trial.136:\n",
      "Epoch 29, avg test_loss: 0.015477, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.004989, train_acc: 0.88\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.002663, train_acc: 0.93\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.004624, train_acc: 0.89\n",
      "alexnet1d, trial.136:\n",
      "Epoch 30, avg test_loss: 0.014406, test_acc: 0.66\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号39个\n",
      "错误信号31个\n",
      "信号正确并预测正确的概率为0.371\n",
      "信号错误并预测正确的概率为0.286\n",
      "总正确率为0.66\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012447, train_acc: 0.38\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012538, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012493, train_acc: 0.52\n",
      "alexnet1d, trial.137:\n",
      "Epoch 0, avg test_loss: 0.009742, test_acc: 0.60\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012263, train_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012459, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012307, train_acc: 0.52\n",
      "alexnet1d, trial.137:\n",
      "Epoch 1, avg test_loss: 0.009674, test_acc: 0.60\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012539, train_acc: 0.48\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012274, train_acc: 0.52\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011941, train_acc: 0.70\n",
      "alexnet1d, trial.137:\n",
      "Epoch 2, avg test_loss: 0.009610, test_acc: 0.60\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012244, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011726, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011799, train_acc: 0.61\n",
      "alexnet1d, trial.137:\n",
      "Epoch 3, avg test_loss: 0.009874, test_acc: 0.60\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012833, train_acc: 0.48\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012101, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011465, train_acc: 0.71\n",
      "alexnet1d, trial.137:\n",
      "Epoch 4, avg test_loss: 0.009829, test_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011619, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011694, train_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011918, train_acc: 0.54\n",
      "alexnet1d, trial.137:\n",
      "Epoch 5, avg test_loss: 0.009955, test_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011546, train_acc: 0.70\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012011, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011508, train_acc: 0.70\n",
      "alexnet1d, trial.137:\n",
      "Epoch 6, avg test_loss: 0.009717, test_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011047, train_acc: 0.68\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011915, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011887, train_acc: 0.61\n",
      "alexnet1d, trial.137:\n",
      "Epoch 7, avg test_loss: 0.009847, test_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010913, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.010249, train_acc: 0.73\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012747, train_acc: 0.62\n",
      "alexnet1d, trial.137:\n",
      "Epoch 8, avg test_loss: 0.010351, test_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012263, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011202, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012033, train_acc: 0.61\n",
      "alexnet1d, trial.137:\n",
      "Epoch 9, avg test_loss: 0.009180, test_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011439, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010721, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010910, train_acc: 0.70\n",
      "alexnet1d, trial.137:\n",
      "Epoch 10, avg test_loss: 0.010076, test_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.009610, train_acc: 0.75\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.012262, train_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.009728, train_acc: 0.71\n",
      "alexnet1d, trial.137:\n",
      "Epoch 11, avg test_loss: 0.009970, test_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009545, train_acc: 0.77\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010916, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.009560, train_acc: 0.77\n",
      "alexnet1d, trial.137:\n",
      "Epoch 12, avg test_loss: 0.009826, test_acc: 0.57\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009772, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.008503, train_acc: 0.77\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.013684, train_acc: 0.66\n",
      "alexnet1d, trial.137:\n",
      "Epoch 13, avg test_loss: 0.011180, test_acc: 0.60\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.006351, train_acc: 0.88\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009120, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011772, train_acc: 0.61\n",
      "alexnet1d, trial.137:\n",
      "Epoch 14, avg test_loss: 0.011690, test_acc: 0.60\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009128, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008974, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010509, train_acc: 0.75\n",
      "alexnet1d, trial.137:\n",
      "Epoch 15, avg test_loss: 0.010353, test_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.007034, train_acc: 0.88\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010019, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.007834, train_acc: 0.84\n",
      "alexnet1d, trial.137:\n",
      "Epoch 16, avg test_loss: 0.012030, test_acc: 0.60\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008516, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007978, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007493, train_acc: 0.80\n",
      "alexnet1d, trial.137:\n",
      "Epoch 17, avg test_loss: 0.014103, test_acc: 0.56\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.006286, train_acc: 0.86\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.012333, train_acc: 0.64\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008280, train_acc: 0.82\n",
      "alexnet1d, trial.137:\n",
      "Epoch 18, avg test_loss: 0.015021, test_acc: 0.53\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006997, train_acc: 0.88\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.006520, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008053, train_acc: 0.80\n",
      "alexnet1d, trial.137:\n",
      "Epoch 19, avg test_loss: 0.013964, test_acc: 0.56\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006468, train_acc: 0.88\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.005323, train_acc: 0.88\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.005911, train_acc: 0.88\n",
      "alexnet1d, trial.137:\n",
      "Epoch 20, avg test_loss: 0.013552, test_acc: 0.63\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.005610, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006379, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.004932, train_acc: 0.89\n",
      "alexnet1d, trial.137:\n",
      "Epoch 21, avg test_loss: 0.018153, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005535, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.004344, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.003103, train_acc: 0.93\n",
      "alexnet1d, trial.137:\n",
      "Epoch 22, avg test_loss: 0.017328, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.004657, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.002714, train_acc: 0.95\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.003957, train_acc: 0.91\n",
      "alexnet1d, trial.137:\n",
      "Epoch 23, avg test_loss: 0.021742, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.003838, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004845, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.002100, train_acc: 0.98\n",
      "alexnet1d, trial.137:\n",
      "Epoch 24, avg test_loss: 0.025007, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号42个\n",
      "错误信号28个\n",
      "信号正确并预测正确的概率为0.314\n",
      "信号错误并预测正确的概率为0.2\n",
      "总正确率为0.51\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012426, train_acc: 0.52\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012120, train_acc: 0.61\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012071, train_acc: 0.66\n",
      "alexnet1d, trial.138:\n",
      "Epoch 0, avg test_loss: 0.009788, test_acc: 0.60\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012244, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012081, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011809, train_acc: 0.62\n",
      "alexnet1d, trial.138:\n",
      "Epoch 1, avg test_loss: 0.009593, test_acc: 0.60\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012350, train_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012260, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012280, train_acc: 0.57\n",
      "alexnet1d, trial.138:\n",
      "Epoch 2, avg test_loss: 0.009539, test_acc: 0.60\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011782, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012932, train_acc: 0.55\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012270, train_acc: 0.57\n",
      "alexnet1d, trial.138:\n",
      "Epoch 3, avg test_loss: 0.009636, test_acc: 0.60\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012201, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012047, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012241, train_acc: 0.54\n",
      "alexnet1d, trial.138:\n",
      "Epoch 4, avg test_loss: 0.009617, test_acc: 0.60\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011611, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012252, train_acc: 0.48\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012161, train_acc: 0.55\n",
      "alexnet1d, trial.138:\n",
      "Epoch 5, avg test_loss: 0.009603, test_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012167, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012041, train_acc: 0.68\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011871, train_acc: 0.66\n",
      "alexnet1d, trial.138:\n",
      "Epoch 6, avg test_loss: 0.009546, test_acc: 0.60\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011730, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012024, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011520, train_acc: 0.75\n",
      "alexnet1d, trial.138:\n",
      "Epoch 7, avg test_loss: 0.009577, test_acc: 0.60\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011945, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011593, train_acc: 0.55\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.010430, train_acc: 0.75\n",
      "alexnet1d, trial.138:\n",
      "Epoch 8, avg test_loss: 0.009315, test_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011203, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010338, train_acc: 0.71\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011890, train_acc: 0.62\n",
      "alexnet1d, trial.138:\n",
      "Epoch 9, avg test_loss: 0.009589, test_acc: 0.60\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010588, train_acc: 0.71\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.009144, train_acc: 0.71\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011992, train_acc: 0.66\n",
      "alexnet1d, trial.138:\n",
      "Epoch 10, avg test_loss: 0.009560, test_acc: 0.60\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010940, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010115, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010405, train_acc: 0.68\n",
      "alexnet1d, trial.138:\n",
      "Epoch 11, avg test_loss: 0.010001, test_acc: 0.51\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.008987, train_acc: 0.79\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010006, train_acc: 0.77\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.009850, train_acc: 0.64\n",
      "alexnet1d, trial.138:\n",
      "Epoch 12, avg test_loss: 0.010327, test_acc: 0.57\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.007261, train_acc: 0.86\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010114, train_acc: 0.79\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009984, train_acc: 0.71\n",
      "alexnet1d, trial.138:\n",
      "Epoch 13, avg test_loss: 0.010630, test_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009294, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.008621, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010614, train_acc: 0.77\n",
      "alexnet1d, trial.138:\n",
      "Epoch 14, avg test_loss: 0.009436, test_acc: 0.69\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010484, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008413, train_acc: 0.86\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008413, train_acc: 0.79\n",
      "alexnet1d, trial.138:\n",
      "Epoch 15, avg test_loss: 0.011168, test_acc: 0.56\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008550, train_acc: 0.82\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.007639, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.006978, train_acc: 0.80\n",
      "alexnet1d, trial.138:\n",
      "Epoch 16, avg test_loss: 0.011191, test_acc: 0.59\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.006181, train_acc: 0.86\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008954, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.004926, train_acc: 0.91\n",
      "alexnet1d, trial.138:\n",
      "Epoch 17, avg test_loss: 0.013307, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.006631, train_acc: 0.91\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008546, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.005826, train_acc: 0.89\n",
      "alexnet1d, trial.138:\n",
      "Epoch 18, avg test_loss: 0.012836, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006054, train_acc: 0.88\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.004876, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008242, train_acc: 0.82\n",
      "alexnet1d, trial.138:\n",
      "Epoch 19, avg test_loss: 0.012984, test_acc: 0.59\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.005002, train_acc: 0.93\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.005982, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.003961, train_acc: 0.93\n",
      "alexnet1d, trial.138:\n",
      "Epoch 20, avg test_loss: 0.012569, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.004562, train_acc: 0.89\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.004826, train_acc: 0.91\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.003993, train_acc: 0.93\n",
      "alexnet1d, trial.138:\n",
      "Epoch 21, avg test_loss: 0.014247, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号42个\n",
      "错误信号28个\n",
      "信号正确并预测正确的概率为0.414\n",
      "信号错误并预测正确的概率为0.157\n",
      "总正确率为0.57\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012501, train_acc: 0.34\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.013098, train_acc: 0.64\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012493, train_acc: 0.46\n",
      "alexnet1d, trial.139:\n",
      "Epoch 0, avg test_loss: 0.009894, test_acc: 0.51\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012304, train_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012282, train_acc: 0.66\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012251, train_acc: 0.57\n",
      "alexnet1d, trial.139:\n",
      "Epoch 1, avg test_loss: 0.009901, test_acc: 0.51\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011970, train_acc: 0.68\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011976, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011861, train_acc: 0.64\n",
      "alexnet1d, trial.139:\n",
      "Epoch 2, avg test_loss: 0.010061, test_acc: 0.51\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012434, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011983, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012528, train_acc: 0.54\n",
      "alexnet1d, trial.139:\n",
      "Epoch 3, avg test_loss: 0.010202, test_acc: 0.51\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011795, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011370, train_acc: 0.68\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011813, train_acc: 0.61\n",
      "alexnet1d, trial.139:\n",
      "Epoch 4, avg test_loss: 0.010083, test_acc: 0.51\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012706, train_acc: 0.52\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011617, train_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012193, train_acc: 0.57\n",
      "alexnet1d, trial.139:\n",
      "Epoch 5, avg test_loss: 0.009999, test_acc: 0.51\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012141, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012083, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011938, train_acc: 0.61\n",
      "alexnet1d, trial.139:\n",
      "Epoch 6, avg test_loss: 0.010109, test_acc: 0.51\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011172, train_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012106, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012292, train_acc: 0.52\n",
      "alexnet1d, trial.139:\n",
      "Epoch 7, avg test_loss: 0.010091, test_acc: 0.51\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012173, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011751, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011437, train_acc: 0.62\n",
      "alexnet1d, trial.139:\n",
      "Epoch 8, avg test_loss: 0.009995, test_acc: 0.51\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011556, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011135, train_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011716, train_acc: 0.48\n",
      "alexnet1d, trial.139:\n",
      "Epoch 9, avg test_loss: 0.010251, test_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011105, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012711, train_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012420, train_acc: 0.57\n",
      "alexnet1d, trial.139:\n",
      "Epoch 10, avg test_loss: 0.010213, test_acc: 0.47\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010975, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011915, train_acc: 0.54\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011276, train_acc: 0.68\n",
      "alexnet1d, trial.139:\n",
      "Epoch 11, avg test_loss: 0.010441, test_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010221, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011819, train_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.012570, train_acc: 0.62\n",
      "alexnet1d, trial.139:\n",
      "Epoch 12, avg test_loss: 0.011371, test_acc: 0.51\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010876, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009449, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.013211, train_acc: 0.64\n",
      "alexnet1d, trial.139:\n",
      "Epoch 13, avg test_loss: 0.011629, test_acc: 0.49\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010914, train_acc: 0.79\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011895, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010797, train_acc: 0.70\n",
      "alexnet1d, trial.139:\n",
      "Epoch 14, avg test_loss: 0.010652, test_acc: 0.41\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009453, train_acc: 0.84\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.011639, train_acc: 0.68\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011729, train_acc: 0.68\n",
      "alexnet1d, trial.139:\n",
      "Epoch 15, avg test_loss: 0.010772, test_acc: 0.49\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009976, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009835, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010163, train_acc: 0.70\n",
      "alexnet1d, trial.139:\n",
      "Epoch 16, avg test_loss: 0.012445, test_acc: 0.54\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007633, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009926, train_acc: 0.66\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009641, train_acc: 0.70\n",
      "alexnet1d, trial.139:\n",
      "Epoch 17, avg test_loss: 0.013442, test_acc: 0.44\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007248, train_acc: 0.86\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009826, train_acc: 0.66\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.012344, train_acc: 0.62\n",
      "alexnet1d, trial.139:\n",
      "Epoch 18, avg test_loss: 0.013121, test_acc: 0.46\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008494, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.011608, train_acc: 0.70\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009126, train_acc: 0.71\n",
      "alexnet1d, trial.139:\n",
      "Epoch 19, avg test_loss: 0.011612, test_acc: 0.51\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.009264, train_acc: 0.73\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008437, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008066, train_acc: 0.75\n",
      "alexnet1d, trial.139:\n",
      "Epoch 20, avg test_loss: 0.013048, test_acc: 0.47\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008942, train_acc: 0.75\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007589, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007995, train_acc: 0.79\n",
      "alexnet1d, trial.139:\n",
      "Epoch 21, avg test_loss: 0.015903, test_acc: 0.49\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007315, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006368, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006595, train_acc: 0.86\n",
      "alexnet1d, trial.139:\n",
      "Epoch 22, avg test_loss: 0.017676, test_acc: 0.51\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007318, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006341, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.005451, train_acc: 0.89\n",
      "alexnet1d, trial.139:\n",
      "Epoch 23, avg test_loss: 0.018065, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.007738, train_acc: 0.77\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004461, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005343, train_acc: 0.89\n",
      "alexnet1d, trial.139:\n",
      "Epoch 24, avg test_loss: 0.022295, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005444, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.003574, train_acc: 0.95\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.008224, train_acc: 0.86\n",
      "alexnet1d, trial.139:\n",
      "Epoch 25, avg test_loss: 0.022753, test_acc: 0.49\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005910, train_acc: 0.86\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.006577, train_acc: 0.84\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004402, train_acc: 0.91\n",
      "alexnet1d, trial.139:\n",
      "Epoch 26, avg test_loss: 0.021435, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.005314, train_acc: 0.86\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.005522, train_acc: 0.86\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004018, train_acc: 0.91\n",
      "alexnet1d, trial.139:\n",
      "Epoch 27, avg test_loss: 0.026351, test_acc: 0.46\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号36个\n",
      "错误信号34个\n",
      "信号正确并预测正确的概率为0.271\n",
      "信号错误并预测正确的概率为0.186\n",
      "总正确率为0.46\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012412, train_acc: 0.38\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.014919, train_acc: 0.50\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012342, train_acc: 0.54\n",
      "alexnet1d, trial.140:\n",
      "Epoch 0, avg test_loss: 0.009848, test_acc: 0.66\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012334, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012318, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012518, train_acc: 0.50\n",
      "alexnet1d, trial.140:\n",
      "Epoch 1, avg test_loss: 0.009563, test_acc: 0.66\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012027, train_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012109, train_acc: 0.52\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012533, train_acc: 0.54\n",
      "alexnet1d, trial.140:\n",
      "Epoch 2, avg test_loss: 0.009515, test_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012088, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012258, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011910, train_acc: 0.54\n",
      "alexnet1d, trial.140:\n",
      "Epoch 3, avg test_loss: 0.009818, test_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011802, train_acc: 0.66\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.013428, train_acc: 0.45\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012155, train_acc: 0.55\n",
      "alexnet1d, trial.140:\n",
      "Epoch 4, avg test_loss: 0.009804, test_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012187, train_acc: 0.45\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012434, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011821, train_acc: 0.59\n",
      "alexnet1d, trial.140:\n",
      "Epoch 5, avg test_loss: 0.009794, test_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011734, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011269, train_acc: 0.75\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.015062, train_acc: 0.59\n",
      "alexnet1d, trial.140:\n",
      "Epoch 6, avg test_loss: 0.010481, test_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.010408, train_acc: 0.70\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012541, train_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012893, train_acc: 0.48\n",
      "alexnet1d, trial.140:\n",
      "Epoch 7, avg test_loss: 0.010109, test_acc: 0.51\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011533, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012159, train_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012071, train_acc: 0.57\n",
      "alexnet1d, trial.140:\n",
      "Epoch 8, avg test_loss: 0.009714, test_acc: 0.60\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011326, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011717, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012283, train_acc: 0.48\n",
      "alexnet1d, trial.140:\n",
      "Epoch 9, avg test_loss: 0.009887, test_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011259, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010861, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012444, train_acc: 0.57\n",
      "alexnet1d, trial.140:\n",
      "Epoch 10, avg test_loss: 0.011132, test_acc: 0.50\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011803, train_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010877, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010541, train_acc: 0.71\n",
      "alexnet1d, trial.140:\n",
      "Epoch 11, avg test_loss: 0.010206, test_acc: 0.60\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009616, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.013291, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.012471, train_acc: 0.61\n",
      "alexnet1d, trial.140:\n",
      "Epoch 12, avg test_loss: 0.011104, test_acc: 0.54\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011098, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.012181, train_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010809, train_acc: 0.62\n",
      "alexnet1d, trial.140:\n",
      "Epoch 13, avg test_loss: 0.009749, test_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009779, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011197, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011184, train_acc: 0.62\n",
      "alexnet1d, trial.140:\n",
      "Epoch 14, avg test_loss: 0.010486, test_acc: 0.54\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009984, train_acc: 0.62\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009625, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011498, train_acc: 0.64\n",
      "alexnet1d, trial.140:\n",
      "Epoch 15, avg test_loss: 0.011395, test_acc: 0.60\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010284, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.011156, train_acc: 0.64\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009073, train_acc: 0.77\n",
      "alexnet1d, trial.140:\n",
      "Epoch 16, avg test_loss: 0.011145, test_acc: 0.53\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009909, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009722, train_acc: 0.70\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009438, train_acc: 0.73\n",
      "alexnet1d, trial.140:\n",
      "Epoch 17, avg test_loss: 0.011922, test_acc: 0.57\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.010215, train_acc: 0.71\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007703, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.010884, train_acc: 0.71\n",
      "alexnet1d, trial.140:\n",
      "Epoch 18, avg test_loss: 0.011881, test_acc: 0.60\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007133, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007635, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.011457, train_acc: 0.66\n",
      "alexnet1d, trial.140:\n",
      "Epoch 19, avg test_loss: 0.012471, test_acc: 0.60\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008159, train_acc: 0.75\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008329, train_acc: 0.77\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008862, train_acc: 0.79\n",
      "alexnet1d, trial.140:\n",
      "Epoch 20, avg test_loss: 0.011416, test_acc: 0.61\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007648, train_acc: 0.77\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007870, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.011941, train_acc: 0.64\n",
      "alexnet1d, trial.140:\n",
      "Epoch 21, avg test_loss: 0.011609, test_acc: 0.63\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.008419, train_acc: 0.75\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007657, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.009342, train_acc: 0.80\n",
      "alexnet1d, trial.140:\n",
      "Epoch 22, avg test_loss: 0.013200, test_acc: 0.60\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007290, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007061, train_acc: 0.80\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.011103, train_acc: 0.71\n",
      "alexnet1d, trial.140:\n",
      "Epoch 23, avg test_loss: 0.012389, test_acc: 0.60\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.007560, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.008966, train_acc: 0.79\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.007921, train_acc: 0.77\n",
      "alexnet1d, trial.140:\n",
      "Epoch 24, avg test_loss: 0.011197, test_acc: 0.63\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.008230, train_acc: 0.79\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.007779, train_acc: 0.79\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006123, train_acc: 0.84\n",
      "alexnet1d, trial.140:\n",
      "Epoch 25, avg test_loss: 0.012320, test_acc: 0.63\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.007213, train_acc: 0.75\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005899, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.006600, train_acc: 0.88\n",
      "alexnet1d, trial.140:\n",
      "Epoch 26, avg test_loss: 0.016640, test_acc: 0.59\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.007043, train_acc: 0.86\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.007397, train_acc: 0.82\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.007072, train_acc: 0.82\n",
      "alexnet1d, trial.140:\n",
      "Epoch 27, avg test_loss: 0.016010, test_acc: 0.61\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.005084, train_acc: 0.89\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.006721, train_acc: 0.82\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.006648, train_acc: 0.80\n",
      "alexnet1d, trial.140:\n",
      "Epoch 28, avg test_loss: 0.014686, test_acc: 0.54\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.006645, train_acc: 0.84\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.005336, train_acc: 0.88\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.005153, train_acc: 0.88\n",
      "alexnet1d, trial.140:\n",
      "Epoch 29, avg test_loss: 0.016988, test_acc: 0.66\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.004977, train_acc: 0.88\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.005191, train_acc: 0.88\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.005997, train_acc: 0.86\n",
      "alexnet1d, trial.140:\n",
      "Epoch 30, avg test_loss: 0.017910, test_acc: 0.59\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.004578, train_acc: 0.89\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.004810, train_acc: 0.84\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.005779, train_acc: 0.88\n",
      "alexnet1d, trial.140:\n",
      "Epoch 31, avg test_loss: 0.017610, test_acc: 0.63\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.005974, train_acc: 0.88\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.004946, train_acc: 0.89\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.003144, train_acc: 0.93\n",
      "alexnet1d, trial.140:\n",
      "Epoch 32, avg test_loss: 0.017973, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 33, lr: 0.000377, 0/280, avg loss: 0.004259, train_acc: 0.89\n",
      "Train Epoch 33, lr: 0.000377, 112/280, avg loss: 0.003494, train_acc: 0.95\n",
      "Train Epoch 33, lr: 0.000377, 224/280, avg loss: 0.004477, train_acc: 0.88\n",
      "alexnet1d, trial.140:\n",
      "Epoch 33, avg test_loss: 0.019293, test_acc: 0.60\n",
      "Train Epoch 34, lr: 0.000377, 0/280, avg loss: 0.002281, train_acc: 0.95\n",
      "Train Epoch 34, lr: 0.000377, 112/280, avg loss: 0.001773, train_acc: 1.00\n",
      "Train Epoch 34, lr: 0.000377, 224/280, avg loss: 0.006259, train_acc: 0.89\n",
      "alexnet1d, trial.140:\n",
      "Epoch 34, avg test_loss: 0.025675, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 35, lr: 0.000321, 0/280, avg loss: 0.003020, train_acc: 0.91\n",
      "Train Epoch 35, lr: 0.000321, 112/280, avg loss: 0.002137, train_acc: 0.98\n",
      "Train Epoch 35, lr: 0.000321, 224/280, avg loss: 0.004954, train_acc: 0.88\n",
      "alexnet1d, trial.140:\n",
      "Epoch 35, avg test_loss: 0.022983, test_acc: 0.61\n",
      "Train Epoch 36, lr: 0.000321, 0/280, avg loss: 0.003550, train_acc: 0.91\n",
      "Train Epoch 36, lr: 0.000321, 112/280, avg loss: 0.003991, train_acc: 0.89\n",
      "Train Epoch 36, lr: 0.000321, 224/280, avg loss: 0.002315, train_acc: 0.95\n",
      "alexnet1d, trial.140:\n",
      "Epoch 36, avg test_loss: 0.024395, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 37, lr: 0.000321, 0/280, avg loss: 0.001970, train_acc: 0.95\n",
      "Train Epoch 37, lr: 0.000321, 112/280, avg loss: 0.006062, train_acc: 0.88\n",
      "Train Epoch 37, lr: 0.000321, 224/280, avg loss: 0.006132, train_acc: 0.91\n",
      "alexnet1d, trial.140:\n",
      "Epoch 37, avg test_loss: 0.021754, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号46个\n",
      "错误信号24个\n",
      "信号正确并预测正确的概率为0.386\n",
      "信号错误并预测正确的概率为0.157\n",
      "总正确率为0.54\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012140, train_acc: 0.71\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012324, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012119, train_acc: 0.59\n",
      "alexnet1d, trial.141:\n",
      "Epoch 0, avg test_loss: 0.009960, test_acc: 0.51\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011989, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012027, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012790, train_acc: 0.52\n",
      "alexnet1d, trial.141:\n",
      "Epoch 1, avg test_loss: 0.010339, test_acc: 0.51\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011716, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012123, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011963, train_acc: 0.61\n",
      "alexnet1d, trial.141:\n",
      "Epoch 2, avg test_loss: 0.009939, test_acc: 0.51\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012066, train_acc: 0.55\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012187, train_acc: 0.55\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012895, train_acc: 0.59\n",
      "alexnet1d, trial.141:\n",
      "Epoch 3, avg test_loss: 0.010160, test_acc: 0.51\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011587, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012054, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011903, train_acc: 0.64\n",
      "alexnet1d, trial.141:\n",
      "Epoch 4, avg test_loss: 0.009858, test_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011936, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011908, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011147, train_acc: 0.68\n",
      "alexnet1d, trial.141:\n",
      "Epoch 5, avg test_loss: 0.009991, test_acc: 0.53\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011970, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011027, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012045, train_acc: 0.55\n",
      "alexnet1d, trial.141:\n",
      "Epoch 6, avg test_loss: 0.009980, test_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011144, train_acc: 0.70\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011415, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012050, train_acc: 0.66\n",
      "alexnet1d, trial.141:\n",
      "Epoch 7, avg test_loss: 0.009708, test_acc: 0.63\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011528, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011910, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012161, train_acc: 0.55\n",
      "alexnet1d, trial.141:\n",
      "Epoch 8, avg test_loss: 0.009635, test_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011749, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010700, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010597, train_acc: 0.70\n",
      "alexnet1d, trial.141:\n",
      "Epoch 9, avg test_loss: 0.010552, test_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010970, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010781, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.009732, train_acc: 0.71\n",
      "alexnet1d, trial.141:\n",
      "Epoch 10, avg test_loss: 0.009554, test_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010054, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.009780, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.012938, train_acc: 0.55\n",
      "alexnet1d, trial.141:\n",
      "Epoch 11, avg test_loss: 0.010200, test_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.008872, train_acc: 0.77\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010240, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010204, train_acc: 0.70\n",
      "alexnet1d, trial.141:\n",
      "Epoch 12, avg test_loss: 0.010973, test_acc: 0.54\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009721, train_acc: 0.75\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010315, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009682, train_acc: 0.70\n",
      "alexnet1d, trial.141:\n",
      "Epoch 13, avg test_loss: 0.010039, test_acc: 0.57\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009794, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.008112, train_acc: 0.80\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010184, train_acc: 0.71\n",
      "alexnet1d, trial.141:\n",
      "Epoch 14, avg test_loss: 0.010851, test_acc: 0.53\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008460, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009788, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.007336, train_acc: 0.84\n",
      "alexnet1d, trial.141:\n",
      "Epoch 15, avg test_loss: 0.011532, test_acc: 0.60\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.007588, train_acc: 0.82\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008871, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010065, train_acc: 0.73\n",
      "alexnet1d, trial.141:\n",
      "Epoch 16, avg test_loss: 0.012742, test_acc: 0.56\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.005956, train_acc: 0.86\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.006489, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009142, train_acc: 0.71\n",
      "alexnet1d, trial.141:\n",
      "Epoch 17, avg test_loss: 0.014385, test_acc: 0.54\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.006637, train_acc: 0.88\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007018, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.006932, train_acc: 0.84\n",
      "alexnet1d, trial.141:\n",
      "Epoch 18, avg test_loss: 0.012387, test_acc: 0.67\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006381, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.006507, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.006800, train_acc: 0.86\n",
      "alexnet1d, trial.141:\n",
      "Epoch 19, avg test_loss: 0.015775, test_acc: 0.56\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.004617, train_acc: 0.93\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.004458, train_acc: 0.89\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006096, train_acc: 0.82\n",
      "alexnet1d, trial.141:\n",
      "Epoch 20, avg test_loss: 0.014683, test_acc: 0.54\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.003886, train_acc: 0.93\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.003721, train_acc: 0.91\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006094, train_acc: 0.86\n",
      "alexnet1d, trial.141:\n",
      "Epoch 21, avg test_loss: 0.018752, test_acc: 0.51\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005294, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.004444, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008186, train_acc: 0.79\n",
      "alexnet1d, trial.141:\n",
      "Epoch 22, avg test_loss: 0.023602, test_acc: 0.51\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.003677, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.008469, train_acc: 0.79\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.004641, train_acc: 0.88\n",
      "alexnet1d, trial.141:\n",
      "Epoch 23, avg test_loss: 0.019516, test_acc: 0.51\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004925, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.003422, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.004214, train_acc: 0.93\n",
      "alexnet1d, trial.141:\n",
      "Epoch 24, avg test_loss: 0.019941, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.003307, train_acc: 0.95\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004033, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.001815, train_acc: 0.98\n",
      "alexnet1d, trial.141:\n",
      "Epoch 25, avg test_loss: 0.021822, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.001631, train_acc: 0.98\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.002743, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.001079, train_acc: 0.98\n",
      "alexnet1d, trial.141:\n",
      "Epoch 26, avg test_loss: 0.027566, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.001478, train_acc: 0.96\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.001155, train_acc: 0.98\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.002071, train_acc: 0.95\n",
      "alexnet1d, trial.141:\n",
      "Epoch 27, avg test_loss: 0.033061, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号36个\n",
      "错误信号34个\n",
      "信号正确并预测正确的概率为0.286\n",
      "信号错误并预测正确的概率为0.243\n",
      "总正确率为0.53\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012314, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.011934, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012203, train_acc: 0.55\n",
      "alexnet1d, trial.142:\n",
      "Epoch 0, avg test_loss: 0.009883, test_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012185, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012392, train_acc: 0.52\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012113, train_acc: 0.68\n",
      "alexnet1d, trial.142:\n",
      "Epoch 1, avg test_loss: 0.009831, test_acc: 0.56\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012443, train_acc: 0.48\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012165, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012349, train_acc: 0.55\n",
      "alexnet1d, trial.142:\n",
      "Epoch 2, avg test_loss: 0.009781, test_acc: 0.56\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012298, train_acc: 0.55\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012015, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011700, train_acc: 0.61\n",
      "alexnet1d, trial.142:\n",
      "Epoch 3, avg test_loss: 0.009874, test_acc: 0.56\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011490, train_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011579, train_acc: 0.66\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.013046, train_acc: 0.50\n",
      "alexnet1d, trial.142:\n",
      "Epoch 4, avg test_loss: 0.009882, test_acc: 0.56\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011977, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012206, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012259, train_acc: 0.55\n",
      "alexnet1d, trial.142:\n",
      "Epoch 5, avg test_loss: 0.009816, test_acc: 0.56\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011666, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011985, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012025, train_acc: 0.61\n",
      "alexnet1d, trial.142:\n",
      "Epoch 6, avg test_loss: 0.009876, test_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011486, train_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012047, train_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012736, train_acc: 0.59\n",
      "alexnet1d, trial.142:\n",
      "Epoch 7, avg test_loss: 0.010150, test_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012572, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011715, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.010995, train_acc: 0.71\n",
      "alexnet1d, trial.142:\n",
      "Epoch 8, avg test_loss: 0.010035, test_acc: 0.53\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011510, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011347, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010958, train_acc: 0.66\n",
      "alexnet1d, trial.142:\n",
      "Epoch 9, avg test_loss: 0.010424, test_acc: 0.56\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.009985, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.009741, train_acc: 0.71\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011679, train_acc: 0.64\n",
      "alexnet1d, trial.142:\n",
      "Epoch 10, avg test_loss: 0.010595, test_acc: 0.54\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010345, train_acc: 0.73\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010613, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.012649, train_acc: 0.61\n",
      "alexnet1d, trial.142:\n",
      "Epoch 11, avg test_loss: 0.009771, test_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011662, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010881, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010168, train_acc: 0.68\n",
      "alexnet1d, trial.142:\n",
      "Epoch 12, avg test_loss: 0.011768, test_acc: 0.49\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009951, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011805, train_acc: 0.57\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011582, train_acc: 0.62\n",
      "alexnet1d, trial.142:\n",
      "Epoch 13, avg test_loss: 0.010076, test_acc: 0.59\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.011118, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010701, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009579, train_acc: 0.73\n",
      "alexnet1d, trial.142:\n",
      "Epoch 14, avg test_loss: 0.010950, test_acc: 0.59\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008111, train_acc: 0.84\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009554, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011859, train_acc: 0.66\n",
      "alexnet1d, trial.142:\n",
      "Epoch 15, avg test_loss: 0.012359, test_acc: 0.54\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008387, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008440, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.011105, train_acc: 0.73\n",
      "alexnet1d, trial.142:\n",
      "Epoch 16, avg test_loss: 0.012609, test_acc: 0.59\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008818, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009854, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008592, train_acc: 0.73\n",
      "alexnet1d, trial.142:\n",
      "Epoch 17, avg test_loss: 0.014322, test_acc: 0.57\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007561, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008122, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009656, train_acc: 0.73\n",
      "alexnet1d, trial.142:\n",
      "Epoch 18, avg test_loss: 0.014224, test_acc: 0.56\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006251, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008439, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008956, train_acc: 0.80\n",
      "alexnet1d, trial.142:\n",
      "Epoch 19, avg test_loss: 0.014277, test_acc: 0.57\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006242, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008694, train_acc: 0.77\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007419, train_acc: 0.80\n",
      "alexnet1d, trial.142:\n",
      "Epoch 20, avg test_loss: 0.015408, test_acc: 0.56\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006799, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006687, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.005791, train_acc: 0.88\n",
      "alexnet1d, trial.142:\n",
      "Epoch 21, avg test_loss: 0.017020, test_acc: 0.61\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005390, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005146, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006529, train_acc: 0.84\n",
      "alexnet1d, trial.142:\n",
      "Epoch 22, avg test_loss: 0.018391, test_acc: 0.56\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.004985, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005922, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.004132, train_acc: 0.95\n",
      "alexnet1d, trial.142:\n",
      "Epoch 23, avg test_loss: 0.020372, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.003042, train_acc: 0.98\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006107, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005022, train_acc: 0.88\n",
      "alexnet1d, trial.142:\n",
      "Epoch 24, avg test_loss: 0.024021, test_acc: 0.56\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.006549, train_acc: 0.82\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005002, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005361, train_acc: 0.91\n",
      "alexnet1d, trial.142:\n",
      "Epoch 25, avg test_loss: 0.025818, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.003909, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004334, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.005723, train_acc: 0.88\n",
      "alexnet1d, trial.142:\n",
      "Epoch 26, avg test_loss: 0.027587, test_acc: 0.51\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.005025, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004004, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003198, train_acc: 0.91\n",
      "alexnet1d, trial.142:\n",
      "Epoch 27, avg test_loss: 0.027562, test_acc: 0.49\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.005572, train_acc: 0.88\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.002376, train_acc: 0.96\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.005316, train_acc: 0.86\n",
      "alexnet1d, trial.142:\n",
      "Epoch 28, avg test_loss: 0.028696, test_acc: 0.59\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.004577, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.006699, train_acc: 0.89\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.002499, train_acc: 0.93\n",
      "alexnet1d, trial.142:\n",
      "Epoch 29, avg test_loss: 0.031679, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号39个\n",
      "错误信号31个\n",
      "信号正确并预测正确的概率为0.371\n",
      "信号错误并预测正确的概率为0.214\n",
      "总正确率为0.59\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012205, train_acc: 0.66\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.013133, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012292, train_acc: 0.66\n",
      "alexnet1d, trial.143:\n",
      "Epoch 0, avg test_loss: 0.009848, test_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012207, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.011932, train_acc: 0.62\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011816, train_acc: 0.61\n",
      "alexnet1d, trial.143:\n",
      "Epoch 1, avg test_loss: 0.009891, test_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011916, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012489, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012145, train_acc: 0.57\n",
      "alexnet1d, trial.143:\n",
      "Epoch 2, avg test_loss: 0.009841, test_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012130, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012085, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011976, train_acc: 0.66\n",
      "alexnet1d, trial.143:\n",
      "Epoch 3, avg test_loss: 0.009847, test_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012523, train_acc: 0.46\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011981, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012003, train_acc: 0.57\n",
      "alexnet1d, trial.143:\n",
      "Epoch 4, avg test_loss: 0.009913, test_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012531, train_acc: 0.52\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011433, train_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011076, train_acc: 0.64\n",
      "alexnet1d, trial.143:\n",
      "Epoch 5, avg test_loss: 0.010057, test_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012169, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011278, train_acc: 0.66\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011807, train_acc: 0.52\n",
      "alexnet1d, trial.143:\n",
      "Epoch 6, avg test_loss: 0.010015, test_acc: 0.53\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012197, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011555, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011083, train_acc: 0.70\n",
      "alexnet1d, trial.143:\n",
      "Epoch 7, avg test_loss: 0.010459, test_acc: 0.53\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012322, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011219, train_acc: 0.70\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012332, train_acc: 0.64\n",
      "alexnet1d, trial.143:\n",
      "Epoch 8, avg test_loss: 0.009950, test_acc: 0.56\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011519, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011326, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011210, train_acc: 0.70\n",
      "alexnet1d, trial.143:\n",
      "Epoch 9, avg test_loss: 0.010429, test_acc: 0.56\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011423, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012646, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010011, train_acc: 0.73\n",
      "alexnet1d, trial.143:\n",
      "Epoch 10, avg test_loss: 0.010379, test_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010547, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010878, train_acc: 0.73\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010605, train_acc: 0.70\n",
      "alexnet1d, trial.143:\n",
      "Epoch 11, avg test_loss: 0.010090, test_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009417, train_acc: 0.79\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.009271, train_acc: 0.75\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010432, train_acc: 0.68\n",
      "alexnet1d, trial.143:\n",
      "Epoch 12, avg test_loss: 0.010933, test_acc: 0.56\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009418, train_acc: 0.77\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009164, train_acc: 0.77\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009724, train_acc: 0.79\n",
      "alexnet1d, trial.143:\n",
      "Epoch 13, avg test_loss: 0.010213, test_acc: 0.54\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.008682, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011030, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011966, train_acc: 0.59\n",
      "alexnet1d, trial.143:\n",
      "Epoch 14, avg test_loss: 0.010437, test_acc: 0.56\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009196, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.012024, train_acc: 0.57\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011566, train_acc: 0.66\n",
      "alexnet1d, trial.143:\n",
      "Epoch 15, avg test_loss: 0.011080, test_acc: 0.63\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009585, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010515, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009238, train_acc: 0.79\n",
      "alexnet1d, trial.143:\n",
      "Epoch 16, avg test_loss: 0.010319, test_acc: 0.54\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009350, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009157, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008670, train_acc: 0.79\n",
      "alexnet1d, trial.143:\n",
      "Epoch 17, avg test_loss: 0.010893, test_acc: 0.53\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.010582, train_acc: 0.68\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007803, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.010350, train_acc: 0.64\n",
      "alexnet1d, trial.143:\n",
      "Epoch 18, avg test_loss: 0.011876, test_acc: 0.60\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009165, train_acc: 0.75\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008907, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009953, train_acc: 0.73\n",
      "alexnet1d, trial.143:\n",
      "Epoch 19, avg test_loss: 0.010636, test_acc: 0.57\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008540, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.009811, train_acc: 0.73\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008358, train_acc: 0.86\n",
      "alexnet1d, trial.143:\n",
      "Epoch 20, avg test_loss: 0.011003, test_acc: 0.61\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006583, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006967, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007709, train_acc: 0.82\n",
      "alexnet1d, trial.143:\n",
      "Epoch 21, avg test_loss: 0.011093, test_acc: 0.60\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007126, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005001, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008517, train_acc: 0.82\n",
      "alexnet1d, trial.143:\n",
      "Epoch 22, avg test_loss: 0.012036, test_acc: 0.64\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007518, train_acc: 0.80\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.009902, train_acc: 0.71\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.009469, train_acc: 0.71\n",
      "alexnet1d, trial.143:\n",
      "Epoch 23, avg test_loss: 0.012420, test_acc: 0.57\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.008084, train_acc: 0.80\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006199, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006391, train_acc: 0.88\n",
      "alexnet1d, trial.143:\n",
      "Epoch 24, avg test_loss: 0.011000, test_acc: 0.67\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005900, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004664, train_acc: 0.96\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.009195, train_acc: 0.73\n",
      "alexnet1d, trial.143:\n",
      "Epoch 25, avg test_loss: 0.011904, test_acc: 0.60\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004895, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005847, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.005698, train_acc: 0.86\n",
      "alexnet1d, trial.143:\n",
      "Epoch 26, avg test_loss: 0.011911, test_acc: 0.56\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003827, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.007030, train_acc: 0.80\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.005218, train_acc: 0.86\n",
      "alexnet1d, trial.143:\n",
      "Epoch 27, avg test_loss: 0.013033, test_acc: 0.61\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004745, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.005358, train_acc: 0.89\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.006993, train_acc: 0.82\n",
      "alexnet1d, trial.143:\n",
      "Epoch 28, avg test_loss: 0.013317, test_acc: 0.60\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.004053, train_acc: 0.89\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.004877, train_acc: 0.89\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.004261, train_acc: 0.91\n",
      "alexnet1d, trial.143:\n",
      "Epoch 29, avg test_loss: 0.012527, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.004040, train_acc: 0.93\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.002758, train_acc: 0.98\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.003270, train_acc: 0.95\n",
      "alexnet1d, trial.143:\n",
      "Epoch 30, avg test_loss: 0.014504, test_acc: 0.66\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.003138, train_acc: 0.91\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.004595, train_acc: 0.91\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.001927, train_acc: 0.96\n",
      "alexnet1d, trial.143:\n",
      "Epoch 31, avg test_loss: 0.015178, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.001742, train_acc: 0.96\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.002686, train_acc: 0.95\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.001887, train_acc: 0.96\n",
      "alexnet1d, trial.143:\n",
      "Epoch 32, avg test_loss: 0.020318, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号38个\n",
      "错误信号32个\n",
      "信号正确并预测正确的概率为0.443\n",
      "信号错误并预测正确的概率为0.171\n",
      "总正确率为0.61\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012282, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012202, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.020026, train_acc: 0.55\n",
      "alexnet1d, trial.144:\n",
      "Epoch 0, avg test_loss: 0.009718, test_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012036, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012768, train_acc: 0.32\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012216, train_acc: 0.66\n",
      "alexnet1d, trial.144:\n",
      "Epoch 1, avg test_loss: 0.009845, test_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012349, train_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012169, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012229, train_acc: 0.57\n",
      "alexnet1d, trial.144:\n",
      "Epoch 2, avg test_loss: 0.009739, test_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012405, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011969, train_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012039, train_acc: 0.61\n",
      "alexnet1d, trial.144:\n",
      "Epoch 3, avg test_loss: 0.009668, test_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012014, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012817, train_acc: 0.50\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012211, train_acc: 0.57\n",
      "alexnet1d, trial.144:\n",
      "Epoch 4, avg test_loss: 0.009664, test_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012356, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011739, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012489, train_acc: 0.50\n",
      "alexnet1d, trial.144:\n",
      "Epoch 5, avg test_loss: 0.009661, test_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012044, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011660, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011735, train_acc: 0.62\n",
      "alexnet1d, trial.144:\n",
      "Epoch 6, avg test_loss: 0.009618, test_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012696, train_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011269, train_acc: 0.70\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012285, train_acc: 0.48\n",
      "alexnet1d, trial.144:\n",
      "Epoch 7, avg test_loss: 0.009595, test_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011726, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011434, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012861, train_acc: 0.45\n",
      "alexnet1d, trial.144:\n",
      "Epoch 8, avg test_loss: 0.009564, test_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012941, train_acc: 0.46\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011547, train_acc: 0.73\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011806, train_acc: 0.62\n",
      "alexnet1d, trial.144:\n",
      "Epoch 9, avg test_loss: 0.009590, test_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011848, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011578, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012265, train_acc: 0.61\n",
      "alexnet1d, trial.144:\n",
      "Epoch 10, avg test_loss: 0.009565, test_acc: 0.60\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010441, train_acc: 0.75\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010469, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.012362, train_acc: 0.62\n",
      "alexnet1d, trial.144:\n",
      "Epoch 11, avg test_loss: 0.010070, test_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011782, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010964, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011145, train_acc: 0.57\n",
      "alexnet1d, trial.144:\n",
      "Epoch 12, avg test_loss: 0.009621, test_acc: 0.60\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011490, train_acc: 0.57\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.012119, train_acc: 0.54\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011075, train_acc: 0.66\n",
      "alexnet1d, trial.144:\n",
      "Epoch 13, avg test_loss: 0.009421, test_acc: 0.63\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010377, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010610, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.012467, train_acc: 0.54\n",
      "alexnet1d, trial.144:\n",
      "Epoch 14, avg test_loss: 0.010436, test_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010430, train_acc: 0.68\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010964, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010172, train_acc: 0.66\n",
      "alexnet1d, trial.144:\n",
      "Epoch 15, avg test_loss: 0.009569, test_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010695, train_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.011354, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010236, train_acc: 0.73\n",
      "alexnet1d, trial.144:\n",
      "Epoch 16, avg test_loss: 0.010764, test_acc: 0.56\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009500, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.011736, train_acc: 0.66\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.012585, train_acc: 0.64\n",
      "alexnet1d, trial.144:\n",
      "Epoch 17, avg test_loss: 0.010205, test_acc: 0.60\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009043, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.010733, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.010375, train_acc: 0.66\n",
      "alexnet1d, trial.144:\n",
      "Epoch 18, avg test_loss: 0.010422, test_acc: 0.61\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008415, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.012500, train_acc: 0.61\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008293, train_acc: 0.84\n",
      "alexnet1d, trial.144:\n",
      "Epoch 19, avg test_loss: 0.010362, test_acc: 0.60\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008636, train_acc: 0.75\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.009999, train_acc: 0.70\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009931, train_acc: 0.68\n",
      "alexnet1d, trial.144:\n",
      "Epoch 20, avg test_loss: 0.010923, test_acc: 0.60\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006998, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.008494, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.010631, train_acc: 0.68\n",
      "alexnet1d, trial.144:\n",
      "Epoch 21, avg test_loss: 0.012771, test_acc: 0.63\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.009134, train_acc: 0.73\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.008347, train_acc: 0.79\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007673, train_acc: 0.84\n",
      "alexnet1d, trial.144:\n",
      "Epoch 22, avg test_loss: 0.011299, test_acc: 0.59\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007068, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.009373, train_acc: 0.79\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.008153, train_acc: 0.75\n",
      "alexnet1d, trial.144:\n",
      "Epoch 23, avg test_loss: 0.012594, test_acc: 0.59\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.007937, train_acc: 0.80\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.007624, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005533, train_acc: 0.91\n",
      "alexnet1d, trial.144:\n",
      "Epoch 24, avg test_loss: 0.014273, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.006954, train_acc: 0.80\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005240, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004663, train_acc: 0.89\n",
      "alexnet1d, trial.144:\n",
      "Epoch 25, avg test_loss: 0.016095, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.006096, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.003717, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.007170, train_acc: 0.84\n",
      "alexnet1d, trial.144:\n",
      "Epoch 26, avg test_loss: 0.017829, test_acc: 0.53\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003729, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.005915, train_acc: 0.86\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003260, train_acc: 0.93\n",
      "alexnet1d, trial.144:\n",
      "Epoch 27, avg test_loss: 0.018992, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003199, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.002859, train_acc: 0.96\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.005267, train_acc: 0.88\n",
      "alexnet1d, trial.144:\n",
      "Epoch 28, avg test_loss: 0.023020, test_acc: 0.50\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.003652, train_acc: 0.91\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.001516, train_acc: 0.98\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.003634, train_acc: 0.95\n",
      "alexnet1d, trial.144:\n",
      "Epoch 29, avg test_loss: 0.023283, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号41个\n",
      "错误信号29个\n",
      "信号正确并预测正确的概率为0.429\n",
      "信号错误并预测正确的概率为0.171\n",
      "总正确率为0.60\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012400, train_acc: 0.46\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012724, train_acc: 0.34\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.011807, train_acc: 0.61\n",
      "alexnet1d, trial.145:\n",
      "Epoch 0, avg test_loss: 0.010582, test_acc: 0.53\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.013342, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012957, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011776, train_acc: 0.64\n",
      "alexnet1d, trial.145:\n",
      "Epoch 1, avg test_loss: 0.009922, test_acc: 0.53\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011586, train_acc: 0.70\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012116, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012246, train_acc: 0.57\n",
      "alexnet1d, trial.145:\n",
      "Epoch 2, avg test_loss: 0.009997, test_acc: 0.53\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012401, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012271, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011990, train_acc: 0.62\n",
      "alexnet1d, trial.145:\n",
      "Epoch 3, avg test_loss: 0.009978, test_acc: 0.53\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012285, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012117, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011656, train_acc: 0.59\n",
      "alexnet1d, trial.145:\n",
      "Epoch 4, avg test_loss: 0.010051, test_acc: 0.53\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012021, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012454, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011601, train_acc: 0.59\n",
      "alexnet1d, trial.145:\n",
      "Epoch 5, avg test_loss: 0.009969, test_acc: 0.53\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011643, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012274, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011306, train_acc: 0.61\n",
      "alexnet1d, trial.145:\n",
      "Epoch 6, avg test_loss: 0.010023, test_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011298, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011471, train_acc: 0.68\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.013151, train_acc: 0.48\n",
      "alexnet1d, trial.145:\n",
      "Epoch 7, avg test_loss: 0.010018, test_acc: 0.56\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011624, train_acc: 0.68\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011672, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011386, train_acc: 0.68\n",
      "alexnet1d, trial.145:\n",
      "Epoch 8, avg test_loss: 0.010099, test_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011125, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011497, train_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012188, train_acc: 0.61\n",
      "alexnet1d, trial.145:\n",
      "Epoch 9, avg test_loss: 0.010018, test_acc: 0.60\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010737, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010195, train_acc: 0.75\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011432, train_acc: 0.66\n",
      "alexnet1d, trial.145:\n",
      "Epoch 10, avg test_loss: 0.010362, test_acc: 0.60\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.009483, train_acc: 0.75\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011434, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010005, train_acc: 0.68\n",
      "alexnet1d, trial.145:\n",
      "Epoch 11, avg test_loss: 0.010870, test_acc: 0.50\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010094, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.012096, train_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011978, train_acc: 0.64\n",
      "alexnet1d, trial.145:\n",
      "Epoch 12, avg test_loss: 0.009990, test_acc: 0.57\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010037, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010286, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.013325, train_acc: 0.62\n",
      "alexnet1d, trial.145:\n",
      "Epoch 13, avg test_loss: 0.011144, test_acc: 0.57\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.011082, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010620, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.008849, train_acc: 0.79\n",
      "alexnet1d, trial.145:\n",
      "Epoch 14, avg test_loss: 0.010703, test_acc: 0.54\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008949, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010287, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009719, train_acc: 0.70\n",
      "alexnet1d, trial.145:\n",
      "Epoch 15, avg test_loss: 0.010229, test_acc: 0.53\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009024, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009656, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008929, train_acc: 0.75\n",
      "alexnet1d, trial.145:\n",
      "Epoch 16, avg test_loss: 0.011094, test_acc: 0.57\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007957, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010549, train_acc: 0.68\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008357, train_acc: 0.84\n",
      "alexnet1d, trial.145:\n",
      "Epoch 17, avg test_loss: 0.012159, test_acc: 0.56\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009526, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008285, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008546, train_acc: 0.71\n",
      "alexnet1d, trial.145:\n",
      "Epoch 18, avg test_loss: 0.011906, test_acc: 0.60\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006967, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.010258, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.013462, train_acc: 0.61\n",
      "alexnet1d, trial.145:\n",
      "Epoch 19, avg test_loss: 0.012545, test_acc: 0.56\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006122, train_acc: 0.88\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007354, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007753, train_acc: 0.80\n",
      "alexnet1d, trial.145:\n",
      "Epoch 20, avg test_loss: 0.013025, test_acc: 0.50\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007038, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007227, train_acc: 0.77\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008223, train_acc: 0.79\n",
      "alexnet1d, trial.145:\n",
      "Epoch 21, avg test_loss: 0.012617, test_acc: 0.53\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007451, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007710, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.005743, train_acc: 0.86\n",
      "alexnet1d, trial.145:\n",
      "Epoch 22, avg test_loss: 0.013493, test_acc: 0.51\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006276, train_acc: 0.82\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006212, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.008660, train_acc: 0.75\n",
      "alexnet1d, trial.145:\n",
      "Epoch 23, avg test_loss: 0.016737, test_acc: 0.51\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006412, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004183, train_acc: 0.93\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006007, train_acc: 0.82\n",
      "alexnet1d, trial.145:\n",
      "Epoch 24, avg test_loss: 0.018511, test_acc: 0.50\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.003598, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.003901, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.007483, train_acc: 0.77\n",
      "alexnet1d, trial.145:\n",
      "Epoch 25, avg test_loss: 0.019142, test_acc: 0.54\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.003231, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005215, train_acc: 0.82\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.003257, train_acc: 0.93\n",
      "alexnet1d, trial.145:\n",
      "Epoch 26, avg test_loss: 0.022892, test_acc: 0.50\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003819, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.005545, train_acc: 0.84\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.005219, train_acc: 0.91\n",
      "alexnet1d, trial.145:\n",
      "Epoch 27, avg test_loss: 0.022971, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.002534, train_acc: 0.98\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.003300, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.006832, train_acc: 0.86\n",
      "alexnet1d, trial.145:\n",
      "Epoch 28, avg test_loss: 0.025893, test_acc: 0.51\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.001729, train_acc: 0.98\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.002292, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.004978, train_acc: 0.88\n",
      "alexnet1d, trial.145:\n",
      "Epoch 29, avg test_loss: 0.026204, test_acc: 0.57\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.002138, train_acc: 0.93\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.001348, train_acc: 0.98\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.002960, train_acc: 0.91\n",
      "alexnet1d, trial.145:\n",
      "Epoch 30, avg test_loss: 0.028096, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.001310, train_acc: 0.98\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.002212, train_acc: 0.95\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.001117, train_acc: 1.00\n",
      "alexnet1d, trial.145:\n",
      "Epoch 31, avg test_loss: 0.034412, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号37个\n",
      "错误信号33个\n",
      "信号正确并预测正确的概率为0.329\n",
      "信号错误并预测正确的概率为0.186\n",
      "总正确率为0.51\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012281, train_acc: 0.68\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.015023, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012229, train_acc: 0.55\n",
      "alexnet1d, trial.146:\n",
      "Epoch 0, avg test_loss: 0.009883, test_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012378, train_acc: 0.52\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012352, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012419, train_acc: 0.48\n",
      "alexnet1d, trial.146:\n",
      "Epoch 1, avg test_loss: 0.009818, test_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012241, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012097, train_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012142, train_acc: 0.57\n",
      "alexnet1d, trial.146:\n",
      "Epoch 2, avg test_loss: 0.009757, test_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011931, train_acc: 0.62\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011943, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.013018, train_acc: 0.52\n",
      "alexnet1d, trial.146:\n",
      "Epoch 3, avg test_loss: 0.009761, test_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012132, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012189, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011829, train_acc: 0.66\n",
      "alexnet1d, trial.146:\n",
      "Epoch 4, avg test_loss: 0.009773, test_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012135, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012141, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011976, train_acc: 0.61\n",
      "alexnet1d, trial.146:\n",
      "Epoch 5, avg test_loss: 0.009775, test_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011905, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012070, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011866, train_acc: 0.61\n",
      "alexnet1d, trial.146:\n",
      "Epoch 6, avg test_loss: 0.009911, test_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011342, train_acc: 0.71\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012208, train_acc: 0.55\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011842, train_acc: 0.61\n",
      "alexnet1d, trial.146:\n",
      "Epoch 7, avg test_loss: 0.010300, test_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012016, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011161, train_acc: 0.70\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011011, train_acc: 0.77\n",
      "alexnet1d, trial.146:\n",
      "Epoch 8, avg test_loss: 0.010461, test_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010751, train_acc: 0.71\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011864, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012136, train_acc: 0.54\n",
      "alexnet1d, trial.146:\n",
      "Epoch 9, avg test_loss: 0.010158, test_acc: 0.53\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010839, train_acc: 0.73\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010826, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012484, train_acc: 0.55\n",
      "alexnet1d, trial.146:\n",
      "Epoch 10, avg test_loss: 0.010605, test_acc: 0.54\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010202, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011517, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010668, train_acc: 0.66\n",
      "alexnet1d, trial.146:\n",
      "Epoch 11, avg test_loss: 0.010329, test_acc: 0.56\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011649, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010145, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010349, train_acc: 0.68\n",
      "alexnet1d, trial.146:\n",
      "Epoch 12, avg test_loss: 0.010321, test_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009739, train_acc: 0.79\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.012013, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011491, train_acc: 0.64\n",
      "alexnet1d, trial.146:\n",
      "Epoch 13, avg test_loss: 0.010293, test_acc: 0.50\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.011006, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010111, train_acc: 0.79\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010493, train_acc: 0.71\n",
      "alexnet1d, trial.146:\n",
      "Epoch 14, avg test_loss: 0.010383, test_acc: 0.56\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009909, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009440, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011411, train_acc: 0.70\n",
      "alexnet1d, trial.146:\n",
      "Epoch 15, avg test_loss: 0.011332, test_acc: 0.56\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010223, train_acc: 0.68\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009778, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009017, train_acc: 0.77\n",
      "alexnet1d, trial.146:\n",
      "Epoch 16, avg test_loss: 0.011216, test_acc: 0.59\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009265, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008285, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009419, train_acc: 0.73\n",
      "alexnet1d, trial.146:\n",
      "Epoch 17, avg test_loss: 0.013405, test_acc: 0.59\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.010534, train_acc: 0.70\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007709, train_acc: 0.88\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009059, train_acc: 0.75\n",
      "alexnet1d, trial.146:\n",
      "Epoch 18, avg test_loss: 0.012865, test_acc: 0.54\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007546, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008474, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009944, train_acc: 0.75\n",
      "alexnet1d, trial.146:\n",
      "Epoch 19, avg test_loss: 0.013125, test_acc: 0.57\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.005634, train_acc: 0.89\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008767, train_acc: 0.77\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008330, train_acc: 0.80\n",
      "alexnet1d, trial.146:\n",
      "Epoch 20, avg test_loss: 0.015710, test_acc: 0.56\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008230, train_acc: 0.75\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.008736, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006865, train_acc: 0.88\n",
      "alexnet1d, trial.146:\n",
      "Epoch 21, avg test_loss: 0.017133, test_acc: 0.53\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006309, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.004944, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.009073, train_acc: 0.77\n",
      "alexnet1d, trial.146:\n",
      "Epoch 22, avg test_loss: 0.014284, test_acc: 0.54\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.008545, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005472, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.007241, train_acc: 0.80\n",
      "alexnet1d, trial.146:\n",
      "Epoch 23, avg test_loss: 0.017182, test_acc: 0.56\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006576, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005922, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.007230, train_acc: 0.82\n",
      "alexnet1d, trial.146:\n",
      "Epoch 24, avg test_loss: 0.018615, test_acc: 0.54\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.003973, train_acc: 0.95\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004075, train_acc: 0.95\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.007895, train_acc: 0.80\n",
      "alexnet1d, trial.146:\n",
      "Epoch 25, avg test_loss: 0.019347, test_acc: 0.53\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005251, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004211, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004548, train_acc: 0.91\n",
      "alexnet1d, trial.146:\n",
      "Epoch 26, avg test_loss: 0.022435, test_acc: 0.49\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003047, train_acc: 0.96\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.005059, train_acc: 0.88\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004880, train_acc: 0.89\n",
      "alexnet1d, trial.146:\n",
      "Epoch 27, avg test_loss: 0.022623, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003603, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004113, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.002779, train_acc: 0.95\n",
      "alexnet1d, trial.146:\n",
      "Epoch 28, avg test_loss: 0.027698, test_acc: 0.50\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.000897, train_acc: 1.00\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.003637, train_acc: 0.91\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.001305, train_acc: 1.00\n",
      "alexnet1d, trial.146:\n",
      "Epoch 29, avg test_loss: 0.024269, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号40个\n",
      "错误信号30个\n",
      "信号正确并预测正确的概率为0.386\n",
      "信号错误并预测正确的概率为0.143\n",
      "总正确率为0.53\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012295, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012040, train_acc: 0.64\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.013416, train_acc: 0.52\n",
      "alexnet1d, trial.147:\n",
      "Epoch 0, avg test_loss: 0.009846, test_acc: 0.53\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012435, train_acc: 0.52\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012208, train_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012332, train_acc: 0.55\n",
      "alexnet1d, trial.147:\n",
      "Epoch 1, avg test_loss: 0.009872, test_acc: 0.53\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012084, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011981, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012839, train_acc: 0.54\n",
      "alexnet1d, trial.147:\n",
      "Epoch 2, avg test_loss: 0.010058, test_acc: 0.53\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012912, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011726, train_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012155, train_acc: 0.59\n",
      "alexnet1d, trial.147:\n",
      "Epoch 3, avg test_loss: 0.009909, test_acc: 0.53\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012229, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011844, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012145, train_acc: 0.57\n",
      "alexnet1d, trial.147:\n",
      "Epoch 4, avg test_loss: 0.009946, test_acc: 0.53\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012001, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012830, train_acc: 0.48\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012325, train_acc: 0.57\n",
      "alexnet1d, trial.147:\n",
      "Epoch 5, avg test_loss: 0.009989, test_acc: 0.53\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012299, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012575, train_acc: 0.50\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011579, train_acc: 0.64\n",
      "alexnet1d, trial.147:\n",
      "Epoch 6, avg test_loss: 0.009905, test_acc: 0.53\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011910, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012205, train_acc: 0.55\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012486, train_acc: 0.52\n",
      "alexnet1d, trial.147:\n",
      "Epoch 7, avg test_loss: 0.009949, test_acc: 0.53\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011420, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012347, train_acc: 0.50\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012024, train_acc: 0.54\n",
      "alexnet1d, trial.147:\n",
      "Epoch 8, avg test_loss: 0.009671, test_acc: 0.56\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011627, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011790, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012845, train_acc: 0.46\n",
      "alexnet1d, trial.147:\n",
      "Epoch 9, avg test_loss: 0.009788, test_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011560, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011572, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011671, train_acc: 0.52\n",
      "alexnet1d, trial.147:\n",
      "Epoch 10, avg test_loss: 0.009818, test_acc: 0.53\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011214, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011931, train_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011929, train_acc: 0.62\n",
      "alexnet1d, trial.147:\n",
      "Epoch 11, avg test_loss: 0.009546, test_acc: 0.60\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010865, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011848, train_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.012525, train_acc: 0.52\n",
      "alexnet1d, trial.147:\n",
      "Epoch 12, avg test_loss: 0.009267, test_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010745, train_acc: 0.75\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011190, train_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011632, train_acc: 0.66\n",
      "alexnet1d, trial.147:\n",
      "Epoch 13, avg test_loss: 0.009680, test_acc: 0.64\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.011813, train_acc: 0.57\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011786, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011065, train_acc: 0.71\n",
      "alexnet1d, trial.147:\n",
      "Epoch 14, avg test_loss: 0.009263, test_acc: 0.63\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.011109, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009949, train_acc: 0.68\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010331, train_acc: 0.66\n",
      "alexnet1d, trial.147:\n",
      "Epoch 15, avg test_loss: 0.009113, test_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010777, train_acc: 0.61\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009542, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008919, train_acc: 0.75\n",
      "alexnet1d, trial.147:\n",
      "Epoch 16, avg test_loss: 0.009199, test_acc: 0.64\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009770, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.011098, train_acc: 0.62\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008927, train_acc: 0.82\n",
      "alexnet1d, trial.147:\n",
      "Epoch 17, avg test_loss: 0.009631, test_acc: 0.69\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.010587, train_acc: 0.68\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008792, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.010171, train_acc: 0.77\n",
      "alexnet1d, trial.147:\n",
      "Epoch 18, avg test_loss: 0.010594, test_acc: 0.61\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.011567, train_acc: 0.64\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009002, train_acc: 0.70\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009527, train_acc: 0.73\n",
      "alexnet1d, trial.147:\n",
      "Epoch 19, avg test_loss: 0.008673, test_acc: 0.73\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008676, train_acc: 0.73\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.009020, train_acc: 0.77\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008364, train_acc: 0.82\n",
      "alexnet1d, trial.147:\n",
      "Epoch 20, avg test_loss: 0.010236, test_acc: 0.67\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008125, train_acc: 0.77\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.009521, train_acc: 0.77\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.010794, train_acc: 0.68\n",
      "alexnet1d, trial.147:\n",
      "Epoch 21, avg test_loss: 0.010979, test_acc: 0.69\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006854, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.008341, train_acc: 0.77\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.005746, train_acc: 0.88\n",
      "alexnet1d, trial.147:\n",
      "Epoch 22, avg test_loss: 0.011786, test_acc: 0.71\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006586, train_acc: 0.82\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.009172, train_acc: 0.79\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006335, train_acc: 0.82\n",
      "alexnet1d, trial.147:\n",
      "Epoch 23, avg test_loss: 0.010934, test_acc: 0.69\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006332, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006735, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005655, train_acc: 0.88\n",
      "alexnet1d, trial.147:\n",
      "Epoch 24, avg test_loss: 0.013337, test_acc: 0.73\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.003962, train_acc: 0.95\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005280, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.008575, train_acc: 0.80\n",
      "alexnet1d, trial.147:\n",
      "Epoch 25, avg test_loss: 0.015174, test_acc: 0.67\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004556, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.006242, train_acc: 0.86\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.006933, train_acc: 0.84\n",
      "alexnet1d, trial.147:\n",
      "Epoch 26, avg test_loss: 0.015043, test_acc: 0.70\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.005879, train_acc: 0.84\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.002533, train_acc: 0.96\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004697, train_acc: 0.91\n",
      "alexnet1d, trial.147:\n",
      "Epoch 27, avg test_loss: 0.015583, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003295, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004864, train_acc: 0.88\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.003750, train_acc: 0.95\n",
      "alexnet1d, trial.147:\n",
      "Epoch 28, avg test_loss: 0.015718, test_acc: 0.69\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.003204, train_acc: 0.96\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.003152, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.005701, train_acc: 0.84\n",
      "alexnet1d, trial.147:\n",
      "Epoch 29, avg test_loss: 0.022342, test_acc: 0.69\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.002902, train_acc: 0.96\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.006010, train_acc: 0.88\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.002637, train_acc: 0.96\n",
      "alexnet1d, trial.147:\n",
      "Epoch 30, avg test_loss: 0.021645, test_acc: 0.70\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.002605, train_acc: 0.96\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.001995, train_acc: 0.96\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.005826, train_acc: 0.88\n",
      "alexnet1d, trial.147:\n",
      "Epoch 31, avg test_loss: 0.020766, test_acc: 0.71\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.001574, train_acc: 0.96\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.003687, train_acc: 0.95\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.005573, train_acc: 0.88\n",
      "alexnet1d, trial.147:\n",
      "Epoch 32, avg test_loss: 0.025744, test_acc: 0.63\n",
      "Train Epoch 33, lr: 0.000377, 0/280, avg loss: 0.002746, train_acc: 0.96\n",
      "Train Epoch 33, lr: 0.000377, 112/280, avg loss: 0.002038, train_acc: 0.98\n",
      "Train Epoch 33, lr: 0.000377, 224/280, avg loss: 0.005062, train_acc: 0.88\n",
      "alexnet1d, trial.147:\n",
      "Epoch 33, avg test_loss: 0.022923, test_acc: 0.66\n",
      "Train Epoch 34, lr: 0.000377, 0/280, avg loss: 0.004121, train_acc: 0.89\n",
      "Train Epoch 34, lr: 0.000377, 112/280, avg loss: 0.002903, train_acc: 0.96\n",
      "Train Epoch 34, lr: 0.000377, 224/280, avg loss: 0.003775, train_acc: 0.95\n",
      "alexnet1d, trial.147:\n",
      "Epoch 34, avg test_loss: 0.027189, test_acc: 0.70\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号37个\n",
      "错误信号33个\n",
      "信号正确并预测正确的概率为0.286\n",
      "信号错误并预测正确的概率为0.414\n",
      "总正确率为0.70\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012382, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.013930, train_acc: 0.45\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012103, train_acc: 0.71\n",
      "alexnet1d, trial.148:\n",
      "Epoch 0, avg test_loss: 0.009795, test_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012395, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012514, train_acc: 0.46\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011974, train_acc: 0.61\n",
      "alexnet1d, trial.148:\n",
      "Epoch 1, avg test_loss: 0.009525, test_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011959, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012245, train_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012456, train_acc: 0.54\n",
      "alexnet1d, trial.148:\n",
      "Epoch 2, avg test_loss: 0.009580, test_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012325, train_acc: 0.55\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012658, train_acc: 0.38\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012051, train_acc: 0.62\n",
      "alexnet1d, trial.148:\n",
      "Epoch 3, avg test_loss: 0.009716, test_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012160, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011843, train_acc: 0.68\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012506, train_acc: 0.52\n",
      "alexnet1d, trial.148:\n",
      "Epoch 4, avg test_loss: 0.009505, test_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011874, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012495, train_acc: 0.52\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012235, train_acc: 0.54\n",
      "alexnet1d, trial.148:\n",
      "Epoch 5, avg test_loss: 0.009496, test_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011906, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011743, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012256, train_acc: 0.52\n",
      "alexnet1d, trial.148:\n",
      "Epoch 6, avg test_loss: 0.009487, test_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011904, train_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012194, train_acc: 0.52\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011667, train_acc: 0.62\n",
      "alexnet1d, trial.148:\n",
      "Epoch 7, avg test_loss: 0.009698, test_acc: 0.67\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011434, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011108, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011654, train_acc: 0.55\n",
      "alexnet1d, trial.148:\n",
      "Epoch 8, avg test_loss: 0.009778, test_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011755, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011663, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011291, train_acc: 0.66\n",
      "alexnet1d, trial.148:\n",
      "Epoch 9, avg test_loss: 0.010289, test_acc: 0.67\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.009751, train_acc: 0.70\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012096, train_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012108, train_acc: 0.68\n",
      "alexnet1d, trial.148:\n",
      "Epoch 10, avg test_loss: 0.009272, test_acc: 0.63\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010895, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011633, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010591, train_acc: 0.71\n",
      "alexnet1d, trial.148:\n",
      "Epoch 11, avg test_loss: 0.009181, test_acc: 0.63\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010939, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010948, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010606, train_acc: 0.66\n",
      "alexnet1d, trial.148:\n",
      "Epoch 12, avg test_loss: 0.010064, test_acc: 0.67\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010088, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011267, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010654, train_acc: 0.71\n",
      "alexnet1d, trial.148:\n",
      "Epoch 13, avg test_loss: 0.010022, test_acc: 0.63\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009666, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010100, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.008272, train_acc: 0.75\n",
      "alexnet1d, trial.148:\n",
      "Epoch 14, avg test_loss: 0.011449, test_acc: 0.57\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009108, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009477, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.007616, train_acc: 0.84\n",
      "alexnet1d, trial.148:\n",
      "Epoch 15, avg test_loss: 0.011131, test_acc: 0.61\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008178, train_acc: 0.80\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010730, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009364, train_acc: 0.75\n",
      "alexnet1d, trial.148:\n",
      "Epoch 16, avg test_loss: 0.011755, test_acc: 0.60\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007386, train_acc: 0.84\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008688, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007292, train_acc: 0.84\n",
      "alexnet1d, trial.148:\n",
      "Epoch 17, avg test_loss: 0.011238, test_acc: 0.64\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008014, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.005595, train_acc: 0.91\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.012604, train_acc: 0.75\n",
      "alexnet1d, trial.148:\n",
      "Epoch 18, avg test_loss: 0.017811, test_acc: 0.57\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.005923, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007097, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009599, train_acc: 0.75\n",
      "alexnet1d, trial.148:\n",
      "Epoch 19, avg test_loss: 0.012989, test_acc: 0.63\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008468, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008239, train_acc: 0.75\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008408, train_acc: 0.75\n",
      "alexnet1d, trial.148:\n",
      "Epoch 20, avg test_loss: 0.010986, test_acc: 0.64\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008540, train_acc: 0.70\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006986, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006839, train_acc: 0.84\n",
      "alexnet1d, trial.148:\n",
      "Epoch 21, avg test_loss: 0.014561, test_acc: 0.67\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005920, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007060, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.004898, train_acc: 0.89\n",
      "alexnet1d, trial.148:\n",
      "Epoch 22, avg test_loss: 0.014752, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005239, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.004513, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.004968, train_acc: 0.88\n",
      "alexnet1d, trial.148:\n",
      "Epoch 23, avg test_loss: 0.019068, test_acc: 0.61\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004329, train_acc: 0.95\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005814, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.007317, train_acc: 0.80\n",
      "alexnet1d, trial.148:\n",
      "Epoch 24, avg test_loss: 0.017111, test_acc: 0.63\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004197, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.002782, train_acc: 0.96\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.002872, train_acc: 0.96\n",
      "alexnet1d, trial.148:\n",
      "Epoch 25, avg test_loss: 0.019102, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.003003, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.002267, train_acc: 0.96\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.006094, train_acc: 0.88\n",
      "alexnet1d, trial.148:\n",
      "Epoch 26, avg test_loss: 0.019957, test_acc: 0.64\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.002386, train_acc: 0.96\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.005266, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.002812, train_acc: 0.95\n",
      "alexnet1d, trial.148:\n",
      "Epoch 27, avg test_loss: 0.025115, test_acc: 0.66\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003744, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.003617, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.003211, train_acc: 0.93\n",
      "alexnet1d, trial.148:\n",
      "Epoch 28, avg test_loss: 0.024566, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号43个\n",
      "错误信号27个\n",
      "信号正确并预测正确的概率为0.486\n",
      "信号错误并预测正确的概率为0.143\n",
      "总正确率为0.63\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012516, train_acc: 0.36\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.017279, train_acc: 0.50\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012357, train_acc: 0.50\n",
      "alexnet1d, trial.149:\n",
      "Epoch 0, avg test_loss: 0.009933, test_acc: 0.44\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012388, train_acc: 0.52\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012312, train_acc: 0.70\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012412, train_acc: 0.38\n",
      "alexnet1d, trial.149:\n",
      "Epoch 1, avg test_loss: 0.009879, test_acc: 0.56\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012321, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012278, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012029, train_acc: 0.66\n",
      "alexnet1d, trial.149:\n",
      "Epoch 2, avg test_loss: 0.009809, test_acc: 0.56\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011987, train_acc: 0.62\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012444, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012357, train_acc: 0.55\n",
      "alexnet1d, trial.149:\n",
      "Epoch 3, avg test_loss: 0.009824, test_acc: 0.56\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012022, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012502, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012369, train_acc: 0.54\n",
      "alexnet1d, trial.149:\n",
      "Epoch 4, avg test_loss: 0.009823, test_acc: 0.56\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012296, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.013000, train_acc: 0.45\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011969, train_acc: 0.62\n",
      "alexnet1d, trial.149:\n",
      "Epoch 5, avg test_loss: 0.009790, test_acc: 0.56\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012469, train_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012008, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011851, train_acc: 0.62\n",
      "alexnet1d, trial.149:\n",
      "Epoch 6, avg test_loss: 0.009746, test_acc: 0.56\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011375, train_acc: 0.70\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012313, train_acc: 0.50\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.013172, train_acc: 0.48\n",
      "alexnet1d, trial.149:\n",
      "Epoch 7, avg test_loss: 0.009747, test_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012470, train_acc: 0.55\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012093, train_acc: 0.55\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011198, train_acc: 0.66\n",
      "alexnet1d, trial.149:\n",
      "Epoch 8, avg test_loss: 0.009678, test_acc: 0.56\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011319, train_acc: 0.73\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012000, train_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010804, train_acc: 0.75\n",
      "alexnet1d, trial.149:\n",
      "Epoch 9, avg test_loss: 0.009654, test_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011633, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010448, train_acc: 0.70\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011369, train_acc: 0.64\n",
      "alexnet1d, trial.149:\n",
      "Epoch 10, avg test_loss: 0.009782, test_acc: 0.56\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011791, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.012749, train_acc: 0.52\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010200, train_acc: 0.75\n",
      "alexnet1d, trial.149:\n",
      "Epoch 11, avg test_loss: 0.009948, test_acc: 0.54\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010018, train_acc: 0.75\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011405, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010774, train_acc: 0.73\n",
      "alexnet1d, trial.149:\n",
      "Epoch 12, avg test_loss: 0.009850, test_acc: 0.57\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009997, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009510, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010659, train_acc: 0.64\n",
      "alexnet1d, trial.149:\n",
      "Epoch 13, avg test_loss: 0.010815, test_acc: 0.53\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010297, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009532, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010743, train_acc: 0.64\n",
      "alexnet1d, trial.149:\n",
      "Epoch 14, avg test_loss: 0.010279, test_acc: 0.53\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009289, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008723, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011054, train_acc: 0.70\n",
      "alexnet1d, trial.149:\n",
      "Epoch 15, avg test_loss: 0.010501, test_acc: 0.54\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008559, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009969, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008602, train_acc: 0.73\n",
      "alexnet1d, trial.149:\n",
      "Epoch 16, avg test_loss: 0.010149, test_acc: 0.60\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007938, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008376, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009281, train_acc: 0.73\n",
      "alexnet1d, trial.149:\n",
      "Epoch 17, avg test_loss: 0.012311, test_acc: 0.49\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.006609, train_acc: 0.86\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.010395, train_acc: 0.64\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.010146, train_acc: 0.71\n",
      "alexnet1d, trial.149:\n",
      "Epoch 18, avg test_loss: 0.011129, test_acc: 0.54\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009649, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008531, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.010328, train_acc: 0.71\n",
      "alexnet1d, trial.149:\n",
      "Epoch 19, avg test_loss: 0.010350, test_acc: 0.61\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007491, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008170, train_acc: 0.75\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009171, train_acc: 0.77\n",
      "alexnet1d, trial.149:\n",
      "Epoch 20, avg test_loss: 0.010343, test_acc: 0.57\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007224, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007110, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007485, train_acc: 0.84\n",
      "alexnet1d, trial.149:\n",
      "Epoch 21, avg test_loss: 0.011178, test_acc: 0.60\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006184, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007599, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007879, train_acc: 0.88\n",
      "alexnet1d, trial.149:\n",
      "Epoch 22, avg test_loss: 0.013103, test_acc: 0.57\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005170, train_acc: 0.95\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005784, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.004795, train_acc: 0.91\n",
      "alexnet1d, trial.149:\n",
      "Epoch 23, avg test_loss: 0.013494, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005257, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.009244, train_acc: 0.82\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006816, train_acc: 0.88\n",
      "alexnet1d, trial.149:\n",
      "Epoch 24, avg test_loss: 0.012744, test_acc: 0.63\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004280, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005106, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.003700, train_acc: 0.95\n",
      "alexnet1d, trial.149:\n",
      "Epoch 25, avg test_loss: 0.012205, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005246, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005816, train_acc: 0.86\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.003363, train_acc: 0.95\n",
      "alexnet1d, trial.149:\n",
      "Epoch 26, avg test_loss: 0.015226, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.005239, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.003219, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003601, train_acc: 0.95\n",
      "alexnet1d, trial.149:\n",
      "Epoch 27, avg test_loss: 0.014644, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号39个\n",
      "错误信号31个\n",
      "信号正确并预测正确的概率为0.457\n",
      "信号错误并预测正确的概率为0.143\n",
      "总正确率为0.60\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012400, train_acc: 0.46\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.011512, train_acc: 0.61\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.011712, train_acc: 0.66\n",
      "alexnet1d, trial.150:\n",
      "Epoch 0, avg test_loss: 0.009935, test_acc: 0.47\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012377, train_acc: 0.52\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012301, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012321, train_acc: 0.59\n",
      "alexnet1d, trial.150:\n",
      "Epoch 1, avg test_loss: 0.009903, test_acc: 0.53\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012197, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012090, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012559, train_acc: 0.54\n",
      "alexnet1d, trial.150:\n",
      "Epoch 2, avg test_loss: 0.010426, test_acc: 0.53\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012036, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012775, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011333, train_acc: 0.71\n",
      "alexnet1d, trial.150:\n",
      "Epoch 3, avg test_loss: 0.010042, test_acc: 0.53\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012329, train_acc: 0.52\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012174, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011746, train_acc: 0.57\n",
      "alexnet1d, trial.150:\n",
      "Epoch 4, avg test_loss: 0.010207, test_acc: 0.53\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012054, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011985, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012142, train_acc: 0.59\n",
      "alexnet1d, trial.150:\n",
      "Epoch 5, avg test_loss: 0.010851, test_acc: 0.53\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011989, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011978, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011715, train_acc: 0.61\n",
      "alexnet1d, trial.150:\n",
      "Epoch 6, avg test_loss: 0.010305, test_acc: 0.53\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011879, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012329, train_acc: 0.52\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011838, train_acc: 0.55\n",
      "alexnet1d, trial.150:\n",
      "Epoch 7, avg test_loss: 0.010907, test_acc: 0.53\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012573, train_acc: 0.50\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011971, train_acc: 0.55\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011455, train_acc: 0.64\n",
      "alexnet1d, trial.150:\n",
      "Epoch 8, avg test_loss: 0.010918, test_acc: 0.53\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011477, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011944, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011533, train_acc: 0.61\n",
      "alexnet1d, trial.150:\n",
      "Epoch 9, avg test_loss: 0.010984, test_acc: 0.53\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011383, train_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011699, train_acc: 0.52\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011059, train_acc: 0.61\n",
      "alexnet1d, trial.150:\n",
      "Epoch 10, avg test_loss: 0.012280, test_acc: 0.54\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011452, train_acc: 0.54\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011411, train_acc: 0.55\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010901, train_acc: 0.71\n",
      "alexnet1d, trial.150:\n",
      "Epoch 11, avg test_loss: 0.011293, test_acc: 0.54\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010759, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010562, train_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.009835, train_acc: 0.75\n",
      "alexnet1d, trial.150:\n",
      "Epoch 12, avg test_loss: 0.014691, test_acc: 0.53\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010774, train_acc: 0.55\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009723, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009668, train_acc: 0.77\n",
      "alexnet1d, trial.150:\n",
      "Epoch 13, avg test_loss: 0.015037, test_acc: 0.56\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009767, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010649, train_acc: 0.57\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011070, train_acc: 0.66\n",
      "alexnet1d, trial.150:\n",
      "Epoch 14, avg test_loss: 0.020862, test_acc: 0.47\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009938, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008540, train_acc: 0.80\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009962, train_acc: 0.77\n",
      "alexnet1d, trial.150:\n",
      "Epoch 15, avg test_loss: 0.017734, test_acc: 0.53\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009233, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008723, train_acc: 0.82\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.007484, train_acc: 0.80\n",
      "alexnet1d, trial.150:\n",
      "Epoch 16, avg test_loss: 0.021037, test_acc: 0.53\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.006545, train_acc: 0.88\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.011131, train_acc: 0.62\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010438, train_acc: 0.75\n",
      "alexnet1d, trial.150:\n",
      "Epoch 17, avg test_loss: 0.022251, test_acc: 0.50\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009009, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007927, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008755, train_acc: 0.75\n",
      "alexnet1d, trial.150:\n",
      "Epoch 18, avg test_loss: 0.017721, test_acc: 0.51\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008916, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.006719, train_acc: 0.89\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007931, train_acc: 0.77\n",
      "alexnet1d, trial.150:\n",
      "Epoch 19, avg test_loss: 0.019728, test_acc: 0.51\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008200, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007163, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006205, train_acc: 0.88\n",
      "alexnet1d, trial.150:\n",
      "Epoch 20, avg test_loss: 0.020554, test_acc: 0.50\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006768, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.005152, train_acc: 0.91\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.004671, train_acc: 0.91\n",
      "alexnet1d, trial.150:\n",
      "Epoch 21, avg test_loss: 0.027885, test_acc: 0.47\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.004991, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007063, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.005966, train_acc: 0.86\n",
      "alexnet1d, trial.150:\n",
      "Epoch 22, avg test_loss: 0.031722, test_acc: 0.43\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.004118, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006989, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.003559, train_acc: 0.91\n",
      "alexnet1d, trial.150:\n",
      "Epoch 23, avg test_loss: 0.033032, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.003581, train_acc: 0.95\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005238, train_acc: 0.93\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.003582, train_acc: 0.93\n",
      "alexnet1d, trial.150:\n",
      "Epoch 24, avg test_loss: 0.032818, test_acc: 0.47\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004905, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006482, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.002774, train_acc: 0.95\n",
      "alexnet1d, trial.150:\n",
      "Epoch 25, avg test_loss: 0.036310, test_acc: 0.49\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号37个\n",
      "错误信号33个\n",
      "信号正确并预测正确的概率为0.3\n",
      "信号错误并预测正确的概率为0.186\n",
      "总正确率为0.49\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012411, train_acc: 0.50\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012947, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012284, train_acc: 0.59\n",
      "alexnet1d, trial.151:\n",
      "Epoch 0, avg test_loss: 0.009885, test_acc: 0.56\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012366, train_acc: 0.46\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012236, train_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012443, train_acc: 0.48\n",
      "alexnet1d, trial.151:\n",
      "Epoch 1, avg test_loss: 0.009785, test_acc: 0.56\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012346, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012460, train_acc: 0.52\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011991, train_acc: 0.59\n",
      "alexnet1d, trial.151:\n",
      "Epoch 2, avg test_loss: 0.009808, test_acc: 0.56\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011655, train_acc: 0.68\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012177, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011877, train_acc: 0.64\n",
      "alexnet1d, trial.151:\n",
      "Epoch 3, avg test_loss: 0.009832, test_acc: 0.56\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012553, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011938, train_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011999, train_acc: 0.59\n",
      "alexnet1d, trial.151:\n",
      "Epoch 4, avg test_loss: 0.009842, test_acc: 0.56\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012054, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011900, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011948, train_acc: 0.61\n",
      "alexnet1d, trial.151:\n",
      "Epoch 5, avg test_loss: 0.010016, test_acc: 0.56\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012550, train_acc: 0.52\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011814, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011419, train_acc: 0.68\n",
      "alexnet1d, trial.151:\n",
      "Epoch 6, avg test_loss: 0.009846, test_acc: 0.56\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012373, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011725, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011418, train_acc: 0.66\n",
      "alexnet1d, trial.151:\n",
      "Epoch 7, avg test_loss: 0.009854, test_acc: 0.56\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012046, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011161, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012769, train_acc: 0.54\n",
      "alexnet1d, trial.151:\n",
      "Epoch 8, avg test_loss: 0.010107, test_acc: 0.51\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011807, train_acc: 0.55\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011809, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011976, train_acc: 0.55\n",
      "alexnet1d, trial.151:\n",
      "Epoch 9, avg test_loss: 0.009528, test_acc: 0.56\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011811, train_acc: 0.52\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012146, train_acc: 0.52\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011359, train_acc: 0.61\n",
      "alexnet1d, trial.151:\n",
      "Epoch 10, avg test_loss: 0.010044, test_acc: 0.54\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010603, train_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010763, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.012078, train_acc: 0.48\n",
      "alexnet1d, trial.151:\n",
      "Epoch 11, avg test_loss: 0.010037, test_acc: 0.56\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011038, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011366, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010681, train_acc: 0.66\n",
      "alexnet1d, trial.151:\n",
      "Epoch 12, avg test_loss: 0.011033, test_acc: 0.60\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010593, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010319, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009947, train_acc: 0.71\n",
      "alexnet1d, trial.151:\n",
      "Epoch 13, avg test_loss: 0.010987, test_acc: 0.57\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010104, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010757, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010083, train_acc: 0.62\n",
      "alexnet1d, trial.151:\n",
      "Epoch 14, avg test_loss: 0.010843, test_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010221, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010083, train_acc: 0.68\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008531, train_acc: 0.77\n",
      "alexnet1d, trial.151:\n",
      "Epoch 15, avg test_loss: 0.014502, test_acc: 0.50\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009305, train_acc: 0.68\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008820, train_acc: 0.68\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.011876, train_acc: 0.59\n",
      "alexnet1d, trial.151:\n",
      "Epoch 16, avg test_loss: 0.012483, test_acc: 0.59\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008441, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010418, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008753, train_acc: 0.77\n",
      "alexnet1d, trial.151:\n",
      "Epoch 17, avg test_loss: 0.013044, test_acc: 0.54\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007438, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008366, train_acc: 0.71\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.011665, train_acc: 0.62\n",
      "alexnet1d, trial.151:\n",
      "Epoch 18, avg test_loss: 0.013566, test_acc: 0.56\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008967, train_acc: 0.71\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008103, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009520, train_acc: 0.73\n",
      "alexnet1d, trial.151:\n",
      "Epoch 19, avg test_loss: 0.012449, test_acc: 0.66\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006325, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007954, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.005902, train_acc: 0.86\n",
      "alexnet1d, trial.151:\n",
      "Epoch 20, avg test_loss: 0.014924, test_acc: 0.64\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006782, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006822, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.005641, train_acc: 0.84\n",
      "alexnet1d, trial.151:\n",
      "Epoch 21, avg test_loss: 0.016043, test_acc: 0.59\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.004450, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005973, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008418, train_acc: 0.75\n",
      "alexnet1d, trial.151:\n",
      "Epoch 22, avg test_loss: 0.018975, test_acc: 0.54\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.004896, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005969, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.008803, train_acc: 0.80\n",
      "alexnet1d, trial.151:\n",
      "Epoch 23, avg test_loss: 0.015977, test_acc: 0.66\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005242, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005645, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005866, train_acc: 0.89\n",
      "alexnet1d, trial.151:\n",
      "Epoch 24, avg test_loss: 0.017999, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004164, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006023, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004741, train_acc: 0.93\n",
      "alexnet1d, trial.151:\n",
      "Epoch 25, avg test_loss: 0.016795, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004124, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004612, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.002642, train_acc: 0.89\n",
      "alexnet1d, trial.151:\n",
      "Epoch 26, avg test_loss: 0.021392, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004345, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.005693, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003260, train_acc: 0.91\n",
      "alexnet1d, trial.151:\n",
      "Epoch 27, avg test_loss: 0.023698, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号39个\n",
      "错误信号31个\n",
      "信号正确并预测正确的概率为0.386\n",
      "信号错误并预测正确的概率为0.186\n",
      "总正确率为0.57\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012415, train_acc: 0.41\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.011323, train_acc: 0.71\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.013424, train_acc: 0.43\n",
      "alexnet1d, trial.152:\n",
      "Epoch 0, avg test_loss: 0.009952, test_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011973, train_acc: 0.66\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012285, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012255, train_acc: 0.57\n",
      "alexnet1d, trial.152:\n",
      "Epoch 1, avg test_loss: 0.009914, test_acc: 0.50\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012181, train_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011957, train_acc: 0.68\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012408, train_acc: 0.55\n",
      "alexnet1d, trial.152:\n",
      "Epoch 2, avg test_loss: 0.010175, test_acc: 0.50\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012404, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012160, train_acc: 0.62\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011959, train_acc: 0.61\n",
      "alexnet1d, trial.152:\n",
      "Epoch 3, avg test_loss: 0.010236, test_acc: 0.50\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011843, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012523, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011943, train_acc: 0.62\n",
      "alexnet1d, trial.152:\n",
      "Epoch 4, avg test_loss: 0.010228, test_acc: 0.50\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012361, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011893, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011965, train_acc: 0.59\n",
      "alexnet1d, trial.152:\n",
      "Epoch 5, avg test_loss: 0.010218, test_acc: 0.50\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011971, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011639, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011361, train_acc: 0.64\n",
      "alexnet1d, trial.152:\n",
      "Epoch 6, avg test_loss: 0.010444, test_acc: 0.50\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011826, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011328, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012259, train_acc: 0.57\n",
      "alexnet1d, trial.152:\n",
      "Epoch 7, avg test_loss: 0.010040, test_acc: 0.50\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011449, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011169, train_acc: 0.70\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012418, train_acc: 0.54\n",
      "alexnet1d, trial.152:\n",
      "Epoch 8, avg test_loss: 0.010046, test_acc: 0.50\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011584, train_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012034, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011371, train_acc: 0.66\n",
      "alexnet1d, trial.152:\n",
      "Epoch 9, avg test_loss: 0.010083, test_acc: 0.50\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011024, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012222, train_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011571, train_acc: 0.59\n",
      "alexnet1d, trial.152:\n",
      "Epoch 10, avg test_loss: 0.010221, test_acc: 0.50\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010871, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010221, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.012776, train_acc: 0.64\n",
      "alexnet1d, trial.152:\n",
      "Epoch 11, avg test_loss: 0.009911, test_acc: 0.54\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011360, train_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011636, train_acc: 0.52\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011690, train_acc: 0.57\n",
      "alexnet1d, trial.152:\n",
      "Epoch 12, avg test_loss: 0.009845, test_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010818, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010985, train_acc: 0.62\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011460, train_acc: 0.64\n",
      "alexnet1d, trial.152:\n",
      "Epoch 13, avg test_loss: 0.009689, test_acc: 0.60\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010346, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.008906, train_acc: 0.82\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.015495, train_acc: 0.57\n",
      "alexnet1d, trial.152:\n",
      "Epoch 14, avg test_loss: 0.009737, test_acc: 0.56\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010037, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010516, train_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008295, train_acc: 0.86\n",
      "alexnet1d, trial.152:\n",
      "Epoch 15, avg test_loss: 0.010878, test_acc: 0.53\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010030, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.011095, train_acc: 0.68\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010933, train_acc: 0.61\n",
      "alexnet1d, trial.152:\n",
      "Epoch 16, avg test_loss: 0.010104, test_acc: 0.57\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009684, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010548, train_acc: 0.64\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010955, train_acc: 0.75\n",
      "alexnet1d, trial.152:\n",
      "Epoch 17, avg test_loss: 0.009711, test_acc: 0.63\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009169, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007667, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.010918, train_acc: 0.71\n",
      "alexnet1d, trial.152:\n",
      "Epoch 18, avg test_loss: 0.011065, test_acc: 0.56\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009340, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.006670, train_acc: 0.89\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.010042, train_acc: 0.70\n",
      "alexnet1d, trial.152:\n",
      "Epoch 19, avg test_loss: 0.013337, test_acc: 0.57\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.009429, train_acc: 0.75\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.010362, train_acc: 0.70\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.010138, train_acc: 0.62\n",
      "alexnet1d, trial.152:\n",
      "Epoch 20, avg test_loss: 0.010984, test_acc: 0.56\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008750, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.009337, train_acc: 0.75\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.009805, train_acc: 0.75\n",
      "alexnet1d, trial.152:\n",
      "Epoch 21, avg test_loss: 0.010985, test_acc: 0.56\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006497, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.011057, train_acc: 0.68\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007240, train_acc: 0.82\n",
      "alexnet1d, trial.152:\n",
      "Epoch 22, avg test_loss: 0.012513, test_acc: 0.57\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006541, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.008383, train_acc: 0.79\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.008375, train_acc: 0.80\n",
      "alexnet1d, trial.152:\n",
      "Epoch 23, avg test_loss: 0.012920, test_acc: 0.57\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006101, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.007466, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005532, train_acc: 0.88\n",
      "alexnet1d, trial.152:\n",
      "Epoch 24, avg test_loss: 0.014639, test_acc: 0.60\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.007954, train_acc: 0.75\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006348, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.007157, train_acc: 0.82\n",
      "alexnet1d, trial.152:\n",
      "Epoch 25, avg test_loss: 0.015397, test_acc: 0.57\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005558, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.007354, train_acc: 0.84\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004408, train_acc: 0.91\n",
      "alexnet1d, trial.152:\n",
      "Epoch 26, avg test_loss: 0.016088, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.006058, train_acc: 0.84\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004329, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.006019, train_acc: 0.86\n",
      "alexnet1d, trial.152:\n",
      "Epoch 27, avg test_loss: 0.017286, test_acc: 0.59\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.005368, train_acc: 0.86\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004513, train_acc: 0.89\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.005344, train_acc: 0.84\n",
      "alexnet1d, trial.152:\n",
      "Epoch 28, avg test_loss: 0.017521, test_acc: 0.59\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.006206, train_acc: 0.86\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.010676, train_acc: 0.77\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.007153, train_acc: 0.79\n",
      "alexnet1d, trial.152:\n",
      "Epoch 29, avg test_loss: 0.014616, test_acc: 0.56\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.005650, train_acc: 0.86\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.006640, train_acc: 0.86\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.007089, train_acc: 0.86\n",
      "alexnet1d, trial.152:\n",
      "Epoch 30, avg test_loss: 0.014376, test_acc: 0.57\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.005210, train_acc: 0.91\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.006737, train_acc: 0.82\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.004387, train_acc: 0.89\n",
      "alexnet1d, trial.152:\n",
      "Epoch 31, avg test_loss: 0.014470, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.002861, train_acc: 0.98\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.006546, train_acc: 0.80\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.002530, train_acc: 0.96\n",
      "alexnet1d, trial.152:\n",
      "Epoch 32, avg test_loss: 0.016204, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 33, lr: 0.000377, 0/280, avg loss: 0.005278, train_acc: 0.88\n",
      "Train Epoch 33, lr: 0.000377, 112/280, avg loss: 0.002862, train_acc: 0.95\n",
      "Train Epoch 33, lr: 0.000377, 224/280, avg loss: 0.002888, train_acc: 0.95\n",
      "alexnet1d, trial.152:\n",
      "Epoch 33, avg test_loss: 0.018041, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号35个\n",
      "错误信号35个\n",
      "信号正确并预测正确的概率为0.357\n",
      "信号错误并预测正确的概率为0.214\n",
      "总正确率为0.57\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012445, train_acc: 0.41\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012159, train_acc: 0.61\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012998, train_acc: 0.57\n",
      "alexnet1d, trial.153:\n",
      "Epoch 0, avg test_loss: 0.009746, test_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011810, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012590, train_acc: 0.46\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012207, train_acc: 0.68\n",
      "alexnet1d, trial.153:\n",
      "Epoch 1, avg test_loss: 0.009859, test_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012227, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011847, train_acc: 0.68\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.013090, train_acc: 0.46\n",
      "alexnet1d, trial.153:\n",
      "Epoch 2, avg test_loss: 0.009939, test_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012029, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012144, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012009, train_acc: 0.61\n",
      "alexnet1d, trial.153:\n",
      "Epoch 3, avg test_loss: 0.009924, test_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012134, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012233, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011951, train_acc: 0.64\n",
      "alexnet1d, trial.153:\n",
      "Epoch 4, avg test_loss: 0.009852, test_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012114, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011898, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011811, train_acc: 0.64\n",
      "alexnet1d, trial.153:\n",
      "Epoch 5, avg test_loss: 0.009830, test_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011895, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012418, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012021, train_acc: 0.55\n",
      "alexnet1d, trial.153:\n",
      "Epoch 6, avg test_loss: 0.009747, test_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011852, train_acc: 0.55\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012314, train_acc: 0.50\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011844, train_acc: 0.62\n",
      "alexnet1d, trial.153:\n",
      "Epoch 7, avg test_loss: 0.009505, test_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011764, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011268, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011487, train_acc: 0.64\n",
      "alexnet1d, trial.153:\n",
      "Epoch 8, avg test_loss: 0.010020, test_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010702, train_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011722, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011868, train_acc: 0.61\n",
      "alexnet1d, trial.153:\n",
      "Epoch 9, avg test_loss: 0.009459, test_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010433, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010928, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011920, train_acc: 0.66\n",
      "alexnet1d, trial.153:\n",
      "Epoch 10, avg test_loss: 0.010100, test_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011358, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010358, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011540, train_acc: 0.59\n",
      "alexnet1d, trial.153:\n",
      "Epoch 11, avg test_loss: 0.010081, test_acc: 0.63\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010235, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.008966, train_acc: 0.75\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010177, train_acc: 0.66\n",
      "alexnet1d, trial.153:\n",
      "Epoch 12, avg test_loss: 0.010193, test_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.008991, train_acc: 0.77\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.007244, train_acc: 0.79\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010621, train_acc: 0.68\n",
      "alexnet1d, trial.153:\n",
      "Epoch 13, avg test_loss: 0.010795, test_acc: 0.63\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.008424, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.008324, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.008046, train_acc: 0.75\n",
      "alexnet1d, trial.153:\n",
      "Epoch 14, avg test_loss: 0.014679, test_acc: 0.51\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008303, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008941, train_acc: 0.80\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011773, train_acc: 0.66\n",
      "alexnet1d, trial.153:\n",
      "Epoch 15, avg test_loss: 0.012552, test_acc: 0.53\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.007698, train_acc: 0.80\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009436, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.007987, train_acc: 0.80\n",
      "alexnet1d, trial.153:\n",
      "Epoch 16, avg test_loss: 0.011348, test_acc: 0.64\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008532, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007202, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009551, train_acc: 0.70\n",
      "alexnet1d, trial.153:\n",
      "Epoch 17, avg test_loss: 0.012341, test_acc: 0.50\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008680, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007676, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.004848, train_acc: 0.95\n",
      "alexnet1d, trial.153:\n",
      "Epoch 18, avg test_loss: 0.014619, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.005120, train_acc: 0.93\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008139, train_acc: 0.71\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.005832, train_acc: 0.89\n",
      "alexnet1d, trial.153:\n",
      "Epoch 19, avg test_loss: 0.017446, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007404, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.005302, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.004106, train_acc: 0.91\n",
      "alexnet1d, trial.153:\n",
      "Epoch 20, avg test_loss: 0.016478, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.004326, train_acc: 0.89\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.004287, train_acc: 0.96\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.004347, train_acc: 0.91\n",
      "alexnet1d, trial.153:\n",
      "Epoch 21, avg test_loss: 0.020898, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号38个\n",
      "错误信号32个\n",
      "信号正确并预测正确的概率为0.4\n",
      "信号错误并预测正确的概率为0.143\n",
      "总正确率为0.54\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012406, train_acc: 0.39\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012505, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012016, train_acc: 0.59\n",
      "alexnet1d, trial.154:\n",
      "Epoch 0, avg test_loss: 0.009912, test_acc: 0.51\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012167, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012189, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011790, train_acc: 0.64\n",
      "alexnet1d, trial.154:\n",
      "Epoch 1, avg test_loss: 0.010091, test_acc: 0.51\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012099, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012134, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011964, train_acc: 0.61\n",
      "alexnet1d, trial.154:\n",
      "Epoch 2, avg test_loss: 0.010048, test_acc: 0.51\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011345, train_acc: 0.70\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012657, train_acc: 0.48\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012448, train_acc: 0.54\n",
      "alexnet1d, trial.154:\n",
      "Epoch 3, avg test_loss: 0.010014, test_acc: 0.51\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012579, train_acc: 0.48\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011537, train_acc: 0.68\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012102, train_acc: 0.61\n",
      "alexnet1d, trial.154:\n",
      "Epoch 4, avg test_loss: 0.010061, test_acc: 0.51\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011330, train_acc: 0.68\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012726, train_acc: 0.50\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011636, train_acc: 0.64\n",
      "alexnet1d, trial.154:\n",
      "Epoch 5, avg test_loss: 0.010032, test_acc: 0.51\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011760, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011263, train_acc: 0.66\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012732, train_acc: 0.52\n",
      "alexnet1d, trial.154:\n",
      "Epoch 6, avg test_loss: 0.010138, test_acc: 0.51\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011215, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.010507, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012987, train_acc: 0.50\n",
      "alexnet1d, trial.154:\n",
      "Epoch 7, avg test_loss: 0.010180, test_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011737, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011323, train_acc: 0.71\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011358, train_acc: 0.62\n",
      "alexnet1d, trial.154:\n",
      "Epoch 8, avg test_loss: 0.009814, test_acc: 0.60\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011463, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012550, train_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011201, train_acc: 0.64\n",
      "alexnet1d, trial.154:\n",
      "Epoch 9, avg test_loss: 0.010677, test_acc: 0.53\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011057, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010301, train_acc: 0.71\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010596, train_acc: 0.70\n",
      "alexnet1d, trial.154:\n",
      "Epoch 10, avg test_loss: 0.011053, test_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.009790, train_acc: 0.73\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.009356, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010725, train_acc: 0.73\n",
      "alexnet1d, trial.154:\n",
      "Epoch 11, avg test_loss: 0.010566, test_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011424, train_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.008775, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.009429, train_acc: 0.73\n",
      "alexnet1d, trial.154:\n",
      "Epoch 12, avg test_loss: 0.011739, test_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010333, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010493, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009494, train_acc: 0.75\n",
      "alexnet1d, trial.154:\n",
      "Epoch 13, avg test_loss: 0.012086, test_acc: 0.60\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.006848, train_acc: 0.88\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009185, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.008802, train_acc: 0.77\n",
      "alexnet1d, trial.154:\n",
      "Epoch 14, avg test_loss: 0.014198, test_acc: 0.63\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.006834, train_acc: 0.84\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009560, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010091, train_acc: 0.70\n",
      "alexnet1d, trial.154:\n",
      "Epoch 15, avg test_loss: 0.013291, test_acc: 0.56\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009433, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008982, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010243, train_acc: 0.70\n",
      "alexnet1d, trial.154:\n",
      "Epoch 16, avg test_loss: 0.013437, test_acc: 0.60\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008400, train_acc: 0.71\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007095, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007856, train_acc: 0.82\n",
      "alexnet1d, trial.154:\n",
      "Epoch 17, avg test_loss: 0.014844, test_acc: 0.57\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007589, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008201, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007408, train_acc: 0.86\n",
      "alexnet1d, trial.154:\n",
      "Epoch 18, avg test_loss: 0.016565, test_acc: 0.59\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006192, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007784, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.005615, train_acc: 0.82\n",
      "alexnet1d, trial.154:\n",
      "Epoch 19, avg test_loss: 0.019676, test_acc: 0.59\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006194, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.006883, train_acc: 0.77\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.004215, train_acc: 0.93\n",
      "alexnet1d, trial.154:\n",
      "Epoch 20, avg test_loss: 0.018867, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.005110, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.004389, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006177, train_acc: 0.84\n",
      "alexnet1d, trial.154:\n",
      "Epoch 21, avg test_loss: 0.022807, test_acc: 0.57\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.003224, train_acc: 0.93\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.004918, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.002912, train_acc: 0.95\n",
      "alexnet1d, trial.154:\n",
      "Epoch 22, avg test_loss: 0.024680, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.003270, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.004717, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006061, train_acc: 0.84\n",
      "alexnet1d, trial.154:\n",
      "Epoch 23, avg test_loss: 0.026664, test_acc: 0.59\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.002621, train_acc: 0.95\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004558, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.003673, train_acc: 0.91\n",
      "alexnet1d, trial.154:\n",
      "Epoch 24, avg test_loss: 0.027259, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004712, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.002866, train_acc: 0.95\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004943, train_acc: 0.93\n",
      "alexnet1d, trial.154:\n",
      "Epoch 25, avg test_loss: 0.020705, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号36个\n",
      "错误信号34个\n",
      "信号正确并预测正确的概率为0.343\n",
      "信号错误并预测正确的概率为0.271\n",
      "总正确率为0.61\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012471, train_acc: 0.43\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.017107, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012300, train_acc: 0.64\n",
      "alexnet1d, trial.155:\n",
      "Epoch 0, avg test_loss: 0.009911, test_acc: 0.47\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012290, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012167, train_acc: 0.66\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012649, train_acc: 0.48\n",
      "alexnet1d, trial.155:\n",
      "Epoch 1, avg test_loss: 0.010237, test_acc: 0.47\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011815, train_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012059, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011669, train_acc: 0.62\n",
      "alexnet1d, trial.155:\n",
      "Epoch 2, avg test_loss: 0.010687, test_acc: 0.47\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012164, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011562, train_acc: 0.62\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012165, train_acc: 0.59\n",
      "alexnet1d, trial.155:\n",
      "Epoch 3, avg test_loss: 0.010612, test_acc: 0.47\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012119, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011721, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011157, train_acc: 0.71\n",
      "alexnet1d, trial.155:\n",
      "Epoch 4, avg test_loss: 0.010530, test_acc: 0.47\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012121, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011278, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011229, train_acc: 0.70\n",
      "alexnet1d, trial.155:\n",
      "Epoch 5, avg test_loss: 0.011443, test_acc: 0.47\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011077, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.010680, train_acc: 0.66\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011437, train_acc: 0.66\n",
      "alexnet1d, trial.155:\n",
      "Epoch 6, avg test_loss: 0.010331, test_acc: 0.47\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012705, train_acc: 0.45\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011829, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011305, train_acc: 0.59\n",
      "alexnet1d, trial.155:\n",
      "Epoch 7, avg test_loss: 0.011053, test_acc: 0.50\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011228, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012300, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012121, train_acc: 0.55\n",
      "alexnet1d, trial.155:\n",
      "Epoch 8, avg test_loss: 0.010553, test_acc: 0.53\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010972, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010930, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011981, train_acc: 0.59\n",
      "alexnet1d, trial.155:\n",
      "Epoch 9, avg test_loss: 0.011087, test_acc: 0.53\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010595, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011943, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011134, train_acc: 0.66\n",
      "alexnet1d, trial.155:\n",
      "Epoch 10, avg test_loss: 0.011492, test_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010990, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010388, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010650, train_acc: 0.66\n",
      "alexnet1d, trial.155:\n",
      "Epoch 11, avg test_loss: 0.011586, test_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009758, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011203, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.009479, train_acc: 0.73\n",
      "alexnet1d, trial.155:\n",
      "Epoch 12, avg test_loss: 0.011600, test_acc: 0.63\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010132, train_acc: 0.62\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009576, train_acc: 0.77\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009761, train_acc: 0.68\n",
      "alexnet1d, trial.155:\n",
      "Epoch 13, avg test_loss: 0.012123, test_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.008068, train_acc: 0.84\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.008881, train_acc: 0.80\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011218, train_acc: 0.66\n",
      "alexnet1d, trial.155:\n",
      "Epoch 14, avg test_loss: 0.014631, test_acc: 0.54\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009824, train_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008421, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008424, train_acc: 0.75\n",
      "alexnet1d, trial.155:\n",
      "Epoch 15, avg test_loss: 0.014134, test_acc: 0.64\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009689, train_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008725, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008056, train_acc: 0.80\n",
      "alexnet1d, trial.155:\n",
      "Epoch 16, avg test_loss: 0.016385, test_acc: 0.66\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007912, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007576, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008455, train_acc: 0.73\n",
      "alexnet1d, trial.155:\n",
      "Epoch 17, avg test_loss: 0.015597, test_acc: 0.60\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.006727, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.006238, train_acc: 0.88\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007436, train_acc: 0.84\n",
      "alexnet1d, trial.155:\n",
      "Epoch 18, avg test_loss: 0.023579, test_acc: 0.60\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009011, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.005028, train_acc: 0.91\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.005683, train_acc: 0.84\n",
      "alexnet1d, trial.155:\n",
      "Epoch 19, avg test_loss: 0.018129, test_acc: 0.66\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006520, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.006346, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.005751, train_acc: 0.86\n",
      "alexnet1d, trial.155:\n",
      "Epoch 20, avg test_loss: 0.022019, test_acc: 0.67\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.003405, train_acc: 0.93\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006912, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006514, train_acc: 0.89\n",
      "alexnet1d, trial.155:\n",
      "Epoch 21, avg test_loss: 0.023962, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.004447, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.004918, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.005919, train_acc: 0.80\n",
      "alexnet1d, trial.155:\n",
      "Epoch 22, avg test_loss: 0.025144, test_acc: 0.66\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.004220, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.008377, train_acc: 0.82\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.010050, train_acc: 0.73\n",
      "alexnet1d, trial.155:\n",
      "Epoch 23, avg test_loss: 0.022460, test_acc: 0.63\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006188, train_acc: 0.82\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004347, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.004955, train_acc: 0.88\n",
      "alexnet1d, trial.155:\n",
      "Epoch 24, avg test_loss: 0.021741, test_acc: 0.53\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005877, train_acc: 0.80\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006465, train_acc: 0.80\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004950, train_acc: 0.86\n",
      "alexnet1d, trial.155:\n",
      "Epoch 25, avg test_loss: 0.017319, test_acc: 0.66\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004340, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005173, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004906, train_acc: 0.86\n",
      "alexnet1d, trial.155:\n",
      "Epoch 26, avg test_loss: 0.028079, test_acc: 0.61\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.002529, train_acc: 0.98\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.003844, train_acc: 0.88\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003953, train_acc: 0.89\n",
      "alexnet1d, trial.155:\n",
      "Epoch 27, avg test_loss: 0.028297, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.001486, train_acc: 0.96\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.002620, train_acc: 0.96\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.001949, train_acc: 0.95\n",
      "alexnet1d, trial.155:\n",
      "Epoch 28, avg test_loss: 0.042638, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.004349, train_acc: 0.89\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.002078, train_acc: 0.98\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.005180, train_acc: 0.88\n",
      "alexnet1d, trial.155:\n",
      "Epoch 29, avg test_loss: 0.033830, test_acc: 0.63\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.000745, train_acc: 1.00\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.002142, train_acc: 0.96\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.001468, train_acc: 0.98\n",
      "alexnet1d, trial.155:\n",
      "Epoch 30, avg test_loss: 0.041712, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号33个\n",
      "错误信号37个\n",
      "信号正确并预测正确的概率为0.443\n",
      "信号错误并预测正确的概率为0.171\n",
      "总正确率为0.61\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012349, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012256, train_acc: 0.52\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.013049, train_acc: 0.55\n",
      "alexnet1d, trial.156:\n",
      "Epoch 0, avg test_loss: 0.009909, test_acc: 0.60\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012697, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012210, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.013040, train_acc: 0.45\n",
      "alexnet1d, trial.156:\n",
      "Epoch 1, avg test_loss: 0.009703, test_acc: 0.60\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012308, train_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012065, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011950, train_acc: 0.55\n",
      "alexnet1d, trial.156:\n",
      "Epoch 2, avg test_loss: 0.009749, test_acc: 0.60\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011592, train_acc: 0.62\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.014761, train_acc: 0.41\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011963, train_acc: 0.70\n",
      "alexnet1d, trial.156:\n",
      "Epoch 3, avg test_loss: 0.009819, test_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012225, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012243, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012086, train_acc: 0.70\n",
      "alexnet1d, trial.156:\n",
      "Epoch 4, avg test_loss: 0.009844, test_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012055, train_acc: 0.68\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012290, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012044, train_acc: 0.57\n",
      "alexnet1d, trial.156:\n",
      "Epoch 5, avg test_loss: 0.009716, test_acc: 0.60\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012169, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012423, train_acc: 0.52\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011504, train_acc: 0.68\n",
      "alexnet1d, trial.156:\n",
      "Epoch 6, avg test_loss: 0.009807, test_acc: 0.60\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011469, train_acc: 0.68\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012607, train_acc: 0.46\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011539, train_acc: 0.59\n",
      "alexnet1d, trial.156:\n",
      "Epoch 7, avg test_loss: 0.009896, test_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011941, train_acc: 0.55\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011392, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012842, train_acc: 0.48\n",
      "alexnet1d, trial.156:\n",
      "Epoch 8, avg test_loss: 0.010062, test_acc: 0.60\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012087, train_acc: 0.55\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012117, train_acc: 0.55\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011922, train_acc: 0.61\n",
      "alexnet1d, trial.156:\n",
      "Epoch 9, avg test_loss: 0.009704, test_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011468, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011637, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011794, train_acc: 0.55\n",
      "alexnet1d, trial.156:\n",
      "Epoch 10, avg test_loss: 0.009685, test_acc: 0.60\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011482, train_acc: 0.55\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010916, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.012661, train_acc: 0.45\n",
      "alexnet1d, trial.156:\n",
      "Epoch 11, avg test_loss: 0.009583, test_acc: 0.63\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010561, train_acc: 0.75\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010945, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011838, train_acc: 0.55\n",
      "alexnet1d, trial.156:\n",
      "Epoch 12, avg test_loss: 0.009891, test_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011126, train_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010896, train_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010704, train_acc: 0.68\n",
      "alexnet1d, trial.156:\n",
      "Epoch 13, avg test_loss: 0.010902, test_acc: 0.63\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010749, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010978, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009562, train_acc: 0.79\n",
      "alexnet1d, trial.156:\n",
      "Epoch 14, avg test_loss: 0.010722, test_acc: 0.59\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009398, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009638, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.012543, train_acc: 0.59\n",
      "alexnet1d, trial.156:\n",
      "Epoch 15, avg test_loss: 0.010960, test_acc: 0.63\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009859, train_acc: 0.68\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008704, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.013335, train_acc: 0.62\n",
      "alexnet1d, trial.156:\n",
      "Epoch 16, avg test_loss: 0.011443, test_acc: 0.63\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.010103, train_acc: 0.66\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010304, train_acc: 0.70\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010001, train_acc: 0.66\n",
      "alexnet1d, trial.156:\n",
      "Epoch 17, avg test_loss: 0.010125, test_acc: 0.61\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.010203, train_acc: 0.64\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009753, train_acc: 0.71\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009466, train_acc: 0.77\n",
      "alexnet1d, trial.156:\n",
      "Epoch 18, avg test_loss: 0.010786, test_acc: 0.63\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007909, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009551, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007838, train_acc: 0.75\n",
      "alexnet1d, trial.156:\n",
      "Epoch 19, avg test_loss: 0.013282, test_acc: 0.63\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007906, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007912, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.010480, train_acc: 0.75\n",
      "alexnet1d, trial.156:\n",
      "Epoch 20, avg test_loss: 0.012225, test_acc: 0.66\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008848, train_acc: 0.73\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.009022, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007209, train_acc: 0.84\n",
      "alexnet1d, trial.156:\n",
      "Epoch 21, avg test_loss: 0.012688, test_acc: 0.67\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.008557, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.008486, train_acc: 0.79\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008574, train_acc: 0.77\n",
      "alexnet1d, trial.156:\n",
      "Epoch 22, avg test_loss: 0.013226, test_acc: 0.67\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007683, train_acc: 0.75\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005711, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.008383, train_acc: 0.75\n",
      "alexnet1d, trial.156:\n",
      "Epoch 23, avg test_loss: 0.013909, test_acc: 0.63\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006729, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.008133, train_acc: 0.75\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005662, train_acc: 0.86\n",
      "alexnet1d, trial.156:\n",
      "Epoch 24, avg test_loss: 0.016353, test_acc: 0.63\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.008028, train_acc: 0.79\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006205, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006972, train_acc: 0.84\n",
      "alexnet1d, trial.156:\n",
      "Epoch 25, avg test_loss: 0.014695, test_acc: 0.64\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005418, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.006087, train_acc: 0.82\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.005125, train_acc: 0.91\n",
      "alexnet1d, trial.156:\n",
      "Epoch 26, avg test_loss: 0.016918, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.006259, train_acc: 0.88\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004015, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.007483, train_acc: 0.80\n",
      "alexnet1d, trial.156:\n",
      "Epoch 27, avg test_loss: 0.019672, test_acc: 0.66\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003929, train_acc: 0.86\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004243, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.006592, train_acc: 0.84\n",
      "alexnet1d, trial.156:\n",
      "Epoch 28, avg test_loss: 0.021458, test_acc: 0.64\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.003780, train_acc: 0.91\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.002901, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.003563, train_acc: 0.91\n",
      "alexnet1d, trial.156:\n",
      "Epoch 29, avg test_loss: 0.021495, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.003806, train_acc: 0.91\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.004366, train_acc: 0.86\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.002164, train_acc: 0.96\n",
      "alexnet1d, trial.156:\n",
      "Epoch 30, avg test_loss: 0.023762, test_acc: 0.67\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.004756, train_acc: 0.86\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.001070, train_acc: 1.00\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.003842, train_acc: 0.93\n",
      "alexnet1d, trial.156:\n",
      "Epoch 31, avg test_loss: 0.028040, test_acc: 0.70\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号42个\n",
      "错误信号28个\n",
      "信号正确并预测正确的概率为0.471\n",
      "信号错误并预测正确的概率为0.229\n",
      "总正确率为0.70\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012409, train_acc: 0.43\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012422, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012140, train_acc: 0.54\n",
      "alexnet1d, trial.157:\n",
      "Epoch 0, avg test_loss: 0.009641, test_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012526, train_acc: 0.48\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012264, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012360, train_acc: 0.59\n",
      "alexnet1d, trial.157:\n",
      "Epoch 1, avg test_loss: 0.009644, test_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012701, train_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011877, train_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012619, train_acc: 0.46\n",
      "alexnet1d, trial.157:\n",
      "Epoch 2, avg test_loss: 0.009667, test_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012788, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011901, train_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012503, train_acc: 0.46\n",
      "alexnet1d, trial.157:\n",
      "Epoch 3, avg test_loss: 0.009686, test_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012398, train_acc: 0.48\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011403, train_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012349, train_acc: 0.52\n",
      "alexnet1d, trial.157:\n",
      "Epoch 4, avg test_loss: 0.009754, test_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012219, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012054, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012079, train_acc: 0.52\n",
      "alexnet1d, trial.157:\n",
      "Epoch 5, avg test_loss: 0.010083, test_acc: 0.56\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011666, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011396, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.010877, train_acc: 0.70\n",
      "alexnet1d, trial.157:\n",
      "Epoch 6, avg test_loss: 0.010185, test_acc: 0.56\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.010552, train_acc: 0.73\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011366, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012052, train_acc: 0.62\n",
      "alexnet1d, trial.157:\n",
      "Epoch 7, avg test_loss: 0.011097, test_acc: 0.40\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010559, train_acc: 0.73\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012155, train_acc: 0.55\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011417, train_acc: 0.59\n",
      "alexnet1d, trial.157:\n",
      "Epoch 8, avg test_loss: 0.009886, test_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011458, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011085, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011181, train_acc: 0.66\n",
      "alexnet1d, trial.157:\n",
      "Epoch 9, avg test_loss: 0.011113, test_acc: 0.49\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011212, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011087, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.009866, train_acc: 0.75\n",
      "alexnet1d, trial.157:\n",
      "Epoch 10, avg test_loss: 0.009769, test_acc: 0.63\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.009688, train_acc: 0.75\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010346, train_acc: 0.75\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.012437, train_acc: 0.57\n",
      "alexnet1d, trial.157:\n",
      "Epoch 11, avg test_loss: 0.011093, test_acc: 0.50\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009158, train_acc: 0.77\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.009915, train_acc: 0.75\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010756, train_acc: 0.77\n",
      "alexnet1d, trial.157:\n",
      "Epoch 12, avg test_loss: 0.011333, test_acc: 0.54\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010018, train_acc: 0.75\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.008593, train_acc: 0.82\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.008696, train_acc: 0.84\n",
      "alexnet1d, trial.157:\n",
      "Epoch 13, avg test_loss: 0.012954, test_acc: 0.50\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.008057, train_acc: 0.82\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.008949, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.008580, train_acc: 0.82\n",
      "alexnet1d, trial.157:\n",
      "Epoch 14, avg test_loss: 0.013352, test_acc: 0.54\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.007998, train_acc: 0.80\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.007364, train_acc: 0.82\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008692, train_acc: 0.80\n",
      "alexnet1d, trial.157:\n",
      "Epoch 15, avg test_loss: 0.015815, test_acc: 0.50\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.007252, train_acc: 0.88\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.006599, train_acc: 0.80\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009085, train_acc: 0.77\n",
      "alexnet1d, trial.157:\n",
      "Epoch 16, avg test_loss: 0.015214, test_acc: 0.50\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.005292, train_acc: 0.86\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.004480, train_acc: 0.96\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.006963, train_acc: 0.82\n",
      "alexnet1d, trial.157:\n",
      "Epoch 17, avg test_loss: 0.018769, test_acc: 0.51\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.006843, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007141, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007049, train_acc: 0.84\n",
      "alexnet1d, trial.157:\n",
      "Epoch 18, avg test_loss: 0.021202, test_acc: 0.47\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.005829, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.004942, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.003929, train_acc: 0.89\n",
      "alexnet1d, trial.157:\n",
      "Epoch 19, avg test_loss: 0.021280, test_acc: 0.50\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006636, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.003310, train_acc: 0.93\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.005944, train_acc: 0.82\n",
      "alexnet1d, trial.157:\n",
      "Epoch 20, avg test_loss: 0.020732, test_acc: 0.49\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.003335, train_acc: 0.93\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.002357, train_acc: 0.95\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.005171, train_acc: 0.93\n",
      "alexnet1d, trial.157:\n",
      "Epoch 21, avg test_loss: 0.032898, test_acc: 0.49\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.003679, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.003983, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006251, train_acc: 0.84\n",
      "alexnet1d, trial.157:\n",
      "Epoch 22, avg test_loss: 0.022089, test_acc: 0.53\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.003327, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.003065, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.002668, train_acc: 0.96\n",
      "alexnet1d, trial.157:\n",
      "Epoch 23, avg test_loss: 0.028264, test_acc: 0.44\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.002479, train_acc: 0.96\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006668, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.001612, train_acc: 0.96\n",
      "alexnet1d, trial.157:\n",
      "Epoch 24, avg test_loss: 0.028968, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号43个\n",
      "错误信号27个\n",
      "信号正确并预测正确的概率为0.386\n",
      "信号错误并预测正确的概率为0.171\n",
      "总正确率为0.56\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012374, train_acc: 0.50\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012463, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012349, train_acc: 0.55\n",
      "alexnet1d, trial.158:\n",
      "Epoch 0, avg test_loss: 0.009475, test_acc: 0.66\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012547, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012538, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012059, train_acc: 0.54\n",
      "alexnet1d, trial.158:\n",
      "Epoch 1, avg test_loss: 0.009471, test_acc: 0.66\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012880, train_acc: 0.43\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012443, train_acc: 0.48\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012168, train_acc: 0.68\n",
      "alexnet1d, trial.158:\n",
      "Epoch 2, avg test_loss: 0.009551, test_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012194, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011752, train_acc: 0.62\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012065, train_acc: 0.64\n",
      "alexnet1d, trial.158:\n",
      "Epoch 3, avg test_loss: 0.009711, test_acc: 0.60\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011959, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011696, train_acc: 0.71\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011942, train_acc: 0.64\n",
      "alexnet1d, trial.158:\n",
      "Epoch 4, avg test_loss: 0.009906, test_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011968, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011186, train_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.014995, train_acc: 0.59\n",
      "alexnet1d, trial.158:\n",
      "Epoch 5, avg test_loss: 0.010113, test_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012907, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.010988, train_acc: 0.68\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011997, train_acc: 0.64\n",
      "alexnet1d, trial.158:\n",
      "Epoch 6, avg test_loss: 0.009799, test_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012052, train_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011110, train_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011939, train_acc: 0.57\n",
      "alexnet1d, trial.158:\n",
      "Epoch 7, avg test_loss: 0.010175, test_acc: 0.49\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011398, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012001, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.010732, train_acc: 0.70\n",
      "alexnet1d, trial.158:\n",
      "Epoch 8, avg test_loss: 0.010887, test_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011234, train_acc: 0.77\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.008209, train_acc: 0.80\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012533, train_acc: 0.57\n",
      "alexnet1d, trial.158:\n",
      "Epoch 9, avg test_loss: 0.010939, test_acc: 0.50\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.008160, train_acc: 0.84\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010814, train_acc: 0.71\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.009297, train_acc: 0.80\n",
      "alexnet1d, trial.158:\n",
      "Epoch 10, avg test_loss: 0.011029, test_acc: 0.56\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.009458, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.013003, train_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.009238, train_acc: 0.80\n",
      "alexnet1d, trial.158:\n",
      "Epoch 11, avg test_loss: 0.010917, test_acc: 0.51\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009697, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.008240, train_acc: 0.84\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011089, train_acc: 0.71\n",
      "alexnet1d, trial.158:\n",
      "Epoch 12, avg test_loss: 0.011270, test_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011488, train_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009338, train_acc: 0.79\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009865, train_acc: 0.77\n",
      "alexnet1d, trial.158:\n",
      "Epoch 13, avg test_loss: 0.011444, test_acc: 0.51\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009546, train_acc: 0.79\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009582, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.008163, train_acc: 0.82\n",
      "alexnet1d, trial.158:\n",
      "Epoch 14, avg test_loss: 0.011101, test_acc: 0.60\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.007701, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008073, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009588, train_acc: 0.79\n",
      "alexnet1d, trial.158:\n",
      "Epoch 15, avg test_loss: 0.011073, test_acc: 0.53\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009228, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.007208, train_acc: 0.84\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008415, train_acc: 0.84\n",
      "alexnet1d, trial.158:\n",
      "Epoch 16, avg test_loss: 0.010867, test_acc: 0.57\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007580, train_acc: 0.84\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008090, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007154, train_acc: 0.82\n",
      "alexnet1d, trial.158:\n",
      "Epoch 17, avg test_loss: 0.012896, test_acc: 0.56\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007765, train_acc: 0.86\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.006260, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.006133, train_acc: 0.84\n",
      "alexnet1d, trial.158:\n",
      "Epoch 18, avg test_loss: 0.012231, test_acc: 0.53\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008425, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007480, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008223, train_acc: 0.79\n",
      "alexnet1d, trial.158:\n",
      "Epoch 19, avg test_loss: 0.015120, test_acc: 0.51\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.005378, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.005922, train_acc: 0.89\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006367, train_acc: 0.84\n",
      "alexnet1d, trial.158:\n",
      "Epoch 20, avg test_loss: 0.013428, test_acc: 0.61\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006586, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.004430, train_acc: 0.95\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.002687, train_acc: 0.96\n",
      "alexnet1d, trial.158:\n",
      "Epoch 21, avg test_loss: 0.018193, test_acc: 0.44\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005783, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.004063, train_acc: 0.95\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.003681, train_acc: 0.91\n",
      "alexnet1d, trial.158:\n",
      "Epoch 22, avg test_loss: 0.017257, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.002787, train_acc: 0.95\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005971, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.005628, train_acc: 0.86\n",
      "alexnet1d, trial.158:\n",
      "Epoch 23, avg test_loss: 0.021940, test_acc: 0.49\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.002819, train_acc: 0.93\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.002475, train_acc: 0.96\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.004461, train_acc: 0.88\n",
      "alexnet1d, trial.158:\n",
      "Epoch 24, avg test_loss: 0.020033, test_acc: 0.57\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.001572, train_acc: 0.98\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.003123, train_acc: 0.96\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.003222, train_acc: 0.93\n",
      "alexnet1d, trial.158:\n",
      "Epoch 25, avg test_loss: 0.018280, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004503, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005626, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.002436, train_acc: 0.95\n",
      "alexnet1d, trial.158:\n",
      "Epoch 26, avg test_loss: 0.021482, test_acc: 0.50\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号46个\n",
      "错误信号24个\n",
      "信号正确并预测正确的概率为0.343\n",
      "信号错误并预测正确的概率为0.157\n",
      "总正确率为0.50\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012381, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012448, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012160, train_acc: 0.64\n",
      "alexnet1d, trial.159:\n",
      "Epoch 0, avg test_loss: 0.009623, test_acc: 0.63\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012440, train_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012580, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012596, train_acc: 0.48\n",
      "alexnet1d, trial.159:\n",
      "Epoch 1, avg test_loss: 0.009611, test_acc: 0.63\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012119, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012213, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012042, train_acc: 0.55\n",
      "alexnet1d, trial.159:\n",
      "Epoch 2, avg test_loss: 0.009573, test_acc: 0.63\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012189, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012761, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012200, train_acc: 0.59\n",
      "alexnet1d, trial.159:\n",
      "Epoch 3, avg test_loss: 0.009560, test_acc: 0.63\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012176, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011911, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.013050, train_acc: 0.46\n",
      "alexnet1d, trial.159:\n",
      "Epoch 4, avg test_loss: 0.009523, test_acc: 0.63\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012041, train_acc: 0.52\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011793, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012232, train_acc: 0.55\n",
      "alexnet1d, trial.159:\n",
      "Epoch 5, avg test_loss: 0.009586, test_acc: 0.63\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011938, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012334, train_acc: 0.46\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012225, train_acc: 0.50\n",
      "alexnet1d, trial.159:\n",
      "Epoch 6, avg test_loss: 0.009381, test_acc: 0.63\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012220, train_acc: 0.50\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012142, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011260, train_acc: 0.70\n",
      "alexnet1d, trial.159:\n",
      "Epoch 7, avg test_loss: 0.009642, test_acc: 0.63\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011071, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012766, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011859, train_acc: 0.66\n",
      "alexnet1d, trial.159:\n",
      "Epoch 8, avg test_loss: 0.009366, test_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010944, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012376, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012389, train_acc: 0.52\n",
      "alexnet1d, trial.159:\n",
      "Epoch 9, avg test_loss: 0.009975, test_acc: 0.56\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011699, train_acc: 0.55\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011286, train_acc: 0.70\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012025, train_acc: 0.64\n",
      "alexnet1d, trial.159:\n",
      "Epoch 10, avg test_loss: 0.009308, test_acc: 0.60\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011760, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010650, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011826, train_acc: 0.55\n",
      "alexnet1d, trial.159:\n",
      "Epoch 11, avg test_loss: 0.009162, test_acc: 0.60\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009437, train_acc: 0.84\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.012649, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010609, train_acc: 0.70\n",
      "alexnet1d, trial.159:\n",
      "Epoch 12, avg test_loss: 0.010272, test_acc: 0.63\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009835, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010844, train_acc: 0.62\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010683, train_acc: 0.66\n",
      "alexnet1d, trial.159:\n",
      "Epoch 13, avg test_loss: 0.009418, test_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010463, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010837, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010564, train_acc: 0.66\n",
      "alexnet1d, trial.159:\n",
      "Epoch 14, avg test_loss: 0.010213, test_acc: 0.56\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009978, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.011149, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011178, train_acc: 0.61\n",
      "alexnet1d, trial.159:\n",
      "Epoch 15, avg test_loss: 0.010494, test_acc: 0.63\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008322, train_acc: 0.80\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008877, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010786, train_acc: 0.62\n",
      "alexnet1d, trial.159:\n",
      "Epoch 16, avg test_loss: 0.010506, test_acc: 0.60\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.010111, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010575, train_acc: 0.66\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008212, train_acc: 0.80\n",
      "alexnet1d, trial.159:\n",
      "Epoch 17, avg test_loss: 0.010235, test_acc: 0.64\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008826, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008241, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.010692, train_acc: 0.71\n",
      "alexnet1d, trial.159:\n",
      "Epoch 18, avg test_loss: 0.011295, test_acc: 0.60\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006472, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008738, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007383, train_acc: 0.80\n",
      "alexnet1d, trial.159:\n",
      "Epoch 19, avg test_loss: 0.013262, test_acc: 0.54\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007918, train_acc: 0.77\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007010, train_acc: 0.88\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008812, train_acc: 0.79\n",
      "alexnet1d, trial.159:\n",
      "Epoch 20, avg test_loss: 0.014503, test_acc: 0.54\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006851, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006046, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007190, train_acc: 0.79\n",
      "alexnet1d, trial.159:\n",
      "Epoch 21, avg test_loss: 0.018612, test_acc: 0.59\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005552, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007816, train_acc: 0.79\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.004782, train_acc: 0.91\n",
      "alexnet1d, trial.159:\n",
      "Epoch 22, avg test_loss: 0.015365, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006525, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005686, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006569, train_acc: 0.86\n",
      "alexnet1d, trial.159:\n",
      "Epoch 23, avg test_loss: 0.021573, test_acc: 0.54\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006438, train_acc: 0.79\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004831, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.007481, train_acc: 0.82\n",
      "alexnet1d, trial.159:\n",
      "Epoch 24, avg test_loss: 0.017437, test_acc: 0.60\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.006060, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004929, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005557, train_acc: 0.84\n",
      "alexnet1d, trial.159:\n",
      "Epoch 25, avg test_loss: 0.022740, test_acc: 0.51\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005185, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005094, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.002885, train_acc: 0.96\n",
      "alexnet1d, trial.159:\n",
      "Epoch 26, avg test_loss: 0.023837, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003693, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.003588, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.006599, train_acc: 0.84\n",
      "alexnet1d, trial.159:\n",
      "Epoch 27, avg test_loss: 0.025651, test_acc: 0.51\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004324, train_acc: 0.89\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004583, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.003885, train_acc: 0.89\n",
      "alexnet1d, trial.159:\n",
      "Epoch 28, avg test_loss: 0.028040, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.003489, train_acc: 0.91\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.003652, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.002079, train_acc: 0.96\n",
      "alexnet1d, trial.159:\n",
      "Epoch 29, avg test_loss: 0.031202, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号44个\n",
      "错误信号26个\n",
      "信号正确并预测正确的概率为0.329\n",
      "信号错误并预测正确的概率为0.214\n",
      "总正确率为0.54\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012434, train_acc: 0.45\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.015380, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012405, train_acc: 0.45\n",
      "alexnet1d, trial.160:\n",
      "Epoch 0, avg test_loss: 0.009848, test_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012298, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.013400, train_acc: 0.41\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011935, train_acc: 0.61\n",
      "alexnet1d, trial.160:\n",
      "Epoch 1, avg test_loss: 0.009821, test_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012573, train_acc: 0.46\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011916, train_acc: 0.70\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012237, train_acc: 0.55\n",
      "alexnet1d, trial.160:\n",
      "Epoch 2, avg test_loss: 0.009975, test_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011788, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012321, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011919, train_acc: 0.66\n",
      "alexnet1d, trial.160:\n",
      "Epoch 3, avg test_loss: 0.009925, test_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011933, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011778, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011947, train_acc: 0.59\n",
      "alexnet1d, trial.160:\n",
      "Epoch 4, avg test_loss: 0.009990, test_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012152, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.010709, train_acc: 0.79\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.014228, train_acc: 0.45\n",
      "alexnet1d, trial.160:\n",
      "Epoch 5, avg test_loss: 0.010201, test_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.010779, train_acc: 0.71\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011879, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011699, train_acc: 0.62\n",
      "alexnet1d, trial.160:\n",
      "Epoch 6, avg test_loss: 0.009750, test_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012260, train_acc: 0.48\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011540, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011624, train_acc: 0.62\n",
      "alexnet1d, trial.160:\n",
      "Epoch 7, avg test_loss: 0.009948, test_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011501, train_acc: 0.68\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012151, train_acc: 0.55\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.010976, train_acc: 0.66\n",
      "alexnet1d, trial.160:\n",
      "Epoch 8, avg test_loss: 0.009709, test_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011480, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011446, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011343, train_acc: 0.61\n",
      "alexnet1d, trial.160:\n",
      "Epoch 9, avg test_loss: 0.009430, test_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011254, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011875, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011869, train_acc: 0.62\n",
      "alexnet1d, trial.160:\n",
      "Epoch 10, avg test_loss: 0.010271, test_acc: 0.54\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.012042, train_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011180, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011308, train_acc: 0.61\n",
      "alexnet1d, trial.160:\n",
      "Epoch 11, avg test_loss: 0.009494, test_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010547, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010725, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.012923, train_acc: 0.54\n",
      "alexnet1d, trial.160:\n",
      "Epoch 12, avg test_loss: 0.010112, test_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011512, train_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010416, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010858, train_acc: 0.61\n",
      "alexnet1d, trial.160:\n",
      "Epoch 13, avg test_loss: 0.009579, test_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010140, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009651, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010276, train_acc: 0.66\n",
      "alexnet1d, trial.160:\n",
      "Epoch 14, avg test_loss: 0.009491, test_acc: 0.59\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009313, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008930, train_acc: 0.80\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010156, train_acc: 0.70\n",
      "alexnet1d, trial.160:\n",
      "Epoch 15, avg test_loss: 0.010056, test_acc: 0.60\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008264, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010905, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009643, train_acc: 0.70\n",
      "alexnet1d, trial.160:\n",
      "Epoch 16, avg test_loss: 0.009524, test_acc: 0.57\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008020, train_acc: 0.86\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009709, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010259, train_acc: 0.71\n",
      "alexnet1d, trial.160:\n",
      "Epoch 17, avg test_loss: 0.010038, test_acc: 0.56\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008649, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009969, train_acc: 0.68\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008908, train_acc: 0.75\n",
      "alexnet1d, trial.160:\n",
      "Epoch 18, avg test_loss: 0.010147, test_acc: 0.64\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007696, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.010079, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008892, train_acc: 0.75\n",
      "alexnet1d, trial.160:\n",
      "Epoch 19, avg test_loss: 0.011014, test_acc: 0.54\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008386, train_acc: 0.77\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.009610, train_acc: 0.75\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007452, train_acc: 0.86\n",
      "alexnet1d, trial.160:\n",
      "Epoch 20, avg test_loss: 0.010366, test_acc: 0.60\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007017, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.008836, train_acc: 0.71\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.009062, train_acc: 0.77\n",
      "alexnet1d, trial.160:\n",
      "Epoch 21, avg test_loss: 0.011313, test_acc: 0.56\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006728, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007681, train_acc: 0.75\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008131, train_acc: 0.82\n",
      "alexnet1d, trial.160:\n",
      "Epoch 22, avg test_loss: 0.012501, test_acc: 0.56\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006129, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007000, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006663, train_acc: 0.82\n",
      "alexnet1d, trial.160:\n",
      "Epoch 23, avg test_loss: 0.013027, test_acc: 0.56\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006614, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006403, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005450, train_acc: 0.88\n",
      "alexnet1d, trial.160:\n",
      "Epoch 24, avg test_loss: 0.015766, test_acc: 0.57\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005207, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006242, train_acc: 0.80\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005041, train_acc: 0.88\n",
      "alexnet1d, trial.160:\n",
      "Epoch 25, avg test_loss: 0.015437, test_acc: 0.56\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005907, train_acc: 0.82\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004189, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.008207, train_acc: 0.79\n",
      "alexnet1d, trial.160:\n",
      "Epoch 26, avg test_loss: 0.017541, test_acc: 0.47\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.002720, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.007250, train_acc: 0.82\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.007262, train_acc: 0.89\n",
      "alexnet1d, trial.160:\n",
      "Epoch 27, avg test_loss: 0.015143, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.005779, train_acc: 0.86\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.005124, train_acc: 0.88\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.004343, train_acc: 0.95\n",
      "alexnet1d, trial.160:\n",
      "Epoch 28, avg test_loss: 0.016507, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.003251, train_acc: 0.96\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.004949, train_acc: 0.88\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.004255, train_acc: 0.91\n",
      "alexnet1d, trial.160:\n",
      "Epoch 29, avg test_loss: 0.018109, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.002697, train_acc: 0.96\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.002526, train_acc: 0.96\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.003792, train_acc: 0.93\n",
      "alexnet1d, trial.160:\n",
      "Epoch 30, avg test_loss: 0.026912, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号38个\n",
      "错误信号32个\n",
      "信号正确并预测正确的概率为0.343\n",
      "信号错误并预测正确的概率为0.171\n",
      "总正确率为0.51\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012356, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.019849, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012364, train_acc: 0.52\n",
      "alexnet1d, trial.161:\n",
      "Epoch 0, avg test_loss: 0.009833, test_acc: 0.63\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012357, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012341, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012320, train_acc: 0.66\n",
      "alexnet1d, trial.161:\n",
      "Epoch 1, avg test_loss: 0.009825, test_acc: 0.63\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012244, train_acc: 0.66\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012247, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012020, train_acc: 0.66\n",
      "alexnet1d, trial.161:\n",
      "Epoch 2, avg test_loss: 0.009672, test_acc: 0.63\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012569, train_acc: 0.45\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012112, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011998, train_acc: 0.59\n",
      "alexnet1d, trial.161:\n",
      "Epoch 3, avg test_loss: 0.009534, test_acc: 0.63\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011665, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012018, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012737, train_acc: 0.55\n",
      "alexnet1d, trial.161:\n",
      "Epoch 4, avg test_loss: 0.009614, test_acc: 0.63\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012466, train_acc: 0.50\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012040, train_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012252, train_acc: 0.59\n",
      "alexnet1d, trial.161:\n",
      "Epoch 5, avg test_loss: 0.009757, test_acc: 0.63\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012126, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012546, train_acc: 0.43\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012200, train_acc: 0.61\n",
      "alexnet1d, trial.161:\n",
      "Epoch 6, avg test_loss: 0.009753, test_acc: 0.63\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012122, train_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012228, train_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012490, train_acc: 0.46\n",
      "alexnet1d, trial.161:\n",
      "Epoch 7, avg test_loss: 0.009687, test_acc: 0.63\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011962, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012342, train_acc: 0.50\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012286, train_acc: 0.55\n",
      "alexnet1d, trial.161:\n",
      "Epoch 8, avg test_loss: 0.009612, test_acc: 0.63\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012240, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012314, train_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011932, train_acc: 0.59\n",
      "alexnet1d, trial.161:\n",
      "Epoch 9, avg test_loss: 0.009540, test_acc: 0.63\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011879, train_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011906, train_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012579, train_acc: 0.55\n",
      "alexnet1d, trial.161:\n",
      "Epoch 10, avg test_loss: 0.009548, test_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011678, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.013138, train_acc: 0.55\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.012031, train_acc: 0.54\n",
      "alexnet1d, trial.161:\n",
      "Epoch 11, avg test_loss: 0.009530, test_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011983, train_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011583, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011798, train_acc: 0.54\n",
      "alexnet1d, trial.161:\n",
      "Epoch 12, avg test_loss: 0.009581, test_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011375, train_acc: 0.62\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.012143, train_acc: 0.57\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010900, train_acc: 0.68\n",
      "alexnet1d, trial.161:\n",
      "Epoch 13, avg test_loss: 0.009631, test_acc: 0.64\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.011344, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011304, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011718, train_acc: 0.55\n",
      "alexnet1d, trial.161:\n",
      "Epoch 14, avg test_loss: 0.009175, test_acc: 0.67\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010879, train_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.012477, train_acc: 0.54\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011555, train_acc: 0.61\n",
      "alexnet1d, trial.161:\n",
      "Epoch 15, avg test_loss: 0.009238, test_acc: 0.67\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.011099, train_acc: 0.61\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.011203, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.011425, train_acc: 0.62\n",
      "alexnet1d, trial.161:\n",
      "Epoch 16, avg test_loss: 0.010356, test_acc: 0.59\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.010555, train_acc: 0.68\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010859, train_acc: 0.66\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010294, train_acc: 0.66\n",
      "alexnet1d, trial.161:\n",
      "Epoch 17, avg test_loss: 0.009458, test_acc: 0.60\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009731, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.010789, train_acc: 0.70\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.010295, train_acc: 0.71\n",
      "alexnet1d, trial.161:\n",
      "Epoch 18, avg test_loss: 0.009043, test_acc: 0.67\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009824, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009501, train_acc: 0.70\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.010749, train_acc: 0.66\n",
      "alexnet1d, trial.161:\n",
      "Epoch 19, avg test_loss: 0.009850, test_acc: 0.63\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.011169, train_acc: 0.66\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.011105, train_acc: 0.59\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.010193, train_acc: 0.70\n",
      "alexnet1d, trial.161:\n",
      "Epoch 20, avg test_loss: 0.009885, test_acc: 0.60\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.010561, train_acc: 0.68\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.011208, train_acc: 0.57\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.009327, train_acc: 0.68\n",
      "alexnet1d, trial.161:\n",
      "Epoch 21, avg test_loss: 0.011276, test_acc: 0.56\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.009251, train_acc: 0.71\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.009503, train_acc: 0.73\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.009224, train_acc: 0.71\n",
      "alexnet1d, trial.161:\n",
      "Epoch 22, avg test_loss: 0.010366, test_acc: 0.63\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.010119, train_acc: 0.66\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.009646, train_acc: 0.68\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.010850, train_acc: 0.64\n",
      "alexnet1d, trial.161:\n",
      "Epoch 23, avg test_loss: 0.010861, test_acc: 0.60\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.010326, train_acc: 0.64\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.012348, train_acc: 0.61\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.009772, train_acc: 0.77\n",
      "alexnet1d, trial.161:\n",
      "Epoch 24, avg test_loss: 0.010219, test_acc: 0.66\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.009936, train_acc: 0.80\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.009153, train_acc: 0.71\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.009785, train_acc: 0.71\n",
      "alexnet1d, trial.161:\n",
      "Epoch 25, avg test_loss: 0.011633, test_acc: 0.67\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.009068, train_acc: 0.75\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.008794, train_acc: 0.75\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.009721, train_acc: 0.75\n",
      "alexnet1d, trial.161:\n",
      "Epoch 26, avg test_loss: 0.011944, test_acc: 0.64\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.009385, train_acc: 0.73\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.009445, train_acc: 0.75\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.008060, train_acc: 0.77\n",
      "alexnet1d, trial.161:\n",
      "Epoch 27, avg test_loss: 0.011679, test_acc: 0.67\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.008231, train_acc: 0.73\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.010959, train_acc: 0.64\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.008546, train_acc: 0.75\n",
      "alexnet1d, trial.161:\n",
      "Epoch 28, avg test_loss: 0.012838, test_acc: 0.66\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.007195, train_acc: 0.80\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.006685, train_acc: 0.84\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.009304, train_acc: 0.75\n",
      "alexnet1d, trial.161:\n",
      "Epoch 29, avg test_loss: 0.015341, test_acc: 0.56\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.008231, train_acc: 0.80\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.007247, train_acc: 0.79\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.006679, train_acc: 0.86\n",
      "alexnet1d, trial.161:\n",
      "Epoch 30, avg test_loss: 0.015166, test_acc: 0.61\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.006795, train_acc: 0.79\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.006850, train_acc: 0.79\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.007004, train_acc: 0.80\n",
      "alexnet1d, trial.161:\n",
      "Epoch 31, avg test_loss: 0.017621, test_acc: 0.59\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.006761, train_acc: 0.84\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.006445, train_acc: 0.86\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.004843, train_acc: 0.86\n",
      "alexnet1d, trial.161:\n",
      "Epoch 32, avg test_loss: 0.020150, test_acc: 0.54\n",
      "Train Epoch 33, lr: 0.000377, 0/280, avg loss: 0.004616, train_acc: 0.91\n",
      "Train Epoch 33, lr: 0.000377, 112/280, avg loss: 0.006539, train_acc: 0.79\n",
      "Train Epoch 33, lr: 0.000377, 224/280, avg loss: 0.005098, train_acc: 0.93\n",
      "alexnet1d, trial.161:\n",
      "Epoch 33, avg test_loss: 0.024691, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 34, lr: 0.000377, 0/280, avg loss: 0.003637, train_acc: 0.95\n",
      "Train Epoch 34, lr: 0.000377, 112/280, avg loss: 0.003835, train_acc: 0.91\n",
      "Train Epoch 34, lr: 0.000377, 224/280, avg loss: 0.005214, train_acc: 0.86\n",
      "alexnet1d, trial.161:\n",
      "Epoch 34, avg test_loss: 0.023324, test_acc: 0.66\n",
      "Train Epoch 35, lr: 0.000321, 0/280, avg loss: 0.006141, train_acc: 0.86\n",
      "Train Epoch 35, lr: 0.000321, 112/280, avg loss: 0.005526, train_acc: 0.86\n",
      "Train Epoch 35, lr: 0.000321, 224/280, avg loss: 0.007092, train_acc: 0.80\n",
      "alexnet1d, trial.161:\n",
      "Epoch 35, avg test_loss: 0.031445, test_acc: 0.47\n",
      "Train Epoch 36, lr: 0.000321, 0/280, avg loss: 0.005149, train_acc: 0.88\n",
      "Train Epoch 36, lr: 0.000321, 112/280, avg loss: 0.008852, train_acc: 0.77\n",
      "Train Epoch 36, lr: 0.000321, 224/280, avg loss: 0.004451, train_acc: 0.89\n",
      "alexnet1d, trial.161:\n",
      "Epoch 36, avg test_loss: 0.022569, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 37, lr: 0.000321, 0/280, avg loss: 0.003304, train_acc: 0.91\n",
      "Train Epoch 37, lr: 0.000321, 112/280, avg loss: 0.003401, train_acc: 0.91\n",
      "Train Epoch 37, lr: 0.000321, 224/280, avg loss: 0.006765, train_acc: 0.86\n",
      "alexnet1d, trial.161:\n",
      "Epoch 37, avg test_loss: 0.026097, test_acc: 0.56\n",
      "Train Epoch 38, lr: 0.000321, 0/280, avg loss: 0.002346, train_acc: 0.98\n",
      "Train Epoch 38, lr: 0.000321, 112/280, avg loss: 0.003024, train_acc: 0.91\n",
      "Train Epoch 38, lr: 0.000321, 224/280, avg loss: 0.006936, train_acc: 0.84\n",
      "alexnet1d, trial.161:\n",
      "Epoch 38, avg test_loss: 0.025769, test_acc: 0.60\n",
      "Train Epoch 39, lr: 0.000321, 0/280, avg loss: 0.003099, train_acc: 0.89\n",
      "Train Epoch 39, lr: 0.000321, 112/280, avg loss: 0.002635, train_acc: 0.95\n",
      "Train Epoch 39, lr: 0.000321, 224/280, avg loss: 0.003456, train_acc: 0.91\n",
      "alexnet1d, trial.161:\n",
      "Epoch 39, avg test_loss: 0.029110, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 40, lr: 0.000272, 0/280, avg loss: 0.003399, train_acc: 0.88\n",
      "Train Epoch 40, lr: 0.000272, 112/280, avg loss: 0.001627, train_acc: 0.98\n",
      "Train Epoch 40, lr: 0.000272, 224/280, avg loss: 0.002690, train_acc: 0.91\n",
      "alexnet1d, trial.161:\n",
      "Epoch 40, avg test_loss: 0.031072, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号44个\n",
      "错误信号26个\n",
      "信号正确并预测正确的概率为0.386\n",
      "信号错误并预测正确的概率为0.171\n",
      "总正确率为0.56\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012473, train_acc: 0.39\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.013880, train_acc: 0.46\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012314, train_acc: 0.61\n",
      "alexnet1d, trial.162:\n",
      "Epoch 0, avg test_loss: 0.009892, test_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012365, train_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012217, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012305, train_acc: 0.52\n",
      "alexnet1d, trial.162:\n",
      "Epoch 1, avg test_loss: 0.010242, test_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012103, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012097, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011727, train_acc: 0.70\n",
      "alexnet1d, trial.162:\n",
      "Epoch 2, avg test_loss: 0.009891, test_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011927, train_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012457, train_acc: 0.50\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011892, train_acc: 0.61\n",
      "alexnet1d, trial.162:\n",
      "Epoch 3, avg test_loss: 0.010018, test_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011768, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011694, train_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011179, train_acc: 0.62\n",
      "alexnet1d, trial.162:\n",
      "Epoch 4, avg test_loss: 0.010074, test_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012343, train_acc: 0.50\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011695, train_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012418, train_acc: 0.50\n",
      "alexnet1d, trial.162:\n",
      "Epoch 5, avg test_loss: 0.010041, test_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011942, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011597, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011432, train_acc: 0.66\n",
      "alexnet1d, trial.162:\n",
      "Epoch 6, avg test_loss: 0.010395, test_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.009768, train_acc: 0.73\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011560, train_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.010355, train_acc: 0.68\n",
      "alexnet1d, trial.162:\n",
      "Epoch 7, avg test_loss: 0.010275, test_acc: 0.47\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012533, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.010485, train_acc: 0.70\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.010905, train_acc: 0.70\n",
      "alexnet1d, trial.162:\n",
      "Epoch 8, avg test_loss: 0.010744, test_acc: 0.53\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011476, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012458, train_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011327, train_acc: 0.57\n",
      "alexnet1d, trial.162:\n",
      "Epoch 9, avg test_loss: 0.011052, test_acc: 0.49\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010107, train_acc: 0.73\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.009958, train_acc: 0.70\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.009975, train_acc: 0.70\n",
      "alexnet1d, trial.162:\n",
      "Epoch 10, avg test_loss: 0.011157, test_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011570, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011266, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.009127, train_acc: 0.79\n",
      "alexnet1d, trial.162:\n",
      "Epoch 11, avg test_loss: 0.011411, test_acc: 0.47\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009093, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010070, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010504, train_acc: 0.73\n",
      "alexnet1d, trial.162:\n",
      "Epoch 12, avg test_loss: 0.011791, test_acc: 0.54\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011092, train_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011891, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011497, train_acc: 0.75\n",
      "alexnet1d, trial.162:\n",
      "Epoch 13, avg test_loss: 0.012498, test_acc: 0.50\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.007412, train_acc: 0.80\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.008914, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.007469, train_acc: 0.79\n",
      "alexnet1d, trial.162:\n",
      "Epoch 14, avg test_loss: 0.011425, test_acc: 0.50\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008894, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.006359, train_acc: 0.86\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011185, train_acc: 0.71\n",
      "alexnet1d, trial.162:\n",
      "Epoch 15, avg test_loss: 0.013690, test_acc: 0.53\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008873, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008320, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.007544, train_acc: 0.79\n",
      "alexnet1d, trial.162:\n",
      "Epoch 16, avg test_loss: 0.011262, test_acc: 0.61\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.006826, train_acc: 0.84\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008141, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007565, train_acc: 0.80\n",
      "alexnet1d, trial.162:\n",
      "Epoch 17, avg test_loss: 0.016111, test_acc: 0.53\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.006461, train_acc: 0.86\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007744, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.005915, train_acc: 0.88\n",
      "alexnet1d, trial.162:\n",
      "Epoch 18, avg test_loss: 0.014141, test_acc: 0.54\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006500, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008130, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008220, train_acc: 0.79\n",
      "alexnet1d, trial.162:\n",
      "Epoch 19, avg test_loss: 0.016762, test_acc: 0.54\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007113, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.006055, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007390, train_acc: 0.77\n",
      "alexnet1d, trial.162:\n",
      "Epoch 20, avg test_loss: 0.015178, test_acc: 0.56\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007640, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.005340, train_acc: 0.89\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.004976, train_acc: 0.88\n",
      "alexnet1d, trial.162:\n",
      "Epoch 21, avg test_loss: 0.016733, test_acc: 0.50\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007743, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005262, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.004748, train_acc: 0.88\n",
      "alexnet1d, trial.162:\n",
      "Epoch 22, avg test_loss: 0.016210, test_acc: 0.51\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.004117, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.004647, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.005702, train_acc: 0.88\n",
      "alexnet1d, trial.162:\n",
      "Epoch 23, avg test_loss: 0.018700, test_acc: 0.49\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006190, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.002474, train_acc: 0.98\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.002614, train_acc: 0.96\n",
      "alexnet1d, trial.162:\n",
      "Epoch 24, avg test_loss: 0.022171, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.003640, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004609, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.003908, train_acc: 0.95\n",
      "alexnet1d, trial.162:\n",
      "Epoch 25, avg test_loss: 0.022291, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.002006, train_acc: 0.96\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.001992, train_acc: 0.96\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.002600, train_acc: 0.89\n",
      "alexnet1d, trial.162:\n",
      "Epoch 26, avg test_loss: 0.023142, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003232, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.002271, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004593, train_acc: 0.93\n",
      "alexnet1d, trial.162:\n",
      "Epoch 27, avg test_loss: 0.030008, test_acc: 0.47\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号38个\n",
      "错误信号32个\n",
      "信号正确并预测正确的概率为0.3\n",
      "信号错误并预测正确的概率为0.171\n",
      "总正确率为0.47\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012373, train_acc: 0.48\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012839, train_acc: 0.52\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012648, train_acc: 0.50\n",
      "alexnet1d, trial.163:\n",
      "Epoch 0, avg test_loss: 0.009563, test_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012245, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.011906, train_acc: 0.62\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011979, train_acc: 0.57\n",
      "alexnet1d, trial.163:\n",
      "Epoch 1, avg test_loss: 0.009483, test_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012181, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012085, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012265, train_acc: 0.54\n",
      "alexnet1d, trial.163:\n",
      "Epoch 2, avg test_loss: 0.009626, test_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012015, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011930, train_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012798, train_acc: 0.50\n",
      "alexnet1d, trial.163:\n",
      "Epoch 3, avg test_loss: 0.009429, test_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011714, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011912, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012090, train_acc: 0.61\n",
      "alexnet1d, trial.163:\n",
      "Epoch 4, avg test_loss: 0.009589, test_acc: 0.60\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012254, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011945, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012902, train_acc: 0.55\n",
      "alexnet1d, trial.163:\n",
      "Epoch 5, avg test_loss: 0.009528, test_acc: 0.60\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.010974, train_acc: 0.70\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012638, train_acc: 0.46\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012022, train_acc: 0.57\n",
      "alexnet1d, trial.163:\n",
      "Epoch 6, avg test_loss: 0.009652, test_acc: 0.56\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012474, train_acc: 0.45\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011214, train_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011680, train_acc: 0.61\n",
      "alexnet1d, trial.163:\n",
      "Epoch 7, avg test_loss: 0.009739, test_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012199, train_acc: 0.52\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011709, train_acc: 0.68\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011747, train_acc: 0.66\n",
      "alexnet1d, trial.163:\n",
      "Epoch 8, avg test_loss: 0.010628, test_acc: 0.56\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010255, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012445, train_acc: 0.52\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011623, train_acc: 0.55\n",
      "alexnet1d, trial.163:\n",
      "Epoch 9, avg test_loss: 0.012122, test_acc: 0.40\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010350, train_acc: 0.71\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011756, train_acc: 0.55\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011257, train_acc: 0.64\n",
      "alexnet1d, trial.163:\n",
      "Epoch 10, avg test_loss: 0.009694, test_acc: 0.63\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011862, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011552, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010506, train_acc: 0.68\n",
      "alexnet1d, trial.163:\n",
      "Epoch 11, avg test_loss: 0.009868, test_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010562, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.009770, train_acc: 0.77\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011636, train_acc: 0.64\n",
      "alexnet1d, trial.163:\n",
      "Epoch 12, avg test_loss: 0.010596, test_acc: 0.60\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009346, train_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009993, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009707, train_acc: 0.68\n",
      "alexnet1d, trial.163:\n",
      "Epoch 13, avg test_loss: 0.011643, test_acc: 0.63\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009575, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011462, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009702, train_acc: 0.73\n",
      "alexnet1d, trial.163:\n",
      "Epoch 14, avg test_loss: 0.010850, test_acc: 0.59\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.012022, train_acc: 0.62\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008966, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009481, train_acc: 0.73\n",
      "alexnet1d, trial.163:\n",
      "Epoch 15, avg test_loss: 0.010544, test_acc: 0.63\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009571, train_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009490, train_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009501, train_acc: 0.73\n",
      "alexnet1d, trial.163:\n",
      "Epoch 16, avg test_loss: 0.011838, test_acc: 0.57\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009375, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.011282, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007351, train_acc: 0.82\n",
      "alexnet1d, trial.163:\n",
      "Epoch 17, avg test_loss: 0.011840, test_acc: 0.59\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007377, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009845, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008944, train_acc: 0.73\n",
      "alexnet1d, trial.163:\n",
      "Epoch 18, avg test_loss: 0.012415, test_acc: 0.61\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008167, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.010589, train_acc: 0.71\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009412, train_acc: 0.68\n",
      "alexnet1d, trial.163:\n",
      "Epoch 19, avg test_loss: 0.011819, test_acc: 0.57\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006712, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007677, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.011074, train_acc: 0.59\n",
      "alexnet1d, trial.163:\n",
      "Epoch 20, avg test_loss: 0.013325, test_acc: 0.54\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006636, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006977, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007660, train_acc: 0.80\n",
      "alexnet1d, trial.163:\n",
      "Epoch 21, avg test_loss: 0.012836, test_acc: 0.63\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007531, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006488, train_acc: 0.93\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.005035, train_acc: 0.88\n",
      "alexnet1d, trial.163:\n",
      "Epoch 22, avg test_loss: 0.015781, test_acc: 0.61\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006527, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005229, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.007100, train_acc: 0.79\n",
      "alexnet1d, trial.163:\n",
      "Epoch 23, avg test_loss: 0.016724, test_acc: 0.66\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005066, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.002187, train_acc: 0.98\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.008780, train_acc: 0.80\n",
      "alexnet1d, trial.163:\n",
      "Epoch 24, avg test_loss: 0.022725, test_acc: 0.57\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005538, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.003384, train_acc: 0.95\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005895, train_acc: 0.84\n",
      "alexnet1d, trial.163:\n",
      "Epoch 25, avg test_loss: 0.017505, test_acc: 0.67\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004164, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.003609, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004506, train_acc: 0.88\n",
      "alexnet1d, trial.163:\n",
      "Epoch 26, avg test_loss: 0.025493, test_acc: 0.54\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.005136, train_acc: 0.88\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.003393, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004386, train_acc: 0.89\n",
      "alexnet1d, trial.163:\n",
      "Epoch 27, avg test_loss: 0.022028, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.002219, train_acc: 0.96\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.002464, train_acc: 0.96\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.003165, train_acc: 0.93\n",
      "alexnet1d, trial.163:\n",
      "Epoch 28, avg test_loss: 0.025373, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.002282, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.003004, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.002939, train_acc: 0.91\n",
      "alexnet1d, trial.163:\n",
      "Epoch 29, avg test_loss: 0.026340, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.001765, train_acc: 0.98\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.002798, train_acc: 0.96\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.003528, train_acc: 0.91\n",
      "alexnet1d, trial.163:\n",
      "Epoch 30, avg test_loss: 0.031050, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号43个\n",
      "错误信号27个\n",
      "信号正确并预测正确的概率为0.357\n",
      "信号错误并预测正确的概率为0.229\n",
      "总正确率为0.59\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012330, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012080, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012689, train_acc: 0.55\n",
      "alexnet1d, trial.164:\n",
      "Epoch 0, avg test_loss: 0.009825, test_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011858, train_acc: 0.62\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012450, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012187, train_acc: 0.59\n",
      "alexnet1d, trial.164:\n",
      "Epoch 1, avg test_loss: 0.010008, test_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012723, train_acc: 0.48\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012209, train_acc: 0.52\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012078, train_acc: 0.62\n",
      "alexnet1d, trial.164:\n",
      "Epoch 2, avg test_loss: 0.009844, test_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012051, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012352, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011850, train_acc: 0.64\n",
      "alexnet1d, trial.164:\n",
      "Epoch 3, avg test_loss: 0.009932, test_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011504, train_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012877, train_acc: 0.48\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011941, train_acc: 0.61\n",
      "alexnet1d, trial.164:\n",
      "Epoch 4, avg test_loss: 0.009866, test_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011908, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011816, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011654, train_acc: 0.62\n",
      "alexnet1d, trial.164:\n",
      "Epoch 5, avg test_loss: 0.009868, test_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011922, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011145, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011122, train_acc: 0.64\n",
      "alexnet1d, trial.164:\n",
      "Epoch 6, avg test_loss: 0.010171, test_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.010895, train_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012420, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011325, train_acc: 0.66\n",
      "alexnet1d, trial.164:\n",
      "Epoch 7, avg test_loss: 0.009858, test_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010821, train_acc: 0.70\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011279, train_acc: 0.70\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011836, train_acc: 0.62\n",
      "alexnet1d, trial.164:\n",
      "Epoch 8, avg test_loss: 0.010281, test_acc: 0.51\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012206, train_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011432, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010420, train_acc: 0.77\n",
      "alexnet1d, trial.164:\n",
      "Epoch 9, avg test_loss: 0.010410, test_acc: 0.53\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010430, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010128, train_acc: 0.70\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010494, train_acc: 0.64\n",
      "alexnet1d, trial.164:\n",
      "Epoch 10, avg test_loss: 0.010386, test_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011871, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010426, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010098, train_acc: 0.71\n",
      "alexnet1d, trial.164:\n",
      "Epoch 11, avg test_loss: 0.010530, test_acc: 0.53\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009241, train_acc: 0.75\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.009769, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.012152, train_acc: 0.64\n",
      "alexnet1d, trial.164:\n",
      "Epoch 12, avg test_loss: 0.012064, test_acc: 0.60\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009535, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009392, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010072, train_acc: 0.73\n",
      "alexnet1d, trial.164:\n",
      "Epoch 13, avg test_loss: 0.010891, test_acc: 0.56\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010598, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009940, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.008152, train_acc: 0.82\n",
      "alexnet1d, trial.164:\n",
      "Epoch 14, avg test_loss: 0.011300, test_acc: 0.57\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.007653, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008103, train_acc: 0.86\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008298, train_acc: 0.73\n",
      "alexnet1d, trial.164:\n",
      "Epoch 15, avg test_loss: 0.013183, test_acc: 0.53\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.011301, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.011571, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010394, train_acc: 0.70\n",
      "alexnet1d, trial.164:\n",
      "Epoch 16, avg test_loss: 0.012066, test_acc: 0.59\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008832, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009288, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008103, train_acc: 0.82\n",
      "alexnet1d, trial.164:\n",
      "Epoch 17, avg test_loss: 0.011118, test_acc: 0.53\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009532, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.006890, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.010022, train_acc: 0.64\n",
      "alexnet1d, trial.164:\n",
      "Epoch 18, avg test_loss: 0.010779, test_acc: 0.59\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006587, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007680, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007660, train_acc: 0.79\n",
      "alexnet1d, trial.164:\n",
      "Epoch 19, avg test_loss: 0.011940, test_acc: 0.59\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006202, train_acc: 0.89\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.005678, train_acc: 0.88\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006112, train_acc: 0.84\n",
      "alexnet1d, trial.164:\n",
      "Epoch 20, avg test_loss: 0.012976, test_acc: 0.61\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.004571, train_acc: 0.91\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.003789, train_acc: 0.93\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007090, train_acc: 0.80\n",
      "alexnet1d, trial.164:\n",
      "Epoch 21, avg test_loss: 0.012979, test_acc: 0.63\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005609, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006017, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007116, train_acc: 0.82\n",
      "alexnet1d, trial.164:\n",
      "Epoch 22, avg test_loss: 0.014420, test_acc: 0.59\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.004531, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.004781, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.008235, train_acc: 0.80\n",
      "alexnet1d, trial.164:\n",
      "Epoch 23, avg test_loss: 0.015360, test_acc: 0.60\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004467, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.003346, train_acc: 0.95\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.009905, train_acc: 0.80\n",
      "alexnet1d, trial.164:\n",
      "Epoch 24, avg test_loss: 0.014435, test_acc: 0.59\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.002720, train_acc: 0.98\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006092, train_acc: 0.82\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.003244, train_acc: 0.96\n",
      "alexnet1d, trial.164:\n",
      "Epoch 25, avg test_loss: 0.016244, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.002175, train_acc: 0.98\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004859, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.002195, train_acc: 0.96\n",
      "alexnet1d, trial.164:\n",
      "Epoch 26, avg test_loss: 0.018630, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.002077, train_acc: 0.96\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.001983, train_acc: 0.96\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003301, train_acc: 0.93\n",
      "alexnet1d, trial.164:\n",
      "Epoch 27, avg test_loss: 0.016362, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.002580, train_acc: 0.96\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.003561, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.002685, train_acc: 0.95\n",
      "alexnet1d, trial.164:\n",
      "Epoch 28, avg test_loss: 0.022406, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号38个\n",
      "错误信号32个\n",
      "信号正确并预测正确的概率为0.429\n",
      "信号错误并预测正确的概率为0.171\n",
      "总正确率为0.60\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012484, train_acc: 0.43\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012307, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.013529, train_acc: 0.46\n",
      "alexnet1d, trial.165:\n",
      "Epoch 0, avg test_loss: 0.009598, test_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012127, train_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012414, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012309, train_acc: 0.57\n",
      "alexnet1d, trial.165:\n",
      "Epoch 1, avg test_loss: 0.009690, test_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011864, train_acc: 0.68\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012170, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012326, train_acc: 0.55\n",
      "alexnet1d, trial.165:\n",
      "Epoch 2, avg test_loss: 0.009762, test_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012154, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012291, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011879, train_acc: 0.61\n",
      "alexnet1d, trial.165:\n",
      "Epoch 3, avg test_loss: 0.009703, test_acc: 0.60\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012224, train_acc: 0.52\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012398, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012073, train_acc: 0.57\n",
      "alexnet1d, trial.165:\n",
      "Epoch 4, avg test_loss: 0.009658, test_acc: 0.60\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011690, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012246, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012471, train_acc: 0.55\n",
      "alexnet1d, trial.165:\n",
      "Epoch 5, avg test_loss: 0.009934, test_acc: 0.56\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.010837, train_acc: 0.68\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012032, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011916, train_acc: 0.52\n",
      "alexnet1d, trial.165:\n",
      "Epoch 6, avg test_loss: 0.009666, test_acc: 0.60\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011153, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011534, train_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011687, train_acc: 0.61\n",
      "alexnet1d, trial.165:\n",
      "Epoch 7, avg test_loss: 0.009522, test_acc: 0.67\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011131, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012173, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.010823, train_acc: 0.66\n",
      "alexnet1d, trial.165:\n",
      "Epoch 8, avg test_loss: 0.009650, test_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010230, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011033, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011057, train_acc: 0.75\n",
      "alexnet1d, trial.165:\n",
      "Epoch 9, avg test_loss: 0.009645, test_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011074, train_acc: 0.73\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012151, train_acc: 0.55\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010374, train_acc: 0.68\n",
      "alexnet1d, trial.165:\n",
      "Epoch 10, avg test_loss: 0.009477, test_acc: 0.67\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010267, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.013118, train_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010261, train_acc: 0.71\n",
      "alexnet1d, trial.165:\n",
      "Epoch 11, avg test_loss: 0.010220, test_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009991, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011275, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010929, train_acc: 0.64\n",
      "alexnet1d, trial.165:\n",
      "Epoch 12, avg test_loss: 0.008824, test_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010404, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010683, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.008280, train_acc: 0.82\n",
      "alexnet1d, trial.165:\n",
      "Epoch 13, avg test_loss: 0.009691, test_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009945, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.008222, train_acc: 0.80\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010458, train_acc: 0.66\n",
      "alexnet1d, trial.165:\n",
      "Epoch 14, avg test_loss: 0.009323, test_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009292, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010110, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011840, train_acc: 0.68\n",
      "alexnet1d, trial.165:\n",
      "Epoch 15, avg test_loss: 0.009292, test_acc: 0.67\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009739, train_acc: 0.68\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010737, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009007, train_acc: 0.77\n",
      "alexnet1d, trial.165:\n",
      "Epoch 16, avg test_loss: 0.008928, test_acc: 0.69\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007479, train_acc: 0.89\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008902, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009788, train_acc: 0.73\n",
      "alexnet1d, trial.165:\n",
      "Epoch 17, avg test_loss: 0.010146, test_acc: 0.66\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.006676, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007829, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.006932, train_acc: 0.84\n",
      "alexnet1d, trial.165:\n",
      "Epoch 18, avg test_loss: 0.008497, test_acc: 0.71\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.005761, train_acc: 0.91\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008265, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.006872, train_acc: 0.86\n",
      "alexnet1d, trial.165:\n",
      "Epoch 19, avg test_loss: 0.010828, test_acc: 0.64\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.005517, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.010736, train_acc: 0.68\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006869, train_acc: 0.86\n",
      "alexnet1d, trial.165:\n",
      "Epoch 20, avg test_loss: 0.009965, test_acc: 0.66\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006940, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007771, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.005361, train_acc: 0.88\n",
      "alexnet1d, trial.165:\n",
      "Epoch 21, avg test_loss: 0.009959, test_acc: 0.66\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005683, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006084, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006910, train_acc: 0.84\n",
      "alexnet1d, trial.165:\n",
      "Epoch 22, avg test_loss: 0.011114, test_acc: 0.67\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006328, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005718, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.007609, train_acc: 0.80\n",
      "alexnet1d, trial.165:\n",
      "Epoch 23, avg test_loss: 0.009564, test_acc: 0.71\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.003958, train_acc: 0.93\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004115, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.007086, train_acc: 0.86\n",
      "alexnet1d, trial.165:\n",
      "Epoch 24, avg test_loss: 0.015715, test_acc: 0.66\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004484, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.002911, train_acc: 0.95\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005867, train_acc: 0.88\n",
      "alexnet1d, trial.165:\n",
      "Epoch 25, avg test_loss: 0.010858, test_acc: 0.66\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.002338, train_acc: 0.96\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.003062, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.003696, train_acc: 0.95\n",
      "alexnet1d, trial.165:\n",
      "Epoch 26, avg test_loss: 0.013776, test_acc: 0.67\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.001750, train_acc: 0.98\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.002381, train_acc: 0.98\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003465, train_acc: 0.95\n",
      "alexnet1d, trial.165:\n",
      "Epoch 27, avg test_loss: 0.014684, test_acc: 0.66\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.002023, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.002516, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.001520, train_acc: 0.98\n",
      "alexnet1d, trial.165:\n",
      "Epoch 28, avg test_loss: 0.019446, test_acc: 0.66\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.000940, train_acc: 1.00\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.000847, train_acc: 1.00\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.001353, train_acc: 0.98\n",
      "alexnet1d, trial.165:\n",
      "Epoch 29, avg test_loss: 0.021115, test_acc: 0.69\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号43个\n",
      "错误信号27个\n",
      "信号正确并预测正确的概率为0.5\n",
      "信号错误并预测正确的概率为0.186\n",
      "总正确率为0.69\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012385, train_acc: 0.50\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012309, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.011695, train_acc: 0.70\n",
      "alexnet1d, trial.166:\n",
      "Epoch 0, avg test_loss: 0.009669, test_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.013331, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012192, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012095, train_acc: 0.62\n",
      "alexnet1d, trial.166:\n",
      "Epoch 1, avg test_loss: 0.009815, test_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011839, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012075, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011944, train_acc: 0.64\n",
      "alexnet1d, trial.166:\n",
      "Epoch 2, avg test_loss: 0.009813, test_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011790, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012252, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012281, train_acc: 0.57\n",
      "alexnet1d, trial.166:\n",
      "Epoch 3, avg test_loss: 0.009835, test_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011640, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012004, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011763, train_acc: 0.59\n",
      "alexnet1d, trial.166:\n",
      "Epoch 4, avg test_loss: 0.009799, test_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012140, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011747, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011720, train_acc: 0.62\n",
      "alexnet1d, trial.166:\n",
      "Epoch 5, avg test_loss: 0.009894, test_acc: 0.51\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011739, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011128, train_acc: 0.68\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012921, train_acc: 0.48\n",
      "alexnet1d, trial.166:\n",
      "Epoch 6, avg test_loss: 0.010140, test_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011158, train_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012498, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012356, train_acc: 0.52\n",
      "alexnet1d, trial.166:\n",
      "Epoch 7, avg test_loss: 0.010094, test_acc: 0.53\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011578, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011814, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.010834, train_acc: 0.77\n",
      "alexnet1d, trial.166:\n",
      "Epoch 8, avg test_loss: 0.009776, test_acc: 0.56\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011905, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011421, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011937, train_acc: 0.62\n",
      "alexnet1d, trial.166:\n",
      "Epoch 9, avg test_loss: 0.010271, test_acc: 0.56\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011715, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011319, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011730, train_acc: 0.55\n",
      "alexnet1d, trial.166:\n",
      "Epoch 10, avg test_loss: 0.009807, test_acc: 0.53\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011265, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.012073, train_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011258, train_acc: 0.64\n",
      "alexnet1d, trial.166:\n",
      "Epoch 11, avg test_loss: 0.010319, test_acc: 0.56\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010447, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011132, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010197, train_acc: 0.70\n",
      "alexnet1d, trial.166:\n",
      "Epoch 12, avg test_loss: 0.010415, test_acc: 0.54\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009979, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011345, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010835, train_acc: 0.68\n",
      "alexnet1d, trial.166:\n",
      "Epoch 13, avg test_loss: 0.009998, test_acc: 0.63\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010114, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010041, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010230, train_acc: 0.71\n",
      "alexnet1d, trial.166:\n",
      "Epoch 14, avg test_loss: 0.009850, test_acc: 0.59\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.011157, train_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010536, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009953, train_acc: 0.71\n",
      "alexnet1d, trial.166:\n",
      "Epoch 15, avg test_loss: 0.010535, test_acc: 0.61\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009015, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008116, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009543, train_acc: 0.70\n",
      "alexnet1d, trial.166:\n",
      "Epoch 16, avg test_loss: 0.011471, test_acc: 0.50\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009559, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007884, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.011867, train_acc: 0.71\n",
      "alexnet1d, trial.166:\n",
      "Epoch 17, avg test_loss: 0.010350, test_acc: 0.60\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008535, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009237, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009712, train_acc: 0.75\n",
      "alexnet1d, trial.166:\n",
      "Epoch 18, avg test_loss: 0.012511, test_acc: 0.56\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009639, train_acc: 0.71\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008329, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008514, train_acc: 0.77\n",
      "alexnet1d, trial.166:\n",
      "Epoch 19, avg test_loss: 0.011010, test_acc: 0.56\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006972, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008817, train_acc: 0.77\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007673, train_acc: 0.75\n",
      "alexnet1d, trial.166:\n",
      "Epoch 20, avg test_loss: 0.012984, test_acc: 0.56\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007989, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007222, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007749, train_acc: 0.82\n",
      "alexnet1d, trial.166:\n",
      "Epoch 21, avg test_loss: 0.011928, test_acc: 0.59\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007730, train_acc: 0.75\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005243, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.005805, train_acc: 0.86\n",
      "alexnet1d, trial.166:\n",
      "Epoch 22, avg test_loss: 0.012775, test_acc: 0.59\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005613, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.004323, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.008752, train_acc: 0.77\n",
      "alexnet1d, trial.166:\n",
      "Epoch 23, avg test_loss: 0.013275, test_acc: 0.57\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005595, train_acc: 0.80\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004682, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006678, train_acc: 0.89\n",
      "alexnet1d, trial.166:\n",
      "Epoch 24, avg test_loss: 0.017055, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.007147, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006138, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.008893, train_acc: 0.82\n",
      "alexnet1d, trial.166:\n",
      "Epoch 25, avg test_loss: 0.013855, test_acc: 0.56\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005009, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005105, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004874, train_acc: 0.93\n",
      "alexnet1d, trial.166:\n",
      "Epoch 26, avg test_loss: 0.015565, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003556, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.005751, train_acc: 0.88\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004663, train_acc: 0.91\n",
      "alexnet1d, trial.166:\n",
      "Epoch 27, avg test_loss: 0.017049, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.005001, train_acc: 0.86\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.003515, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.005316, train_acc: 0.88\n",
      "alexnet1d, trial.166:\n",
      "Epoch 28, avg test_loss: 0.013816, test_acc: 0.59\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.003826, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.003169, train_acc: 0.96\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.002322, train_acc: 0.95\n",
      "alexnet1d, trial.166:\n",
      "Epoch 29, avg test_loss: 0.015428, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号38个\n",
      "错误信号32个\n",
      "信号正确并预测正确的概率为0.357\n",
      "信号错误并预测正确的概率为0.243\n",
      "总正确率为0.60\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012338, train_acc: 0.62\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012137, train_acc: 0.62\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012495, train_acc: 0.48\n",
      "alexnet1d, trial.167:\n",
      "Epoch 0, avg test_loss: 0.009700, test_acc: 0.60\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012266, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012028, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.013181, train_acc: 0.48\n",
      "alexnet1d, trial.167:\n",
      "Epoch 1, avg test_loss: 0.009746, test_acc: 0.60\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011748, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011904, train_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012721, train_acc: 0.48\n",
      "alexnet1d, trial.167:\n",
      "Epoch 2, avg test_loss: 0.009681, test_acc: 0.60\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011887, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012527, train_acc: 0.50\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012151, train_acc: 0.59\n",
      "alexnet1d, trial.167:\n",
      "Epoch 3, avg test_loss: 0.009675, test_acc: 0.60\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012124, train_acc: 0.50\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011668, train_acc: 0.71\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012294, train_acc: 0.54\n",
      "alexnet1d, trial.167:\n",
      "Epoch 4, avg test_loss: 0.009603, test_acc: 0.60\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011514, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012647, train_acc: 0.52\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012222, train_acc: 0.50\n",
      "alexnet1d, trial.167:\n",
      "Epoch 5, avg test_loss: 0.009541, test_acc: 0.60\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012353, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011897, train_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011413, train_acc: 0.61\n",
      "alexnet1d, trial.167:\n",
      "Epoch 6, avg test_loss: 0.009473, test_acc: 0.60\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011429, train_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011851, train_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.013998, train_acc: 0.54\n",
      "alexnet1d, trial.167:\n",
      "Epoch 7, avg test_loss: 0.009522, test_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011229, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011853, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012696, train_acc: 0.52\n",
      "alexnet1d, trial.167:\n",
      "Epoch 8, avg test_loss: 0.009628, test_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011889, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011236, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012579, train_acc: 0.57\n",
      "alexnet1d, trial.167:\n",
      "Epoch 9, avg test_loss: 0.009495, test_acc: 0.60\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.012341, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012083, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011502, train_acc: 0.61\n",
      "alexnet1d, trial.167:\n",
      "Epoch 10, avg test_loss: 0.009479, test_acc: 0.56\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011470, train_acc: 0.75\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011557, train_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011341, train_acc: 0.68\n",
      "alexnet1d, trial.167:\n",
      "Epoch 11, avg test_loss: 0.009213, test_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011248, train_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011146, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011240, train_acc: 0.62\n",
      "alexnet1d, trial.167:\n",
      "Epoch 12, avg test_loss: 0.009433, test_acc: 0.63\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010413, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010227, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.012010, train_acc: 0.62\n",
      "alexnet1d, trial.167:\n",
      "Epoch 13, avg test_loss: 0.009938, test_acc: 0.51\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009694, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011805, train_acc: 0.59\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010185, train_acc: 0.70\n",
      "alexnet1d, trial.167:\n",
      "Epoch 14, avg test_loss: 0.009149, test_acc: 0.61\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.011160, train_acc: 0.59\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.011461, train_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010042, train_acc: 0.80\n",
      "alexnet1d, trial.167:\n",
      "Epoch 15, avg test_loss: 0.009291, test_acc: 0.63\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010173, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010161, train_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.011204, train_acc: 0.68\n",
      "alexnet1d, trial.167:\n",
      "Epoch 16, avg test_loss: 0.009549, test_acc: 0.61\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008441, train_acc: 0.86\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.011033, train_acc: 0.61\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010281, train_acc: 0.64\n",
      "alexnet1d, trial.167:\n",
      "Epoch 17, avg test_loss: 0.010694, test_acc: 0.56\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.011107, train_acc: 0.62\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009963, train_acc: 0.71\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009804, train_acc: 0.71\n",
      "alexnet1d, trial.167:\n",
      "Epoch 18, avg test_loss: 0.010303, test_acc: 0.54\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006609, train_acc: 0.89\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008889, train_acc: 0.71\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009676, train_acc: 0.71\n",
      "alexnet1d, trial.167:\n",
      "Epoch 19, avg test_loss: 0.010199, test_acc: 0.60\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008010, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008496, train_acc: 0.75\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009111, train_acc: 0.77\n",
      "alexnet1d, trial.167:\n",
      "Epoch 20, avg test_loss: 0.012436, test_acc: 0.49\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008964, train_acc: 0.75\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007376, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008854, train_acc: 0.70\n",
      "alexnet1d, trial.167:\n",
      "Epoch 21, avg test_loss: 0.013400, test_acc: 0.53\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007730, train_acc: 0.79\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006985, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.010652, train_acc: 0.73\n",
      "alexnet1d, trial.167:\n",
      "Epoch 22, avg test_loss: 0.012559, test_acc: 0.56\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.008206, train_acc: 0.77\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.008084, train_acc: 0.79\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006875, train_acc: 0.84\n",
      "alexnet1d, trial.167:\n",
      "Epoch 23, avg test_loss: 0.013425, test_acc: 0.53\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006713, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.007775, train_acc: 0.82\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006174, train_acc: 0.86\n",
      "alexnet1d, trial.167:\n",
      "Epoch 24, avg test_loss: 0.015154, test_acc: 0.60\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004451, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004934, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.007061, train_acc: 0.84\n",
      "alexnet1d, trial.167:\n",
      "Epoch 25, avg test_loss: 0.020617, test_acc: 0.54\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005012, train_acc: 0.86\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004797, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.003676, train_acc: 0.95\n",
      "alexnet1d, trial.167:\n",
      "Epoch 26, avg test_loss: 0.017616, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003212, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.005174, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003974, train_acc: 0.95\n",
      "alexnet1d, trial.167:\n",
      "Epoch 27, avg test_loss: 0.021500, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.003797, train_acc: 0.96\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004578, train_acc: 0.88\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.004861, train_acc: 0.89\n",
      "alexnet1d, trial.167:\n",
      "Epoch 28, avg test_loss: 0.026595, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.003954, train_acc: 0.89\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.003673, train_acc: 0.89\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.005022, train_acc: 0.86\n",
      "alexnet1d, trial.167:\n",
      "Epoch 29, avg test_loss: 0.024518, test_acc: 0.56\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.002712, train_acc: 0.93\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.005690, train_acc: 0.88\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.002342, train_acc: 0.93\n",
      "alexnet1d, trial.167:\n",
      "Epoch 30, avg test_loss: 0.026727, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号42个\n",
      "错误信号28个\n",
      "信号正确并预测正确的概率为0.414\n",
      "信号错误并预测正确的概率为0.171\n",
      "总正确率为0.59\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012412, train_acc: 0.43\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012970, train_acc: 0.52\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012126, train_acc: 0.70\n",
      "alexnet1d, trial.168:\n",
      "Epoch 0, avg test_loss: 0.009940, test_acc: 0.49\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012244, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.011211, train_acc: 0.68\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012587, train_acc: 0.59\n",
      "alexnet1d, trial.168:\n",
      "Epoch 1, avg test_loss: 0.010116, test_acc: 0.49\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012844, train_acc: 0.45\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012120, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012191, train_acc: 0.64\n",
      "alexnet1d, trial.168:\n",
      "Epoch 2, avg test_loss: 0.009942, test_acc: 0.49\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012218, train_acc: 0.62\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012280, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012150, train_acc: 0.61\n",
      "alexnet1d, trial.168:\n",
      "Epoch 3, avg test_loss: 0.010074, test_acc: 0.49\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012043, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012289, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011702, train_acc: 0.62\n",
      "alexnet1d, trial.168:\n",
      "Epoch 4, avg test_loss: 0.010428, test_acc: 0.49\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012436, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011825, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011887, train_acc: 0.64\n",
      "alexnet1d, trial.168:\n",
      "Epoch 5, avg test_loss: 0.010450, test_acc: 0.49\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011757, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012598, train_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011951, train_acc: 0.62\n",
      "alexnet1d, trial.168:\n",
      "Epoch 6, avg test_loss: 0.010247, test_acc: 0.49\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012207, train_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011299, train_acc: 0.71\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.013016, train_acc: 0.46\n",
      "alexnet1d, trial.168:\n",
      "Epoch 7, avg test_loss: 0.010727, test_acc: 0.49\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011453, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011937, train_acc: 0.55\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011473, train_acc: 0.66\n",
      "alexnet1d, trial.168:\n",
      "Epoch 8, avg test_loss: 0.010518, test_acc: 0.49\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011051, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011340, train_acc: 0.55\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.013070, train_acc: 0.54\n",
      "alexnet1d, trial.168:\n",
      "Epoch 9, avg test_loss: 0.011601, test_acc: 0.49\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011722, train_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011874, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011180, train_acc: 0.61\n",
      "alexnet1d, trial.168:\n",
      "Epoch 10, avg test_loss: 0.010878, test_acc: 0.46\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.012206, train_acc: 0.52\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010501, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011544, train_acc: 0.62\n",
      "alexnet1d, trial.168:\n",
      "Epoch 11, avg test_loss: 0.011786, test_acc: 0.50\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010066, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.013813, train_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.009970, train_acc: 0.73\n",
      "alexnet1d, trial.168:\n",
      "Epoch 12, avg test_loss: 0.010703, test_acc: 0.56\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010745, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010546, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011152, train_acc: 0.70\n",
      "alexnet1d, trial.168:\n",
      "Epoch 13, avg test_loss: 0.012318, test_acc: 0.59\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010187, train_acc: 0.79\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011208, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009832, train_acc: 0.66\n",
      "alexnet1d, trial.168:\n",
      "Epoch 14, avg test_loss: 0.011600, test_acc: 0.56\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.011674, train_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009450, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010360, train_acc: 0.70\n",
      "alexnet1d, trial.168:\n",
      "Epoch 15, avg test_loss: 0.010686, test_acc: 0.61\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008660, train_acc: 0.84\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009557, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.011440, train_acc: 0.70\n",
      "alexnet1d, trial.168:\n",
      "Epoch 16, avg test_loss: 0.014212, test_acc: 0.51\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008100, train_acc: 0.86\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008473, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009190, train_acc: 0.77\n",
      "alexnet1d, trial.168:\n",
      "Epoch 17, avg test_loss: 0.011311, test_acc: 0.56\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.010975, train_acc: 0.71\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008362, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007892, train_acc: 0.79\n",
      "alexnet1d, trial.168:\n",
      "Epoch 18, avg test_loss: 0.013384, test_acc: 0.49\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007442, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008379, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.006619, train_acc: 0.86\n",
      "alexnet1d, trial.168:\n",
      "Epoch 19, avg test_loss: 0.012426, test_acc: 0.59\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006336, train_acc: 0.88\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008152, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007519, train_acc: 0.86\n",
      "alexnet1d, trial.168:\n",
      "Epoch 20, avg test_loss: 0.015628, test_acc: 0.53\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.005544, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007073, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.003981, train_acc: 0.89\n",
      "alexnet1d, trial.168:\n",
      "Epoch 21, avg test_loss: 0.014243, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005745, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005106, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006294, train_acc: 0.84\n",
      "alexnet1d, trial.168:\n",
      "Epoch 22, avg test_loss: 0.018658, test_acc: 0.50\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.004119, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006800, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.008898, train_acc: 0.77\n",
      "alexnet1d, trial.168:\n",
      "Epoch 23, avg test_loss: 0.017194, test_acc: 0.56\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005707, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006118, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006667, train_acc: 0.82\n",
      "alexnet1d, trial.168:\n",
      "Epoch 24, avg test_loss: 0.017773, test_acc: 0.54\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004709, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.003947, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004217, train_acc: 0.91\n",
      "alexnet1d, trial.168:\n",
      "Epoch 25, avg test_loss: 0.017310, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004304, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.003352, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.003898, train_acc: 0.91\n",
      "alexnet1d, trial.168:\n",
      "Epoch 26, avg test_loss: 0.019624, test_acc: 0.50\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.002740, train_acc: 0.98\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.002701, train_acc: 0.96\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.002140, train_acc: 0.96\n",
      "alexnet1d, trial.168:\n",
      "Epoch 27, avg test_loss: 0.026403, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号34个\n",
      "错误信号36个\n",
      "信号正确并预测正确的概率为0.371\n",
      "信号错误并预测正确的概率为0.186\n",
      "总正确率为0.56\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012419, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012902, train_acc: 0.52\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.011775, train_acc: 0.70\n",
      "alexnet1d, trial.169:\n",
      "Epoch 0, avg test_loss: 0.010234, test_acc: 0.51\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011901, train_acc: 0.62\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012202, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012080, train_acc: 0.57\n",
      "alexnet1d, trial.169:\n",
      "Epoch 1, avg test_loss: 0.010124, test_acc: 0.51\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012212, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012105, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011751, train_acc: 0.61\n",
      "alexnet1d, trial.169:\n",
      "Epoch 2, avg test_loss: 0.010475, test_acc: 0.51\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011228, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.014350, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012102, train_acc: 0.66\n",
      "alexnet1d, trial.169:\n",
      "Epoch 3, avg test_loss: 0.010079, test_acc: 0.51\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012021, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012350, train_acc: 0.52\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012255, train_acc: 0.55\n",
      "alexnet1d, trial.169:\n",
      "Epoch 4, avg test_loss: 0.009976, test_acc: 0.51\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012582, train_acc: 0.46\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012172, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011843, train_acc: 0.62\n",
      "alexnet1d, trial.169:\n",
      "Epoch 5, avg test_loss: 0.010170, test_acc: 0.51\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011854, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012702, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011900, train_acc: 0.61\n",
      "alexnet1d, trial.169:\n",
      "Epoch 6, avg test_loss: 0.010153, test_acc: 0.51\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012138, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012300, train_acc: 0.52\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012099, train_acc: 0.52\n",
      "alexnet1d, trial.169:\n",
      "Epoch 7, avg test_loss: 0.010303, test_acc: 0.51\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011516, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012032, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011566, train_acc: 0.57\n",
      "alexnet1d, trial.169:\n",
      "Epoch 8, avg test_loss: 0.010673, test_acc: 0.51\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012393, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011031, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012131, train_acc: 0.54\n",
      "alexnet1d, trial.169:\n",
      "Epoch 9, avg test_loss: 0.010931, test_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011261, train_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011952, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010864, train_acc: 0.68\n",
      "alexnet1d, trial.169:\n",
      "Epoch 10, avg test_loss: 0.010848, test_acc: 0.54\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010912, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011449, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010927, train_acc: 0.62\n",
      "alexnet1d, trial.169:\n",
      "Epoch 11, avg test_loss: 0.013265, test_acc: 0.49\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010252, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010206, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011010, train_acc: 0.77\n",
      "alexnet1d, trial.169:\n",
      "Epoch 12, avg test_loss: 0.010032, test_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011403, train_acc: 0.57\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010416, train_acc: 0.77\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010958, train_acc: 0.62\n",
      "alexnet1d, trial.169:\n",
      "Epoch 13, avg test_loss: 0.012469, test_acc: 0.51\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010130, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010796, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009991, train_acc: 0.71\n",
      "alexnet1d, trial.169:\n",
      "Epoch 14, avg test_loss: 0.012960, test_acc: 0.54\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009783, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010099, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009481, train_acc: 0.71\n",
      "alexnet1d, trial.169:\n",
      "Epoch 15, avg test_loss: 0.011478, test_acc: 0.51\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009583, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009679, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010365, train_acc: 0.68\n",
      "alexnet1d, trial.169:\n",
      "Epoch 16, avg test_loss: 0.012926, test_acc: 0.56\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008521, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009163, train_acc: 0.68\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009548, train_acc: 0.73\n",
      "alexnet1d, trial.169:\n",
      "Epoch 17, avg test_loss: 0.012594, test_acc: 0.59\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008487, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.011132, train_acc: 0.66\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009018, train_acc: 0.75\n",
      "alexnet1d, trial.169:\n",
      "Epoch 18, avg test_loss: 0.013324, test_acc: 0.54\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009677, train_acc: 0.70\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008197, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007628, train_acc: 0.80\n",
      "alexnet1d, trial.169:\n",
      "Epoch 19, avg test_loss: 0.012700, test_acc: 0.54\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007435, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008000, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007272, train_acc: 0.82\n",
      "alexnet1d, trial.169:\n",
      "Epoch 20, avg test_loss: 0.015573, test_acc: 0.53\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007182, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006648, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.005798, train_acc: 0.89\n",
      "alexnet1d, trial.169:\n",
      "Epoch 21, avg test_loss: 0.015684, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006704, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005423, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.005703, train_acc: 0.86\n",
      "alexnet1d, trial.169:\n",
      "Epoch 22, avg test_loss: 0.016730, test_acc: 0.54\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.004696, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007525, train_acc: 0.82\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.002850, train_acc: 0.95\n",
      "alexnet1d, trial.169:\n",
      "Epoch 23, avg test_loss: 0.018075, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004232, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.007143, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.009090, train_acc: 0.73\n",
      "alexnet1d, trial.169:\n",
      "Epoch 24, avg test_loss: 0.022229, test_acc: 0.46\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.006862, train_acc: 0.80\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005684, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.007358, train_acc: 0.82\n",
      "alexnet1d, trial.169:\n",
      "Epoch 25, avg test_loss: 0.018345, test_acc: 0.50\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.006179, train_acc: 0.84\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.006044, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004867, train_acc: 0.89\n",
      "alexnet1d, trial.169:\n",
      "Epoch 26, avg test_loss: 0.019294, test_acc: 0.50\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003863, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.008896, train_acc: 0.80\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004212, train_acc: 0.95\n",
      "alexnet1d, trial.169:\n",
      "Epoch 27, avg test_loss: 0.016537, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号36个\n",
      "错误信号34个\n",
      "信号正确并预测正确的概率为0.3\n",
      "信号错误并预测正确的概率为0.214\n",
      "总正确率为0.51\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012378, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.011436, train_acc: 0.75\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.014274, train_acc: 0.45\n",
      "alexnet1d, trial.170:\n",
      "Epoch 0, avg test_loss: 0.009887, test_acc: 0.56\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012247, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012395, train_acc: 0.46\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012487, train_acc: 0.34\n",
      "alexnet1d, trial.170:\n",
      "Epoch 1, avg test_loss: 0.009905, test_acc: 0.40\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012369, train_acc: 0.50\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012297, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012084, train_acc: 0.66\n",
      "alexnet1d, trial.170:\n",
      "Epoch 2, avg test_loss: 0.009831, test_acc: 0.56\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012307, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011635, train_acc: 0.70\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012003, train_acc: 0.61\n",
      "alexnet1d, trial.170:\n",
      "Epoch 3, avg test_loss: 0.009984, test_acc: 0.56\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.013092, train_acc: 0.46\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012613, train_acc: 0.52\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011797, train_acc: 0.62\n",
      "alexnet1d, trial.170:\n",
      "Epoch 4, avg test_loss: 0.009910, test_acc: 0.56\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012008, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012454, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011542, train_acc: 0.64\n",
      "alexnet1d, trial.170:\n",
      "Epoch 5, avg test_loss: 0.010027, test_acc: 0.56\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012174, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012122, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011566, train_acc: 0.59\n",
      "alexnet1d, trial.170:\n",
      "Epoch 6, avg test_loss: 0.010090, test_acc: 0.56\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011801, train_acc: 0.55\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011512, train_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012314, train_acc: 0.46\n",
      "alexnet1d, trial.170:\n",
      "Epoch 7, avg test_loss: 0.010365, test_acc: 0.60\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011581, train_acc: 0.52\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.010856, train_acc: 0.68\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011895, train_acc: 0.64\n",
      "alexnet1d, trial.170:\n",
      "Epoch 8, avg test_loss: 0.010699, test_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010828, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010730, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011292, train_acc: 0.68\n",
      "alexnet1d, trial.170:\n",
      "Epoch 9, avg test_loss: 0.010302, test_acc: 0.56\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011558, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010316, train_acc: 0.77\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011075, train_acc: 0.62\n",
      "alexnet1d, trial.170:\n",
      "Epoch 10, avg test_loss: 0.011753, test_acc: 0.54\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.009852, train_acc: 0.79\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.009692, train_acc: 0.73\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.012359, train_acc: 0.62\n",
      "alexnet1d, trial.170:\n",
      "Epoch 11, avg test_loss: 0.011907, test_acc: 0.60\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010778, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010573, train_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010933, train_acc: 0.73\n",
      "alexnet1d, trial.170:\n",
      "Epoch 12, avg test_loss: 0.010749, test_acc: 0.54\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010381, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009934, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.008624, train_acc: 0.86\n",
      "alexnet1d, trial.170:\n",
      "Epoch 13, avg test_loss: 0.011184, test_acc: 0.60\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.008314, train_acc: 0.79\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009071, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010337, train_acc: 0.71\n",
      "alexnet1d, trial.170:\n",
      "Epoch 14, avg test_loss: 0.013372, test_acc: 0.51\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010111, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010361, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008642, train_acc: 0.77\n",
      "alexnet1d, trial.170:\n",
      "Epoch 15, avg test_loss: 0.012824, test_acc: 0.56\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008099, train_acc: 0.84\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.007705, train_acc: 0.82\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008718, train_acc: 0.75\n",
      "alexnet1d, trial.170:\n",
      "Epoch 16, avg test_loss: 0.012338, test_acc: 0.56\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009428, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010006, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.006659, train_acc: 0.86\n",
      "alexnet1d, trial.170:\n",
      "Epoch 17, avg test_loss: 0.016654, test_acc: 0.47\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007199, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.005808, train_acc: 0.86\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009998, train_acc: 0.77\n",
      "alexnet1d, trial.170:\n",
      "Epoch 18, avg test_loss: 0.015673, test_acc: 0.49\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007697, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007955, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007016, train_acc: 0.82\n",
      "alexnet1d, trial.170:\n",
      "Epoch 19, avg test_loss: 0.016655, test_acc: 0.49\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.005010, train_acc: 0.89\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.004850, train_acc: 0.91\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006181, train_acc: 0.88\n",
      "alexnet1d, trial.170:\n",
      "Epoch 20, avg test_loss: 0.021207, test_acc: 0.47\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006155, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006389, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.004322, train_acc: 0.91\n",
      "alexnet1d, trial.170:\n",
      "Epoch 21, avg test_loss: 0.023148, test_acc: 0.50\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.003631, train_acc: 0.95\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007288, train_acc: 0.75\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.004998, train_acc: 0.93\n",
      "alexnet1d, trial.170:\n",
      "Epoch 22, avg test_loss: 0.023829, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.004772, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005596, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.003435, train_acc: 0.93\n",
      "alexnet1d, trial.170:\n",
      "Epoch 23, avg test_loss: 0.023164, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005298, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.003825, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006326, train_acc: 0.86\n",
      "alexnet1d, trial.170:\n",
      "Epoch 24, avg test_loss: 0.022740, test_acc: 0.49\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.003427, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006414, train_acc: 0.80\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.003285, train_acc: 0.91\n",
      "alexnet1d, trial.170:\n",
      "Epoch 25, avg test_loss: 0.025240, test_acc: 0.49\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号39个\n",
      "错误信号31个\n",
      "信号正确并预测正确的概率为0.286\n",
      "信号错误并预测正确的概率为0.2\n",
      "总正确率为0.49\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012532, train_acc: 0.30\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012269, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012405, train_acc: 0.55\n",
      "alexnet1d, trial.171:\n",
      "Epoch 0, avg test_loss: 0.009756, test_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012182, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.011843, train_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012367, train_acc: 0.61\n",
      "alexnet1d, trial.171:\n",
      "Epoch 1, avg test_loss: 0.009734, test_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012297, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012154, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012199, train_acc: 0.55\n",
      "alexnet1d, trial.171:\n",
      "Epoch 2, avg test_loss: 0.009736, test_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012054, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011909, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011692, train_acc: 0.64\n",
      "alexnet1d, trial.171:\n",
      "Epoch 3, avg test_loss: 0.009887, test_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012058, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012089, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011918, train_acc: 0.62\n",
      "alexnet1d, trial.171:\n",
      "Epoch 4, avg test_loss: 0.009966, test_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011227, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011739, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011929, train_acc: 0.48\n",
      "alexnet1d, trial.171:\n",
      "Epoch 5, avg test_loss: 0.010172, test_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011628, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012066, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012416, train_acc: 0.59\n",
      "alexnet1d, trial.171:\n",
      "Epoch 6, avg test_loss: 0.010334, test_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.010639, train_acc: 0.71\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011422, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011815, train_acc: 0.50\n",
      "alexnet1d, trial.171:\n",
      "Epoch 7, avg test_loss: 0.010293, test_acc: 0.56\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012168, train_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011730, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.010382, train_acc: 0.68\n",
      "alexnet1d, trial.171:\n",
      "Epoch 8, avg test_loss: 0.011167, test_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010192, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012741, train_acc: 0.52\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.009857, train_acc: 0.70\n",
      "alexnet1d, trial.171:\n",
      "Epoch 9, avg test_loss: 0.010285, test_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010459, train_acc: 0.75\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011163, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010365, train_acc: 0.61\n",
      "alexnet1d, trial.171:\n",
      "Epoch 10, avg test_loss: 0.010307, test_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010697, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.009591, train_acc: 0.73\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010007, train_acc: 0.61\n",
      "alexnet1d, trial.171:\n",
      "Epoch 11, avg test_loss: 0.011925, test_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011068, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010365, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011368, train_acc: 0.54\n",
      "alexnet1d, trial.171:\n",
      "Epoch 12, avg test_loss: 0.011779, test_acc: 0.60\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009917, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009345, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009822, train_acc: 0.71\n",
      "alexnet1d, trial.171:\n",
      "Epoch 13, avg test_loss: 0.013083, test_acc: 0.60\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009049, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.008985, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010186, train_acc: 0.71\n",
      "alexnet1d, trial.171:\n",
      "Epoch 14, avg test_loss: 0.012904, test_acc: 0.59\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008734, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010047, train_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009733, train_acc: 0.79\n",
      "alexnet1d, trial.171:\n",
      "Epoch 15, avg test_loss: 0.013084, test_acc: 0.63\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009626, train_acc: 0.80\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008552, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008935, train_acc: 0.71\n",
      "alexnet1d, trial.171:\n",
      "Epoch 16, avg test_loss: 0.013361, test_acc: 0.60\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007156, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008140, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007861, train_acc: 0.80\n",
      "alexnet1d, trial.171:\n",
      "Epoch 17, avg test_loss: 0.014551, test_acc: 0.57\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008412, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007588, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007011, train_acc: 0.84\n",
      "alexnet1d, trial.171:\n",
      "Epoch 18, avg test_loss: 0.017042, test_acc: 0.64\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.005664, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.010040, train_acc: 0.70\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.010144, train_acc: 0.73\n",
      "alexnet1d, trial.171:\n",
      "Epoch 19, avg test_loss: 0.016406, test_acc: 0.60\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007799, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.005569, train_acc: 0.89\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007164, train_acc: 0.88\n",
      "alexnet1d, trial.171:\n",
      "Epoch 20, avg test_loss: 0.016908, test_acc: 0.56\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007794, train_acc: 0.77\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.005480, train_acc: 0.89\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.004712, train_acc: 0.88\n",
      "alexnet1d, trial.171:\n",
      "Epoch 21, avg test_loss: 0.018366, test_acc: 0.54\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006620, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.004607, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.004802, train_acc: 0.86\n",
      "alexnet1d, trial.171:\n",
      "Epoch 22, avg test_loss: 0.023568, test_acc: 0.54\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.003163, train_acc: 0.96\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005504, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.003192, train_acc: 0.91\n",
      "alexnet1d, trial.171:\n",
      "Epoch 23, avg test_loss: 0.028003, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.003648, train_acc: 0.95\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.003338, train_acc: 0.93\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.004781, train_acc: 0.89\n",
      "alexnet1d, trial.171:\n",
      "Epoch 24, avg test_loss: 0.033638, test_acc: 0.49\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.002954, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005657, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.009339, train_acc: 0.82\n",
      "alexnet1d, trial.171:\n",
      "Epoch 25, avg test_loss: 0.036225, test_acc: 0.54\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.003064, train_acc: 0.96\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004515, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.002948, train_acc: 0.93\n",
      "alexnet1d, trial.171:\n",
      "Epoch 26, avg test_loss: 0.027031, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004984, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.001529, train_acc: 0.98\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.001703, train_acc: 0.96\n",
      "alexnet1d, trial.171:\n",
      "Epoch 27, avg test_loss: 0.038375, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号40个\n",
      "错误信号30个\n",
      "信号正确并预测正确的概率为0.414\n",
      "信号错误并预测正确的概率为0.157\n",
      "总正确率为0.57\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012452, train_acc: 0.39\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012548, train_acc: 0.48\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.013118, train_acc: 0.48\n",
      "alexnet1d, trial.172:\n",
      "Epoch 0, avg test_loss: 0.009570, test_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011809, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012144, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012113, train_acc: 0.57\n",
      "alexnet1d, trial.172:\n",
      "Epoch 1, avg test_loss: 0.009666, test_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012451, train_acc: 0.50\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012285, train_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012314, train_acc: 0.57\n",
      "alexnet1d, trial.172:\n",
      "Epoch 2, avg test_loss: 0.009494, test_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012179, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011901, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012286, train_acc: 0.59\n",
      "alexnet1d, trial.172:\n",
      "Epoch 3, avg test_loss: 0.009418, test_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011877, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011763, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012621, train_acc: 0.52\n",
      "alexnet1d, trial.172:\n",
      "Epoch 4, avg test_loss: 0.009304, test_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012710, train_acc: 0.39\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011939, train_acc: 0.73\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011568, train_acc: 0.61\n",
      "alexnet1d, trial.172:\n",
      "Epoch 5, avg test_loss: 0.009408, test_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012010, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011831, train_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011159, train_acc: 0.62\n",
      "alexnet1d, trial.172:\n",
      "Epoch 6, avg test_loss: 0.009203, test_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.010904, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.013373, train_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012754, train_acc: 0.57\n",
      "alexnet1d, trial.172:\n",
      "Epoch 7, avg test_loss: 0.009339, test_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012408, train_acc: 0.55\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011873, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011368, train_acc: 0.73\n",
      "alexnet1d, trial.172:\n",
      "Epoch 8, avg test_loss: 0.009183, test_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011367, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011766, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011307, train_acc: 0.66\n",
      "alexnet1d, trial.172:\n",
      "Epoch 9, avg test_loss: 0.009549, test_acc: 0.60\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010132, train_acc: 0.71\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011232, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011135, train_acc: 0.77\n",
      "alexnet1d, trial.172:\n",
      "Epoch 10, avg test_loss: 0.009005, test_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.012476, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011549, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010501, train_acc: 0.71\n",
      "alexnet1d, trial.172:\n",
      "Epoch 11, avg test_loss: 0.008722, test_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010646, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010581, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010932, train_acc: 0.75\n",
      "alexnet1d, trial.172:\n",
      "Epoch 12, avg test_loss: 0.009032, test_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010306, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010732, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010321, train_acc: 0.75\n",
      "alexnet1d, trial.172:\n",
      "Epoch 13, avg test_loss: 0.009146, test_acc: 0.60\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009440, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009331, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009825, train_acc: 0.75\n",
      "alexnet1d, trial.172:\n",
      "Epoch 14, avg test_loss: 0.009233, test_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009261, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009598, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008244, train_acc: 0.79\n",
      "alexnet1d, trial.172:\n",
      "Epoch 15, avg test_loss: 0.009132, test_acc: 0.67\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008226, train_acc: 0.84\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008733, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008011, train_acc: 0.77\n",
      "alexnet1d, trial.172:\n",
      "Epoch 16, avg test_loss: 0.010595, test_acc: 0.66\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007787, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007484, train_acc: 0.84\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.006408, train_acc: 0.86\n",
      "alexnet1d, trial.172:\n",
      "Epoch 17, avg test_loss: 0.010663, test_acc: 0.63\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.006531, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.004531, train_acc: 0.89\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.006144, train_acc: 0.84\n",
      "alexnet1d, trial.172:\n",
      "Epoch 18, avg test_loss: 0.012137, test_acc: 0.64\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009452, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.006058, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.004998, train_acc: 0.89\n",
      "alexnet1d, trial.172:\n",
      "Epoch 19, avg test_loss: 0.011725, test_acc: 0.67\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.004041, train_acc: 0.91\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.006392, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006238, train_acc: 0.93\n",
      "alexnet1d, trial.172:\n",
      "Epoch 20, avg test_loss: 0.015772, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.002021, train_acc: 0.98\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.003787, train_acc: 0.95\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.004144, train_acc: 0.91\n",
      "alexnet1d, trial.172:\n",
      "Epoch 21, avg test_loss: 0.013961, test_acc: 0.66\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.003486, train_acc: 0.95\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.003023, train_acc: 0.95\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.003797, train_acc: 0.89\n",
      "alexnet1d, trial.172:\n",
      "Epoch 22, avg test_loss: 0.014862, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号45个\n",
      "错误信号25个\n",
      "信号正确并预测正确的概率为0.4\n",
      "信号错误并预测正确的概率为0.243\n",
      "总正确率为0.64\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012262, train_acc: 0.66\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.011779, train_acc: 0.68\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.014036, train_acc: 0.46\n",
      "alexnet1d, trial.173:\n",
      "Epoch 0, avg test_loss: 0.009803, test_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012160, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012278, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011933, train_acc: 0.73\n",
      "alexnet1d, trial.173:\n",
      "Epoch 1, avg test_loss: 0.009849, test_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012255, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012061, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011716, train_acc: 0.64\n",
      "alexnet1d, trial.173:\n",
      "Epoch 2, avg test_loss: 0.009898, test_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011949, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012357, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012058, train_acc: 0.57\n",
      "alexnet1d, trial.173:\n",
      "Epoch 3, avg test_loss: 0.009938, test_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012353, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011465, train_acc: 0.68\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.013060, train_acc: 0.52\n",
      "alexnet1d, trial.173:\n",
      "Epoch 4, avg test_loss: 0.010256, test_acc: 0.53\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011766, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011419, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012035, train_acc: 0.57\n",
      "alexnet1d, trial.173:\n",
      "Epoch 5, avg test_loss: 0.010214, test_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011758, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011375, train_acc: 0.68\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011464, train_acc: 0.57\n",
      "alexnet1d, trial.173:\n",
      "Epoch 6, avg test_loss: 0.010188, test_acc: 0.56\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012518, train_acc: 0.52\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011178, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.010366, train_acc: 0.71\n",
      "alexnet1d, trial.173:\n",
      "Epoch 7, avg test_loss: 0.010966, test_acc: 0.53\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010878, train_acc: 0.68\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.009818, train_acc: 0.80\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.013650, train_acc: 0.59\n",
      "alexnet1d, trial.173:\n",
      "Epoch 8, avg test_loss: 0.011486, test_acc: 0.53\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.013297, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010769, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012594, train_acc: 0.46\n",
      "alexnet1d, trial.173:\n",
      "Epoch 9, avg test_loss: 0.010840, test_acc: 0.47\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011281, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010275, train_acc: 0.75\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010962, train_acc: 0.68\n",
      "alexnet1d, trial.173:\n",
      "Epoch 10, avg test_loss: 0.010439, test_acc: 0.56\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011200, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010445, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010957, train_acc: 0.70\n",
      "alexnet1d, trial.173:\n",
      "Epoch 11, avg test_loss: 0.010408, test_acc: 0.51\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009869, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010112, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.009995, train_acc: 0.71\n",
      "alexnet1d, trial.173:\n",
      "Epoch 12, avg test_loss: 0.011666, test_acc: 0.53\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010713, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011172, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009772, train_acc: 0.68\n",
      "alexnet1d, trial.173:\n",
      "Epoch 13, avg test_loss: 0.011741, test_acc: 0.53\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009227, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.008381, train_acc: 0.79\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010697, train_acc: 0.66\n",
      "alexnet1d, trial.173:\n",
      "Epoch 14, avg test_loss: 0.012344, test_acc: 0.50\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008674, train_acc: 0.80\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009166, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.007311, train_acc: 0.80\n",
      "alexnet1d, trial.173:\n",
      "Epoch 15, avg test_loss: 0.012589, test_acc: 0.53\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.007427, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009329, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.011133, train_acc: 0.71\n",
      "alexnet1d, trial.173:\n",
      "Epoch 16, avg test_loss: 0.012440, test_acc: 0.54\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007764, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008584, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007425, train_acc: 0.80\n",
      "alexnet1d, trial.173:\n",
      "Epoch 17, avg test_loss: 0.012348, test_acc: 0.54\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007914, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008358, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008986, train_acc: 0.73\n",
      "alexnet1d, trial.173:\n",
      "Epoch 18, avg test_loss: 0.013553, test_acc: 0.59\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008575, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007947, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007147, train_acc: 0.84\n",
      "alexnet1d, trial.173:\n",
      "Epoch 19, avg test_loss: 0.014143, test_acc: 0.57\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006892, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.004175, train_acc: 0.93\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009815, train_acc: 0.77\n",
      "alexnet1d, trial.173:\n",
      "Epoch 20, avg test_loss: 0.014868, test_acc: 0.60\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.005539, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007731, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.004756, train_acc: 0.93\n",
      "alexnet1d, trial.173:\n",
      "Epoch 21, avg test_loss: 0.016587, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005513, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.003333, train_acc: 0.93\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007168, train_acc: 0.80\n",
      "alexnet1d, trial.173:\n",
      "Epoch 22, avg test_loss: 0.019914, test_acc: 0.56\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.002652, train_acc: 0.96\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.002953, train_acc: 0.95\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006070, train_acc: 0.88\n",
      "alexnet1d, trial.173:\n",
      "Epoch 23, avg test_loss: 0.024620, test_acc: 0.53\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005562, train_acc: 0.82\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006471, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.003942, train_acc: 0.93\n",
      "alexnet1d, trial.173:\n",
      "Epoch 24, avg test_loss: 0.021560, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004429, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.008026, train_acc: 0.79\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.003127, train_acc: 0.96\n",
      "alexnet1d, trial.173:\n",
      "Epoch 25, avg test_loss: 0.022220, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004523, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004698, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.002530, train_acc: 1.00\n",
      "alexnet1d, trial.173:\n",
      "Epoch 26, avg test_loss: 0.020190, test_acc: 0.49\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号38个\n",
      "错误信号32个\n",
      "信号正确并预测正确的概率为0.4\n",
      "信号错误并预测正确的概率为0.086\n",
      "总正确率为0.49\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012376, train_acc: 0.48\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012365, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012273, train_acc: 0.55\n",
      "alexnet1d, trial.174:\n",
      "Epoch 0, avg test_loss: 0.009741, test_acc: 0.56\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012007, train_acc: 0.62\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.013479, train_acc: 0.48\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012024, train_acc: 0.66\n",
      "alexnet1d, trial.174:\n",
      "Epoch 1, avg test_loss: 0.009799, test_acc: 0.56\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012397, train_acc: 0.52\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011993, train_acc: 0.77\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012299, train_acc: 0.57\n",
      "alexnet1d, trial.174:\n",
      "Epoch 2, avg test_loss: 0.009738, test_acc: 0.56\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012072, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012059, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011615, train_acc: 0.64\n",
      "alexnet1d, trial.174:\n",
      "Epoch 3, avg test_loss: 0.009672, test_acc: 0.56\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012395, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011805, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012518, train_acc: 0.54\n",
      "alexnet1d, trial.174:\n",
      "Epoch 4, avg test_loss: 0.009715, test_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011173, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011877, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012952, train_acc: 0.59\n",
      "alexnet1d, trial.174:\n",
      "Epoch 5, avg test_loss: 0.009837, test_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011427, train_acc: 0.66\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011655, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012450, train_acc: 0.54\n",
      "alexnet1d, trial.174:\n",
      "Epoch 6, avg test_loss: 0.009935, test_acc: 0.53\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011566, train_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011214, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012692, train_acc: 0.55\n",
      "alexnet1d, trial.174:\n",
      "Epoch 7, avg test_loss: 0.009924, test_acc: 0.56\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011498, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012909, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011596, train_acc: 0.66\n",
      "alexnet1d, trial.174:\n",
      "Epoch 8, avg test_loss: 0.010231, test_acc: 0.50\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011646, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010773, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012068, train_acc: 0.64\n",
      "alexnet1d, trial.174:\n",
      "Epoch 9, avg test_loss: 0.009884, test_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011579, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011716, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011757, train_acc: 0.61\n",
      "alexnet1d, trial.174:\n",
      "Epoch 10, avg test_loss: 0.009839, test_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010966, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011309, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010672, train_acc: 0.68\n",
      "alexnet1d, trial.174:\n",
      "Epoch 11, avg test_loss: 0.010090, test_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010119, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010322, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.009960, train_acc: 0.71\n",
      "alexnet1d, trial.174:\n",
      "Epoch 12, avg test_loss: 0.011582, test_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.012282, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010279, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011269, train_acc: 0.75\n",
      "alexnet1d, trial.174:\n",
      "Epoch 13, avg test_loss: 0.009894, test_acc: 0.54\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009910, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010874, train_acc: 0.64\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010465, train_acc: 0.68\n",
      "alexnet1d, trial.174:\n",
      "Epoch 14, avg test_loss: 0.010407, test_acc: 0.56\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009675, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009955, train_acc: 0.62\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009876, train_acc: 0.75\n",
      "alexnet1d, trial.174:\n",
      "Epoch 15, avg test_loss: 0.009862, test_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008667, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009817, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010302, train_acc: 0.75\n",
      "alexnet1d, trial.174:\n",
      "Epoch 16, avg test_loss: 0.011044, test_acc: 0.63\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008991, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008627, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010202, train_acc: 0.73\n",
      "alexnet1d, trial.174:\n",
      "Epoch 17, avg test_loss: 0.011406, test_acc: 0.57\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009149, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007991, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008179, train_acc: 0.79\n",
      "alexnet1d, trial.174:\n",
      "Epoch 18, avg test_loss: 0.011775, test_acc: 0.51\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009507, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008333, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008829, train_acc: 0.82\n",
      "alexnet1d, trial.174:\n",
      "Epoch 19, avg test_loss: 0.011942, test_acc: 0.57\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007564, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008897, train_acc: 0.77\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008104, train_acc: 0.79\n",
      "alexnet1d, trial.174:\n",
      "Epoch 20, avg test_loss: 0.010188, test_acc: 0.66\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007743, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007422, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008015, train_acc: 0.80\n",
      "alexnet1d, trial.174:\n",
      "Epoch 21, avg test_loss: 0.013064, test_acc: 0.66\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006640, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005900, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008747, train_acc: 0.75\n",
      "alexnet1d, trial.174:\n",
      "Epoch 22, avg test_loss: 0.012493, test_acc: 0.64\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006291, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006557, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.008909, train_acc: 0.71\n",
      "alexnet1d, trial.174:\n",
      "Epoch 23, avg test_loss: 0.011962, test_acc: 0.59\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004329, train_acc: 0.93\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006977, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.010408, train_acc: 0.79\n",
      "alexnet1d, trial.174:\n",
      "Epoch 24, avg test_loss: 0.016123, test_acc: 0.64\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004081, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005569, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005170, train_acc: 0.88\n",
      "alexnet1d, trial.174:\n",
      "Epoch 25, avg test_loss: 0.010931, test_acc: 0.71\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004846, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.003586, train_acc: 0.96\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.005799, train_acc: 0.80\n",
      "alexnet1d, trial.174:\n",
      "Epoch 26, avg test_loss: 0.018686, test_acc: 0.63\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.002925, train_acc: 0.96\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.009979, train_acc: 0.79\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003774, train_acc: 0.89\n",
      "alexnet1d, trial.174:\n",
      "Epoch 27, avg test_loss: 0.017721, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.006766, train_acc: 0.80\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004997, train_acc: 0.88\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.006272, train_acc: 0.80\n",
      "alexnet1d, trial.174:\n",
      "Epoch 28, avg test_loss: 0.016755, test_acc: 0.71\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.004542, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.004511, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.007380, train_acc: 0.82\n",
      "alexnet1d, trial.174:\n",
      "Epoch 29, avg test_loss: 0.022655, test_acc: 0.57\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.004392, train_acc: 0.91\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.004124, train_acc: 0.91\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.003823, train_acc: 0.95\n",
      "alexnet1d, trial.174:\n",
      "Epoch 30, avg test_loss: 0.016385, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.003652, train_acc: 0.95\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.004680, train_acc: 0.86\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.004891, train_acc: 0.88\n",
      "alexnet1d, trial.174:\n",
      "Epoch 31, avg test_loss: 0.021428, test_acc: 0.59\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.003976, train_acc: 0.93\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.003039, train_acc: 0.91\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.002514, train_acc: 0.98\n",
      "alexnet1d, trial.174:\n",
      "Epoch 32, avg test_loss: 0.019912, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 33, lr: 0.000377, 0/280, avg loss: 0.001169, train_acc: 1.00\n",
      "Train Epoch 33, lr: 0.000377, 112/280, avg loss: 0.002618, train_acc: 0.95\n",
      "Train Epoch 33, lr: 0.000377, 224/280, avg loss: 0.001545, train_acc: 0.98\n",
      "alexnet1d, trial.174:\n",
      "Epoch 33, avg test_loss: 0.021998, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号39个\n",
      "错误信号31个\n",
      "信号正确并预测正确的概率为0.414\n",
      "信号错误并预测正确的概率为0.229\n",
      "总正确率为0.64\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012379, train_acc: 0.46\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012492, train_acc: 0.39\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012496, train_acc: 0.61\n",
      "alexnet1d, trial.175:\n",
      "Epoch 0, avg test_loss: 0.009434, test_acc: 0.66\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011734, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.011972, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012552, train_acc: 0.55\n",
      "alexnet1d, trial.175:\n",
      "Epoch 1, avg test_loss: 0.009598, test_acc: 0.66\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012631, train_acc: 0.45\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012205, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012276, train_acc: 0.55\n",
      "alexnet1d, trial.175:\n",
      "Epoch 2, avg test_loss: 0.009608, test_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012593, train_acc: 0.50\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011939, train_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011985, train_acc: 0.59\n",
      "alexnet1d, trial.175:\n",
      "Epoch 3, avg test_loss: 0.009609, test_acc: 0.66\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012098, train_acc: 0.48\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012465, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011775, train_acc: 0.68\n",
      "alexnet1d, trial.175:\n",
      "Epoch 4, avg test_loss: 0.009515, test_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011789, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011863, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012139, train_acc: 0.54\n",
      "alexnet1d, trial.175:\n",
      "Epoch 5, avg test_loss: 0.009358, test_acc: 0.66\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011389, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.010974, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012661, train_acc: 0.48\n",
      "alexnet1d, trial.175:\n",
      "Epoch 6, avg test_loss: 0.009753, test_acc: 0.60\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.010320, train_acc: 0.75\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011595, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011889, train_acc: 0.62\n",
      "alexnet1d, trial.175:\n",
      "Epoch 7, avg test_loss: 0.009568, test_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011409, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.010743, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.010912, train_acc: 0.71\n",
      "alexnet1d, trial.175:\n",
      "Epoch 8, avg test_loss: 0.009635, test_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010566, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011182, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010511, train_acc: 0.68\n",
      "alexnet1d, trial.175:\n",
      "Epoch 9, avg test_loss: 0.009654, test_acc: 0.67\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010780, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012151, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.009576, train_acc: 0.82\n",
      "alexnet1d, trial.175:\n",
      "Epoch 10, avg test_loss: 0.009671, test_acc: 0.56\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011071, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011758, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.012053, train_acc: 0.62\n",
      "alexnet1d, trial.175:\n",
      "Epoch 11, avg test_loss: 0.010277, test_acc: 0.54\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010344, train_acc: 0.77\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010427, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010217, train_acc: 0.70\n",
      "alexnet1d, trial.175:\n",
      "Epoch 12, avg test_loss: 0.009554, test_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010261, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010761, train_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.008533, train_acc: 0.79\n",
      "alexnet1d, trial.175:\n",
      "Epoch 13, avg test_loss: 0.010373, test_acc: 0.59\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.011006, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009491, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010525, train_acc: 0.68\n",
      "alexnet1d, trial.175:\n",
      "Epoch 14, avg test_loss: 0.011357, test_acc: 0.53\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.007945, train_acc: 0.84\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010470, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008682, train_acc: 0.73\n",
      "alexnet1d, trial.175:\n",
      "Epoch 15, avg test_loss: 0.011276, test_acc: 0.49\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.007201, train_acc: 0.86\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010165, train_acc: 0.68\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009849, train_acc: 0.75\n",
      "alexnet1d, trial.175:\n",
      "Epoch 16, avg test_loss: 0.011540, test_acc: 0.51\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009171, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.006962, train_acc: 0.91\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008608, train_acc: 0.77\n",
      "alexnet1d, trial.175:\n",
      "Epoch 17, avg test_loss: 0.012128, test_acc: 0.51\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008948, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007911, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007415, train_acc: 0.80\n",
      "alexnet1d, trial.175:\n",
      "Epoch 18, avg test_loss: 0.013471, test_acc: 0.53\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008685, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008034, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.006209, train_acc: 0.86\n",
      "alexnet1d, trial.175:\n",
      "Epoch 19, avg test_loss: 0.012569, test_acc: 0.53\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006888, train_acc: 0.88\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.006346, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.005860, train_acc: 0.88\n",
      "alexnet1d, trial.175:\n",
      "Epoch 20, avg test_loss: 0.014952, test_acc: 0.51\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.005737, train_acc: 0.91\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.004310, train_acc: 0.93\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007451, train_acc: 0.82\n",
      "alexnet1d, trial.175:\n",
      "Epoch 21, avg test_loss: 0.016726, test_acc: 0.51\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.004371, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.004380, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.004525, train_acc: 0.88\n",
      "alexnet1d, trial.175:\n",
      "Epoch 22, avg test_loss: 0.017262, test_acc: 0.56\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.003736, train_acc: 0.96\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.003002, train_acc: 0.95\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.004181, train_acc: 0.93\n",
      "alexnet1d, trial.175:\n",
      "Epoch 23, avg test_loss: 0.018894, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.002623, train_acc: 0.96\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.002687, train_acc: 0.96\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006116, train_acc: 0.84\n",
      "alexnet1d, trial.175:\n",
      "Epoch 24, avg test_loss: 0.026485, test_acc: 0.50\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004742, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004343, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005140, train_acc: 0.95\n",
      "alexnet1d, trial.175:\n",
      "Epoch 25, avg test_loss: 0.027109, test_acc: 0.50\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.003471, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.002129, train_acc: 0.96\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.003511, train_acc: 0.93\n",
      "alexnet1d, trial.175:\n",
      "Epoch 26, avg test_loss: 0.026091, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003419, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.003200, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.002943, train_acc: 0.93\n",
      "alexnet1d, trial.175:\n",
      "Epoch 27, avg test_loss: 0.024726, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号46个\n",
      "错误信号24个\n",
      "信号正确并预测正确的概率为0.429\n",
      "信号错误并预测正确的概率为0.129\n",
      "总正确率为0.56\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012408, train_acc: 0.43\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012488, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012105, train_acc: 0.59\n",
      "alexnet1d, trial.176:\n",
      "Epoch 0, avg test_loss: 0.010087, test_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011774, train_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012259, train_acc: 0.62\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012023, train_acc: 0.54\n",
      "alexnet1d, trial.176:\n",
      "Epoch 1, avg test_loss: 0.009863, test_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012139, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012256, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012529, train_acc: 0.45\n",
      "alexnet1d, trial.176:\n",
      "Epoch 2, avg test_loss: 0.009830, test_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011980, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012816, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012201, train_acc: 0.59\n",
      "alexnet1d, trial.176:\n",
      "Epoch 3, avg test_loss: 0.009841, test_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012120, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011937, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011940, train_acc: 0.59\n",
      "alexnet1d, trial.176:\n",
      "Epoch 4, avg test_loss: 0.009862, test_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012777, train_acc: 0.50\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012465, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011919, train_acc: 0.61\n",
      "alexnet1d, trial.176:\n",
      "Epoch 5, avg test_loss: 0.009939, test_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011585, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011721, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012439, train_acc: 0.52\n",
      "alexnet1d, trial.176:\n",
      "Epoch 6, avg test_loss: 0.009879, test_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012433, train_acc: 0.50\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012150, train_acc: 0.55\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011691, train_acc: 0.66\n",
      "alexnet1d, trial.176:\n",
      "Epoch 7, avg test_loss: 0.009707, test_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012164, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012039, train_acc: 0.55\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011358, train_acc: 0.66\n",
      "alexnet1d, trial.176:\n",
      "Epoch 8, avg test_loss: 0.009810, test_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011282, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011632, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.013541, train_acc: 0.46\n",
      "alexnet1d, trial.176:\n",
      "Epoch 9, avg test_loss: 0.009748, test_acc: 0.60\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011366, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011799, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011305, train_acc: 0.70\n",
      "alexnet1d, trial.176:\n",
      "Epoch 10, avg test_loss: 0.009556, test_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010914, train_acc: 0.84\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010944, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011196, train_acc: 0.59\n",
      "alexnet1d, trial.176:\n",
      "Epoch 11, avg test_loss: 0.009721, test_acc: 0.60\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011497, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010616, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011722, train_acc: 0.61\n",
      "alexnet1d, trial.176:\n",
      "Epoch 12, avg test_loss: 0.009586, test_acc: 0.57\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009886, train_acc: 0.75\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011647, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011554, train_acc: 0.64\n",
      "alexnet1d, trial.176:\n",
      "Epoch 13, avg test_loss: 0.009544, test_acc: 0.56\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.011124, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010539, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010784, train_acc: 0.61\n",
      "alexnet1d, trial.176:\n",
      "Epoch 14, avg test_loss: 0.010184, test_acc: 0.57\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.011493, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010345, train_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009245, train_acc: 0.75\n",
      "alexnet1d, trial.176:\n",
      "Epoch 15, avg test_loss: 0.009908, test_acc: 0.57\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009642, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009578, train_acc: 0.80\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.011370, train_acc: 0.61\n",
      "alexnet1d, trial.176:\n",
      "Epoch 16, avg test_loss: 0.009434, test_acc: 0.54\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.010891, train_acc: 0.64\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010417, train_acc: 0.66\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009195, train_acc: 0.70\n",
      "alexnet1d, trial.176:\n",
      "Epoch 17, avg test_loss: 0.010130, test_acc: 0.59\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009349, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007566, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009582, train_acc: 0.75\n",
      "alexnet1d, trial.176:\n",
      "Epoch 18, avg test_loss: 0.009887, test_acc: 0.60\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008925, train_acc: 0.71\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009822, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009805, train_acc: 0.75\n",
      "alexnet1d, trial.176:\n",
      "Epoch 19, avg test_loss: 0.010379, test_acc: 0.63\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.009371, train_acc: 0.73\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007176, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008421, train_acc: 0.77\n",
      "alexnet1d, trial.176:\n",
      "Epoch 20, avg test_loss: 0.009562, test_acc: 0.63\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007672, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007129, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.009502, train_acc: 0.73\n",
      "alexnet1d, trial.176:\n",
      "Epoch 21, avg test_loss: 0.010305, test_acc: 0.64\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.008368, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007122, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007791, train_acc: 0.79\n",
      "alexnet1d, trial.176:\n",
      "Epoch 22, avg test_loss: 0.010034, test_acc: 0.66\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006854, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007885, train_acc: 0.77\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.009820, train_acc: 0.80\n",
      "alexnet1d, trial.176:\n",
      "Epoch 23, avg test_loss: 0.010444, test_acc: 0.70\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.007473, train_acc: 0.80\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006210, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.007207, train_acc: 0.80\n",
      "alexnet1d, trial.176:\n",
      "Epoch 24, avg test_loss: 0.009500, test_acc: 0.73\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.008279, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005211, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.007398, train_acc: 0.82\n",
      "alexnet1d, trial.176:\n",
      "Epoch 25, avg test_loss: 0.011041, test_acc: 0.66\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005076, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005377, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.007333, train_acc: 0.84\n",
      "alexnet1d, trial.176:\n",
      "Epoch 26, avg test_loss: 0.011673, test_acc: 0.67\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003548, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004782, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.008433, train_acc: 0.77\n",
      "alexnet1d, trial.176:\n",
      "Epoch 27, avg test_loss: 0.011159, test_acc: 0.74\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.005864, train_acc: 0.79\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.005730, train_acc: 0.88\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.004265, train_acc: 0.95\n",
      "alexnet1d, trial.176:\n",
      "Epoch 28, avg test_loss: 0.013631, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.004251, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.004174, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.003317, train_acc: 0.95\n",
      "alexnet1d, trial.176:\n",
      "Epoch 29, avg test_loss: 0.015418, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.003211, train_acc: 0.91\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.004891, train_acc: 0.89\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.002522, train_acc: 0.96\n",
      "alexnet1d, trial.176:\n",
      "Epoch 30, avg test_loss: 0.013303, test_acc: 0.70\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.003362, train_acc: 0.95\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.001358, train_acc: 0.98\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.003291, train_acc: 0.95\n",
      "alexnet1d, trial.176:\n",
      "Epoch 31, avg test_loss: 0.017150, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号40个\n",
      "错误信号30个\n",
      "信号正确并预测正确的概率为0.357\n",
      "信号错误并预测正确的概率为0.271\n",
      "总正确率为0.63\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012351, train_acc: 0.52\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.011828, train_acc: 0.61\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012480, train_acc: 0.61\n",
      "alexnet1d, trial.177:\n",
      "Epoch 0, avg test_loss: 0.009915, test_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012148, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012332, train_acc: 0.52\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012959, train_acc: 0.59\n",
      "alexnet1d, trial.177:\n",
      "Epoch 1, avg test_loss: 0.009847, test_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.013049, train_acc: 0.46\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011865, train_acc: 0.64\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011657, train_acc: 0.68\n",
      "alexnet1d, trial.177:\n",
      "Epoch 2, avg test_loss: 0.010011, test_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012627, train_acc: 0.48\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011192, train_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012714, train_acc: 0.59\n",
      "alexnet1d, trial.177:\n",
      "Epoch 3, avg test_loss: 0.010174, test_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011577, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012336, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011780, train_acc: 0.66\n",
      "alexnet1d, trial.177:\n",
      "Epoch 4, avg test_loss: 0.009862, test_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012186, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012033, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011796, train_acc: 0.61\n",
      "alexnet1d, trial.177:\n",
      "Epoch 5, avg test_loss: 0.009863, test_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011528, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012136, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.013989, train_acc: 0.43\n",
      "alexnet1d, trial.177:\n",
      "Epoch 6, avg test_loss: 0.010098, test_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011944, train_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012284, train_acc: 0.55\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011611, train_acc: 0.64\n",
      "alexnet1d, trial.177:\n",
      "Epoch 7, avg test_loss: 0.009740, test_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011125, train_acc: 0.68\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012265, train_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012373, train_acc: 0.50\n",
      "alexnet1d, trial.177:\n",
      "Epoch 8, avg test_loss: 0.009802, test_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012030, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011694, train_acc: 0.52\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011369, train_acc: 0.64\n",
      "alexnet1d, trial.177:\n",
      "Epoch 9, avg test_loss: 0.009697, test_acc: 0.56\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.012396, train_acc: 0.55\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011687, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010729, train_acc: 0.70\n",
      "alexnet1d, trial.177:\n",
      "Epoch 10, avg test_loss: 0.009466, test_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011540, train_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011475, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011351, train_acc: 0.59\n",
      "alexnet1d, trial.177:\n",
      "Epoch 11, avg test_loss: 0.009541, test_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011034, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010208, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011293, train_acc: 0.59\n",
      "alexnet1d, trial.177:\n",
      "Epoch 12, avg test_loss: 0.009676, test_acc: 0.63\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010987, train_acc: 0.62\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011026, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011625, train_acc: 0.64\n",
      "alexnet1d, trial.177:\n",
      "Epoch 13, avg test_loss: 0.009290, test_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010136, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011726, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011501, train_acc: 0.55\n",
      "alexnet1d, trial.177:\n",
      "Epoch 14, avg test_loss: 0.009407, test_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009150, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009349, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.012109, train_acc: 0.71\n",
      "alexnet1d, trial.177:\n",
      "Epoch 15, avg test_loss: 0.009887, test_acc: 0.59\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010520, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010993, train_acc: 0.59\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010287, train_acc: 0.73\n",
      "alexnet1d, trial.177:\n",
      "Epoch 16, avg test_loss: 0.010015, test_acc: 0.60\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009842, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010746, train_acc: 0.61\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.011146, train_acc: 0.73\n",
      "alexnet1d, trial.177:\n",
      "Epoch 17, avg test_loss: 0.011442, test_acc: 0.59\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008435, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.011222, train_acc: 0.70\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008751, train_acc: 0.80\n",
      "alexnet1d, trial.177:\n",
      "Epoch 18, avg test_loss: 0.009951, test_acc: 0.66\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009662, train_acc: 0.71\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008420, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.010953, train_acc: 0.71\n",
      "alexnet1d, trial.177:\n",
      "Epoch 19, avg test_loss: 0.012760, test_acc: 0.66\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.009434, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007902, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009126, train_acc: 0.75\n",
      "alexnet1d, trial.177:\n",
      "Epoch 20, avg test_loss: 0.011679, test_acc: 0.57\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007471, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007917, train_acc: 0.77\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006928, train_acc: 0.82\n",
      "alexnet1d, trial.177:\n",
      "Epoch 21, avg test_loss: 0.014004, test_acc: 0.61\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007323, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006862, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007240, train_acc: 0.79\n",
      "alexnet1d, trial.177:\n",
      "Epoch 22, avg test_loss: 0.014801, test_acc: 0.63\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007722, train_acc: 0.84\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005969, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.005736, train_acc: 0.86\n",
      "alexnet1d, trial.177:\n",
      "Epoch 23, avg test_loss: 0.014974, test_acc: 0.61\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006297, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006593, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006863, train_acc: 0.86\n",
      "alexnet1d, trial.177:\n",
      "Epoch 24, avg test_loss: 0.016675, test_acc: 0.63\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.006217, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004511, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005615, train_acc: 0.86\n",
      "alexnet1d, trial.177:\n",
      "Epoch 25, avg test_loss: 0.018381, test_acc: 0.66\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.002734, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.007210, train_acc: 0.84\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.005659, train_acc: 0.86\n",
      "alexnet1d, trial.177:\n",
      "Epoch 26, avg test_loss: 0.023192, test_acc: 0.61\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003078, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.008369, train_acc: 0.80\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004299, train_acc: 0.91\n",
      "alexnet1d, trial.177:\n",
      "Epoch 27, avg test_loss: 0.019126, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.005456, train_acc: 0.84\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.003248, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.002910, train_acc: 0.95\n",
      "alexnet1d, trial.177:\n",
      "Epoch 28, avg test_loss: 0.024413, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.007409, train_acc: 0.84\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.002131, train_acc: 0.96\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.003853, train_acc: 0.93\n",
      "alexnet1d, trial.177:\n",
      "Epoch 29, avg test_loss: 0.022242, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.005875, train_acc: 0.88\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.004088, train_acc: 0.91\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.003160, train_acc: 0.93\n",
      "alexnet1d, trial.177:\n",
      "Epoch 30, avg test_loss: 0.022361, test_acc: 0.67\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号38个\n",
      "错误信号32个\n",
      "信号正确并预测正确的概率为0.414\n",
      "信号错误并预测正确的概率为0.257\n",
      "总正确率为0.67\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012317, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.011785, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012583, train_acc: 0.52\n",
      "alexnet1d, trial.178:\n",
      "Epoch 0, avg test_loss: 0.009848, test_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012387, train_acc: 0.52\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012423, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012068, train_acc: 0.62\n",
      "alexnet1d, trial.178:\n",
      "Epoch 1, avg test_loss: 0.009810, test_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011879, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011846, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012584, train_acc: 0.50\n",
      "alexnet1d, trial.178:\n",
      "Epoch 2, avg test_loss: 0.009850, test_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012019, train_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012063, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.013268, train_acc: 0.45\n",
      "alexnet1d, trial.178:\n",
      "Epoch 3, avg test_loss: 0.009857, test_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011789, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012449, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012541, train_acc: 0.55\n",
      "alexnet1d, trial.178:\n",
      "Epoch 4, avg test_loss: 0.009927, test_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012400, train_acc: 0.48\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011740, train_acc: 0.68\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012214, train_acc: 0.59\n",
      "alexnet1d, trial.178:\n",
      "Epoch 5, avg test_loss: 0.010024, test_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011997, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011869, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011642, train_acc: 0.61\n",
      "alexnet1d, trial.178:\n",
      "Epoch 6, avg test_loss: 0.010586, test_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.010953, train_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011723, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012093, train_acc: 0.59\n",
      "alexnet1d, trial.178:\n",
      "Epoch 7, avg test_loss: 0.010011, test_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011632, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012128, train_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011737, train_acc: 0.61\n",
      "alexnet1d, trial.178:\n",
      "Epoch 8, avg test_loss: 0.010110, test_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011697, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011322, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011913, train_acc: 0.59\n",
      "alexnet1d, trial.178:\n",
      "Epoch 9, avg test_loss: 0.010771, test_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011464, train_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011117, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012022, train_acc: 0.66\n",
      "alexnet1d, trial.178:\n",
      "Epoch 10, avg test_loss: 0.009979, test_acc: 0.60\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.012179, train_acc: 0.52\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011402, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011055, train_acc: 0.68\n",
      "alexnet1d, trial.178:\n",
      "Epoch 11, avg test_loss: 0.010639, test_acc: 0.60\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011260, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010252, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.012185, train_acc: 0.57\n",
      "alexnet1d, trial.178:\n",
      "Epoch 12, avg test_loss: 0.010708, test_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010544, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011283, train_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011487, train_acc: 0.66\n",
      "alexnet1d, trial.178:\n",
      "Epoch 13, avg test_loss: 0.010211, test_acc: 0.63\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010336, train_acc: 0.82\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010283, train_acc: 0.62\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011360, train_acc: 0.66\n",
      "alexnet1d, trial.178:\n",
      "Epoch 14, avg test_loss: 0.011133, test_acc: 0.60\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010038, train_acc: 0.62\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010508, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.012490, train_acc: 0.64\n",
      "alexnet1d, trial.178:\n",
      "Epoch 15, avg test_loss: 0.011198, test_acc: 0.59\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009350, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010907, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010723, train_acc: 0.68\n",
      "alexnet1d, trial.178:\n",
      "Epoch 16, avg test_loss: 0.010730, test_acc: 0.60\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009316, train_acc: 0.71\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008808, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.011031, train_acc: 0.64\n",
      "alexnet1d, trial.178:\n",
      "Epoch 17, avg test_loss: 0.010524, test_acc: 0.64\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009224, train_acc: 0.71\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009933, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009289, train_acc: 0.75\n",
      "alexnet1d, trial.178:\n",
      "Epoch 18, avg test_loss: 0.011775, test_acc: 0.61\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008584, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008455, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008479, train_acc: 0.75\n",
      "alexnet1d, trial.178:\n",
      "Epoch 19, avg test_loss: 0.011957, test_acc: 0.53\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.010159, train_acc: 0.73\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007286, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009967, train_acc: 0.75\n",
      "alexnet1d, trial.178:\n",
      "Epoch 20, avg test_loss: 0.014249, test_acc: 0.56\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008053, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007842, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007542, train_acc: 0.75\n",
      "alexnet1d, trial.178:\n",
      "Epoch 21, avg test_loss: 0.014083, test_acc: 0.60\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.008376, train_acc: 0.75\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.008220, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006601, train_acc: 0.86\n",
      "alexnet1d, trial.178:\n",
      "Epoch 22, avg test_loss: 0.014488, test_acc: 0.54\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006756, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005396, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006779, train_acc: 0.88\n",
      "alexnet1d, trial.178:\n",
      "Epoch 23, avg test_loss: 0.017207, test_acc: 0.57\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.007418, train_acc: 0.79\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006261, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005606, train_acc: 0.88\n",
      "alexnet1d, trial.178:\n",
      "Epoch 24, avg test_loss: 0.020834, test_acc: 0.60\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004908, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005336, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.008602, train_acc: 0.82\n",
      "alexnet1d, trial.178:\n",
      "Epoch 25, avg test_loss: 0.016504, test_acc: 0.54\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004644, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005796, train_acc: 0.84\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004264, train_acc: 0.88\n",
      "alexnet1d, trial.178:\n",
      "Epoch 26, avg test_loss: 0.019825, test_acc: 0.54\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.005770, train_acc: 0.86\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004546, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004445, train_acc: 0.89\n",
      "alexnet1d, trial.178:\n",
      "Epoch 27, avg test_loss: 0.021535, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004924, train_acc: 0.89\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.003745, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.005604, train_acc: 0.89\n",
      "alexnet1d, trial.178:\n",
      "Epoch 28, avg test_loss: 0.020633, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.003491, train_acc: 0.96\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.003438, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.003855, train_acc: 0.89\n",
      "alexnet1d, trial.178:\n",
      "Epoch 29, avg test_loss: 0.026092, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.005165, train_acc: 0.95\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.003437, train_acc: 0.93\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.004837, train_acc: 0.89\n",
      "alexnet1d, trial.178:\n",
      "Epoch 30, avg test_loss: 0.026854, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号41个\n",
      "错误信号29个\n",
      "信号正确并预测正确的概率为0.371\n",
      "信号错误并预测正确的概率为0.186\n",
      "总正确率为0.56\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012298, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.014043, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012169, train_acc: 0.54\n",
      "alexnet1d, trial.179:\n",
      "Epoch 0, avg test_loss: 0.009806, test_acc: 0.60\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012138, train_acc: 0.66\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012437, train_acc: 0.48\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012333, train_acc: 0.54\n",
      "alexnet1d, trial.179:\n",
      "Epoch 1, avg test_loss: 0.009666, test_acc: 0.60\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012344, train_acc: 0.50\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012518, train_acc: 0.48\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011757, train_acc: 0.71\n",
      "alexnet1d, trial.179:\n",
      "Epoch 2, avg test_loss: 0.009659, test_acc: 0.60\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012238, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012108, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012500, train_acc: 0.54\n",
      "alexnet1d, trial.179:\n",
      "Epoch 3, avg test_loss: 0.009655, test_acc: 0.60\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011633, train_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012025, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012644, train_acc: 0.48\n",
      "alexnet1d, trial.179:\n",
      "Epoch 4, avg test_loss: 0.009594, test_acc: 0.60\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012676, train_acc: 0.48\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011841, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012277, train_acc: 0.50\n",
      "alexnet1d, trial.179:\n",
      "Epoch 5, avg test_loss: 0.009583, test_acc: 0.60\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011665, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011822, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012527, train_acc: 0.46\n",
      "alexnet1d, trial.179:\n",
      "Epoch 6, avg test_loss: 0.009446, test_acc: 0.60\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012776, train_acc: 0.46\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011944, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011620, train_acc: 0.64\n",
      "alexnet1d, trial.179:\n",
      "Epoch 7, avg test_loss: 0.009458, test_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011891, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011707, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011908, train_acc: 0.57\n",
      "alexnet1d, trial.179:\n",
      "Epoch 8, avg test_loss: 0.009088, test_acc: 0.63\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010837, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012106, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011763, train_acc: 0.61\n",
      "alexnet1d, trial.179:\n",
      "Epoch 9, avg test_loss: 0.009645, test_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011209, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011168, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011184, train_acc: 0.66\n",
      "alexnet1d, trial.179:\n",
      "Epoch 10, avg test_loss: 0.009339, test_acc: 0.63\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011149, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010875, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010594, train_acc: 0.71\n",
      "alexnet1d, trial.179:\n",
      "Epoch 11, avg test_loss: 0.009298, test_acc: 0.67\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011636, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011571, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011553, train_acc: 0.66\n",
      "alexnet1d, trial.179:\n",
      "Epoch 12, avg test_loss: 0.009361, test_acc: 0.63\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010278, train_acc: 0.79\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010657, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011296, train_acc: 0.70\n",
      "alexnet1d, trial.179:\n",
      "Epoch 13, avg test_loss: 0.009705, test_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009643, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009594, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010340, train_acc: 0.73\n",
      "alexnet1d, trial.179:\n",
      "Epoch 14, avg test_loss: 0.009541, test_acc: 0.60\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009942, train_acc: 0.68\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008631, train_acc: 0.84\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010595, train_acc: 0.68\n",
      "alexnet1d, trial.179:\n",
      "Epoch 15, avg test_loss: 0.009725, test_acc: 0.59\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.006524, train_acc: 0.91\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009792, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009321, train_acc: 0.77\n",
      "alexnet1d, trial.179:\n",
      "Epoch 16, avg test_loss: 0.010864, test_acc: 0.53\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008544, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008072, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008568, train_acc: 0.77\n",
      "alexnet1d, trial.179:\n",
      "Epoch 17, avg test_loss: 0.011522, test_acc: 0.56\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.006014, train_acc: 0.91\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008817, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007908, train_acc: 0.80\n",
      "alexnet1d, trial.179:\n",
      "Epoch 18, avg test_loss: 0.011125, test_acc: 0.59\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006596, train_acc: 0.91\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.006019, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009108, train_acc: 0.79\n",
      "alexnet1d, trial.179:\n",
      "Epoch 19, avg test_loss: 0.012858, test_acc: 0.53\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007044, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.005434, train_acc: 0.89\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006069, train_acc: 0.89\n",
      "alexnet1d, trial.179:\n",
      "Epoch 20, avg test_loss: 0.011173, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006412, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006686, train_acc: 0.89\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007591, train_acc: 0.84\n",
      "alexnet1d, trial.179:\n",
      "Epoch 21, avg test_loss: 0.013638, test_acc: 0.51\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007206, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.003916, train_acc: 0.93\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.004241, train_acc: 0.93\n",
      "alexnet1d, trial.179:\n",
      "Epoch 22, avg test_loss: 0.012781, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006520, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.003760, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.003630, train_acc: 0.95\n",
      "alexnet1d, trial.179:\n",
      "Epoch 23, avg test_loss: 0.016097, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004634, train_acc: 0.95\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004253, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.002779, train_acc: 0.95\n",
      "alexnet1d, trial.179:\n",
      "Epoch 24, avg test_loss: 0.020839, test_acc: 0.47\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号42个\n",
      "错误信号28个\n",
      "信号正确并预测正确的概率为0.271\n",
      "信号错误并预测正确的概率为0.2\n",
      "总正确率为0.47\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012420, train_acc: 0.48\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.013070, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012521, train_acc: 0.41\n",
      "alexnet1d, trial.180:\n",
      "Epoch 0, avg test_loss: 0.009908, test_acc: 0.46\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012345, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012262, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011791, train_acc: 0.64\n",
      "alexnet1d, trial.180:\n",
      "Epoch 1, avg test_loss: 0.009616, test_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012079, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012613, train_acc: 0.50\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012016, train_acc: 0.61\n",
      "alexnet1d, trial.180:\n",
      "Epoch 2, avg test_loss: 0.009702, test_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012239, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011807, train_acc: 0.70\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012467, train_acc: 0.52\n",
      "alexnet1d, trial.180:\n",
      "Epoch 3, avg test_loss: 0.009707, test_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012310, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011831, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012088, train_acc: 0.52\n",
      "alexnet1d, trial.180:\n",
      "Epoch 4, avg test_loss: 0.010109, test_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012027, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011810, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012267, train_acc: 0.50\n",
      "alexnet1d, trial.180:\n",
      "Epoch 5, avg test_loss: 0.009789, test_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011822, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011325, train_acc: 0.68\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011055, train_acc: 0.62\n",
      "alexnet1d, trial.180:\n",
      "Epoch 6, avg test_loss: 0.010425, test_acc: 0.63\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.010810, train_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012006, train_acc: 0.55\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011768, train_acc: 0.68\n",
      "alexnet1d, trial.180:\n",
      "Epoch 7, avg test_loss: 0.009755, test_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010749, train_acc: 0.68\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011777, train_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011491, train_acc: 0.59\n",
      "alexnet1d, trial.180:\n",
      "Epoch 8, avg test_loss: 0.010128, test_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010728, train_acc: 0.73\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011279, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011980, train_acc: 0.55\n",
      "alexnet1d, trial.180:\n",
      "Epoch 9, avg test_loss: 0.010873, test_acc: 0.60\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011129, train_acc: 0.48\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010649, train_acc: 0.77\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010388, train_acc: 0.66\n",
      "alexnet1d, trial.180:\n",
      "Epoch 10, avg test_loss: 0.010547, test_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010341, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010729, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010667, train_acc: 0.62\n",
      "alexnet1d, trial.180:\n",
      "Epoch 11, avg test_loss: 0.011444, test_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010077, train_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011106, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011133, train_acc: 0.73\n",
      "alexnet1d, trial.180:\n",
      "Epoch 12, avg test_loss: 0.011011, test_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009960, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011476, train_acc: 0.62\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009102, train_acc: 0.75\n",
      "alexnet1d, trial.180:\n",
      "Epoch 13, avg test_loss: 0.012725, test_acc: 0.60\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009176, train_acc: 0.79\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009164, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009914, train_acc: 0.66\n",
      "alexnet1d, trial.180:\n",
      "Epoch 14, avg test_loss: 0.012602, test_acc: 0.51\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008638, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009583, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011006, train_acc: 0.66\n",
      "alexnet1d, trial.180:\n",
      "Epoch 15, avg test_loss: 0.015540, test_acc: 0.61\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009488, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009547, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.007879, train_acc: 0.79\n",
      "alexnet1d, trial.180:\n",
      "Epoch 16, avg test_loss: 0.011660, test_acc: 0.63\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008709, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008268, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007838, train_acc: 0.80\n",
      "alexnet1d, trial.180:\n",
      "Epoch 17, avg test_loss: 0.013745, test_acc: 0.64\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007159, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007171, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.010574, train_acc: 0.70\n",
      "alexnet1d, trial.180:\n",
      "Epoch 18, avg test_loss: 0.015681, test_acc: 0.60\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007396, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008326, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008737, train_acc: 0.79\n",
      "alexnet1d, trial.180:\n",
      "Epoch 19, avg test_loss: 0.016425, test_acc: 0.51\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007241, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.005926, train_acc: 0.86\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006870, train_acc: 0.86\n",
      "alexnet1d, trial.180:\n",
      "Epoch 20, avg test_loss: 0.018838, test_acc: 0.56\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006518, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007021, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006125, train_acc: 0.82\n",
      "alexnet1d, trial.180:\n",
      "Epoch 21, avg test_loss: 0.015581, test_acc: 0.63\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.004977, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005118, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006289, train_acc: 0.86\n",
      "alexnet1d, trial.180:\n",
      "Epoch 22, avg test_loss: 0.020918, test_acc: 0.60\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.003000, train_acc: 0.95\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.005301, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.011018, train_acc: 0.73\n",
      "alexnet1d, trial.180:\n",
      "Epoch 23, avg test_loss: 0.021498, test_acc: 0.59\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004290, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.007454, train_acc: 0.80\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.004624, train_acc: 0.91\n",
      "alexnet1d, trial.180:\n",
      "Epoch 24, avg test_loss: 0.022770, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004716, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006366, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004549, train_acc: 0.89\n",
      "alexnet1d, trial.180:\n",
      "Epoch 25, avg test_loss: 0.018687, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005146, train_acc: 0.84\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005968, train_acc: 0.86\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004927, train_acc: 0.89\n",
      "alexnet1d, trial.180:\n",
      "Epoch 26, avg test_loss: 0.020221, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003357, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004677, train_acc: 0.88\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003439, train_acc: 0.98\n",
      "alexnet1d, trial.180:\n",
      "Epoch 27, avg test_loss: 0.020875, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号43个\n",
      "错误信号27个\n",
      "信号正确并预测正确的概率为0.443\n",
      "信号错误并预测正确的概率为0.143\n",
      "总正确率为0.59\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012444, train_acc: 0.39\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012411, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012289, train_acc: 0.59\n",
      "alexnet1d, trial.181:\n",
      "Epoch 0, avg test_loss: 0.009810, test_acc: 0.63\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012090, train_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012838, train_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012156, train_acc: 0.59\n",
      "alexnet1d, trial.181:\n",
      "Epoch 1, avg test_loss: 0.009801, test_acc: 0.63\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012350, train_acc: 0.48\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012159, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011772, train_acc: 0.64\n",
      "alexnet1d, trial.181:\n",
      "Epoch 2, avg test_loss: 0.010061, test_acc: 0.63\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.013154, train_acc: 0.48\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011601, train_acc: 0.70\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012181, train_acc: 0.59\n",
      "alexnet1d, trial.181:\n",
      "Epoch 3, avg test_loss: 0.010137, test_acc: 0.63\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011879, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.010657, train_acc: 0.68\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011647, train_acc: 0.57\n",
      "alexnet1d, trial.181:\n",
      "Epoch 4, avg test_loss: 0.010139, test_acc: 0.63\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012108, train_acc: 0.50\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011901, train_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011239, train_acc: 0.59\n",
      "alexnet1d, trial.181:\n",
      "Epoch 5, avg test_loss: 0.010651, test_acc: 0.67\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011832, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011790, train_acc: 0.52\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011692, train_acc: 0.64\n",
      "alexnet1d, trial.181:\n",
      "Epoch 6, avg test_loss: 0.011737, test_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012044, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.013231, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012035, train_acc: 0.62\n",
      "alexnet1d, trial.181:\n",
      "Epoch 7, avg test_loss: 0.010210, test_acc: 0.44\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011941, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011017, train_acc: 0.68\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011873, train_acc: 0.59\n",
      "alexnet1d, trial.181:\n",
      "Epoch 8, avg test_loss: 0.011031, test_acc: 0.69\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012462, train_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011341, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010704, train_acc: 0.68\n",
      "alexnet1d, trial.181:\n",
      "Epoch 9, avg test_loss: 0.010766, test_acc: 0.56\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011949, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010848, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010483, train_acc: 0.71\n",
      "alexnet1d, trial.181:\n",
      "Epoch 10, avg test_loss: 0.011179, test_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010442, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.009525, train_acc: 0.77\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010334, train_acc: 0.71\n",
      "alexnet1d, trial.181:\n",
      "Epoch 11, avg test_loss: 0.010748, test_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010394, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.012372, train_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010072, train_acc: 0.73\n",
      "alexnet1d, trial.181:\n",
      "Epoch 12, avg test_loss: 0.013409, test_acc: 0.53\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.008622, train_acc: 0.82\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009465, train_acc: 0.77\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010427, train_acc: 0.66\n",
      "alexnet1d, trial.181:\n",
      "Epoch 13, avg test_loss: 0.012600, test_acc: 0.46\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009625, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010007, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009904, train_acc: 0.68\n",
      "alexnet1d, trial.181:\n",
      "Epoch 14, avg test_loss: 0.010762, test_acc: 0.60\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009540, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009004, train_acc: 0.80\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009359, train_acc: 0.77\n",
      "alexnet1d, trial.181:\n",
      "Epoch 15, avg test_loss: 0.013992, test_acc: 0.57\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010151, train_acc: 0.62\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.007959, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.007075, train_acc: 0.84\n",
      "alexnet1d, trial.181:\n",
      "Epoch 16, avg test_loss: 0.016330, test_acc: 0.56\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008621, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.006189, train_acc: 0.84\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.006522, train_acc: 0.89\n",
      "alexnet1d, trial.181:\n",
      "Epoch 17, avg test_loss: 0.018868, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.005863, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007505, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008218, train_acc: 0.80\n",
      "alexnet1d, trial.181:\n",
      "Epoch 18, avg test_loss: 0.019591, test_acc: 0.46\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007846, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007481, train_acc: 0.88\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009636, train_acc: 0.79\n",
      "alexnet1d, trial.181:\n",
      "Epoch 19, avg test_loss: 0.025452, test_acc: 0.60\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.010509, train_acc: 0.70\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.009414, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007992, train_acc: 0.84\n",
      "alexnet1d, trial.181:\n",
      "Epoch 20, avg test_loss: 0.018284, test_acc: 0.47\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008282, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.008052, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.009968, train_acc: 0.70\n",
      "alexnet1d, trial.181:\n",
      "Epoch 21, avg test_loss: 0.018143, test_acc: 0.46\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007209, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.008692, train_acc: 0.77\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007948, train_acc: 0.80\n",
      "alexnet1d, trial.181:\n",
      "Epoch 22, avg test_loss: 0.016565, test_acc: 0.54\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006745, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006209, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006052, train_acc: 0.84\n",
      "alexnet1d, trial.181:\n",
      "Epoch 23, avg test_loss: 0.017879, test_acc: 0.53\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005360, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005084, train_acc: 0.93\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.004435, train_acc: 0.91\n",
      "alexnet1d, trial.181:\n",
      "Epoch 24, avg test_loss: 0.022908, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005113, train_acc: 0.91\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.003290, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006268, train_acc: 0.89\n",
      "alexnet1d, trial.181:\n",
      "Epoch 25, avg test_loss: 0.031343, test_acc: 0.49\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005608, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004417, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.002860, train_acc: 0.95\n",
      "alexnet1d, trial.181:\n",
      "Epoch 26, avg test_loss: 0.033959, test_acc: 0.47\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号44个\n",
      "错误信号26个\n",
      "信号正确并预测正确的概率为0.343\n",
      "信号错误并预测正确的概率为0.129\n",
      "总正确率为0.47\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012473, train_acc: 0.43\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012958, train_acc: 0.62\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012490, train_acc: 0.52\n",
      "alexnet1d, trial.182:\n",
      "Epoch 0, avg test_loss: 0.009821, test_acc: 0.53\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011861, train_acc: 0.68\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012282, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011866, train_acc: 0.64\n",
      "alexnet1d, trial.182:\n",
      "Epoch 1, avg test_loss: 0.009842, test_acc: 0.53\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012086, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012111, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011973, train_acc: 0.57\n",
      "alexnet1d, trial.182:\n",
      "Epoch 2, avg test_loss: 0.009961, test_acc: 0.53\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011586, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.010732, train_acc: 0.71\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012234, train_acc: 0.62\n",
      "alexnet1d, trial.182:\n",
      "Epoch 3, avg test_loss: 0.009884, test_acc: 0.53\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011540, train_acc: 0.68\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011915, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012365, train_acc: 0.55\n",
      "alexnet1d, trial.182:\n",
      "Epoch 4, avg test_loss: 0.009851, test_acc: 0.53\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011974, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011693, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012978, train_acc: 0.52\n",
      "alexnet1d, trial.182:\n",
      "Epoch 5, avg test_loss: 0.009853, test_acc: 0.53\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012522, train_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011008, train_acc: 0.70\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012865, train_acc: 0.52\n",
      "alexnet1d, trial.182:\n",
      "Epoch 6, avg test_loss: 0.009795, test_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012356, train_acc: 0.50\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.010784, train_acc: 0.71\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012677, train_acc: 0.50\n",
      "alexnet1d, trial.182:\n",
      "Epoch 7, avg test_loss: 0.010023, test_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011669, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011475, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011601, train_acc: 0.57\n",
      "alexnet1d, trial.182:\n",
      "Epoch 8, avg test_loss: 0.009871, test_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011064, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011891, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.009705, train_acc: 0.73\n",
      "alexnet1d, trial.182:\n",
      "Epoch 9, avg test_loss: 0.010102, test_acc: 0.60\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010655, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011701, train_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.009976, train_acc: 0.70\n",
      "alexnet1d, trial.182:\n",
      "Epoch 10, avg test_loss: 0.010156, test_acc: 0.60\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.009300, train_acc: 0.73\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010194, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.009868, train_acc: 0.61\n",
      "alexnet1d, trial.182:\n",
      "Epoch 11, avg test_loss: 0.012760, test_acc: 0.60\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.012052, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.009507, train_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011299, train_acc: 0.61\n",
      "alexnet1d, trial.182:\n",
      "Epoch 12, avg test_loss: 0.012333, test_acc: 0.54\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.008968, train_acc: 0.75\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010063, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011159, train_acc: 0.59\n",
      "alexnet1d, trial.182:\n",
      "Epoch 13, avg test_loss: 0.010971, test_acc: 0.64\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.007931, train_acc: 0.80\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.007864, train_acc: 0.80\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009372, train_acc: 0.73\n",
      "alexnet1d, trial.182:\n",
      "Epoch 14, avg test_loss: 0.013753, test_acc: 0.61\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008512, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008500, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009286, train_acc: 0.75\n",
      "alexnet1d, trial.182:\n",
      "Epoch 15, avg test_loss: 0.012665, test_acc: 0.61\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008493, train_acc: 0.80\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.007110, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.007655, train_acc: 0.84\n",
      "alexnet1d, trial.182:\n",
      "Epoch 16, avg test_loss: 0.013331, test_acc: 0.57\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007018, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.006233, train_acc: 0.88\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.006112, train_acc: 0.86\n",
      "alexnet1d, trial.182:\n",
      "Epoch 17, avg test_loss: 0.017143, test_acc: 0.63\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.004687, train_acc: 0.89\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007243, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.010009, train_acc: 0.75\n",
      "alexnet1d, trial.182:\n",
      "Epoch 18, avg test_loss: 0.018262, test_acc: 0.61\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.005721, train_acc: 0.88\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.006607, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.006156, train_acc: 0.84\n",
      "alexnet1d, trial.182:\n",
      "Epoch 19, avg test_loss: 0.018197, test_acc: 0.60\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006442, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.005216, train_acc: 0.91\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007651, train_acc: 0.82\n",
      "alexnet1d, trial.182:\n",
      "Epoch 20, avg test_loss: 0.015892, test_acc: 0.63\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.005453, train_acc: 0.89\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.005069, train_acc: 0.86\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006512, train_acc: 0.84\n",
      "alexnet1d, trial.182:\n",
      "Epoch 21, avg test_loss: 0.016013, test_acc: 0.60\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005414, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006912, train_acc: 0.84\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.004328, train_acc: 0.93\n",
      "alexnet1d, trial.182:\n",
      "Epoch 22, avg test_loss: 0.017387, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005977, train_acc: 0.82\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.004856, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.005099, train_acc: 0.88\n",
      "alexnet1d, trial.182:\n",
      "Epoch 23, avg test_loss: 0.020435, test_acc: 0.59\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005236, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.002114, train_acc: 0.96\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005646, train_acc: 0.88\n",
      "alexnet1d, trial.182:\n",
      "Epoch 24, avg test_loss: 0.022942, test_acc: 0.61\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.002342, train_acc: 0.96\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.003853, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.003056, train_acc: 0.95\n",
      "alexnet1d, trial.182:\n",
      "Epoch 25, avg test_loss: 0.021924, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.003560, train_acc: 0.91\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.002130, train_acc: 0.96\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004093, train_acc: 0.89\n",
      "alexnet1d, trial.182:\n",
      "Epoch 26, avg test_loss: 0.025793, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003216, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004012, train_acc: 0.86\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.005749, train_acc: 0.91\n",
      "alexnet1d, trial.182:\n",
      "Epoch 27, avg test_loss: 0.029059, test_acc: 0.66\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号37个\n",
      "错误信号33个\n",
      "信号正确并预测正确的概率为0.443\n",
      "信号错误并预测正确的概率为0.214\n",
      "总正确率为0.66\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012445, train_acc: 0.38\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012643, train_acc: 0.48\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012081, train_acc: 0.61\n",
      "alexnet1d, trial.183:\n",
      "Epoch 0, avg test_loss: 0.009600, test_acc: 0.66\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012341, train_acc: 0.48\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.013492, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012350, train_acc: 0.48\n",
      "alexnet1d, trial.183:\n",
      "Epoch 1, avg test_loss: 0.009995, test_acc: 0.34\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012497, train_acc: 0.39\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012365, train_acc: 0.52\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012394, train_acc: 0.57\n",
      "alexnet1d, trial.183:\n",
      "Epoch 2, avg test_loss: 0.009660, test_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012331, train_acc: 0.52\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011554, train_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.013218, train_acc: 0.57\n",
      "alexnet1d, trial.183:\n",
      "Epoch 3, avg test_loss: 0.009603, test_acc: 0.66\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012497, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012118, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012592, train_acc: 0.45\n",
      "alexnet1d, trial.183:\n",
      "Epoch 4, avg test_loss: 0.009657, test_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011991, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012408, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012077, train_acc: 0.52\n",
      "alexnet1d, trial.183:\n",
      "Epoch 5, avg test_loss: 0.009675, test_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012294, train_acc: 0.52\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011648, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012279, train_acc: 0.48\n",
      "alexnet1d, trial.183:\n",
      "Epoch 6, avg test_loss: 0.009969, test_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012164, train_acc: 0.52\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012045, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011740, train_acc: 0.54\n",
      "alexnet1d, trial.183:\n",
      "Epoch 7, avg test_loss: 0.010042, test_acc: 0.63\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011625, train_acc: 0.52\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011926, train_acc: 0.55\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011690, train_acc: 0.66\n",
      "alexnet1d, trial.183:\n",
      "Epoch 8, avg test_loss: 0.010011, test_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011587, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011447, train_acc: 0.71\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010577, train_acc: 0.75\n",
      "alexnet1d, trial.183:\n",
      "Epoch 9, avg test_loss: 0.010460, test_acc: 0.60\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011386, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010181, train_acc: 0.71\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012460, train_acc: 0.64\n",
      "alexnet1d, trial.183:\n",
      "Epoch 10, avg test_loss: 0.012010, test_acc: 0.47\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010367, train_acc: 0.71\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011216, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011622, train_acc: 0.57\n",
      "alexnet1d, trial.183:\n",
      "Epoch 11, avg test_loss: 0.010311, test_acc: 0.63\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011110, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.012547, train_acc: 0.50\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010192, train_acc: 0.79\n",
      "alexnet1d, trial.183:\n",
      "Epoch 12, avg test_loss: 0.010314, test_acc: 0.60\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011219, train_acc: 0.62\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010948, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011645, train_acc: 0.62\n",
      "alexnet1d, trial.183:\n",
      "Epoch 13, avg test_loss: 0.011293, test_acc: 0.54\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010417, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009560, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010540, train_acc: 0.70\n",
      "alexnet1d, trial.183:\n",
      "Epoch 14, avg test_loss: 0.011669, test_acc: 0.51\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009581, train_acc: 0.84\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010446, train_acc: 0.68\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010800, train_acc: 0.62\n",
      "alexnet1d, trial.183:\n",
      "Epoch 15, avg test_loss: 0.011686, test_acc: 0.56\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010295, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.011439, train_acc: 0.59\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009834, train_acc: 0.77\n",
      "alexnet1d, trial.183:\n",
      "Epoch 16, avg test_loss: 0.011529, test_acc: 0.53\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009388, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009298, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.011410, train_acc: 0.68\n",
      "alexnet1d, trial.183:\n",
      "Epoch 17, avg test_loss: 0.012773, test_acc: 0.56\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.010069, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007880, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.011212, train_acc: 0.70\n",
      "alexnet1d, trial.183:\n",
      "Epoch 18, avg test_loss: 0.011640, test_acc: 0.54\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007761, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.010306, train_acc: 0.70\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.010595, train_acc: 0.75\n",
      "alexnet1d, trial.183:\n",
      "Epoch 19, avg test_loss: 0.013065, test_acc: 0.56\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.010542, train_acc: 0.64\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007676, train_acc: 0.88\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.009322, train_acc: 0.77\n",
      "alexnet1d, trial.183:\n",
      "Epoch 20, avg test_loss: 0.011098, test_acc: 0.54\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008401, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.008208, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008057, train_acc: 0.79\n",
      "alexnet1d, trial.183:\n",
      "Epoch 21, avg test_loss: 0.013719, test_acc: 0.57\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.011402, train_acc: 0.68\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007400, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.006061, train_acc: 0.91\n",
      "alexnet1d, trial.183:\n",
      "Epoch 22, avg test_loss: 0.011808, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.008658, train_acc: 0.79\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006744, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.008100, train_acc: 0.79\n",
      "alexnet1d, trial.183:\n",
      "Epoch 23, avg test_loss: 0.013884, test_acc: 0.60\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.008515, train_acc: 0.79\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.007884, train_acc: 0.80\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.008554, train_acc: 0.79\n",
      "alexnet1d, trial.183:\n",
      "Epoch 24, avg test_loss: 0.012346, test_acc: 0.59\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.009983, train_acc: 0.68\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.007877, train_acc: 0.82\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005610, train_acc: 0.86\n",
      "alexnet1d, trial.183:\n",
      "Epoch 25, avg test_loss: 0.014139, test_acc: 0.53\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005383, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004766, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.007704, train_acc: 0.79\n",
      "alexnet1d, trial.183:\n",
      "Epoch 26, avg test_loss: 0.015455, test_acc: 0.53\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.005953, train_acc: 0.88\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.007776, train_acc: 0.84\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004577, train_acc: 0.93\n",
      "alexnet1d, trial.183:\n",
      "Epoch 27, avg test_loss: 0.016057, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.007288, train_acc: 0.82\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.007037, train_acc: 0.79\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.003992, train_acc: 0.93\n",
      "alexnet1d, trial.183:\n",
      "Epoch 28, avg test_loss: 0.015359, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.003883, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.002546, train_acc: 0.96\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.005897, train_acc: 0.89\n",
      "alexnet1d, trial.183:\n",
      "Epoch 29, avg test_loss: 0.022180, test_acc: 0.49\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号46个\n",
      "错误信号24个\n",
      "信号正确并预测正确的概率为0.343\n",
      "信号错误并预测正确的概率为0.143\n",
      "总正确率为0.49\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012349, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012711, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012347, train_acc: 0.55\n",
      "alexnet1d, trial.184:\n",
      "Epoch 0, avg test_loss: 0.009837, test_acc: 0.56\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012170, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012266, train_acc: 0.54\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012643, train_acc: 0.52\n",
      "alexnet1d, trial.184:\n",
      "Epoch 1, avg test_loss: 0.009945, test_acc: 0.56\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011688, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.013403, train_acc: 0.48\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012105, train_acc: 0.57\n",
      "alexnet1d, trial.184:\n",
      "Epoch 2, avg test_loss: 0.010118, test_acc: 0.56\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011867, train_acc: 0.55\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011434, train_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011400, train_acc: 0.61\n",
      "alexnet1d, trial.184:\n",
      "Epoch 3, avg test_loss: 0.010835, test_acc: 0.49\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011098, train_acc: 0.62\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011465, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011194, train_acc: 0.50\n",
      "alexnet1d, trial.184:\n",
      "Epoch 4, avg test_loss: 0.010244, test_acc: 0.44\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012309, train_acc: 0.50\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011752, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.010478, train_acc: 0.75\n",
      "alexnet1d, trial.184:\n",
      "Epoch 5, avg test_loss: 0.010939, test_acc: 0.46\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011127, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.010777, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.010184, train_acc: 0.75\n",
      "alexnet1d, trial.184:\n",
      "Epoch 6, avg test_loss: 0.011967, test_acc: 0.44\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.010224, train_acc: 0.68\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011703, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.010205, train_acc: 0.71\n",
      "alexnet1d, trial.184:\n",
      "Epoch 7, avg test_loss: 0.013232, test_acc: 0.46\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010302, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.010468, train_acc: 0.71\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.010554, train_acc: 0.66\n",
      "alexnet1d, trial.184:\n",
      "Epoch 8, avg test_loss: 0.012221, test_acc: 0.39\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011931, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010547, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011549, train_acc: 0.62\n",
      "alexnet1d, trial.184:\n",
      "Epoch 9, avg test_loss: 0.012394, test_acc: 0.47\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010865, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010114, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010907, train_acc: 0.64\n",
      "alexnet1d, trial.184:\n",
      "Epoch 10, avg test_loss: 0.013137, test_acc: 0.49\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011679, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.009517, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011269, train_acc: 0.66\n",
      "alexnet1d, trial.184:\n",
      "Epoch 11, avg test_loss: 0.015653, test_acc: 0.43\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.008789, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010254, train_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011222, train_acc: 0.64\n",
      "alexnet1d, trial.184:\n",
      "Epoch 12, avg test_loss: 0.014618, test_acc: 0.50\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010855, train_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.008379, train_acc: 0.80\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009427, train_acc: 0.77\n",
      "alexnet1d, trial.184:\n",
      "Epoch 13, avg test_loss: 0.016464, test_acc: 0.50\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.008285, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009584, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009195, train_acc: 0.71\n",
      "alexnet1d, trial.184:\n",
      "Epoch 14, avg test_loss: 0.018045, test_acc: 0.50\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008538, train_acc: 0.80\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008664, train_acc: 0.79\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008240, train_acc: 0.77\n",
      "alexnet1d, trial.184:\n",
      "Epoch 15, avg test_loss: 0.024604, test_acc: 0.53\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010092, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.006861, train_acc: 0.84\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008769, train_acc: 0.80\n",
      "alexnet1d, trial.184:\n",
      "Epoch 16, avg test_loss: 0.018512, test_acc: 0.51\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009259, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009648, train_acc: 0.62\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.006606, train_acc: 0.88\n",
      "alexnet1d, trial.184:\n",
      "Epoch 17, avg test_loss: 0.022200, test_acc: 0.44\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008759, train_acc: 0.75\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008187, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.012886, train_acc: 0.64\n",
      "alexnet1d, trial.184:\n",
      "Epoch 18, avg test_loss: 0.018291, test_acc: 0.46\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006107, train_acc: 0.89\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.011307, train_acc: 0.66\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.010068, train_acc: 0.64\n",
      "alexnet1d, trial.184:\n",
      "Epoch 19, avg test_loss: 0.017948, test_acc: 0.46\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008773, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.009704, train_acc: 0.73\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.010823, train_acc: 0.68\n",
      "alexnet1d, trial.184:\n",
      "Epoch 20, avg test_loss: 0.020433, test_acc: 0.46\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.009056, train_acc: 0.73\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006829, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.009972, train_acc: 0.68\n",
      "alexnet1d, trial.184:\n",
      "Epoch 21, avg test_loss: 0.017007, test_acc: 0.49\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007943, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007541, train_acc: 0.79\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008867, train_acc: 0.79\n",
      "alexnet1d, trial.184:\n",
      "Epoch 22, avg test_loss: 0.018501, test_acc: 0.46\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007536, train_acc: 0.77\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007960, train_acc: 0.79\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.007108, train_acc: 0.84\n",
      "alexnet1d, trial.184:\n",
      "Epoch 23, avg test_loss: 0.022810, test_acc: 0.53\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006171, train_acc: 0.80\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006623, train_acc: 0.79\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.008433, train_acc: 0.77\n",
      "alexnet1d, trial.184:\n",
      "Epoch 24, avg test_loss: 0.023128, test_acc: 0.49\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005924, train_acc: 0.82\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006525, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006393, train_acc: 0.86\n",
      "alexnet1d, trial.184:\n",
      "Epoch 25, avg test_loss: 0.026960, test_acc: 0.47\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004493, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005455, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.006090, train_acc: 0.86\n",
      "alexnet1d, trial.184:\n",
      "Epoch 26, avg test_loss: 0.035605, test_acc: 0.49\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.006225, train_acc: 0.84\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.004271, train_acc: 0.88\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.007083, train_acc: 0.82\n",
      "alexnet1d, trial.184:\n",
      "Epoch 27, avg test_loss: 0.027419, test_acc: 0.53\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.006988, train_acc: 0.82\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.003102, train_acc: 0.95\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.008173, train_acc: 0.79\n",
      "alexnet1d, trial.184:\n",
      "Epoch 28, avg test_loss: 0.044585, test_acc: 0.51\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.005664, train_acc: 0.82\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.003184, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.005162, train_acc: 0.84\n",
      "alexnet1d, trial.184:\n",
      "Epoch 29, avg test_loss: 0.029700, test_acc: 0.54\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.003763, train_acc: 0.91\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.006277, train_acc: 0.86\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.005623, train_acc: 0.84\n",
      "alexnet1d, trial.184:\n",
      "Epoch 30, avg test_loss: 0.037287, test_acc: 0.53\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.003380, train_acc: 0.91\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.004319, train_acc: 0.93\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.005211, train_acc: 0.91\n",
      "alexnet1d, trial.184:\n",
      "Epoch 31, avg test_loss: 0.039437, test_acc: 0.50\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.004644, train_acc: 0.88\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.002565, train_acc: 0.96\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.004868, train_acc: 0.86\n",
      "alexnet1d, trial.184:\n",
      "Epoch 32, avg test_loss: 0.038716, test_acc: 0.54\n",
      "Train Epoch 33, lr: 0.000377, 0/280, avg loss: 0.003377, train_acc: 0.93\n",
      "Train Epoch 33, lr: 0.000377, 112/280, avg loss: 0.005125, train_acc: 0.88\n",
      "Train Epoch 33, lr: 0.000377, 224/280, avg loss: 0.002945, train_acc: 0.98\n",
      "alexnet1d, trial.184:\n",
      "Epoch 33, avg test_loss: 0.045533, test_acc: 0.49\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 34, lr: 0.000377, 0/280, avg loss: 0.001749, train_acc: 0.98\n",
      "Train Epoch 34, lr: 0.000377, 112/280, avg loss: 0.001548, train_acc: 0.98\n",
      "Train Epoch 34, lr: 0.000377, 224/280, avg loss: 0.003107, train_acc: 0.89\n",
      "alexnet1d, trial.184:\n",
      "Epoch 34, avg test_loss: 0.053724, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 35, lr: 0.000321, 0/280, avg loss: 0.004155, train_acc: 0.88\n",
      "Train Epoch 35, lr: 0.000321, 112/280, avg loss: 0.003187, train_acc: 0.96\n",
      "Train Epoch 35, lr: 0.000321, 224/280, avg loss: 0.001531, train_acc: 0.98\n",
      "alexnet1d, trial.184:\n",
      "Epoch 35, avg test_loss: 0.053237, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号39个\n",
      "错误信号31个\n",
      "信号正确并预测正确的概率为0.429\n",
      "信号错误并预测正确的概率为0.114\n",
      "总正确率为0.54\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012364, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012478, train_acc: 0.61\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012259, train_acc: 0.61\n",
      "alexnet1d, trial.185:\n",
      "Epoch 0, avg test_loss: 0.009799, test_acc: 0.66\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012282, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012129, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012234, train_acc: 0.59\n",
      "alexnet1d, trial.185:\n",
      "Epoch 1, avg test_loss: 0.009528, test_acc: 0.66\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012227, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.013622, train_acc: 0.45\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012374, train_acc: 0.55\n",
      "alexnet1d, trial.185:\n",
      "Epoch 2, avg test_loss: 0.009635, test_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012063, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012459, train_acc: 0.48\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012200, train_acc: 0.62\n",
      "alexnet1d, trial.185:\n",
      "Epoch 3, avg test_loss: 0.009719, test_acc: 0.67\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011999, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012404, train_acc: 0.48\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011824, train_acc: 0.59\n",
      "alexnet1d, trial.185:\n",
      "Epoch 4, avg test_loss: 0.010301, test_acc: 0.67\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012604, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011306, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011414, train_acc: 0.61\n",
      "alexnet1d, trial.185:\n",
      "Epoch 5, avg test_loss: 0.010586, test_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011190, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012808, train_acc: 0.52\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011389, train_acc: 0.64\n",
      "alexnet1d, trial.185:\n",
      "Epoch 6, avg test_loss: 0.012091, test_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.010153, train_acc: 0.73\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012521, train_acc: 0.55\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012985, train_acc: 0.59\n",
      "alexnet1d, trial.185:\n",
      "Epoch 7, avg test_loss: 0.012065, test_acc: 0.50\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011687, train_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012342, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011963, train_acc: 0.61\n",
      "alexnet1d, trial.185:\n",
      "Epoch 8, avg test_loss: 0.010064, test_acc: 0.60\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011334, train_acc: 0.73\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012002, train_acc: 0.55\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010843, train_acc: 0.61\n",
      "alexnet1d, trial.185:\n",
      "Epoch 9, avg test_loss: 0.010736, test_acc: 0.63\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010678, train_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.013633, train_acc: 0.55\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010025, train_acc: 0.70\n",
      "alexnet1d, trial.185:\n",
      "Epoch 10, avg test_loss: 0.012547, test_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010886, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011152, train_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.009575, train_acc: 0.77\n",
      "alexnet1d, trial.185:\n",
      "Epoch 11, avg test_loss: 0.011774, test_acc: 0.63\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010701, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.009995, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010836, train_acc: 0.61\n",
      "alexnet1d, trial.185:\n",
      "Epoch 12, avg test_loss: 0.014525, test_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010161, train_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010800, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009540, train_acc: 0.77\n",
      "alexnet1d, trial.185:\n",
      "Epoch 13, avg test_loss: 0.014582, test_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009519, train_acc: 0.80\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009052, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011995, train_acc: 0.59\n",
      "alexnet1d, trial.185:\n",
      "Epoch 14, avg test_loss: 0.016196, test_acc: 0.54\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008447, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008559, train_acc: 0.80\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011292, train_acc: 0.64\n",
      "alexnet1d, trial.185:\n",
      "Epoch 15, avg test_loss: 0.013029, test_acc: 0.60\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008742, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008250, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009883, train_acc: 0.68\n",
      "alexnet1d, trial.185:\n",
      "Epoch 16, avg test_loss: 0.016022, test_acc: 0.67\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009194, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007434, train_acc: 0.84\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008079, train_acc: 0.80\n",
      "alexnet1d, trial.185:\n",
      "Epoch 17, avg test_loss: 0.019639, test_acc: 0.59\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.006494, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008349, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007539, train_acc: 0.79\n",
      "alexnet1d, trial.185:\n",
      "Epoch 18, avg test_loss: 0.018143, test_acc: 0.59\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008740, train_acc: 0.70\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007276, train_acc: 0.75\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008262, train_acc: 0.84\n",
      "alexnet1d, trial.185:\n",
      "Epoch 19, avg test_loss: 0.017814, test_acc: 0.53\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006634, train_acc: 0.88\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007850, train_acc: 0.73\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008829, train_acc: 0.70\n",
      "alexnet1d, trial.185:\n",
      "Epoch 20, avg test_loss: 0.019738, test_acc: 0.57\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.005424, train_acc: 0.89\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.009320, train_acc: 0.77\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006398, train_acc: 0.80\n",
      "alexnet1d, trial.185:\n",
      "Epoch 21, avg test_loss: 0.023075, test_acc: 0.44\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.004960, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.003404, train_acc: 0.93\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.005853, train_acc: 0.82\n",
      "alexnet1d, trial.185:\n",
      "Epoch 22, avg test_loss: 0.022706, test_acc: 0.57\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.004423, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.004379, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.005222, train_acc: 0.89\n",
      "alexnet1d, trial.185:\n",
      "Epoch 23, avg test_loss: 0.028828, test_acc: 0.49\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004829, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.007755, train_acc: 0.82\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005261, train_acc: 0.88\n",
      "alexnet1d, trial.185:\n",
      "Epoch 24, avg test_loss: 0.026012, test_acc: 0.53\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004877, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.004791, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004260, train_acc: 0.89\n",
      "alexnet1d, trial.185:\n",
      "Epoch 25, avg test_loss: 0.026592, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004354, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004110, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.003698, train_acc: 0.93\n",
      "alexnet1d, trial.185:\n",
      "Epoch 26, avg test_loss: 0.026803, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.003179, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.003914, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004506, train_acc: 0.88\n",
      "alexnet1d, trial.185:\n",
      "Epoch 27, avg test_loss: 0.029700, test_acc: 0.54\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.002835, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.002754, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.003165, train_acc: 0.95\n",
      "alexnet1d, trial.185:\n",
      "Epoch 28, avg test_loss: 0.033961, test_acc: 0.64\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号46个\n",
      "错误信号24个\n",
      "信号正确并预测正确的概率为0.443\n",
      "信号错误并预测正确的概率为0.2\n",
      "总正确率为0.64\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012391, train_acc: 0.52\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.015799, train_acc: 0.48\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012394, train_acc: 0.50\n",
      "alexnet1d, trial.186:\n",
      "Epoch 0, avg test_loss: 0.009897, test_acc: 0.51\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012236, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.011962, train_acc: 0.62\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012077, train_acc: 0.59\n",
      "alexnet1d, trial.186:\n",
      "Epoch 1, avg test_loss: 0.010105, test_acc: 0.51\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011904, train_acc: 0.61\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012319, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011867, train_acc: 0.66\n",
      "alexnet1d, trial.186:\n",
      "Epoch 2, avg test_loss: 0.009951, test_acc: 0.51\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011991, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011612, train_acc: 0.68\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011616, train_acc: 0.64\n",
      "alexnet1d, trial.186:\n",
      "Epoch 3, avg test_loss: 0.010037, test_acc: 0.51\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011908, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011225, train_acc: 0.68\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.013327, train_acc: 0.48\n",
      "alexnet1d, trial.186:\n",
      "Epoch 4, avg test_loss: 0.010192, test_acc: 0.51\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011377, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011042, train_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011952, train_acc: 0.57\n",
      "alexnet1d, trial.186:\n",
      "Epoch 5, avg test_loss: 0.009986, test_acc: 0.50\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011509, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011229, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012043, train_acc: 0.61\n",
      "alexnet1d, trial.186:\n",
      "Epoch 6, avg test_loss: 0.010080, test_acc: 0.53\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011011, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.010325, train_acc: 0.71\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.013433, train_acc: 0.62\n",
      "alexnet1d, trial.186:\n",
      "Epoch 7, avg test_loss: 0.009985, test_acc: 0.56\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010553, train_acc: 0.70\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011145, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011702, train_acc: 0.61\n",
      "alexnet1d, trial.186:\n",
      "Epoch 8, avg test_loss: 0.009940, test_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011932, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011066, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012025, train_acc: 0.68\n",
      "alexnet1d, trial.186:\n",
      "Epoch 9, avg test_loss: 0.009773, test_acc: 0.56\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011009, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010877, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.013040, train_acc: 0.52\n",
      "alexnet1d, trial.186:\n",
      "Epoch 10, avg test_loss: 0.010213, test_acc: 0.56\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011017, train_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010648, train_acc: 0.64\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011401, train_acc: 0.70\n",
      "alexnet1d, trial.186:\n",
      "Epoch 11, avg test_loss: 0.010260, test_acc: 0.60\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009679, train_acc: 0.75\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010139, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011637, train_acc: 0.71\n",
      "alexnet1d, trial.186:\n",
      "Epoch 12, avg test_loss: 0.010363, test_acc: 0.57\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009995, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011659, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010541, train_acc: 0.71\n",
      "alexnet1d, trial.186:\n",
      "Epoch 13, avg test_loss: 0.009979, test_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010938, train_acc: 0.61\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009751, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.012616, train_acc: 0.70\n",
      "alexnet1d, trial.186:\n",
      "Epoch 14, avg test_loss: 0.010847, test_acc: 0.63\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008667, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010076, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.012451, train_acc: 0.59\n",
      "alexnet1d, trial.186:\n",
      "Epoch 15, avg test_loss: 0.011023, test_acc: 0.56\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.011390, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010227, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009707, train_acc: 0.84\n",
      "alexnet1d, trial.186:\n",
      "Epoch 16, avg test_loss: 0.009792, test_acc: 0.60\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009773, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009462, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008646, train_acc: 0.82\n",
      "alexnet1d, trial.186:\n",
      "Epoch 17, avg test_loss: 0.011127, test_acc: 0.61\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.010415, train_acc: 0.66\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007622, train_acc: 0.86\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007574, train_acc: 0.79\n",
      "alexnet1d, trial.186:\n",
      "Epoch 18, avg test_loss: 0.012320, test_acc: 0.53\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008498, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008640, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.010109, train_acc: 0.79\n",
      "alexnet1d, trial.186:\n",
      "Epoch 19, avg test_loss: 0.012804, test_acc: 0.59\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008018, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008035, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006390, train_acc: 0.86\n",
      "alexnet1d, trial.186:\n",
      "Epoch 20, avg test_loss: 0.012887, test_acc: 0.53\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.005859, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006493, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.005590, train_acc: 0.86\n",
      "alexnet1d, trial.186:\n",
      "Epoch 21, avg test_loss: 0.014091, test_acc: 0.57\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006652, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006197, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.005889, train_acc: 0.91\n",
      "alexnet1d, trial.186:\n",
      "Epoch 22, avg test_loss: 0.015991, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.004137, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.010000, train_acc: 0.82\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006129, train_acc: 0.84\n",
      "alexnet1d, trial.186:\n",
      "Epoch 23, avg test_loss: 0.014084, test_acc: 0.53\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006045, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.007914, train_acc: 0.79\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005054, train_acc: 0.91\n",
      "alexnet1d, trial.186:\n",
      "Epoch 24, avg test_loss: 0.014930, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.006651, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.002662, train_acc: 0.96\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004810, train_acc: 0.88\n",
      "alexnet1d, trial.186:\n",
      "Epoch 25, avg test_loss: 0.018378, test_acc: 0.64\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.002921, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004645, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004595, train_acc: 0.89\n",
      "alexnet1d, trial.186:\n",
      "Epoch 26, avg test_loss: 0.019007, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.007834, train_acc: 0.80\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.001856, train_acc: 0.98\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.004049, train_acc: 0.89\n",
      "alexnet1d, trial.186:\n",
      "Epoch 27, avg test_loss: 0.021500, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号36个\n",
      "错误信号34个\n",
      "信号正确并预测正确的概率为0.371\n",
      "信号错误并预测正确的概率为0.243\n",
      "总正确率为0.61\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012314, train_acc: 0.68\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.011910, train_acc: 0.64\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012335, train_acc: 0.57\n",
      "alexnet1d, trial.187:\n",
      "Epoch 0, avg test_loss: 0.010208, test_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011716, train_acc: 0.66\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012304, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011809, train_acc: 0.61\n",
      "alexnet1d, trial.187:\n",
      "Epoch 1, avg test_loss: 0.010368, test_acc: 0.50\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011884, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012243, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012775, train_acc: 0.59\n",
      "alexnet1d, trial.187:\n",
      "Epoch 2, avg test_loss: 0.010652, test_acc: 0.50\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.010990, train_acc: 0.70\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011448, train_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011871, train_acc: 0.62\n",
      "alexnet1d, trial.187:\n",
      "Epoch 3, avg test_loss: 0.010441, test_acc: 0.50\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011632, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012159, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011501, train_acc: 0.62\n",
      "alexnet1d, trial.187:\n",
      "Epoch 4, avg test_loss: 0.010571, test_acc: 0.51\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.010795, train_acc: 0.71\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012278, train_acc: 0.52\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012484, train_acc: 0.50\n",
      "alexnet1d, trial.187:\n",
      "Epoch 5, avg test_loss: 0.010829, test_acc: 0.53\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011778, train_acc: 0.50\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011731, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011137, train_acc: 0.54\n",
      "alexnet1d, trial.187:\n",
      "Epoch 6, avg test_loss: 0.011342, test_acc: 0.56\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011214, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012130, train_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012575, train_acc: 0.59\n",
      "alexnet1d, trial.187:\n",
      "Epoch 7, avg test_loss: 0.011781, test_acc: 0.56\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012359, train_acc: 0.48\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011173, train_acc: 0.68\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011369, train_acc: 0.61\n",
      "alexnet1d, trial.187:\n",
      "Epoch 8, avg test_loss: 0.010794, test_acc: 0.56\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010528, train_acc: 0.68\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010943, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011355, train_acc: 0.75\n",
      "alexnet1d, trial.187:\n",
      "Epoch 9, avg test_loss: 0.011857, test_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011719, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010608, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.009982, train_acc: 0.75\n",
      "alexnet1d, trial.187:\n",
      "Epoch 10, avg test_loss: 0.011760, test_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011817, train_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.009816, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.009112, train_acc: 0.77\n",
      "alexnet1d, trial.187:\n",
      "Epoch 11, avg test_loss: 0.013121, test_acc: 0.54\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010078, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.009979, train_acc: 0.77\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.012132, train_acc: 0.66\n",
      "alexnet1d, trial.187:\n",
      "Epoch 12, avg test_loss: 0.013920, test_acc: 0.54\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009032, train_acc: 0.77\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009816, train_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009755, train_acc: 0.70\n",
      "alexnet1d, trial.187:\n",
      "Epoch 13, avg test_loss: 0.014279, test_acc: 0.57\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.008843, train_acc: 0.84\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009032, train_acc: 0.79\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.008696, train_acc: 0.73\n",
      "alexnet1d, trial.187:\n",
      "Epoch 14, avg test_loss: 0.012247, test_acc: 0.53\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008058, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009654, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009853, train_acc: 0.68\n",
      "alexnet1d, trial.187:\n",
      "Epoch 15, avg test_loss: 0.013559, test_acc: 0.49\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008571, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.006984, train_acc: 0.82\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008271, train_acc: 0.80\n",
      "alexnet1d, trial.187:\n",
      "Epoch 16, avg test_loss: 0.017430, test_acc: 0.49\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.005176, train_acc: 0.95\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009488, train_acc: 0.66\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.005020, train_acc: 0.84\n",
      "alexnet1d, trial.187:\n",
      "Epoch 17, avg test_loss: 0.019755, test_acc: 0.50\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.005726, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.006257, train_acc: 0.86\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009615, train_acc: 0.75\n",
      "alexnet1d, trial.187:\n",
      "Epoch 18, avg test_loss: 0.018884, test_acc: 0.46\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006413, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.006953, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.005572, train_acc: 0.82\n",
      "alexnet1d, trial.187:\n",
      "Epoch 19, avg test_loss: 0.020672, test_acc: 0.47\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006695, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.005492, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006482, train_acc: 0.89\n",
      "alexnet1d, trial.187:\n",
      "Epoch 20, avg test_loss: 0.025206, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.004096, train_acc: 0.91\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.004744, train_acc: 0.91\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.004763, train_acc: 0.93\n",
      "alexnet1d, trial.187:\n",
      "Epoch 21, avg test_loss: 0.022439, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.003845, train_acc: 0.93\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.003032, train_acc: 0.93\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.003032, train_acc: 0.95\n",
      "alexnet1d, trial.187:\n",
      "Epoch 22, avg test_loss: 0.035328, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.003919, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.003407, train_acc: 0.95\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.003171, train_acc: 0.96\n",
      "alexnet1d, trial.187:\n",
      "Epoch 23, avg test_loss: 0.035093, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号35个\n",
      "错误信号35个\n",
      "信号正确并预测正确的概率为0.357\n",
      "信号错误并预测正确的概率为0.186\n",
      "总正确率为0.54\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012336, train_acc: 0.57\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012178, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012058, train_acc: 0.64\n",
      "alexnet1d, trial.188:\n",
      "Epoch 0, avg test_loss: 0.010520, test_acc: 0.56\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.013662, train_acc: 0.43\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.011947, train_acc: 0.70\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012149, train_acc: 0.64\n",
      "alexnet1d, trial.188:\n",
      "Epoch 1, avg test_loss: 0.009855, test_acc: 0.56\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012213, train_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011856, train_acc: 0.66\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012119, train_acc: 0.57\n",
      "alexnet1d, trial.188:\n",
      "Epoch 2, avg test_loss: 0.010588, test_acc: 0.56\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011016, train_acc: 0.70\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011378, train_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.013470, train_acc: 0.43\n",
      "alexnet1d, trial.188:\n",
      "Epoch 3, avg test_loss: 0.010063, test_acc: 0.56\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011631, train_acc: 0.64\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012159, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012004, train_acc: 0.62\n",
      "alexnet1d, trial.188:\n",
      "Epoch 4, avg test_loss: 0.009920, test_acc: 0.56\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011713, train_acc: 0.71\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011829, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012805, train_acc: 0.59\n",
      "alexnet1d, trial.188:\n",
      "Epoch 5, avg test_loss: 0.010509, test_acc: 0.56\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011962, train_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011702, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011791, train_acc: 0.61\n",
      "alexnet1d, trial.188:\n",
      "Epoch 6, avg test_loss: 0.011011, test_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011641, train_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.010377, train_acc: 0.77\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011441, train_acc: 0.64\n",
      "alexnet1d, trial.188:\n",
      "Epoch 7, avg test_loss: 0.011342, test_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011624, train_acc: 0.55\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.010458, train_acc: 0.68\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011756, train_acc: 0.64\n",
      "alexnet1d, trial.188:\n",
      "Epoch 8, avg test_loss: 0.011374, test_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.008412, train_acc: 0.84\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011299, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012562, train_acc: 0.59\n",
      "alexnet1d, trial.188:\n",
      "Epoch 9, avg test_loss: 0.011893, test_acc: 0.59\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010790, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010052, train_acc: 0.73\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.009153, train_acc: 0.82\n",
      "alexnet1d, trial.188:\n",
      "Epoch 10, avg test_loss: 0.011161, test_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010083, train_acc: 0.75\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.009461, train_acc: 0.75\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.009304, train_acc: 0.73\n",
      "alexnet1d, trial.188:\n",
      "Epoch 11, avg test_loss: 0.014734, test_acc: 0.53\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.008901, train_acc: 0.80\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011211, train_acc: 0.64\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.008908, train_acc: 0.75\n",
      "alexnet1d, trial.188:\n",
      "Epoch 12, avg test_loss: 0.011600, test_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009476, train_acc: 0.71\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010911, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.008632, train_acc: 0.75\n",
      "alexnet1d, trial.188:\n",
      "Epoch 13, avg test_loss: 0.014306, test_acc: 0.54\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.008571, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.008543, train_acc: 0.80\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.006856, train_acc: 0.80\n",
      "alexnet1d, trial.188:\n",
      "Epoch 14, avg test_loss: 0.016442, test_acc: 0.50\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.006969, train_acc: 0.82\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.006866, train_acc: 0.82\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008849, train_acc: 0.73\n",
      "alexnet1d, trial.188:\n",
      "Epoch 15, avg test_loss: 0.021056, test_acc: 0.53\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.007864, train_acc: 0.82\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.005716, train_acc: 0.88\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.007116, train_acc: 0.82\n",
      "alexnet1d, trial.188:\n",
      "Epoch 16, avg test_loss: 0.018082, test_acc: 0.50\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.006732, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.006480, train_acc: 0.91\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.006955, train_acc: 0.84\n",
      "alexnet1d, trial.188:\n",
      "Epoch 17, avg test_loss: 0.025109, test_acc: 0.51\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.006212, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.006349, train_acc: 0.89\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.007514, train_acc: 0.79\n",
      "alexnet1d, trial.188:\n",
      "Epoch 18, avg test_loss: 0.021707, test_acc: 0.54\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.012369, train_acc: 0.64\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007202, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008324, train_acc: 0.75\n",
      "alexnet1d, trial.188:\n",
      "Epoch 19, avg test_loss: 0.019142, test_acc: 0.53\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006254, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.006839, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.005482, train_acc: 0.89\n",
      "alexnet1d, trial.188:\n",
      "Epoch 20, avg test_loss: 0.020138, test_acc: 0.47\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.004618, train_acc: 0.93\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.005648, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.004573, train_acc: 0.89\n",
      "alexnet1d, trial.188:\n",
      "Epoch 21, avg test_loss: 0.025221, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005550, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.004191, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.003153, train_acc: 0.93\n",
      "alexnet1d, trial.188:\n",
      "Epoch 22, avg test_loss: 0.031156, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.002406, train_acc: 0.96\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.004270, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.001417, train_acc: 1.00\n",
      "alexnet1d, trial.188:\n",
      "Epoch 23, avg test_loss: 0.037712, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号39个\n",
      "错误信号31个\n",
      "信号正确并预测正确的概率为0.314\n",
      "信号错误并预测正确的概率为0.214\n",
      "总正确率为0.53\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012383, train_acc: 0.43\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012018, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.011771, train_acc: 0.61\n",
      "alexnet1d, trial.189:\n",
      "Epoch 0, avg test_loss: 0.009802, test_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011769, train_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.011935, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012473, train_acc: 0.57\n",
      "alexnet1d, trial.189:\n",
      "Epoch 1, avg test_loss: 0.009806, test_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011947, train_acc: 0.59\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012322, train_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012438, train_acc: 0.52\n",
      "alexnet1d, trial.189:\n",
      "Epoch 2, avg test_loss: 0.009808, test_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012201, train_acc: 0.55\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012217, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012603, train_acc: 0.52\n",
      "alexnet1d, trial.189:\n",
      "Epoch 3, avg test_loss: 0.009798, test_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012864, train_acc: 0.45\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012290, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012031, train_acc: 0.55\n",
      "alexnet1d, trial.189:\n",
      "Epoch 4, avg test_loss: 0.009868, test_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012375, train_acc: 0.54\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011326, train_acc: 0.70\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012210, train_acc: 0.54\n",
      "alexnet1d, trial.189:\n",
      "Epoch 5, avg test_loss: 0.009848, test_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012650, train_acc: 0.52\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011241, train_acc: 0.70\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011837, train_acc: 0.57\n",
      "alexnet1d, trial.189:\n",
      "Epoch 6, avg test_loss: 0.009884, test_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011778, train_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.010489, train_acc: 0.70\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.013588, train_acc: 0.54\n",
      "alexnet1d, trial.189:\n",
      "Epoch 7, avg test_loss: 0.009874, test_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011142, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011476, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011570, train_acc: 0.61\n",
      "alexnet1d, trial.189:\n",
      "Epoch 8, avg test_loss: 0.009794, test_acc: 0.56\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011680, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011704, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012319, train_acc: 0.59\n",
      "alexnet1d, trial.189:\n",
      "Epoch 9, avg test_loss: 0.010374, test_acc: 0.53\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011322, train_acc: 0.75\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012763, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012935, train_acc: 0.55\n",
      "alexnet1d, trial.189:\n",
      "Epoch 10, avg test_loss: 0.010237, test_acc: 0.53\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010055, train_acc: 0.77\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010626, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.012362, train_acc: 0.55\n",
      "alexnet1d, trial.189:\n",
      "Epoch 11, avg test_loss: 0.010469, test_acc: 0.56\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011599, train_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.012080, train_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010262, train_acc: 0.75\n",
      "alexnet1d, trial.189:\n",
      "Epoch 12, avg test_loss: 0.010098, test_acc: 0.57\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010764, train_acc: 0.77\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011682, train_acc: 0.57\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009203, train_acc: 0.77\n",
      "alexnet1d, trial.189:\n",
      "Epoch 13, avg test_loss: 0.011211, test_acc: 0.56\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.008273, train_acc: 0.82\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.008479, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011095, train_acc: 0.71\n",
      "alexnet1d, trial.189:\n",
      "Epoch 14, avg test_loss: 0.012243, test_acc: 0.57\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010197, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.011178, train_acc: 0.71\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010477, train_acc: 0.70\n",
      "alexnet1d, trial.189:\n",
      "Epoch 15, avg test_loss: 0.010527, test_acc: 0.56\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010808, train_acc: 0.68\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009645, train_acc: 0.77\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010669, train_acc: 0.66\n",
      "alexnet1d, trial.189:\n",
      "Epoch 16, avg test_loss: 0.011750, test_acc: 0.54\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008496, train_acc: 0.73\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009546, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009308, train_acc: 0.73\n",
      "alexnet1d, trial.189:\n",
      "Epoch 17, avg test_loss: 0.012465, test_acc: 0.54\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008358, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007811, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.010183, train_acc: 0.77\n",
      "alexnet1d, trial.189:\n",
      "Epoch 18, avg test_loss: 0.012278, test_acc: 0.61\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008658, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.006833, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007532, train_acc: 0.89\n",
      "alexnet1d, trial.189:\n",
      "Epoch 19, avg test_loss: 0.013205, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007650, train_acc: 0.88\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.006747, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007921, train_acc: 0.73\n",
      "alexnet1d, trial.189:\n",
      "Epoch 20, avg test_loss: 0.015391, test_acc: 0.54\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.006638, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.005875, train_acc: 0.91\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.010522, train_acc: 0.66\n",
      "alexnet1d, trial.189:\n",
      "Epoch 21, avg test_loss: 0.016155, test_acc: 0.56\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006944, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007054, train_acc: 0.82\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.005178, train_acc: 0.89\n",
      "alexnet1d, trial.189:\n",
      "Epoch 22, avg test_loss: 0.014581, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.005355, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007451, train_acc: 0.79\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.004079, train_acc: 0.93\n",
      "alexnet1d, trial.189:\n",
      "Epoch 23, avg test_loss: 0.016890, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.007801, train_acc: 0.80\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005217, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005119, train_acc: 0.88\n",
      "alexnet1d, trial.189:\n",
      "Epoch 24, avg test_loss: 0.021039, test_acc: 0.51\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.006680, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005484, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004135, train_acc: 0.93\n",
      "alexnet1d, trial.189:\n",
      "Epoch 25, avg test_loss: 0.020525, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号40个\n",
      "错误信号30个\n",
      "信号正确并预测正确的概率为0.314\n",
      "信号错误并预测正确的概率为0.214\n",
      "总正确率为0.53\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012331, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012485, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012168, train_acc: 0.59\n",
      "alexnet1d, trial.190:\n",
      "Epoch 0, avg test_loss: 0.009730, test_acc: 0.60\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012402, train_acc: 0.50\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.011758, train_acc: 0.71\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.013191, train_acc: 0.48\n",
      "alexnet1d, trial.190:\n",
      "Epoch 1, avg test_loss: 0.010296, test_acc: 0.60\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.013033, train_acc: 0.45\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012051, train_acc: 0.66\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012373, train_acc: 0.52\n",
      "alexnet1d, trial.190:\n",
      "Epoch 2, avg test_loss: 0.009813, test_acc: 0.60\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011767, train_acc: 0.71\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012155, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012351, train_acc: 0.55\n",
      "alexnet1d, trial.190:\n",
      "Epoch 3, avg test_loss: 0.010360, test_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011769, train_acc: 0.55\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012346, train_acc: 0.61\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011713, train_acc: 0.61\n",
      "alexnet1d, trial.190:\n",
      "Epoch 4, avg test_loss: 0.011174, test_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011274, train_acc: 0.68\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011630, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011166, train_acc: 0.66\n",
      "alexnet1d, trial.190:\n",
      "Epoch 5, avg test_loss: 0.010270, test_acc: 0.60\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011762, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011480, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011000, train_acc: 0.68\n",
      "alexnet1d, trial.190:\n",
      "Epoch 6, avg test_loss: 0.010472, test_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.009626, train_acc: 0.79\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012112, train_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011896, train_acc: 0.66\n",
      "alexnet1d, trial.190:\n",
      "Epoch 7, avg test_loss: 0.009989, test_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010236, train_acc: 0.77\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.010670, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012024, train_acc: 0.59\n",
      "alexnet1d, trial.190:\n",
      "Epoch 8, avg test_loss: 0.011082, test_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011807, train_acc: 0.55\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.009457, train_acc: 0.75\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010586, train_acc: 0.64\n",
      "alexnet1d, trial.190:\n",
      "Epoch 9, avg test_loss: 0.011528, test_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.009584, train_acc: 0.75\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010393, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011539, train_acc: 0.62\n",
      "alexnet1d, trial.190:\n",
      "Epoch 10, avg test_loss: 0.011560, test_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.009062, train_acc: 0.79\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010449, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010107, train_acc: 0.73\n",
      "alexnet1d, trial.190:\n",
      "Epoch 11, avg test_loss: 0.012566, test_acc: 0.63\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009544, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.009551, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.009214, train_acc: 0.77\n",
      "alexnet1d, trial.190:\n",
      "Epoch 12, avg test_loss: 0.014983, test_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010855, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.008026, train_acc: 0.77\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009746, train_acc: 0.73\n",
      "alexnet1d, trial.190:\n",
      "Epoch 13, avg test_loss: 0.013350, test_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.007658, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.007528, train_acc: 0.82\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011956, train_acc: 0.66\n",
      "alexnet1d, trial.190:\n",
      "Epoch 14, avg test_loss: 0.019001, test_acc: 0.63\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.006792, train_acc: 0.80\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008277, train_acc: 0.82\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008682, train_acc: 0.79\n",
      "alexnet1d, trial.190:\n",
      "Epoch 15, avg test_loss: 0.013980, test_acc: 0.61\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.007575, train_acc: 0.84\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.006074, train_acc: 0.91\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008664, train_acc: 0.77\n",
      "alexnet1d, trial.190:\n",
      "Epoch 16, avg test_loss: 0.021827, test_acc: 0.63\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.006405, train_acc: 0.86\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007505, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.006288, train_acc: 0.88\n",
      "alexnet1d, trial.190:\n",
      "Epoch 17, avg test_loss: 0.019642, test_acc: 0.60\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007011, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.006134, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.010442, train_acc: 0.73\n",
      "alexnet1d, trial.190:\n",
      "Epoch 18, avg test_loss: 0.026605, test_acc: 0.53\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.005159, train_acc: 0.89\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008863, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009089, train_acc: 0.77\n",
      "alexnet1d, trial.190:\n",
      "Epoch 19, avg test_loss: 0.026212, test_acc: 0.63\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006213, train_acc: 0.89\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007226, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.004889, train_acc: 0.86\n",
      "alexnet1d, trial.190:\n",
      "Epoch 20, avg test_loss: 0.022705, test_acc: 0.59\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.004645, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.003630, train_acc: 0.96\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.004126, train_acc: 0.91\n",
      "alexnet1d, trial.190:\n",
      "Epoch 21, avg test_loss: 0.022327, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005155, train_acc: 0.89\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005283, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.004166, train_acc: 0.89\n",
      "alexnet1d, trial.190:\n",
      "Epoch 22, avg test_loss: 0.026320, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.004214, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.003232, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.003160, train_acc: 0.95\n",
      "alexnet1d, trial.190:\n",
      "Epoch 23, avg test_loss: 0.030371, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.002996, train_acc: 0.95\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.001919, train_acc: 0.96\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.001715, train_acc: 0.98\n",
      "alexnet1d, trial.190:\n",
      "Epoch 24, avg test_loss: 0.034516, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号42个\n",
      "错误信号28个\n",
      "信号正确并预测正确的概率为0.414\n",
      "信号错误并预测正确的概率为0.157\n",
      "总正确率为0.57\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012357, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012398, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012300, train_acc: 0.55\n",
      "alexnet1d, trial.191:\n",
      "Epoch 0, avg test_loss: 0.010094, test_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.013068, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012573, train_acc: 0.46\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012042, train_acc: 0.64\n",
      "alexnet1d, trial.191:\n",
      "Epoch 1, avg test_loss: 0.009754, test_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012334, train_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012787, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011857, train_acc: 0.61\n",
      "alexnet1d, trial.191:\n",
      "Epoch 2, avg test_loss: 0.009748, test_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012475, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012124, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011966, train_acc: 0.62\n",
      "alexnet1d, trial.191:\n",
      "Epoch 3, avg test_loss: 0.009765, test_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012299, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012242, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011508, train_acc: 0.70\n",
      "alexnet1d, trial.191:\n",
      "Epoch 4, avg test_loss: 0.009902, test_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011597, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012820, train_acc: 0.48\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011329, train_acc: 0.68\n",
      "alexnet1d, trial.191:\n",
      "Epoch 5, avg test_loss: 0.009811, test_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012571, train_acc: 0.52\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011969, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011326, train_acc: 0.59\n",
      "alexnet1d, trial.191:\n",
      "Epoch 6, avg test_loss: 0.009907, test_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011077, train_acc: 0.68\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012325, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012142, train_acc: 0.52\n",
      "alexnet1d, trial.191:\n",
      "Epoch 7, avg test_loss: 0.009797, test_acc: 0.56\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.012697, train_acc: 0.46\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012122, train_acc: 0.55\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011650, train_acc: 0.64\n",
      "alexnet1d, trial.191:\n",
      "Epoch 8, avg test_loss: 0.009758, test_acc: 0.53\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012126, train_acc: 0.55\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011358, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010914, train_acc: 0.62\n",
      "alexnet1d, trial.191:\n",
      "Epoch 9, avg test_loss: 0.009980, test_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010972, train_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010615, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011713, train_acc: 0.59\n",
      "alexnet1d, trial.191:\n",
      "Epoch 10, avg test_loss: 0.009713, test_acc: 0.54\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010716, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.013346, train_acc: 0.52\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011498, train_acc: 0.62\n",
      "alexnet1d, trial.191:\n",
      "Epoch 11, avg test_loss: 0.009696, test_acc: 0.60\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010512, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010491, train_acc: 0.77\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011969, train_acc: 0.62\n",
      "alexnet1d, trial.191:\n",
      "Epoch 12, avg test_loss: 0.010060, test_acc: 0.60\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011013, train_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010055, train_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011542, train_acc: 0.70\n",
      "alexnet1d, trial.191:\n",
      "Epoch 13, avg test_loss: 0.010450, test_acc: 0.57\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009131, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.012200, train_acc: 0.64\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011364, train_acc: 0.73\n",
      "alexnet1d, trial.191:\n",
      "Epoch 14, avg test_loss: 0.011668, test_acc: 0.56\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009595, train_acc: 0.75\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.012641, train_acc: 0.55\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009743, train_acc: 0.71\n",
      "alexnet1d, trial.191:\n",
      "Epoch 15, avg test_loss: 0.010401, test_acc: 0.57\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.009743, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.008329, train_acc: 0.89\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009307, train_acc: 0.75\n",
      "alexnet1d, trial.191:\n",
      "Epoch 16, avg test_loss: 0.010038, test_acc: 0.63\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009061, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009274, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009157, train_acc: 0.71\n",
      "alexnet1d, trial.191:\n",
      "Epoch 17, avg test_loss: 0.011837, test_acc: 0.63\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009634, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008116, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009319, train_acc: 0.79\n",
      "alexnet1d, trial.191:\n",
      "Epoch 18, avg test_loss: 0.011463, test_acc: 0.61\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009744, train_acc: 0.73\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.008460, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.009418, train_acc: 0.71\n",
      "alexnet1d, trial.191:\n",
      "Epoch 19, avg test_loss: 0.010459, test_acc: 0.61\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.008159, train_acc: 0.84\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.009394, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.010730, train_acc: 0.68\n",
      "alexnet1d, trial.191:\n",
      "Epoch 20, avg test_loss: 0.010954, test_acc: 0.61\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007984, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.008559, train_acc: 0.73\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006837, train_acc: 0.79\n",
      "alexnet1d, trial.191:\n",
      "Epoch 21, avg test_loss: 0.011953, test_acc: 0.63\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.006729, train_acc: 0.86\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007015, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.005910, train_acc: 0.82\n",
      "alexnet1d, trial.191:\n",
      "Epoch 22, avg test_loss: 0.013357, test_acc: 0.61\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006570, train_acc: 0.80\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.007943, train_acc: 0.86\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006404, train_acc: 0.89\n",
      "alexnet1d, trial.191:\n",
      "Epoch 23, avg test_loss: 0.013506, test_acc: 0.66\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005959, train_acc: 0.86\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.005989, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.007220, train_acc: 0.80\n",
      "alexnet1d, trial.191:\n",
      "Epoch 24, avg test_loss: 0.014323, test_acc: 0.61\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.006189, train_acc: 0.82\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006127, train_acc: 0.82\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.005900, train_acc: 0.86\n",
      "alexnet1d, trial.191:\n",
      "Epoch 25, avg test_loss: 0.013935, test_acc: 0.64\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005078, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.007060, train_acc: 0.77\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.005629, train_acc: 0.89\n",
      "alexnet1d, trial.191:\n",
      "Epoch 26, avg test_loss: 0.013463, test_acc: 0.66\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004223, train_acc: 0.88\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.005823, train_acc: 0.82\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.006987, train_acc: 0.84\n",
      "alexnet1d, trial.191:\n",
      "Epoch 27, avg test_loss: 0.016619, test_acc: 0.61\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004552, train_acc: 0.89\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004486, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.005554, train_acc: 0.86\n",
      "alexnet1d, trial.191:\n",
      "Epoch 28, avg test_loss: 0.018136, test_acc: 0.61\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.004123, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.005007, train_acc: 0.88\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.003945, train_acc: 0.96\n",
      "alexnet1d, trial.191:\n",
      "Epoch 29, avg test_loss: 0.016679, test_acc: 0.66\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.004170, train_acc: 0.91\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.004597, train_acc: 0.89\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.003099, train_acc: 0.95\n",
      "alexnet1d, trial.191:\n",
      "Epoch 30, avg test_loss: 0.019039, test_acc: 0.66\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号40个\n",
      "错误信号30个\n",
      "信号正确并预测正确的概率为0.429\n",
      "信号错误并预测正确的概率为0.229\n",
      "总正确率为0.66\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012371, train_acc: 0.61\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.013318, train_acc: 0.52\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012183, train_acc: 0.59\n",
      "alexnet1d, trial.192:\n",
      "Epoch 0, avg test_loss: 0.009772, test_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012297, train_acc: 0.52\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012332, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012061, train_acc: 0.62\n",
      "alexnet1d, trial.192:\n",
      "Epoch 1, avg test_loss: 0.009691, test_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012833, train_acc: 0.52\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011534, train_acc: 0.68\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012375, train_acc: 0.55\n",
      "alexnet1d, trial.192:\n",
      "Epoch 2, avg test_loss: 0.009697, test_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011550, train_acc: 0.64\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011313, train_acc: 0.70\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012976, train_acc: 0.50\n",
      "alexnet1d, trial.192:\n",
      "Epoch 3, avg test_loss: 0.009714, test_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011469, train_acc: 0.66\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011159, train_acc: 0.73\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.013206, train_acc: 0.45\n",
      "alexnet1d, trial.192:\n",
      "Epoch 4, avg test_loss: 0.009648, test_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012054, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011651, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012585, train_acc: 0.50\n",
      "alexnet1d, trial.192:\n",
      "Epoch 5, avg test_loss: 0.009617, test_acc: 0.60\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011387, train_acc: 0.66\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012152, train_acc: 0.55\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011385, train_acc: 0.70\n",
      "alexnet1d, trial.192:\n",
      "Epoch 6, avg test_loss: 0.009576, test_acc: 0.63\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011225, train_acc: 0.68\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011810, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011914, train_acc: 0.59\n",
      "alexnet1d, trial.192:\n",
      "Epoch 7, avg test_loss: 0.009472, test_acc: 0.63\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010327, train_acc: 0.68\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011649, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012553, train_acc: 0.54\n",
      "alexnet1d, trial.192:\n",
      "Epoch 8, avg test_loss: 0.009186, test_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010802, train_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011749, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010161, train_acc: 0.66\n",
      "alexnet1d, trial.192:\n",
      "Epoch 9, avg test_loss: 0.009545, test_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011037, train_acc: 0.70\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011019, train_acc: 0.62\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011185, train_acc: 0.66\n",
      "alexnet1d, trial.192:\n",
      "Epoch 10, avg test_loss: 0.009497, test_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.009655, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011665, train_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010207, train_acc: 0.62\n",
      "alexnet1d, trial.192:\n",
      "Epoch 11, avg test_loss: 0.009794, test_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010551, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.009176, train_acc: 0.75\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.009661, train_acc: 0.68\n",
      "alexnet1d, trial.192:\n",
      "Epoch 12, avg test_loss: 0.009643, test_acc: 0.67\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.008798, train_acc: 0.75\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.008818, train_acc: 0.80\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010654, train_acc: 0.68\n",
      "alexnet1d, trial.192:\n",
      "Epoch 13, avg test_loss: 0.011490, test_acc: 0.59\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010655, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.009463, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.012522, train_acc: 0.61\n",
      "alexnet1d, trial.192:\n",
      "Epoch 14, avg test_loss: 0.010274, test_acc: 0.59\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009875, train_acc: 0.73\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008940, train_acc: 0.82\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010803, train_acc: 0.71\n",
      "alexnet1d, trial.192:\n",
      "Epoch 15, avg test_loss: 0.011189, test_acc: 0.51\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008195, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.011210, train_acc: 0.62\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009266, train_acc: 0.70\n",
      "alexnet1d, trial.192:\n",
      "Epoch 16, avg test_loss: 0.010169, test_acc: 0.64\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009589, train_acc: 0.68\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007678, train_acc: 0.84\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008936, train_acc: 0.73\n",
      "alexnet1d, trial.192:\n",
      "Epoch 17, avg test_loss: 0.010610, test_acc: 0.64\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007956, train_acc: 0.80\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.006518, train_acc: 0.84\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.012380, train_acc: 0.66\n",
      "alexnet1d, trial.192:\n",
      "Epoch 18, avg test_loss: 0.013325, test_acc: 0.59\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007301, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007066, train_acc: 0.82\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007685, train_acc: 0.77\n",
      "alexnet1d, trial.192:\n",
      "Epoch 19, avg test_loss: 0.012274, test_acc: 0.54\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006851, train_acc: 0.77\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007147, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.013427, train_acc: 0.66\n",
      "alexnet1d, trial.192:\n",
      "Epoch 20, avg test_loss: 0.011607, test_acc: 0.64\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007844, train_acc: 0.82\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.010426, train_acc: 0.71\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008442, train_acc: 0.80\n",
      "alexnet1d, trial.192:\n",
      "Epoch 21, avg test_loss: 0.011919, test_acc: 0.57\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.007452, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.006537, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.008020, train_acc: 0.73\n",
      "alexnet1d, trial.192:\n",
      "Epoch 22, avg test_loss: 0.013010, test_acc: 0.51\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006345, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.009566, train_acc: 0.77\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006178, train_acc: 0.82\n",
      "alexnet1d, trial.192:\n",
      "Epoch 23, avg test_loss: 0.011535, test_acc: 0.63\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006335, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006733, train_acc: 0.89\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005781, train_acc: 0.88\n",
      "alexnet1d, trial.192:\n",
      "Epoch 24, avg test_loss: 0.013436, test_acc: 0.59\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.004823, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005145, train_acc: 0.89\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.008574, train_acc: 0.80\n",
      "alexnet1d, trial.192:\n",
      "Epoch 25, avg test_loss: 0.016762, test_acc: 0.59\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.006246, train_acc: 0.88\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005413, train_acc: 0.86\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.006286, train_acc: 0.84\n",
      "alexnet1d, trial.192:\n",
      "Epoch 26, avg test_loss: 0.018440, test_acc: 0.59\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004016, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.005447, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003953, train_acc: 0.91\n",
      "alexnet1d, trial.192:\n",
      "Epoch 27, avg test_loss: 0.018788, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.002449, train_acc: 0.98\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.004125, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.003130, train_acc: 0.98\n",
      "alexnet1d, trial.192:\n",
      "Epoch 28, avg test_loss: 0.022572, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.002219, train_acc: 0.98\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.002747, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.007142, train_acc: 0.86\n",
      "alexnet1d, trial.192:\n",
      "Epoch 29, avg test_loss: 0.024381, test_acc: 0.59\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.003596, train_acc: 0.93\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.006336, train_acc: 0.86\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.003260, train_acc: 0.91\n",
      "alexnet1d, trial.192:\n",
      "Epoch 30, avg test_loss: 0.021748, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.005073, train_acc: 0.89\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.002693, train_acc: 0.91\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.003695, train_acc: 0.91\n",
      "alexnet1d, trial.192:\n",
      "Epoch 31, avg test_loss: 0.022324, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号40个\n",
      "错误信号30个\n",
      "信号正确并预测正确的概率为0.386\n",
      "信号错误并预测正确的概率为0.229\n",
      "总正确率为0.61\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012323, train_acc: 0.62\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.011937, train_acc: 0.61\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012380, train_acc: 0.57\n",
      "alexnet1d, trial.193:\n",
      "Epoch 0, avg test_loss: 0.010290, test_acc: 0.53\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011847, train_acc: 0.59\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012260, train_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.011715, train_acc: 0.62\n",
      "alexnet1d, trial.193:\n",
      "Epoch 1, avg test_loss: 0.009923, test_acc: 0.53\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012323, train_acc: 0.48\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011668, train_acc: 0.75\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.014194, train_acc: 0.52\n",
      "alexnet1d, trial.193:\n",
      "Epoch 2, avg test_loss: 0.010231, test_acc: 0.53\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011978, train_acc: 0.61\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.011809, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012681, train_acc: 0.48\n",
      "alexnet1d, trial.193:\n",
      "Epoch 3, avg test_loss: 0.010096, test_acc: 0.53\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011924, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012432, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011777, train_acc: 0.61\n",
      "alexnet1d, trial.193:\n",
      "Epoch 4, avg test_loss: 0.010476, test_acc: 0.53\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012375, train_acc: 0.46\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011739, train_acc: 0.61\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011715, train_acc: 0.59\n",
      "alexnet1d, trial.193:\n",
      "Epoch 5, avg test_loss: 0.010455, test_acc: 0.53\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012014, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011801, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.010728, train_acc: 0.73\n",
      "alexnet1d, trial.193:\n",
      "Epoch 6, avg test_loss: 0.010543, test_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011295, train_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011646, train_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012317, train_acc: 0.62\n",
      "alexnet1d, trial.193:\n",
      "Epoch 7, avg test_loss: 0.011092, test_acc: 0.51\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011188, train_acc: 0.64\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011070, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011008, train_acc: 0.70\n",
      "alexnet1d, trial.193:\n",
      "Epoch 8, avg test_loss: 0.011797, test_acc: 0.51\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010476, train_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010957, train_acc: 0.62\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010676, train_acc: 0.68\n",
      "alexnet1d, trial.193:\n",
      "Epoch 9, avg test_loss: 0.012303, test_acc: 0.51\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010184, train_acc: 0.68\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.012007, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010115, train_acc: 0.66\n",
      "alexnet1d, trial.193:\n",
      "Epoch 10, avg test_loss: 0.011906, test_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.009137, train_acc: 0.75\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011467, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.009269, train_acc: 0.73\n",
      "alexnet1d, trial.193:\n",
      "Epoch 11, avg test_loss: 0.011649, test_acc: 0.56\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010088, train_acc: 0.66\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011149, train_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.008337, train_acc: 0.84\n",
      "alexnet1d, trial.193:\n",
      "Epoch 12, avg test_loss: 0.012792, test_acc: 0.49\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.008529, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.009122, train_acc: 0.77\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.008711, train_acc: 0.73\n",
      "alexnet1d, trial.193:\n",
      "Epoch 13, avg test_loss: 0.012564, test_acc: 0.56\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.008720, train_acc: 0.80\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.007651, train_acc: 0.79\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.012193, train_acc: 0.66\n",
      "alexnet1d, trial.193:\n",
      "Epoch 14, avg test_loss: 0.015705, test_acc: 0.57\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.007152, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.007343, train_acc: 0.80\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009613, train_acc: 0.75\n",
      "alexnet1d, trial.193:\n",
      "Epoch 15, avg test_loss: 0.014693, test_acc: 0.60\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008612, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.006853, train_acc: 0.82\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008511, train_acc: 0.73\n",
      "alexnet1d, trial.193:\n",
      "Epoch 16, avg test_loss: 0.012273, test_acc: 0.61\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008961, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007925, train_acc: 0.75\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009445, train_acc: 0.79\n",
      "alexnet1d, trial.193:\n",
      "Epoch 17, avg test_loss: 0.014777, test_acc: 0.64\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.006204, train_acc: 0.88\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008169, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009085, train_acc: 0.75\n",
      "alexnet1d, trial.193:\n",
      "Epoch 18, avg test_loss: 0.015484, test_acc: 0.59\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006065, train_acc: 0.89\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.005400, train_acc: 0.89\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.005011, train_acc: 0.91\n",
      "alexnet1d, trial.193:\n",
      "Epoch 19, avg test_loss: 0.020257, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007997, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.004772, train_acc: 0.91\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006014, train_acc: 0.86\n",
      "alexnet1d, trial.193:\n",
      "Epoch 20, avg test_loss: 0.016812, test_acc: 0.53\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.004865, train_acc: 0.91\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007664, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.005779, train_acc: 0.84\n",
      "alexnet1d, trial.193:\n",
      "Epoch 21, avg test_loss: 0.018745, test_acc: 0.54\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.004723, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.003594, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.005392, train_acc: 0.88\n",
      "alexnet1d, trial.193:\n",
      "Epoch 22, avg test_loss: 0.021083, test_acc: 0.57\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.003316, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.003247, train_acc: 0.93\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.001730, train_acc: 1.00\n",
      "alexnet1d, trial.193:\n",
      "Epoch 23, avg test_loss: 0.025810, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.004332, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.002400, train_acc: 0.95\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.004678, train_acc: 0.93\n",
      "alexnet1d, trial.193:\n",
      "Epoch 24, avg test_loss: 0.024140, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.002545, train_acc: 0.96\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.003038, train_acc: 0.93\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.004665, train_acc: 0.91\n",
      "alexnet1d, trial.193:\n",
      "Epoch 25, avg test_loss: 0.031491, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号37个\n",
      "错误信号33个\n",
      "信号正确并预测正确的概率为0.414\n",
      "信号错误并预测正确的概率为0.171\n",
      "总正确率为0.59\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012396, train_acc: 0.50\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012109, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.011752, train_acc: 0.61\n",
      "alexnet1d, trial.194:\n",
      "Epoch 0, avg test_loss: 0.009822, test_acc: 0.56\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011689, train_acc: 0.71\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012609, train_acc: 0.52\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012344, train_acc: 0.54\n",
      "alexnet1d, trial.194:\n",
      "Epoch 1, avg test_loss: 0.009806, test_acc: 0.56\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.011651, train_acc: 0.66\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011111, train_acc: 0.73\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.013332, train_acc: 0.46\n",
      "alexnet1d, trial.194:\n",
      "Epoch 2, avg test_loss: 0.009836, test_acc: 0.56\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011975, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012336, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012198, train_acc: 0.57\n",
      "alexnet1d, trial.194:\n",
      "Epoch 3, avg test_loss: 0.009820, test_acc: 0.56\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012164, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011866, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011527, train_acc: 0.64\n",
      "alexnet1d, trial.194:\n",
      "Epoch 4, avg test_loss: 0.010030, test_acc: 0.56\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011584, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012027, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012755, train_acc: 0.48\n",
      "alexnet1d, trial.194:\n",
      "Epoch 5, avg test_loss: 0.009720, test_acc: 0.54\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011578, train_acc: 0.62\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011637, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011744, train_acc: 0.59\n",
      "alexnet1d, trial.194:\n",
      "Epoch 6, avg test_loss: 0.009975, test_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011894, train_acc: 0.52\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011631, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012251, train_acc: 0.61\n",
      "alexnet1d, trial.194:\n",
      "Epoch 7, avg test_loss: 0.009841, test_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.010760, train_acc: 0.75\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012585, train_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012894, train_acc: 0.48\n",
      "alexnet1d, trial.194:\n",
      "Epoch 8, avg test_loss: 0.010034, test_acc: 0.51\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011209, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011878, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.010950, train_acc: 0.66\n",
      "alexnet1d, trial.194:\n",
      "Epoch 9, avg test_loss: 0.009599, test_acc: 0.61\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.012193, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011000, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012438, train_acc: 0.62\n",
      "alexnet1d, trial.194:\n",
      "Epoch 10, avg test_loss: 0.009853, test_acc: 0.63\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010924, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010094, train_acc: 0.75\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.009935, train_acc: 0.75\n",
      "alexnet1d, trial.194:\n",
      "Epoch 11, avg test_loss: 0.010245, test_acc: 0.60\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010071, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010772, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011832, train_acc: 0.66\n",
      "alexnet1d, trial.194:\n",
      "Epoch 12, avg test_loss: 0.011477, test_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010029, train_acc: 0.70\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.007285, train_acc: 0.86\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.009104, train_acc: 0.70\n",
      "alexnet1d, trial.194:\n",
      "Epoch 13, avg test_loss: 0.009876, test_acc: 0.57\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.008844, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.012003, train_acc: 0.64\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010487, train_acc: 0.66\n",
      "alexnet1d, trial.194:\n",
      "Epoch 14, avg test_loss: 0.010927, test_acc: 0.56\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010857, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010169, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.009002, train_acc: 0.79\n",
      "alexnet1d, trial.194:\n",
      "Epoch 15, avg test_loss: 0.010024, test_acc: 0.61\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.007559, train_acc: 0.88\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009291, train_acc: 0.73\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010389, train_acc: 0.71\n",
      "alexnet1d, trial.194:\n",
      "Epoch 16, avg test_loss: 0.013142, test_acc: 0.57\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.008068, train_acc: 0.82\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.007055, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.007425, train_acc: 0.82\n",
      "alexnet1d, trial.194:\n",
      "Epoch 17, avg test_loss: 0.011551, test_acc: 0.59\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.007214, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007826, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.012745, train_acc: 0.70\n",
      "alexnet1d, trial.194:\n",
      "Epoch 18, avg test_loss: 0.013838, test_acc: 0.57\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006560, train_acc: 0.86\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.005134, train_acc: 0.89\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.006158, train_acc: 0.89\n",
      "alexnet1d, trial.194:\n",
      "Epoch 19, avg test_loss: 0.013814, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006142, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.004538, train_acc: 0.93\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.005729, train_acc: 0.86\n",
      "alexnet1d, trial.194:\n",
      "Epoch 20, avg test_loss: 0.013610, test_acc: 0.63\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.004111, train_acc: 0.91\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.006494, train_acc: 0.80\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.005327, train_acc: 0.89\n",
      "alexnet1d, trial.194:\n",
      "Epoch 21, avg test_loss: 0.015998, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.003890, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.003887, train_acc: 0.96\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.005964, train_acc: 0.88\n",
      "alexnet1d, trial.194:\n",
      "Epoch 22, avg test_loss: 0.019984, test_acc: 0.54\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006008, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.002721, train_acc: 0.95\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.004717, train_acc: 0.89\n",
      "alexnet1d, trial.194:\n",
      "Epoch 23, avg test_loss: 0.020456, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.003885, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.004735, train_acc: 0.91\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.003051, train_acc: 0.96\n",
      "alexnet1d, trial.194:\n",
      "Epoch 24, avg test_loss: 0.020677, test_acc: 0.49\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号39个\n",
      "错误信号31个\n",
      "信号正确并预测正确的概率为0.357\n",
      "信号错误并预测正确的概率为0.129\n",
      "总正确率为0.49\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012320, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.014984, train_acc: 0.55\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012170, train_acc: 0.64\n",
      "alexnet1d, trial.195:\n",
      "Epoch 0, avg test_loss: 0.009897, test_acc: 0.51\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012297, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012166, train_acc: 0.64\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012223, train_acc: 0.61\n",
      "alexnet1d, trial.195:\n",
      "Epoch 1, avg test_loss: 0.010003, test_acc: 0.51\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012417, train_acc: 0.54\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011696, train_acc: 0.68\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012247, train_acc: 0.54\n",
      "alexnet1d, trial.195:\n",
      "Epoch 2, avg test_loss: 0.010250, test_acc: 0.51\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012207, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.013130, train_acc: 0.48\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.011452, train_acc: 0.68\n",
      "alexnet1d, trial.195:\n",
      "Epoch 3, avg test_loss: 0.010030, test_acc: 0.51\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011228, train_acc: 0.71\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012134, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012510, train_acc: 0.54\n",
      "alexnet1d, trial.195:\n",
      "Epoch 4, avg test_loss: 0.010195, test_acc: 0.51\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.013175, train_acc: 0.43\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012571, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011471, train_acc: 0.68\n",
      "alexnet1d, trial.195:\n",
      "Epoch 5, avg test_loss: 0.010012, test_acc: 0.51\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011674, train_acc: 0.70\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011657, train_acc: 0.64\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012878, train_acc: 0.52\n",
      "alexnet1d, trial.195:\n",
      "Epoch 6, avg test_loss: 0.010225, test_acc: 0.51\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011525, train_acc: 0.64\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011977, train_acc: 0.59\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012899, train_acc: 0.46\n",
      "alexnet1d, trial.195:\n",
      "Epoch 7, avg test_loss: 0.010126, test_acc: 0.51\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011731, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012005, train_acc: 0.55\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011579, train_acc: 0.66\n",
      "alexnet1d, trial.195:\n",
      "Epoch 8, avg test_loss: 0.010088, test_acc: 0.51\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011213, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011634, train_acc: 0.57\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011539, train_acc: 0.61\n",
      "alexnet1d, trial.195:\n",
      "Epoch 9, avg test_loss: 0.009975, test_acc: 0.51\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011931, train_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011505, train_acc: 0.64\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.012407, train_acc: 0.48\n",
      "alexnet1d, trial.195:\n",
      "Epoch 10, avg test_loss: 0.010207, test_acc: 0.53\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010205, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011543, train_acc: 0.55\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011555, train_acc: 0.55\n",
      "alexnet1d, trial.195:\n",
      "Epoch 11, avg test_loss: 0.009991, test_acc: 0.63\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010996, train_acc: 0.71\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010834, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.010507, train_acc: 0.68\n",
      "alexnet1d, trial.195:\n",
      "Epoch 12, avg test_loss: 0.010309, test_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009677, train_acc: 0.73\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.013562, train_acc: 0.50\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010674, train_acc: 0.68\n",
      "alexnet1d, trial.195:\n",
      "Epoch 13, avg test_loss: 0.010677, test_acc: 0.60\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.011456, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010713, train_acc: 0.68\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.009947, train_acc: 0.71\n",
      "alexnet1d, trial.195:\n",
      "Epoch 14, avg test_loss: 0.010409, test_acc: 0.53\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.010381, train_acc: 0.64\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009047, train_acc: 0.77\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.010701, train_acc: 0.70\n",
      "alexnet1d, trial.195:\n",
      "Epoch 15, avg test_loss: 0.011316, test_acc: 0.60\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.011194, train_acc: 0.64\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010703, train_acc: 0.57\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008800, train_acc: 0.75\n",
      "alexnet1d, trial.195:\n",
      "Epoch 16, avg test_loss: 0.011270, test_acc: 0.60\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.009212, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008765, train_acc: 0.80\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010406, train_acc: 0.70\n",
      "alexnet1d, trial.195:\n",
      "Epoch 17, avg test_loss: 0.011305, test_acc: 0.61\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.011273, train_acc: 0.62\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.011290, train_acc: 0.71\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009923, train_acc: 0.71\n",
      "alexnet1d, trial.195:\n",
      "Epoch 18, avg test_loss: 0.012454, test_acc: 0.60\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.009461, train_acc: 0.68\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009103, train_acc: 0.77\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.010552, train_acc: 0.59\n",
      "alexnet1d, trial.195:\n",
      "Epoch 19, avg test_loss: 0.010375, test_acc: 0.61\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.009184, train_acc: 0.71\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008444, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008510, train_acc: 0.73\n",
      "alexnet1d, trial.195:\n",
      "Epoch 20, avg test_loss: 0.011372, test_acc: 0.53\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.009327, train_acc: 0.73\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007531, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008259, train_acc: 0.77\n",
      "alexnet1d, trial.195:\n",
      "Epoch 21, avg test_loss: 0.013085, test_acc: 0.56\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.008788, train_acc: 0.73\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007730, train_acc: 0.77\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007980, train_acc: 0.77\n",
      "alexnet1d, trial.195:\n",
      "Epoch 22, avg test_loss: 0.013742, test_acc: 0.54\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.006807, train_acc: 0.82\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006721, train_acc: 0.82\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006604, train_acc: 0.80\n",
      "alexnet1d, trial.195:\n",
      "Epoch 23, avg test_loss: 0.015233, test_acc: 0.59\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006691, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.006798, train_acc: 0.84\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.007133, train_acc: 0.84\n",
      "alexnet1d, trial.195:\n",
      "Epoch 24, avg test_loss: 0.016318, test_acc: 0.53\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.007621, train_acc: 0.82\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006865, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006555, train_acc: 0.84\n",
      "alexnet1d, trial.195:\n",
      "Epoch 25, avg test_loss: 0.013967, test_acc: 0.53\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005621, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.005852, train_acc: 0.84\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.007793, train_acc: 0.79\n",
      "alexnet1d, trial.195:\n",
      "Epoch 26, avg test_loss: 0.017498, test_acc: 0.57\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.004944, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.005811, train_acc: 0.86\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003979, train_acc: 0.95\n",
      "alexnet1d, trial.195:\n",
      "Epoch 27, avg test_loss: 0.018269, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.004105, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.005233, train_acc: 0.91\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.005957, train_acc: 0.91\n",
      "alexnet1d, trial.195:\n",
      "Epoch 28, avg test_loss: 0.022741, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.005130, train_acc: 0.91\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.005440, train_acc: 0.88\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.005734, train_acc: 0.84\n",
      "alexnet1d, trial.195:\n",
      "Epoch 29, avg test_loss: 0.023518, test_acc: 0.60\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.002663, train_acc: 0.91\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.004586, train_acc: 0.89\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.005391, train_acc: 0.89\n",
      "alexnet1d, trial.195:\n",
      "Epoch 30, avg test_loss: 0.021793, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.004266, train_acc: 0.91\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.003122, train_acc: 0.91\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.003232, train_acc: 0.95\n",
      "alexnet1d, trial.195:\n",
      "Epoch 31, avg test_loss: 0.019257, test_acc: 0.50\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号36个\n",
      "错误信号34个\n",
      "信号正确并预测正确的概率为0.271\n",
      "信号错误并预测正确的概率为0.229\n",
      "总正确率为0.50\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012418, train_acc: 0.50\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.011920, train_acc: 0.61\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.014209, train_acc: 0.54\n",
      "alexnet1d, trial.196:\n",
      "Epoch 0, avg test_loss: 0.009757, test_acc: 0.60\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012291, train_acc: 0.52\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012356, train_acc: 0.52\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012338, train_acc: 0.50\n",
      "alexnet1d, trial.196:\n",
      "Epoch 1, avg test_loss: 0.009860, test_acc: 0.60\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012324, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012415, train_acc: 0.46\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.012072, train_acc: 0.68\n",
      "alexnet1d, trial.196:\n",
      "Epoch 2, avg test_loss: 0.009705, test_acc: 0.60\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012318, train_acc: 0.55\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012190, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012952, train_acc: 0.46\n",
      "alexnet1d, trial.196:\n",
      "Epoch 3, avg test_loss: 0.009591, test_acc: 0.60\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.011926, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012513, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012115, train_acc: 0.59\n",
      "alexnet1d, trial.196:\n",
      "Epoch 4, avg test_loss: 0.009625, test_acc: 0.60\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012284, train_acc: 0.55\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011940, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012110, train_acc: 0.57\n",
      "alexnet1d, trial.196:\n",
      "Epoch 5, avg test_loss: 0.009633, test_acc: 0.60\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012308, train_acc: 0.52\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011199, train_acc: 0.71\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012001, train_acc: 0.55\n",
      "alexnet1d, trial.196:\n",
      "Epoch 6, avg test_loss: 0.009762, test_acc: 0.60\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.010929, train_acc: 0.71\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011784, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012722, train_acc: 0.50\n",
      "alexnet1d, trial.196:\n",
      "Epoch 7, avg test_loss: 0.009613, test_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011978, train_acc: 0.59\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011775, train_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.012446, train_acc: 0.50\n",
      "alexnet1d, trial.196:\n",
      "Epoch 8, avg test_loss: 0.009723, test_acc: 0.56\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.011332, train_acc: 0.70\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011361, train_acc: 0.64\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011975, train_acc: 0.62\n",
      "alexnet1d, trial.196:\n",
      "Epoch 9, avg test_loss: 0.009739, test_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.012063, train_acc: 0.57\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010933, train_acc: 0.71\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.010517, train_acc: 0.70\n",
      "alexnet1d, trial.196:\n",
      "Epoch 10, avg test_loss: 0.009561, test_acc: 0.59\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010541, train_acc: 0.70\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010986, train_acc: 0.61\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.013848, train_acc: 0.54\n",
      "alexnet1d, trial.196:\n",
      "Epoch 11, avg test_loss: 0.010440, test_acc: 0.56\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.012082, train_acc: 0.62\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010909, train_acc: 0.73\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011554, train_acc: 0.66\n",
      "alexnet1d, trial.196:\n",
      "Epoch 12, avg test_loss: 0.009542, test_acc: 0.60\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.010713, train_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.011037, train_acc: 0.68\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010407, train_acc: 0.71\n",
      "alexnet1d, trial.196:\n",
      "Epoch 13, avg test_loss: 0.009880, test_acc: 0.57\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009138, train_acc: 0.77\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.008958, train_acc: 0.79\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.011204, train_acc: 0.66\n",
      "alexnet1d, trial.196:\n",
      "Epoch 14, avg test_loss: 0.009760, test_acc: 0.54\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.012086, train_acc: 0.68\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.011220, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011103, train_acc: 0.70\n",
      "alexnet1d, trial.196:\n",
      "Epoch 15, avg test_loss: 0.009946, test_acc: 0.57\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010949, train_acc: 0.62\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.010674, train_acc: 0.71\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010171, train_acc: 0.70\n",
      "alexnet1d, trial.196:\n",
      "Epoch 16, avg test_loss: 0.009549, test_acc: 0.60\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.010795, train_acc: 0.68\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.010678, train_acc: 0.66\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010673, train_acc: 0.73\n",
      "alexnet1d, trial.196:\n",
      "Epoch 17, avg test_loss: 0.010130, test_acc: 0.56\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.008712, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009496, train_acc: 0.70\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009701, train_acc: 0.73\n",
      "alexnet1d, trial.196:\n",
      "Epoch 18, avg test_loss: 0.010707, test_acc: 0.56\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008126, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007786, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.007106, train_acc: 0.88\n",
      "alexnet1d, trial.196:\n",
      "Epoch 19, avg test_loss: 0.010943, test_acc: 0.56\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.007864, train_acc: 0.82\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008096, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.007414, train_acc: 0.86\n",
      "alexnet1d, trial.196:\n",
      "Epoch 20, avg test_loss: 0.011671, test_acc: 0.57\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.007081, train_acc: 0.84\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.010346, train_acc: 0.70\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007807, train_acc: 0.80\n",
      "alexnet1d, trial.196:\n",
      "Epoch 21, avg test_loss: 0.010996, test_acc: 0.63\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005752, train_acc: 0.91\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.007601, train_acc: 0.80\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.007517, train_acc: 0.84\n",
      "alexnet1d, trial.196:\n",
      "Epoch 22, avg test_loss: 0.013407, test_acc: 0.51\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.008098, train_acc: 0.77\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.008780, train_acc: 0.77\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.007975, train_acc: 0.82\n",
      "alexnet1d, trial.196:\n",
      "Epoch 23, avg test_loss: 0.012469, test_acc: 0.57\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.005627, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.010095, train_acc: 0.79\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.006170, train_acc: 0.86\n",
      "alexnet1d, trial.196:\n",
      "Epoch 24, avg test_loss: 0.013029, test_acc: 0.53\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.006933, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.006201, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.008054, train_acc: 0.71\n",
      "alexnet1d, trial.196:\n",
      "Epoch 25, avg test_loss: 0.013460, test_acc: 0.59\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005977, train_acc: 0.82\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.006305, train_acc: 0.86\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.004170, train_acc: 0.93\n",
      "alexnet1d, trial.196:\n",
      "Epoch 26, avg test_loss: 0.014779, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.005935, train_acc: 0.89\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.003059, train_acc: 0.93\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.003475, train_acc: 0.91\n",
      "alexnet1d, trial.196:\n",
      "Epoch 27, avg test_loss: 0.015134, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.006774, train_acc: 0.84\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.005540, train_acc: 0.93\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.004429, train_acc: 0.91\n",
      "alexnet1d, trial.196:\n",
      "Epoch 28, avg test_loss: 0.017937, test_acc: 0.53\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.002349, train_acc: 0.95\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.004849, train_acc: 0.86\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.004612, train_acc: 0.89\n",
      "alexnet1d, trial.196:\n",
      "Epoch 29, avg test_loss: 0.017988, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号42个\n",
      "错误信号28个\n",
      "信号正确并预测正确的概率为0.357\n",
      "信号错误并预测正确的概率为0.2\n",
      "总正确率为0.56\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012268, train_acc: 0.64\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012762, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012442, train_acc: 0.48\n",
      "alexnet1d, trial.197:\n",
      "Epoch 0, avg test_loss: 0.009889, test_acc: 0.56\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012262, train_acc: 0.55\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012325, train_acc: 0.57\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012329, train_acc: 0.59\n",
      "alexnet1d, trial.197:\n",
      "Epoch 1, avg test_loss: 0.009792, test_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012390, train_acc: 0.50\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.011983, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011715, train_acc: 0.62\n",
      "alexnet1d, trial.197:\n",
      "Epoch 2, avg test_loss: 0.009713, test_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012039, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012513, train_acc: 0.55\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012095, train_acc: 0.59\n",
      "alexnet1d, trial.197:\n",
      "Epoch 3, avg test_loss: 0.009744, test_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012022, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012590, train_acc: 0.50\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012154, train_acc: 0.57\n",
      "alexnet1d, trial.197:\n",
      "Epoch 4, avg test_loss: 0.009751, test_acc: 0.57\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012054, train_acc: 0.59\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.012461, train_acc: 0.52\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.011880, train_acc: 0.61\n",
      "alexnet1d, trial.197:\n",
      "Epoch 5, avg test_loss: 0.009740, test_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012391, train_acc: 0.52\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011955, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011971, train_acc: 0.59\n",
      "alexnet1d, trial.197:\n",
      "Epoch 6, avg test_loss: 0.009709, test_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.012329, train_acc: 0.54\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012210, train_acc: 0.57\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011526, train_acc: 0.68\n",
      "alexnet1d, trial.197:\n",
      "Epoch 7, avg test_loss: 0.009646, test_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011175, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012739, train_acc: 0.54\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011710, train_acc: 0.59\n",
      "alexnet1d, trial.197:\n",
      "Epoch 8, avg test_loss: 0.009639, test_acc: 0.60\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012344, train_acc: 0.54\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.011456, train_acc: 0.66\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.012921, train_acc: 0.46\n",
      "alexnet1d, trial.197:\n",
      "Epoch 9, avg test_loss: 0.009739, test_acc: 0.56\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.012019, train_acc: 0.54\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011136, train_acc: 0.70\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011111, train_acc: 0.70\n",
      "alexnet1d, trial.197:\n",
      "Epoch 10, avg test_loss: 0.009511, test_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010739, train_acc: 0.73\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.011414, train_acc: 0.68\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.011823, train_acc: 0.64\n",
      "alexnet1d, trial.197:\n",
      "Epoch 11, avg test_loss: 0.009642, test_acc: 0.59\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.010926, train_acc: 0.70\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.010979, train_acc: 0.61\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.012559, train_acc: 0.57\n",
      "alexnet1d, trial.197:\n",
      "Epoch 12, avg test_loss: 0.009799, test_acc: 0.59\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.008868, train_acc: 0.86\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010762, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.011200, train_acc: 0.68\n",
      "alexnet1d, trial.197:\n",
      "Epoch 13, avg test_loss: 0.010062, test_acc: 0.60\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.009654, train_acc: 0.75\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010438, train_acc: 0.70\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.008382, train_acc: 0.82\n",
      "alexnet1d, trial.197:\n",
      "Epoch 14, avg test_loss: 0.010112, test_acc: 0.60\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008102, train_acc: 0.80\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.008719, train_acc: 0.82\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.008013, train_acc: 0.79\n",
      "alexnet1d, trial.197:\n",
      "Epoch 15, avg test_loss: 0.010429, test_acc: 0.64\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.011723, train_acc: 0.70\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009201, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.008486, train_acc: 0.80\n",
      "alexnet1d, trial.197:\n",
      "Epoch 16, avg test_loss: 0.010997, test_acc: 0.53\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.007734, train_acc: 0.84\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008119, train_acc: 0.79\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.009359, train_acc: 0.77\n",
      "alexnet1d, trial.197:\n",
      "Epoch 17, avg test_loss: 0.010154, test_acc: 0.59\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009257, train_acc: 0.79\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.007367, train_acc: 0.82\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.009568, train_acc: 0.75\n",
      "alexnet1d, trial.197:\n",
      "Epoch 18, avg test_loss: 0.009782, test_acc: 0.66\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.006999, train_acc: 0.80\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.007290, train_acc: 0.84\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008408, train_acc: 0.82\n",
      "alexnet1d, trial.197:\n",
      "Epoch 19, avg test_loss: 0.010905, test_acc: 0.59\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.006242, train_acc: 0.89\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.007462, train_acc: 0.80\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.006082, train_acc: 0.88\n",
      "alexnet1d, trial.197:\n",
      "Epoch 20, avg test_loss: 0.012536, test_acc: 0.59\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.004923, train_acc: 0.88\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.005428, train_acc: 0.89\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.006404, train_acc: 0.82\n",
      "alexnet1d, trial.197:\n",
      "Epoch 21, avg test_loss: 0.013326, test_acc: 0.56\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.003835, train_acc: 0.93\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.004434, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.005800, train_acc: 0.89\n",
      "alexnet1d, trial.197:\n",
      "Epoch 22, avg test_loss: 0.017282, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.004728, train_acc: 0.89\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.004479, train_acc: 0.91\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.006029, train_acc: 0.88\n",
      "alexnet1d, trial.197:\n",
      "Epoch 23, avg test_loss: 0.017046, test_acc: 0.57\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.002621, train_acc: 0.96\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.002972, train_acc: 0.95\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.005659, train_acc: 0.84\n",
      "alexnet1d, trial.197:\n",
      "Epoch 24, avg test_loss: 0.014815, test_acc: 0.54\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.005597, train_acc: 0.86\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.002885, train_acc: 0.96\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.003599, train_acc: 0.95\n",
      "alexnet1d, trial.197:\n",
      "Epoch 25, avg test_loss: 0.021089, test_acc: 0.51\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.004292, train_acc: 0.95\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.002843, train_acc: 0.96\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.006440, train_acc: 0.82\n",
      "alexnet1d, trial.197:\n",
      "Epoch 26, avg test_loss: 0.013959, test_acc: 0.59\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.002914, train_acc: 0.95\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.002536, train_acc: 0.96\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.002844, train_acc: 0.95\n",
      "alexnet1d, trial.197:\n",
      "Epoch 27, avg test_loss: 0.012574, test_acc: 0.61\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.002523, train_acc: 0.98\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.002426, train_acc: 0.96\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.003087, train_acc: 0.95\n",
      "alexnet1d, trial.197:\n",
      "Epoch 28, avg test_loss: 0.016149, test_acc: 0.59\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号40个\n",
      "错误信号30个\n",
      "信号正确并预测正确的概率为0.343\n",
      "信号错误并预测正确的概率为0.243\n",
      "总正确率为0.59\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012584, train_acc: 0.32\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.015386, train_acc: 0.54\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.012314, train_acc: 0.55\n",
      "alexnet1d, trial.198:\n",
      "Epoch 0, avg test_loss: 0.009936, test_acc: 0.40\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.012362, train_acc: 0.46\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012427, train_acc: 0.41\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012362, train_acc: 0.54\n",
      "alexnet1d, trial.198:\n",
      "Epoch 1, avg test_loss: 0.009819, test_acc: 0.60\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012309, train_acc: 0.62\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012198, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.013007, train_acc: 0.45\n",
      "alexnet1d, trial.198:\n",
      "Epoch 2, avg test_loss: 0.009578, test_acc: 0.60\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.012003, train_acc: 0.66\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012415, train_acc: 0.54\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012557, train_acc: 0.50\n",
      "alexnet1d, trial.198:\n",
      "Epoch 3, avg test_loss: 0.009642, test_acc: 0.60\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012374, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.012307, train_acc: 0.54\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.011962, train_acc: 0.64\n",
      "alexnet1d, trial.198:\n",
      "Epoch 4, avg test_loss: 0.009697, test_acc: 0.60\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.012049, train_acc: 0.62\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011918, train_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.012272, train_acc: 0.52\n",
      "alexnet1d, trial.198:\n",
      "Epoch 5, avg test_loss: 0.009609, test_acc: 0.60\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.012075, train_acc: 0.59\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.012056, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.011953, train_acc: 0.54\n",
      "alexnet1d, trial.198:\n",
      "Epoch 6, avg test_loss: 0.009574, test_acc: 0.60\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011612, train_acc: 0.61\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.012204, train_acc: 0.52\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.011744, train_acc: 0.59\n",
      "alexnet1d, trial.198:\n",
      "Epoch 7, avg test_loss: 0.009662, test_acc: 0.61\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011919, train_acc: 0.57\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.012933, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011537, train_acc: 0.61\n",
      "alexnet1d, trial.198:\n",
      "Epoch 8, avg test_loss: 0.009565, test_acc: 0.60\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.012307, train_acc: 0.59\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.012044, train_acc: 0.61\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011758, train_acc: 0.64\n",
      "alexnet1d, trial.198:\n",
      "Epoch 9, avg test_loss: 0.009512, test_acc: 0.63\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.011288, train_acc: 0.71\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.011403, train_acc: 0.66\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011428, train_acc: 0.66\n",
      "alexnet1d, trial.198:\n",
      "Epoch 10, avg test_loss: 0.009661, test_acc: 0.60\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.011728, train_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.012744, train_acc: 0.57\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010789, train_acc: 0.71\n",
      "alexnet1d, trial.198:\n",
      "Epoch 11, avg test_loss: 0.009568, test_acc: 0.60\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.011985, train_acc: 0.57\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.011167, train_acc: 0.68\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.011069, train_acc: 0.64\n",
      "alexnet1d, trial.198:\n",
      "Epoch 12, avg test_loss: 0.009526, test_acc: 0.66\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.011754, train_acc: 0.57\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010772, train_acc: 0.64\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010579, train_acc: 0.73\n",
      "alexnet1d, trial.198:\n",
      "Epoch 13, avg test_loss: 0.009608, test_acc: 0.63\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010544, train_acc: 0.66\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.011071, train_acc: 0.59\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010627, train_acc: 0.71\n",
      "alexnet1d, trial.198:\n",
      "Epoch 14, avg test_loss: 0.010742, test_acc: 0.59\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.009955, train_acc: 0.70\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.009812, train_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011902, train_acc: 0.71\n",
      "alexnet1d, trial.198:\n",
      "Epoch 15, avg test_loss: 0.009684, test_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.010621, train_acc: 0.66\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009283, train_acc: 0.79\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.009863, train_acc: 0.68\n",
      "alexnet1d, trial.198:\n",
      "Epoch 16, avg test_loss: 0.010367, test_acc: 0.57\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.010527, train_acc: 0.71\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.009525, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.010323, train_acc: 0.61\n",
      "alexnet1d, trial.198:\n",
      "Epoch 17, avg test_loss: 0.010278, test_acc: 0.64\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.009461, train_acc: 0.73\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.008992, train_acc: 0.77\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.011694, train_acc: 0.68\n",
      "alexnet1d, trial.198:\n",
      "Epoch 18, avg test_loss: 0.013087, test_acc: 0.56\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.008804, train_acc: 0.79\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.011748, train_acc: 0.68\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008970, train_acc: 0.75\n",
      "alexnet1d, trial.198:\n",
      "Epoch 19, avg test_loss: 0.013073, test_acc: 0.56\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.009542, train_acc: 0.70\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008820, train_acc: 0.77\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.010188, train_acc: 0.68\n",
      "alexnet1d, trial.198:\n",
      "Epoch 20, avg test_loss: 0.010030, test_acc: 0.60\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.010050, train_acc: 0.75\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.009559, train_acc: 0.73\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.007843, train_acc: 0.84\n",
      "alexnet1d, trial.198:\n",
      "Epoch 21, avg test_loss: 0.013155, test_acc: 0.63\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.009269, train_acc: 0.77\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.008811, train_acc: 0.79\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.010306, train_acc: 0.70\n",
      "alexnet1d, trial.198:\n",
      "Epoch 22, avg test_loss: 0.011932, test_acc: 0.57\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.008970, train_acc: 0.71\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.006813, train_acc: 0.88\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.007690, train_acc: 0.86\n",
      "alexnet1d, trial.198:\n",
      "Epoch 23, avg test_loss: 0.015234, test_acc: 0.54\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006946, train_acc: 0.88\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.007298, train_acc: 0.82\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.007103, train_acc: 0.84\n",
      "alexnet1d, trial.198:\n",
      "Epoch 24, avg test_loss: 0.014690, test_acc: 0.61\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.007137, train_acc: 0.82\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.005354, train_acc: 0.88\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.006251, train_acc: 0.88\n",
      "alexnet1d, trial.198:\n",
      "Epoch 25, avg test_loss: 0.014799, test_acc: 0.60\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.005271, train_acc: 0.89\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.004335, train_acc: 0.93\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.007768, train_acc: 0.80\n",
      "alexnet1d, trial.198:\n",
      "Epoch 26, avg test_loss: 0.017206, test_acc: 0.61\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.006926, train_acc: 0.82\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.005816, train_acc: 0.86\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.005833, train_acc: 0.89\n",
      "alexnet1d, trial.198:\n",
      "Epoch 27, avg test_loss: 0.016240, test_acc: 0.63\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.005406, train_acc: 0.88\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.005962, train_acc: 0.88\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.005679, train_acc: 0.86\n",
      "alexnet1d, trial.198:\n",
      "Epoch 28, avg test_loss: 0.021632, test_acc: 0.59\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.007184, train_acc: 0.77\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.004254, train_acc: 0.93\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.005187, train_acc: 0.89\n",
      "alexnet1d, trial.198:\n",
      "Epoch 29, avg test_loss: 0.018453, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.003728, train_acc: 0.93\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.002583, train_acc: 0.98\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.006927, train_acc: 0.79\n",
      "alexnet1d, trial.198:\n",
      "Epoch 30, avg test_loss: 0.019167, test_acc: 0.57\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.006824, train_acc: 0.82\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.003887, train_acc: 0.91\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.004757, train_acc: 0.93\n",
      "alexnet1d, trial.198:\n",
      "Epoch 31, avg test_loss: 0.022186, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.004548, train_acc: 0.93\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.002584, train_acc: 0.93\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.005793, train_acc: 0.88\n",
      "alexnet1d, trial.198:\n",
      "Epoch 32, avg test_loss: 0.019505, test_acc: 0.59\n",
      "Train Epoch 33, lr: 0.000377, 0/280, avg loss: 0.003654, train_acc: 0.91\n",
      "Train Epoch 33, lr: 0.000377, 112/280, avg loss: 0.002774, train_acc: 0.95\n",
      "Train Epoch 33, lr: 0.000377, 224/280, avg loss: 0.003303, train_acc: 0.91\n",
      "alexnet1d, trial.198:\n",
      "Epoch 33, avg test_loss: 0.019780, test_acc: 0.57\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号42个\n",
      "错误信号28个\n",
      "信号正确并预测正确的概率为0.357\n",
      "信号错误并预测正确的概率为0.214\n",
      "总正确率为0.57\n",
      "\n",
      "Use binary label \n",
      "\n",
      "Enable one-hot encoding\n",
      "\n",
      "no test loader\n",
      "Train Epoch 0, lr: 0.001000, 0/280, avg loss: 0.012423, train_acc: 0.46\n",
      "Train Epoch 0, lr: 0.001000, 112/280, avg loss: 0.012182, train_acc: 0.59\n",
      "Train Epoch 0, lr: 0.001000, 224/280, avg loss: 0.013360, train_acc: 0.43\n",
      "alexnet1d, trial.199:\n",
      "Epoch 0, avg test_loss: 0.009900, test_acc: 0.56\n",
      "Train Epoch 1, lr: 0.001000, 0/280, avg loss: 0.011946, train_acc: 0.62\n",
      "Train Epoch 1, lr: 0.001000, 112/280, avg loss: 0.012175, train_acc: 0.61\n",
      "Train Epoch 1, lr: 0.001000, 224/280, avg loss: 0.012230, train_acc: 0.55\n",
      "alexnet1d, trial.199:\n",
      "Epoch 1, avg test_loss: 0.009820, test_acc: 0.56\n",
      "Train Epoch 2, lr: 0.001000, 0/280, avg loss: 0.012297, train_acc: 0.55\n",
      "Train Epoch 2, lr: 0.001000, 112/280, avg loss: 0.012099, train_acc: 0.57\n",
      "Train Epoch 2, lr: 0.001000, 224/280, avg loss: 0.011991, train_acc: 0.61\n",
      "alexnet1d, trial.199:\n",
      "Epoch 2, avg test_loss: 0.009826, test_acc: 0.56\n",
      "Train Epoch 3, lr: 0.001000, 0/280, avg loss: 0.011990, train_acc: 0.57\n",
      "Train Epoch 3, lr: 0.001000, 112/280, avg loss: 0.012106, train_acc: 0.59\n",
      "Train Epoch 3, lr: 0.001000, 224/280, avg loss: 0.012170, train_acc: 0.61\n",
      "alexnet1d, trial.199:\n",
      "Epoch 3, avg test_loss: 0.009845, test_acc: 0.56\n",
      "Train Epoch 4, lr: 0.001000, 0/280, avg loss: 0.012002, train_acc: 0.59\n",
      "Train Epoch 4, lr: 0.001000, 112/280, avg loss: 0.011914, train_acc: 0.57\n",
      "Train Epoch 4, lr: 0.001000, 224/280, avg loss: 0.012585, train_acc: 0.54\n",
      "alexnet1d, trial.199:\n",
      "Epoch 4, avg test_loss: 0.009983, test_acc: 0.56\n",
      "Train Epoch 5, lr: 0.000850, 0/280, avg loss: 0.011626, train_acc: 0.66\n",
      "Train Epoch 5, lr: 0.000850, 112/280, avg loss: 0.011140, train_acc: 0.64\n",
      "Train Epoch 5, lr: 0.000850, 224/280, avg loss: 0.013068, train_acc: 0.46\n",
      "alexnet1d, trial.199:\n",
      "Epoch 5, avg test_loss: 0.010028, test_acc: 0.56\n",
      "Train Epoch 6, lr: 0.000850, 0/280, avg loss: 0.011855, train_acc: 0.61\n",
      "Train Epoch 6, lr: 0.000850, 112/280, avg loss: 0.011881, train_acc: 0.57\n",
      "Train Epoch 6, lr: 0.000850, 224/280, avg loss: 0.012283, train_acc: 0.61\n",
      "alexnet1d, trial.199:\n",
      "Epoch 6, avg test_loss: 0.010071, test_acc: 0.56\n",
      "Train Epoch 7, lr: 0.000850, 0/280, avg loss: 0.011441, train_acc: 0.66\n",
      "Train Epoch 7, lr: 0.000850, 112/280, avg loss: 0.011602, train_acc: 0.62\n",
      "Train Epoch 7, lr: 0.000850, 224/280, avg loss: 0.012758, train_acc: 0.54\n",
      "alexnet1d, trial.199:\n",
      "Epoch 7, avg test_loss: 0.010657, test_acc: 0.53\n",
      "Train Epoch 8, lr: 0.000850, 0/280, avg loss: 0.011705, train_acc: 0.62\n",
      "Train Epoch 8, lr: 0.000850, 112/280, avg loss: 0.011545, train_acc: 0.66\n",
      "Train Epoch 8, lr: 0.000850, 224/280, avg loss: 0.011470, train_acc: 0.57\n",
      "alexnet1d, trial.199:\n",
      "Epoch 8, avg test_loss: 0.010460, test_acc: 0.47\n",
      "Train Epoch 9, lr: 0.000850, 0/280, avg loss: 0.010332, train_acc: 0.73\n",
      "Train Epoch 9, lr: 0.000850, 112/280, avg loss: 0.010311, train_acc: 0.75\n",
      "Train Epoch 9, lr: 0.000850, 224/280, avg loss: 0.011316, train_acc: 0.68\n",
      "alexnet1d, trial.199:\n",
      "Epoch 9, avg test_loss: 0.011093, test_acc: 0.53\n",
      "Train Epoch 10, lr: 0.000722, 0/280, avg loss: 0.010733, train_acc: 0.70\n",
      "Train Epoch 10, lr: 0.000722, 112/280, avg loss: 0.010375, train_acc: 0.73\n",
      "Train Epoch 10, lr: 0.000722, 224/280, avg loss: 0.011792, train_acc: 0.62\n",
      "alexnet1d, trial.199:\n",
      "Epoch 10, avg test_loss: 0.011950, test_acc: 0.44\n",
      "Train Epoch 11, lr: 0.000722, 0/280, avg loss: 0.010609, train_acc: 0.62\n",
      "Train Epoch 11, lr: 0.000722, 112/280, avg loss: 0.010667, train_acc: 0.66\n",
      "Train Epoch 11, lr: 0.000722, 224/280, avg loss: 0.010220, train_acc: 0.73\n",
      "alexnet1d, trial.199:\n",
      "Epoch 11, avg test_loss: 0.010649, test_acc: 0.54\n",
      "Train Epoch 12, lr: 0.000722, 0/280, avg loss: 0.009825, train_acc: 0.79\n",
      "Train Epoch 12, lr: 0.000722, 112/280, avg loss: 0.009992, train_acc: 0.75\n",
      "Train Epoch 12, lr: 0.000722, 224/280, avg loss: 0.012486, train_acc: 0.61\n",
      "alexnet1d, trial.199:\n",
      "Epoch 12, avg test_loss: 0.011609, test_acc: 0.46\n",
      "Train Epoch 13, lr: 0.000722, 0/280, avg loss: 0.009849, train_acc: 0.75\n",
      "Train Epoch 13, lr: 0.000722, 112/280, avg loss: 0.010665, train_acc: 0.61\n",
      "Train Epoch 13, lr: 0.000722, 224/280, avg loss: 0.010649, train_acc: 0.70\n",
      "alexnet1d, trial.199:\n",
      "Epoch 13, avg test_loss: 0.010903, test_acc: 0.53\n",
      "Train Epoch 14, lr: 0.000722, 0/280, avg loss: 0.010364, train_acc: 0.71\n",
      "Train Epoch 14, lr: 0.000722, 112/280, avg loss: 0.010003, train_acc: 0.73\n",
      "Train Epoch 14, lr: 0.000722, 224/280, avg loss: 0.010344, train_acc: 0.70\n",
      "alexnet1d, trial.199:\n",
      "Epoch 14, avg test_loss: 0.012070, test_acc: 0.49\n",
      "Train Epoch 15, lr: 0.000614, 0/280, avg loss: 0.008450, train_acc: 0.80\n",
      "Train Epoch 15, lr: 0.000614, 112/280, avg loss: 0.010754, train_acc: 0.66\n",
      "Train Epoch 15, lr: 0.000614, 224/280, avg loss: 0.011138, train_acc: 0.62\n",
      "alexnet1d, trial.199:\n",
      "Epoch 15, avg test_loss: 0.011795, test_acc: 0.50\n",
      "Train Epoch 16, lr: 0.000614, 0/280, avg loss: 0.008937, train_acc: 0.75\n",
      "Train Epoch 16, lr: 0.000614, 112/280, avg loss: 0.009317, train_acc: 0.80\n",
      "Train Epoch 16, lr: 0.000614, 224/280, avg loss: 0.010863, train_acc: 0.62\n",
      "alexnet1d, trial.199:\n",
      "Epoch 16, avg test_loss: 0.011078, test_acc: 0.54\n",
      "Train Epoch 17, lr: 0.000614, 0/280, avg loss: 0.010390, train_acc: 0.66\n",
      "Train Epoch 17, lr: 0.000614, 112/280, avg loss: 0.008406, train_acc: 0.77\n",
      "Train Epoch 17, lr: 0.000614, 224/280, avg loss: 0.008833, train_acc: 0.86\n",
      "alexnet1d, trial.199:\n",
      "Epoch 17, avg test_loss: 0.011554, test_acc: 0.54\n",
      "Train Epoch 18, lr: 0.000614, 0/280, avg loss: 0.010700, train_acc: 0.64\n",
      "Train Epoch 18, lr: 0.000614, 112/280, avg loss: 0.009237, train_acc: 0.71\n",
      "Train Epoch 18, lr: 0.000614, 224/280, avg loss: 0.008558, train_acc: 0.82\n",
      "alexnet1d, trial.199:\n",
      "Epoch 18, avg test_loss: 0.012764, test_acc: 0.53\n",
      "Train Epoch 19, lr: 0.000614, 0/280, avg loss: 0.007745, train_acc: 0.89\n",
      "Train Epoch 19, lr: 0.000614, 112/280, avg loss: 0.009516, train_acc: 0.70\n",
      "Train Epoch 19, lr: 0.000614, 224/280, avg loss: 0.008634, train_acc: 0.73\n",
      "alexnet1d, trial.199:\n",
      "Epoch 19, avg test_loss: 0.011822, test_acc: 0.57\n",
      "Train Epoch 20, lr: 0.000522, 0/280, avg loss: 0.009210, train_acc: 0.75\n",
      "Train Epoch 20, lr: 0.000522, 112/280, avg loss: 0.008500, train_acc: 0.79\n",
      "Train Epoch 20, lr: 0.000522, 224/280, avg loss: 0.008410, train_acc: 0.80\n",
      "alexnet1d, trial.199:\n",
      "Epoch 20, avg test_loss: 0.012478, test_acc: 0.60\n",
      "Train Epoch 21, lr: 0.000522, 0/280, avg loss: 0.008936, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 112/280, avg loss: 0.007281, train_acc: 0.79\n",
      "Train Epoch 21, lr: 0.000522, 224/280, avg loss: 0.008672, train_acc: 0.79\n",
      "alexnet1d, trial.199:\n",
      "Epoch 21, avg test_loss: 0.013729, test_acc: 0.56\n",
      "Train Epoch 22, lr: 0.000522, 0/280, avg loss: 0.005524, train_acc: 0.93\n",
      "Train Epoch 22, lr: 0.000522, 112/280, avg loss: 0.005677, train_acc: 0.88\n",
      "Train Epoch 22, lr: 0.000522, 224/280, avg loss: 0.010753, train_acc: 0.77\n",
      "alexnet1d, trial.199:\n",
      "Epoch 22, avg test_loss: 0.011860, test_acc: 0.54\n",
      "Train Epoch 23, lr: 0.000522, 0/280, avg loss: 0.007099, train_acc: 0.80\n",
      "Train Epoch 23, lr: 0.000522, 112/280, avg loss: 0.009276, train_acc: 0.68\n",
      "Train Epoch 23, lr: 0.000522, 224/280, avg loss: 0.007084, train_acc: 0.82\n",
      "alexnet1d, trial.199:\n",
      "Epoch 23, avg test_loss: 0.015798, test_acc: 0.53\n",
      "Train Epoch 24, lr: 0.000522, 0/280, avg loss: 0.006516, train_acc: 0.82\n",
      "Train Epoch 24, lr: 0.000522, 112/280, avg loss: 0.013463, train_acc: 0.71\n",
      "Train Epoch 24, lr: 0.000522, 224/280, avg loss: 0.007725, train_acc: 0.79\n",
      "alexnet1d, trial.199:\n",
      "Epoch 24, avg test_loss: 0.012567, test_acc: 0.51\n",
      "Train Epoch 25, lr: 0.000444, 0/280, avg loss: 0.007304, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 112/280, avg loss: 0.007976, train_acc: 0.84\n",
      "Train Epoch 25, lr: 0.000444, 224/280, avg loss: 0.009685, train_acc: 0.71\n",
      "alexnet1d, trial.199:\n",
      "Epoch 25, avg test_loss: 0.010792, test_acc: 0.57\n",
      "Train Epoch 26, lr: 0.000444, 0/280, avg loss: 0.007705, train_acc: 0.84\n",
      "Train Epoch 26, lr: 0.000444, 112/280, avg loss: 0.007588, train_acc: 0.79\n",
      "Train Epoch 26, lr: 0.000444, 224/280, avg loss: 0.006978, train_acc: 0.86\n",
      "alexnet1d, trial.199:\n",
      "Epoch 26, avg test_loss: 0.011647, test_acc: 0.56\n",
      "Train Epoch 27, lr: 0.000444, 0/280, avg loss: 0.005622, train_acc: 0.91\n",
      "Train Epoch 27, lr: 0.000444, 112/280, avg loss: 0.006777, train_acc: 0.86\n",
      "Train Epoch 27, lr: 0.000444, 224/280, avg loss: 0.006216, train_acc: 0.84\n",
      "alexnet1d, trial.199:\n",
      "Epoch 27, avg test_loss: 0.014288, test_acc: 0.59\n",
      "Train Epoch 28, lr: 0.000444, 0/280, avg loss: 0.006642, train_acc: 0.84\n",
      "Train Epoch 28, lr: 0.000444, 112/280, avg loss: 0.006491, train_acc: 0.86\n",
      "Train Epoch 28, lr: 0.000444, 224/280, avg loss: 0.004516, train_acc: 0.89\n",
      "alexnet1d, trial.199:\n",
      "Epoch 28, avg test_loss: 0.013974, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 1\n",
      "Train Epoch 29, lr: 0.000444, 0/280, avg loss: 0.004766, train_acc: 0.89\n",
      "Train Epoch 29, lr: 0.000444, 112/280, avg loss: 0.004392, train_acc: 0.89\n",
      "Train Epoch 29, lr: 0.000444, 224/280, avg loss: 0.006502, train_acc: 0.88\n",
      "alexnet1d, trial.199:\n",
      "Epoch 29, avg test_loss: 0.017556, test_acc: 0.57\n",
      "Train Epoch 30, lr: 0.000377, 0/280, avg loss: 0.003146, train_acc: 0.98\n",
      "Train Epoch 30, lr: 0.000377, 112/280, avg loss: 0.006684, train_acc: 0.82\n",
      "Train Epoch 30, lr: 0.000377, 224/280, avg loss: 0.004367, train_acc: 0.88\n",
      "alexnet1d, trial.199:\n",
      "Epoch 30, avg test_loss: 0.017277, test_acc: 0.60\n",
      "Train Epoch 31, lr: 0.000377, 0/280, avg loss: 0.004504, train_acc: 0.89\n",
      "Train Epoch 31, lr: 0.000377, 112/280, avg loss: 0.002910, train_acc: 0.96\n",
      "Train Epoch 31, lr: 0.000377, 224/280, avg loss: 0.004572, train_acc: 0.93\n",
      "alexnet1d, trial.199:\n",
      "Epoch 31, avg test_loss: 0.024256, test_acc: 0.54\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 2\n",
      "Train Epoch 32, lr: 0.000377, 0/280, avg loss: 0.002729, train_acc: 0.95\n",
      "Train Epoch 32, lr: 0.000377, 112/280, avg loss: 0.002509, train_acc: 0.98\n",
      "Train Epoch 32, lr: 0.000377, 224/280, avg loss: 0.002993, train_acc: 0.96\n",
      "alexnet1d, trial.199:\n",
      "Epoch 32, avg test_loss: 0.024380, test_acc: 0.56\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 3\n",
      "Train Epoch 33, lr: 0.000377, 0/280, avg loss: 0.005848, train_acc: 0.86\n",
      "Train Epoch 33, lr: 0.000377, 112/280, avg loss: 0.003649, train_acc: 0.95\n",
      "Train Epoch 33, lr: 0.000377, 224/280, avg loss: 0.003196, train_acc: 0.91\n",
      "alexnet1d, trial.199:\n",
      "Epoch 33, avg test_loss: 0.021775, test_acc: 0.60\n",
      "\n",
      "train data has been fully fitted. Stop training process in 5 epochs. tolerance = 4\n",
      "\n",
      "Training stop\n",
      "在70个结果中:\n",
      "正确信号39个\n",
      "错误信号31个\n",
      "信号正确并预测正确的概率为0.429\n",
      "信号错误并预测正确的概率为0.171\n",
      "总正确率为0.60\n"
     ]
    }
   ],
   "source": [
    "## find candidate model based on parameter sets\n",
    "trial_acc_list = []\n",
    "max_trial = 200 if data_dim == 1 else 50\n",
    "# model_list = []\n",
    "# print(f\"\\nRunning macd prediction process for {target_time_list[0]} {target_time_list[1][0][0]}\")\n",
    "print(\"\\nGenerate candidate models for evaluations\")\n",
    "for trial in range(max_trial):\n",
    "    train_loader, test_loader = prepare_dataloader(img_datasets, load_data.tensors[1], batch_size, label_type, test=False, one_hot_label=one_hot_label) # use different dataloader for every trial\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    # device = 'cpu'\n",
    "    # used_net = \"alexnet1d\"\n",
    "    net = alexnet1d(len(channels), out_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss() if out_dim == 2 else nn.MSELoss()\n",
    "    # criterion = nn.BCEWithLogitsLoss()\n",
    "    # criterion = FocalLoss()\n",
    "    scheduler = lrs.StepLR(optimizer, step_size=5 ,gamma=0.85)\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    tolerance = 0 # 由NNI调参决定\n",
    "    max_epochs = 200\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        net.train()\n",
    "        for batch, (data, label) in enumerate(train_loader):\n",
    "            data = data[:,channels].to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            logits = net(data)\n",
    "            loss = criterion(logits, label)\n",
    "            train_loss.append(loss.item() / len(data))\n",
    "\n",
    "            if label_type == 'binary':\n",
    "                if one_hot_label: \n",
    "                    acc = logits.max(1)[1].eq(label.max(1)[1]).sum().item() / len(label)\n",
    "                else:\n",
    "                    acc = ((logits.flatten() > 0.5 ).flatten() == label.flatten()).sum().item() / len(label)\n",
    "            else:\n",
    "                count = 0\n",
    "                for item in (torch.cat((logits, label), 1) > 0):\n",
    "                    if item[0] == item[1]:\n",
    "                        count += 1\n",
    "                acc = count / len(label)\n",
    "            train_acc.append(acc)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch % 2 == 0:\n",
    "                print(f\"Train Epoch {epoch}, lr: {optimizer.param_groups[0]['lr']:.6f}, {batch * len(data)}/{len(train_loader.dataset)}, avg loss: {loss.item() / len(data):.6f}, train_acc: {train_acc[-1]:.2f}\")\n",
    "        scheduler.step() # update learning rate after an epoch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            for data, label in test_loader:\n",
    "                data = data[:, channels].to(device)\n",
    "                label = label.to(device)\n",
    "                \n",
    "                logits = net(data)\n",
    "                loss = criterion(logits, label)\n",
    "                \n",
    "                if label_type == 'binary':\n",
    "                    if one_hot_label: \n",
    "                        acc = logits.max(1)[1].eq(label.max(1)[1]).sum().item() / len(label)\n",
    "                    else:\n",
    "                        acc = ((logits.flatten() > 0.5 ).flatten() == label.flatten()).sum().item() / len(label)\n",
    "                else:\n",
    "                    count = 0\n",
    "                    for item in (torch.cat((logits, label), 1) > 0):\n",
    "                        if item[0] == item[1]:\n",
    "                            count += 1\n",
    "                    acc = count / len(label)\n",
    "                test_acc.append(acc)\n",
    "\n",
    "                # print(((net(data).flatten() > 0.5 ) == label.flatten()).sum() / len(data))\n",
    "\n",
    "                test_loss.append(loss.item() / len(data))\n",
    "\n",
    "            print(f\"{used_net}, trial.{trial}:\\nEpoch {epoch}, avg test_loss: {loss.item() / len(data):.6f}, test_acc: {acc:.2f}\")\n",
    "        \n",
    "        if train_acc[-1] >= threshold: # early stop\n",
    "            tolerance += 1\n",
    "            print(f\"\\ntrain data has been fully fitted. Stop training process in 5 epochs. tolerance = {tolerance}\")\n",
    "            if tolerance >= patient:\n",
    "                print(\"\\nTraining stop\")\n",
    "                break\n",
    "\n",
    "    # torch.save(net.state_dict(), f\"e:\\\\ml_data\\\\model_checkpoints\\\\{used_net}_{today}_{len(saved_time_list)}_{int(test_acc[-1] * 100)}_{channels}_{label_type}.pth\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        net = net.to('cpu')\n",
    "        net.eval()\n",
    "        for data, label in test_loader:\n",
    "            # if label.max() == 1. and label.min() == 0.:\n",
    "            if label_type == \"binary\" and not one_hot_label:\n",
    "                result = torch.cat((torch.tensor(net(data[:, channels]) > 0.5, dtype=torch.float32), label),1)\n",
    "            elif label_type == \"binary\" and one_hot_label:\n",
    "                result = torch.stack((net(data[:, channels]).max(1)[1], label.max(1)[1]), 1)\n",
    "                pos_corr = 0\n",
    "                neg_corr = 0\n",
    "                for res in result:\n",
    "                    if res[0] == res[1] and res[1] == 1.:\n",
    "                        pos_corr += 1\n",
    "                    elif res[0] == res[1] and res[1] == 0.:\n",
    "                        neg_corr += 1\n",
    "                print(f\"在{len(result)}个结果中:\\n正确信号{sum([item[1] == 1 for item in result])}个\\n错误信号{sum([item[1] == 0 for item in result])}个\\n信号正确并预测正确的概率为{round(pos_corr/len(result), 3)}\\n信号错误并预测正确的概率为{round(neg_corr/len(result),3)}\\n总正确率为{sum([item[0] == item[1] for item in result])/len(result):.2f}\")\n",
    "            \n",
    "            else:\n",
    "                count = 0\n",
    "                for item in torch.cat((net(data[:, channels]), label), 1) > 0:\n",
    "                    if item[0] == item[1]:\n",
    "                        count += 1\n",
    "                result = torch.cat((net(data[:, channels]), label), 1) > 0\n",
    "                pos_corr = 0\n",
    "                neg_corr = 0\n",
    "                for res in result:\n",
    "                    if res[0] == res[1] and res[1] == True:\n",
    "                        pos_corr += 1\n",
    "                    elif res[0] == res[1] and res[1] == False:\n",
    "                        neg_corr += 1          \n",
    "                print(f\"在{len(result)}个结果中:\\n正确信号{sum([item[1] == 1 for item in result])}个\\n错误信号{sum([item[1] == 0 for item in result])}个\\n信号正确并预测正确的概率为{round(pos_corr/len(result), 3)}\\n信号错误并预测正确的概率为{round(neg_corr/len(result),3)}\\n总正确率为{sum([item[0] == item[1] for item in result])/len(result):.2f}\")\n",
    "    # predict = torch.squeeze(predict_on_target(net, channels, target_data)).tolist()\n",
    "    # trial_acc_list.append([test_acc[-1], *predict])\n",
    "    trial_acc_list.append([round(test_acc[-1], 6), net.state_dict()])\n",
    "\n",
    "# predict_tensor_list = analysis_result(trial_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list = []\n",
    "model_list = []\n",
    "for item in trial_acc_list:\n",
    "    acc_list.append(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.742857, 0.7, 0.7, 0.685714, 0.685714, 0.685714, 0.685714, 0.685714, 0.671429, 0.671429]\n",
      "0.691429\n"
     ]
    }
   ],
   "source": [
    "print(sorted(acc_list, reverse=True)[:10])\n",
    "print(round(sum(sorted(acc_list, reverse=True)[:10]) / 10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "for acc in sorted(acc_list, reverse=True)[:10]:\n",
    "    for index, item in enumerate(trial_acc_list): \n",
    "        # print(len(trial_acc_list))\n",
    "        if item[0] == acc:\n",
    "            model_list.append(item[1])\n",
    "            trial_acc_list.pop(index)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"channels\": channels,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"threshold\": threshold,\n",
    "    \"patient\": patient,\n",
    "    \"top_ten_models\": model_list,\n",
    "    \"acc_list\": sorted(acc_list, reverse=True)[:10],\n",
    "}, f\"e:\\\\ml_data\\\\model_checkpoints\\\\model_list_batch_{model_batch}.pth\")\n",
    "model_batch += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "316fd7d0ffbbd585e0a028353d18e07f77a209a844dc0280639c2ac286fb40d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
